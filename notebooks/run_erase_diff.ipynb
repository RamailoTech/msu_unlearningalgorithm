{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Unlearning (MU) for Erase diff algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### **1: Environment Setup**\n",
    " \n",
    "In this section, we set up our Python environment and install the necessary packages. For reproducibility, it’s best to use a virtual environment.\n",
    "\n",
    "\n",
    "**Prerequisities**\n",
    "\n",
    "Ensure conda is installed on your system. You can install Miniconda or Anaconda:\n",
    "\n",
    "* Miniconda (recommended): https://docs.conda.io/en/latest/miniconda.html\n",
    "\n",
    "* Anaconda: https://www.anaconda.com/products/distribution\n",
    "\n",
    "After installing conda, ensure it is available in your PATH by running. You may require to restart the terminal session:\n",
    "\n",
    "\n",
    "Before installing the unlearn_diff package, follow these steps to set up your environment correctly. These instructions ensure compatibility with the required dependencies, including Python, PyTorch, and ONNX Runtime.\n",
    "\n",
    "\n",
    "**Step-by-Step Setup:**\n",
    "\n",
    "1. Create a Conda Environment Create a new Conda environment named myenv with Python 3.8.5:\n",
    "\n",
    "```bash\n",
    "conda create -n myenv python=3.8.5\n",
    "```\n",
    "\n",
    "2. Activate the Environment Activate the environment to work within it:\n",
    "\n",
    "```bash\n",
    "conda activate myenv\n",
    "```\n",
    "\n",
    "3. Install Core Dependencies Install PyTorch, torchvision, CUDA Toolkit, and ONNX Runtime with specific versions:\n",
    "\n",
    "```bash\n",
    "conda install pytorch==1.11.0 torchvision==0.12.0 cudatoolkit=11.3 onnxruntime==1.16.3 -c pytorch -c conda-forge\n",
    "```\n",
    "\n",
    "4. Install our unlearn_diff Package using pip:\n",
    "\n",
    "```bash\n",
    "pip install unlearn_diff\n",
    "```\n",
    "\n",
    "5. Install Additional Git Dependencies:\n",
    "\n",
    " After installing unlearn_diff, install the following Git-based dependencies in the same Conda environment to ensure full functionality:\n",
    "\n",
    " ```bash\n",
    "pip install git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers\n",
    "```\n",
    "\n",
    "```bash\n",
    "pip install git+https://github.com/openai/CLIP.git@main#egg=clip\n",
    "```\n",
    "\n",
    "```bash\n",
    "pip install git+https://github.com/crowsonkb/k-diffusion.git\n",
    "```\n",
    "\n",
    "```bash\n",
    "pip install git+https://github.com/cocodataset/panopticapi.git\n",
    "```\n",
    "\n",
    "```bash\n",
    "pip install git+https://github.com/Phoveran/fastargs.git@main#egg=fastargs\n",
    "```\n",
    "\n",
    "```bash\n",
    "pip install git+https://github.com/boomb0om/text2image-benchmark\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. Downloading the Dataset**\n",
    "\n",
    "After you install the package, you can use the following commands to download.\n",
    "\n",
    "\n",
    "1. quick_canvas:\n",
    "\n",
    "* Sample: \n",
    "\n",
    "```bash \n",
    "     download_data sample quick_canvas\n",
    "```\n",
    "\n",
    "* Full: \n",
    "\n",
    "```bash \n",
    "     download_data full quick_canvas\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloading sample quick_canvas dataset:\n",
    "\n",
    "!download_data sample quick_canvas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have downloaded datasets, verify the Downloaded Files.\n",
    "\n",
    "\n",
    "* ls -lh ./data/quick-canvas-dataset/sample/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3. Downloading models**\n",
    "\n",
    "* compvis: \n",
    "\n",
    "```bash \n",
    "     download_model compvis\n",
    "```\n",
    "\n",
    "* diffuser: \n",
    "\n",
    "```bash\n",
    "     download_model diffuser\n",
    "```\n",
    "\n",
    "* Download best.onnx model\n",
    "\n",
    "     ```bash\n",
    "     download_best_onnx\n",
    "     ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download compvis model\n",
    "\n",
    "!download_model compvis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download diffuser model\n",
    "\n",
    "!download_model diffuser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!download_best_onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. **MU using unlearn canvas dataset**\n",
    "\n",
    "**Run Train**\n",
    "\n",
    "The default configuration for training is provided by erase_diff_train_mu. You can run the training with the default settings as follows:\n",
    "\n",
    "\n",
    "```python\n",
    "from mu.algorithms.erase_diff.algorithm import EraseDiffAlgorithm\n",
    "from mu.algorithms.erase_diff.configs import erase_diff_train_mu\n",
    "\n",
    "algorithm = EraseDiffAlgorithm(\n",
    "    erase_diff_train_mu\n",
    ")\n",
    "algorithm.run()\n",
    "```\n",
    "\n",
    "<br> <br>\n",
    "\n",
    "**Overriding the Default Configuration**\n",
    "\n",
    "If you need to override the existing configuration settings, you can specify your custom parameters (such as ckpt_path and raw_dataset_dir) directly when initializing the algorithm. For example:\n",
    "\n",
    "```python\n",
    "from mu.algorithms.erase_diff.algorithm import EraseDiffAlgorithm\n",
    "from mu.algorithms.erase_diff.configs import erase_diff_train_mu\n",
    "\n",
    "algorithm = EraseDiffAlgorithm(\n",
    "    erase_diff_train_mu,\n",
    "    use_sample = True\n",
    ")\n",
    "algorithm.run()\n",
    "```\n",
    "\n",
    "<span style=\"color: red;\"><br>Note: When fine-tuning the model, if you want to use a sample dataset, set use_sample=True (default).Otherwise, set use_sample=False to use the full dataset.<br></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# machine unlearning with quick canvas dataset\n",
    "\n",
    "from mu.algorithms.erase_diff.algorithm import EraseDiffAlgorithm\n",
    "from mu.algorithms.erase_diff.configs import erase_diff_train_mu\n",
    "\n",
    "algorithm = EraseDiffAlgorithm(\n",
    "    erase_diff_train_mu,\n",
    "    ckpt_path=\"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\", #replace it with your ckpt path\n",
    "    raw_dataset_dir=\"data/quick-canvas-dataset/sample\",\n",
    "    use_sample = True, #uses sample dataset\n",
    "    template_name = \"Abstractionism\",\n",
    "    dataset_type = \"unlearncanvas\",\n",
    "    devices = \"0\"\n",
    ")\n",
    "algorithm.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of MU\n",
    "\n",
    "The evaluation framework is used to assess the performance of models after applying machine unlearning.\n",
    "\n",
    "config descriptions:\n",
    "\n",
    "* erase_diff_evaluation_config : default evaluation config for erase_dif\n",
    "* ckpt_path: finetuned model path for erase_diff algorithm. \n",
    "* classifier_ckpt_path: Path to classifier model. Download the classifier ckpt from here: \n",
    "    https://drive.google.com/drive/folders/1AoazlvDgWgc3bAyHDpqlafqltmn4vm61 \n",
    "\n",
    "* refrence_dir: original dataset dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mu.algorithms.erase_diff import EraseDiffEvaluator\n",
    "from mu.algorithms.erase_diff.configs import (\n",
    "    erase_diff_evaluation_config\n",
    ")\n",
    "from evaluation.metrics.accuracy import accuracy_score\n",
    "from evaluation.metrics.fid import fid_score\n",
    "\n",
    "\n",
    "evaluator = EraseDiffEvaluator(\n",
    "erase_diff_evaluation_config,\n",
    "ckpt_path=\"outputs/erase_diff/finetuned_models/erase_diff_Abstractionism_model.pth\",\n",
    ")\n",
    "generated_images_path = evaluator.generate_images()\n",
    "\n",
    "accuracy = accuracy_score(gen_image_dir=generated_images_path,\n",
    "                        dataset_type = \"unlearncanvas\",\n",
    "                        classifier_ckpt_path = \"models/classifier_ckpt_path/style50_cls.pth\",\n",
    "                        forget_theme=\"Bricks\",\n",
    "                        seed_list = [\"188\"] )\n",
    "print(accuracy['acc'])\n",
    "print(accuracy['loss'])\n",
    "\n",
    "reference_image_dir = \"data/quick-canvas-dataset/sample\"\n",
    "fid, _ = fid_score(generated_image_dir=generated_images_path,\n",
    "                reference_image_dir=reference_image_dir )\n",
    "\n",
    "print(fid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red;\">**Output for evalaution will be saved to the given directory `Outputs/eval_results/mu_results/erase_diff/`** </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Run Attacks**\n",
    "\n",
    "Before running attacks, download dataset for attack. Run the following command in terminal.\n",
    "\n",
    "<span style=\"color:grey;\"> **generate_attack_dataset --prompts_path data/prompts/nudity_sample.csv --concept i2p_nude --save_path outputs/dataset --num_samples 1** </span>\n",
    "\n",
    "Note: If you want to generate image using full prompt then use `data/prompts/nudity.csv` as prompts_path.\n",
    "\n",
    "Here, prompts_path is the path of csv containing prompt, concept is the name of the concept for organizing output file, save_path is the directory where generated images and metadata will be saved, num_samples is the number of images to generate per prompt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hard Prompt Attack – CompVis to Diffusers Conversion**\n",
    "\n",
    "If you have compvis models, you will need to convert the compvis model to diffuser format. Note: For the conversion to take place, set task.`save_diffuser` to True and to use the converted model `task.sld` should be set to None.\n",
    "\n",
    "Note:\n",
    "1.  Enable conversion: When True, the CompVis model is converted to the Diffuser format during the attack process, allowing integration with the Diffusers library for further processing.\n",
    "\n",
    "2. SLD conversion parameters by setting to None. This ensures that no additional SLD modifications interfere with the conversion of the CompVis model to the Diffuser format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mu_attack.configs.nudity import hard_prompt_esd_nudity_P4D_compvis_config\n",
    "from mu_attack.execs.attack import MUAttack\n",
    "from mu.algorithms.erase_diff.configs import erase_diff_train_mu\n",
    "\n",
    "overridable_params = {\n",
    "    \"task.compvis_ckpt_path\":\"outputs/erase_diff/finetuned_models/erase_diff_Abstractionism_model.pth\",\n",
    "    \"task.compvis_config_path\": erase_diff_train_mu.model_config_path,\n",
    "    \"task.dataset_path\":\"outputs/dataset/i2p_nude\",\n",
    "    \"logger.json.root\":\"results/hard_prompt_esd_nudity_P4D_erase_diff_compvis_to_diffuser\",\n",
    "    # \"attacker.no_attack.dataset_path\" : \"/home/ubuntu/Projects/Palistha/unlearn_diff_attack/outputs/dataset/i2p_nude\",\n",
    "    \"task.save_diffuser\": True, # This flag triggers conversion of compvis model to diffuser while performinig attack\n",
    "    \"task.sld\": None, # Set sld to None for conversion of compvis model to diffuser while performinig attack\n",
    "    \"task.model_name\": \"SD-v1-4\",\n",
    "    \"attacker.iteration\": 1\n",
    "}\n",
    "\n",
    "MUAttack(\n",
    "    config=hard_prompt_esd_nudity_P4D_compvis_config,\n",
    "    **overridable_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**No Attack – CompVis to Diffusers Conversion**\n",
    "\n",
    "If you have compvis models, you will need to convert the compvis model to diffuser format. Note: For the conversion to take place, set task.`save_diffuser` to True and to use the converted model `task.sld` should be set to None.\n",
    "\n",
    "Note:\n",
    "1.  Enable conversion: When True, the CompVis model is converted to the Diffuser format during the attack process, allowing integration with the Diffusers library for further processing.\n",
    "\n",
    "2. SLD conversion parameters by setting to None. This ensures that no additional SLD modifications interfere with the conversion of the CompVis model to the Diffuser format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from mu_attack.configs.nudity import no_attack_esd_nudity_classifier_compvis_config\n",
    "from mu_attack.execs.attack import MUAttack\n",
    "from mu.algorithms.erase_diff.configs import erase_diff_train_mu\n",
    "\n",
    "def run_attack_for_nudity():\n",
    "\n",
    "    overridable_params = {\n",
    "    \"task.compvis_ckpt_path\" :\"outputs/erase_diff/finetuned_models/erase_diff_Abstractionism_model.pth\",\n",
    "    \"task.compvis_config_path\" : erase_diff_train_mu.model_config_path ,\n",
    "    \"task.dataset_path\" : \"outputs/dataset/i2p_nude\",\n",
    "    \"logger.json.root\" : \"results/hard_prompt_esd_nudity_P4D_erase_diff_compvis_to_diffuser\",\n",
    "    \"task.save_diffuser\": True, #This flag triggers conversion of compvis model to diffuser while performinig attack\n",
    "    \"task.sld\": None, #  sld should be set to None while conversion of compvis model to diffuser while performinig attack\n",
    "    \"task.model_name\": \"SD-v1-4\"\n",
    "    }\n",
    "\n",
    "    MUAttack(\n",
    "        config=no_attack_esd_nudity_classifier_compvis_config,\n",
    "        **overridable_params\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_attack_for_nudity()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of MU_attack\n",
    "\n",
    "Calculate the performance of mu_attack using logs and images generated during attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.metrics.asr import asr_score\n",
    "from evaluation.metrics.clip import clip_score\n",
    "from evaluation.metrics.fid import fid_score\n",
    "\n",
    "root = \"results/hard_prompt_esd_nudity_P4D_erase_diff/P4d\"\n",
    "root_no_attack = \"results/no_attack_esd_nudity_P4D_erase_diff/NoAttackEsdNudity\"\n",
    "devices = \"0\"\n",
    "image_path = \"results/hard_prompt_esd_nudity_P4D_erase_diff/P4d/images\"\n",
    "log_path = \"results/hard_prompt_esd_nudity_P4D_erase_diff/P4d/log.json\"\n",
    "ref_batch_path = \"results/no_attack_esd_nudity_P4D_erase_diff/NoAttackEsdNudity/images\"  #to calculate fid score, the genrated image path and ref image path should have ssame number of images.\n",
    "sample_batch_path = \"/home/ubuntu/Projects/Palistha/msu_unlearningalgorithm/data/i2p/nude\"\n",
    "\n",
    "asr_val = asr_score(root, root_no_attack)\n",
    "print(asr_val)\n",
    "\n",
    "clip_val = clip_score(image_path, log_path, devices)\n",
    "print(clip_val)\n",
    "\n",
    "fid_val, _ = fid_score(sample_batch_path,ref_batch_path)\n",
    "print(fid_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mu Defense (AdvUnlean)\n",
    "\n",
    "After performing unlearning and attack, we need to perform adversarial unlearning by integrating a soft prompt attack into the training loop. use the following code snippet for advunlearn.\n",
    "\n",
    "\n",
    "Note: import the model_config_path from the relevant algorithm's configuration module in the mu package\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mu_defense.algorithms.adv_unlearn.algorithm import AdvUnlearnAlgorithm\n",
    "from mu_defense.algorithms.adv_unlearn.configs import adv_unlearn_config\n",
    "from mu.algorithms.erase_diff.configs import erase_diff_train_mu\n",
    "\n",
    "def mu_defense():\n",
    "\n",
    "    mu_defense = AdvUnlearnAlgorithm(\n",
    "        config=adv_unlearn_config,\n",
    "        compvis_ckpt_path = \"outputs/erase_diff/finetuned_models/erase_diff_Abstractionism_model.pth\", #path to the finetuned model\n",
    "        attack_step = 2,\n",
    "        backend = \"compvis\",\n",
    "        attack_method = \"fast_at\",\n",
    "        train_method = \"text_encoder_full\",  #training method. check for docs for available train_method \n",
    "        # train_method = \"noxattn\", #training method. check for docs for available train_method \n",
    "        warmup_iter = 1,\n",
    "        iterations = 2,\n",
    "        model_config_path = erase_diff_train_mu.model_config_path  # use same model_config_path for the model you are using\n",
    "\n",
    "    )\n",
    "    mu_defense.run()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mu_defense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation for mu_defense\n",
    "\n",
    "Description of params used:\n",
    "\n",
    "* config: default train config for image generation.\n",
    "* target_ckpt: Model ckpt after running mu_defense (AdvUnleran).\n",
    "* model_config_path: model config path for the model being used for defense.\n",
    "* save_path: output dir to save generated images.\n",
    "* prompts_path: path to the csv with prompts.\n",
    "* num_samples: number of samples to be generated for a prompt.\n",
    "* folder_suffix: suffix for folder name for save path.\n",
    "* devices: devices to be used.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Note: Before performing evalaution:\n",
    "1. Download coco 10k dataset from this link : https://drive.google.com/file/d/1Qgm3nNhp6ykamszN_ZvofvuzjryTsPHB/view \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from mu_defense.algorithms.adv_unlearn import MUDefenseEvaluator\n",
    "from mu_defense.algorithms.adv_unlearn.configs import mu_defense_evaluation_config\n",
    "from mu.algorithms.erase_diff.configs import erase_diff_train_mu\n",
    "from evaluation.metrics.clip import clip_score\n",
    "from evaluation.metrics.fid import fid_score\n",
    "\n",
    "target_ckpt = \"outputs/results_with_retaining/nudity/coco_object/pgd/AttackLr_0.001/text_encoder_full/all/prefix_k/AdvUnlearn-nudity-method_text_encoder_full_all-Attack_pgd-Retain_coco_object_iter_1.0-lr_1e-05-AttackLr_0.001-prefix_k_adv_num_1-word_embd-attack_init_latest-attack_step_30-adv_update_1-warmup_iter_200/models/Diffusers-UNet-noxattn-epoch_0.pt\"\n",
    "evaluator = MUDefenseEvaluator(config=mu_defense_evaluation_config,\n",
    "                               target_ckpt = target_ckpt,\n",
    "                               model_config_path = erase_diff_train_mu.model_config_path,\n",
    "                               save_path = \"outputs/adv_unlearn/results\",\n",
    "                               prompts_path = \"data/prompts/sample_prompt.csv\",\n",
    "                               num_samples = 1,\n",
    "                               folder_suffix = \"imagenette\",\n",
    "                               devices = \"0\",)\n",
    "\n",
    "gen_image_path = evaluator.generate_images() #Genereate sample images before evalaution \n",
    "print(gen_image_path)  \n",
    "\n",
    "prompt_path = \"data/prompts/sample_prompt.csv\"\n",
    "ref_image_path = \"coco_dataset/sample\"\n",
    "device = \"0\"\n",
    "clip_val = clip_score(gen_image_path, prompt_path, device)    \n",
    "print(clip_val)    \n",
    "\n",
    "fid_val, _  = fid_score(gen_image_path, ref_image_path)\n",
    "print(fid_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
