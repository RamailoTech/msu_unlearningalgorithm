{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Unlearn Unlearn is an open-source Python package designed to streamline the development of unlearning algorithms and establish a standardized evaluation pipeline for diffusion models. It provides researchers and practitioners with tools to implement, evaluate, and extend unlearning algorithms effectively. Features Comprehensive Algorithm Support : Includes commonly used concept erasing and machine unlearning algorithms tailored for diffusion models. Each algorithm is encapsulated and standardized in terms of input-output formats. Automated Evaluation : Supports automatic evaluation on datasets like UnlearnCanvas or IP2P. Performs standard and adversarial evaluations, outputting metrics as detailed in UnlearnCanvas and UnlearnDiffAtk. Extensibility : Designed for easy integration of new unlearning algorithms, attack methods, defense mechanisms, and datasets with minimal modifications. Supported Algorithms The initial version includes established methods benchmarked in UnlearnCanvas and defensive unlearning techniques: CA (Concept Ablation) ED (Erase Diff) ESD (Efficient Substitution Distillation) FMN (Forget Me Not) SU (Saliency Unlearning) SH (ScissorHands) SA (Selective Amnesia) SPM (Semi Permeable Membrane) UCE (Unified Concept Editing) For detailed information on each algorithm, please refer to the respective README.md files located inside mu/algorithms . Project Architecture The project is organized to facilitate scalability and maintainability. data/ : Stores data-related files. i2p-dataset/ : contains i2p-dataset sample/ : Sample dataset full/ : Full dataset quick-canvas-dataset/ : contains quick canvas dataset sample/ : Sample dataset full/ : Full dataset docs/ : Documentation, including API references and user guides. outputs/ : Outputs of the trained algorithms. examples/ : Sample code and notebooks demonstrating usage. logs/ : Log files for debugging and auditing. models/ : Repository of trained models and checkpoints. mu/ : Core source code. algorithms/ : Implementation of various algorithms. Each algorithm has its own subdirectory containing code and a README.md with detailed documentation. esd/ : ESD algorithm components. README.md : Documentation specific to the ESD algorithm. algorithm.py : Core implementation of ESD. configs/ : Configuration files for training and generation tasks. constants/const.py : Constant values used across the ESD algorithm. environment.yaml : Environment setup for ESD. model.py : Model architectures specific to ESD. sampler.py : Sampling methods used during training or inference. scripts/train.py : Training script for ESD. trainer.py : Training routines and optimization strategies. utils.py : Utility functions and helpers. ca/ : Components for the CA algorithm. README.md : Documentation specific to the CA algorithm. ...and so on for other algorithms core/ : Foundational classes and utilities. base_algorithm.py : Abstract base class for algorithm implementations. base_data_handler.py : Base class for data handling. base_model.py : Base class for model definitions. base_sampler.py : Base class for sampling methods. base_trainer.py : Base class for training routines. datasets/ : Dataset management and utilities. __init__.py : Initializes the dataset package. dataset.py : Dataset classes and methods. helpers/ : Helper functions for data processing. unlearning_canvas_dataset.py : Specific dataset class for unlearning tasks. helpers/ : Utility functions and helpers. helper.py : General-purpose helper functions. logger.py : Logging utilities to standardize logging practices. path_setup.py : Path configurations and environment setup. tests/ : Test suites for ensuring code reliability. stable_diffusion/ : Components for stable diffusion. lora_diffusion/ : Components for the LoRA Diffusion. Datasets We use the Quick Canvas benchmark dataset, available here . Currently, the algorithms are trained using 5 images belonging to the themes of Abstractionism and Architectures . Usage This section contains the usage guide for the package. Installation pip install unlearn_diff Prerequisities Ensure conda is installed on your system. You can install Miniconda or Anaconda: Miniconda (recommended): https://docs.conda.io/en/latest/miniconda.html Anaconda : https://www.anaconda.com/products/distribution After installing conda , ensure it is available in your PATH by running. You may require to restart the terminal session: conda --version Create environment: create_env <algorithm_name> eg: create_env erase_diff Activate environment: conda activate <environment_name> eg: conda activate mu_erase_diff The has to be one of the folders in the mu/algorithms folder. Downloading data and models. After you install the package, you can use the following commands to download. Dataset : i2p : Sample : download_data sample i2p Full : download_data full i2p quick_canvas : Sample : download_data sample quick_canvas Full : download_data full quick_canvas Model : compvis : download_model compvis diffuser : download_model diffuser Run Train Each algorithm has their own script to run the algorithm, Some also have different process all together. Follow usage section in readme for the algorithm you want to run with the help of the github repository. You will need to create a train_config.yaml anywhere in your machine, and pass it's path as --config_path parameter. Here is an example for Erase_diff algorithm. WANDB_MODE=offline python -m mu.algorithms.erase_diff.scripts.train \\ --config_path <path_to_config_in_your_machine> The default algorithm specific train_config.yaml makes use of the model_config.yaml with default settings. You can also create your own model_config.yaml and update it's path in the train_config.yaml file to tweak the original model parameters. The details about each parameter in config files are written in the readme for each of the algorithm. NOTE Make sure to update these parameters in train_config.yaml . Otherwise, the train script will not run properly. Also, update other parameters as per your usage. model_config_path: \"configs/erase_diff/model_config.yaml\" # path to model_config.yaml. ckpt_path: \"models/compvis/style50/compvis.ckpt\" # Checkpoint path for compvis or diffuser model raw_dataset_dir: \"data/i2p-dataset/sample\" # path where your dataset was downloaded processed_dataset_dir: \"mu/algorithms/erase_diff/data\" # path to directory, where you want the trained model data to be stored","title":"Home"},{"location":"#unlearn","text":"Unlearn is an open-source Python package designed to streamline the development of unlearning algorithms and establish a standardized evaluation pipeline for diffusion models. It provides researchers and practitioners with tools to implement, evaluate, and extend unlearning algorithms effectively.","title":"Unlearn"},{"location":"#features","text":"Comprehensive Algorithm Support : Includes commonly used concept erasing and machine unlearning algorithms tailored for diffusion models. Each algorithm is encapsulated and standardized in terms of input-output formats. Automated Evaluation : Supports automatic evaluation on datasets like UnlearnCanvas or IP2P. Performs standard and adversarial evaluations, outputting metrics as detailed in UnlearnCanvas and UnlearnDiffAtk. Extensibility : Designed for easy integration of new unlearning algorithms, attack methods, defense mechanisms, and datasets with minimal modifications.","title":"Features"},{"location":"#supported-algorithms","text":"The initial version includes established methods benchmarked in UnlearnCanvas and defensive unlearning techniques: CA (Concept Ablation) ED (Erase Diff) ESD (Efficient Substitution Distillation) FMN (Forget Me Not) SU (Saliency Unlearning) SH (ScissorHands) SA (Selective Amnesia) SPM (Semi Permeable Membrane) UCE (Unified Concept Editing) For detailed information on each algorithm, please refer to the respective README.md files located inside mu/algorithms .","title":"Supported Algorithms"},{"location":"#project-architecture","text":"The project is organized to facilitate scalability and maintainability. data/ : Stores data-related files. i2p-dataset/ : contains i2p-dataset sample/ : Sample dataset full/ : Full dataset quick-canvas-dataset/ : contains quick canvas dataset sample/ : Sample dataset full/ : Full dataset docs/ : Documentation, including API references and user guides. outputs/ : Outputs of the trained algorithms. examples/ : Sample code and notebooks demonstrating usage. logs/ : Log files for debugging and auditing. models/ : Repository of trained models and checkpoints. mu/ : Core source code. algorithms/ : Implementation of various algorithms. Each algorithm has its own subdirectory containing code and a README.md with detailed documentation. esd/ : ESD algorithm components. README.md : Documentation specific to the ESD algorithm. algorithm.py : Core implementation of ESD. configs/ : Configuration files for training and generation tasks. constants/const.py : Constant values used across the ESD algorithm. environment.yaml : Environment setup for ESD. model.py : Model architectures specific to ESD. sampler.py : Sampling methods used during training or inference. scripts/train.py : Training script for ESD. trainer.py : Training routines and optimization strategies. utils.py : Utility functions and helpers. ca/ : Components for the CA algorithm. README.md : Documentation specific to the CA algorithm. ...and so on for other algorithms core/ : Foundational classes and utilities. base_algorithm.py : Abstract base class for algorithm implementations. base_data_handler.py : Base class for data handling. base_model.py : Base class for model definitions. base_sampler.py : Base class for sampling methods. base_trainer.py : Base class for training routines. datasets/ : Dataset management and utilities. __init__.py : Initializes the dataset package. dataset.py : Dataset classes and methods. helpers/ : Helper functions for data processing. unlearning_canvas_dataset.py : Specific dataset class for unlearning tasks. helpers/ : Utility functions and helpers. helper.py : General-purpose helper functions. logger.py : Logging utilities to standardize logging practices. path_setup.py : Path configurations and environment setup. tests/ : Test suites for ensuring code reliability. stable_diffusion/ : Components for stable diffusion. lora_diffusion/ : Components for the LoRA Diffusion.","title":"Project Architecture"},{"location":"#datasets","text":"We use the Quick Canvas benchmark dataset, available here . Currently, the algorithms are trained using 5 images belonging to the themes of Abstractionism and Architectures .","title":"Datasets"},{"location":"#usage","text":"This section contains the usage guide for the package.","title":"Usage"},{"location":"#installation","text":"pip install unlearn_diff","title":"Installation"},{"location":"#prerequisities","text":"Ensure conda is installed on your system. You can install Miniconda or Anaconda: Miniconda (recommended): https://docs.conda.io/en/latest/miniconda.html Anaconda : https://www.anaconda.com/products/distribution After installing conda , ensure it is available in your PATH by running. You may require to restart the terminal session: conda --version","title":"Prerequisities"},{"location":"#create-environment","text":"create_env <algorithm_name> eg: create_env erase_diff","title":"Create environment:"},{"location":"#activate-environment","text":"conda activate <environment_name> eg: conda activate mu_erase_diff The has to be one of the folders in the mu/algorithms folder.","title":"Activate environment:"},{"location":"#downloading-data-and-models","text":"After you install the package, you can use the following commands to download. Dataset : i2p : Sample : download_data sample i2p Full : download_data full i2p quick_canvas : Sample : download_data sample quick_canvas Full : download_data full quick_canvas Model : compvis : download_model compvis diffuser : download_model diffuser","title":"Downloading data and models."},{"location":"#run-train","text":"Each algorithm has their own script to run the algorithm, Some also have different process all together. Follow usage section in readme for the algorithm you want to run with the help of the github repository. You will need to create a train_config.yaml anywhere in your machine, and pass it's path as --config_path parameter. Here is an example for Erase_diff algorithm. WANDB_MODE=offline python -m mu.algorithms.erase_diff.scripts.train \\ --config_path <path_to_config_in_your_machine> The default algorithm specific train_config.yaml makes use of the model_config.yaml with default settings. You can also create your own model_config.yaml and update it's path in the train_config.yaml file to tweak the original model parameters. The details about each parameter in config files are written in the readme for each of the algorithm. NOTE Make sure to update these parameters in train_config.yaml . Otherwise, the train script will not run properly. Also, update other parameters as per your usage. model_config_path: \"configs/erase_diff/model_config.yaml\" # path to model_config.yaml. ckpt_path: \"models/compvis/style50/compvis.ckpt\" # Checkpoint path for compvis or diffuser model raw_dataset_dir: \"data/i2p-dataset/sample\" # path where your dataset was downloaded processed_dataset_dir: \"mu/algorithms/erase_diff/data\" # path to directory, where you want the trained model data to be stored","title":"Run Train"},{"location":"algorithms/concept_ablation/","text":"Concept Ablation Algorithm for Machine Unlearning This repository provides an implementation of the Concept Ablation algorithm for machine unlearning in Stable Diffusion models. The Concept Ablation algorithm enables the removal of specific concepts or styles from a pre-trained model without the need for retraining from scratch. Installation Create the Conda Environment First, create and activate the Conda environment using the provided environment.yaml file: conda env create -f mu/algorithms/concept_ablation/environment.yaml -n mu_concept_ablation conda --version Create environment: create_env <algorithm_name> eg: create_env concept_ablation Activate environment: conda activate <environment_name> eg: conda activate concept_ablation The has to be one of the folders in the mu/algorithms folder. Downloading data and models. After you install the package, you can use the following commands to download. Dataset : i2p : Sample : download_data sample i2p Full : download_data full i2p quick_canvas : Sample : download_data sample quick_canvas Full : download_data full quick_canvas Model : compvis : download_model compvis diffuser : download_model diffuser Verify the Downloaded Files After downloading, verify that the datasets have been correctly extracted: ls -lh ./data/i2p-dataset/sample/ ls -lh ./data/quick-canvas-dataset/sample/ Run Train Example Command python -m mu.algorithms.concept_ablation.scripts.train \\ --config_path mu/algorithms/concept_ablation/configs/train_config.yaml \\ --prompts mu/algorithms/concept_ablation/data/anchor_prompts/finetune_prompts/sd_prompt_Architectures_sample.txt Running the Training Script in Offline Mode WANDB_MODE=offline python -m mu.algorithms.concept_ablation.scripts.train \\ --config_path mu/algorithms/concept_ablation/configs/train_config.yaml \\ --prompts /home/ubuntu/Projects/Palistha/msu_unlearningalgorithm/mu/algorithms/concept_ablation/data/anchor_prompts/finetune_prompts/sd_prompt_Architectures_sample.txt Overriding Configuration via Command Line You can override configuration parameters by passing them directly as arguments during runtime. Example Usage with Command-Line Arguments: python -m mu.algorithms.concept_ablation.scripts.train \\ --config_path mu/algorithms/concept_ablation/configs/train_config.yaml \\ --batch_size 8 \\ --base_lr 1e-5 \\ --devices 0,1 \\ --output_dir outputs/experiment_2 Explanation: * --config_path : Specifies the YAML configuration file. * --batch_size : Overrides the batch size to 8. * --base_lr : Updates the base learning rate to 1e-5. * --devices : Specifies the GPUs (e.g., device 0 and 1). * --output_dir : Sets a custom output directory for the experiment. Directory Structure algorithm.py : Core implementation of the Concept Ablation Algorithm. configs/ : Configuration files for training and generation. data_handler.py : Data handling and preprocessing. scripts/train.py : Script to train the Concept Ablation Algorithm. callbacks/ : Custom callbacks for logging and monitoring training. utils.py : Utility functions. How It Works Default Configuration: Loads values from the specified YAML file ( --config_path ). Command-Line Overrides: Updates the configuration with values provided as command-line arguments. Training Execution: Initializes the ConceptAblationAlgorithm and trains the model using the provided dataset, model checkpoint, and configuration. Output: Saves the fine-tuned model and logs training metrics in the specified output directory. Notes Ensure all dependencies are installed as per the environment file. The training process generates logs in the logs/ directory for easy monitoring. Use appropriate CUDA devices for optimal performance during training. Regularly verify dataset and model configurations to avoid errors during execution. Configuration File ( train_config.yaml ) Training Parameters seed: Random seed for reproducibility. Type: int Example: 23 scale_lr: Whether to scale the base learning rate. Type: bool Example: True caption_target: Target style to remove. Type: str Example: \"Abstractionism Style\" regularization: Adds regularization loss during training. Type: bool Example: True n_samples: Number of batch sizes for image generation. Type: int Example: 10 train_size: Number of generated images for training. Type: int Example: 1000 base_lr: Learning rate for the optimizer. Type: float Example: 2.0e-06 Model Configuration model_config_path: Path to the Stable Diffusion model configuration YAML file. Type: str Example: \"/path/to/model_config.yaml\" ckpt_path: Path to the Stable Diffusion model checkpoint. Type: str Example: \"/path/to/compvis.ckpt\" Dataset Directories raw_dataset_dir: Directory containing the raw dataset categorized by themes or classes. Type: str Example: \"/path/to/raw_dataset\" processed_dataset_dir: Directory to save the processed dataset. Type: str Example: \"/path/to/processed_dataset\" dataset_type: Specifies the dataset type for training. Choices: [\"unlearncanvas\", \"i2p\"] Example: \"unlearncanvas\" template: Type of template to use during training. Choices: [\"object\", \"style\", \"i2p\"] Example: \"style\" template_name: Name of the concept or style to erase. Choices: [\"self-harm\", \"Abstractionism\"] Example: \"Abstractionism\" Output Configurations output_dir: Directory to save fine-tuned models and results. Type: str Example: \"outputs/concept_ablation/finetuned_models\" Device Configuration devices: CUDA devices for training (comma-separated). Type: str Example: \"0\" Concept ablation Evaluation Framework This section provides instructions for running the evaluation framework for the Concept ablation algorithm on Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying machine unlearning. Running the Evaluation Framework You can run the evaluation framework using the evaluate.py script located in the mu/algorithms/concept_ablation/scripts/ directory. Basic Command to Run Evaluation: conda activate <env_name> python -m mu.algorithms.concept_ablation.scripts.evaluate \\ --config_path mu/algorithms/concept_ablation/configs/evaluation_config.yaml Running in Offline Mode: WANDB_MODE=offline python -m mu.algorithms.concept_ablation.scripts.evaluate \\ --config_path mu/algorithms/concept_ablation/configs/evaluation_config.yaml Example with CLI Overrides: python -m mu.algorithms.concept_ablation.scripts.evaluate \\ --config_path mu/algorithms/concept_ablation/configs/evaluation_config.yaml \\ --devices \"0\" \\ --seed 123 \\ --cfg_text 8.5 \\ --batch_size 16 Description of parameters in evaluation_config.yaml The evaluation_config.yaml file contains the necessary parameters for running the Concept ablation evaluation framework. Below is a detailed description of each parameter along with examples. Model Configuration: model_config : Path to the YAML file specifying the model architecture and settings. Type: str Example: \"mu/algorithms/concept_ablation/configs/model_config.yaml\" ckpt_path : Path to the finetuned Stable Diffusion checkpoint file to be evaluated. Type: str Example: \"outputs/concept_ablation/finetuned_models/concept_ablation_Abstractionism_model.pth\" classification_model : Specifies the classification model used for evaluating the generated outputs. Type: str Example: \"vit_large_patch16_224\" model_ckpt_path: Path to pretrained Stable Diffusion model. Type : str Example : models/compvis/style50/compvis.ckpt Training and Sampling Parameters: theme : Specifies the theme or concept being evaluated for removal from the model's outputs. Type: str Example: \"Bricks\" devices : CUDA device IDs to be used for the evaluation process. Type: str Example: \"0\" cfg_text : Classifier-free guidance scale value for image generation. Higher values increase the strength of the conditioning prompt. Type: float Example: 9.0 seed : Random seed for reproducibility of results. Type: int Example: 188 ddim_steps : Number of steps for the DDIM (Denoising Diffusion Implicit Models) sampling process. Type: int Example: 100 ddim_eta : DDIM eta value for controlling the amount of randomness during sampling. Set to 0 for deterministic sampling. Type: float Example: 0.0 image_height : Height of the generated images in pixels. Type: int Example: 512 image_width : Width of the generated images in pixels. Type: int Example: 512 Output and Logging Parameters: sampler_output_dir : Directory where generated images will be saved during evaluation. Type: str Example: \"outputs/eval_results/mu_results/concept_ablation/\" eval_output_dir : Directory where evaluation metrics and results will be stored. Type: str Example: \"outputs/eval_results/mu_results/concept_ablation/\" reference_dir : Directory containing original images for comparison during evaluation. Type: str Example: \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\" Performance and Efficiency Parameters: multiprocessing : Enables multiprocessing for faster evaluation for FID score. Recommended for large datasets. Type: bool Example: False batch_size : Batch size used during FID computation and evaluation. Type: int Example: 16 Optimization Parameters: forget_theme : Concept or style intended for removal in the evaluation process. Type: str Example: \"Bricks\" seed_list : List of random seeds for performing multiple evaluations with different randomness levels. Type: list Example: [\"188\"]","title":"Concept Ablation"},{"location":"algorithms/concept_ablation/#concept-ablation-algorithm-for-machine-unlearning","text":"This repository provides an implementation of the Concept Ablation algorithm for machine unlearning in Stable Diffusion models. The Concept Ablation algorithm enables the removal of specific concepts or styles from a pre-trained model without the need for retraining from scratch.","title":"Concept Ablation Algorithm for Machine Unlearning"},{"location":"algorithms/concept_ablation/#installation","text":"","title":"Installation"},{"location":"algorithms/concept_ablation/#create-the-conda-environment","text":"First, create and activate the Conda environment using the provided environment.yaml file: conda env create -f mu/algorithms/concept_ablation/environment.yaml -n mu_concept_ablation conda --version","title":"Create the Conda Environment"},{"location":"algorithms/concept_ablation/#create-environment","text":"create_env <algorithm_name> eg: create_env concept_ablation","title":"Create environment:"},{"location":"algorithms/concept_ablation/#activate-environment","text":"conda activate <environment_name> eg: conda activate concept_ablation The has to be one of the folders in the mu/algorithms folder.","title":"Activate environment:"},{"location":"algorithms/concept_ablation/#downloading-data-and-models","text":"After you install the package, you can use the following commands to download. Dataset : i2p : Sample : download_data sample i2p Full : download_data full i2p quick_canvas : Sample : download_data sample quick_canvas Full : download_data full quick_canvas Model : compvis : download_model compvis diffuser : download_model diffuser Verify the Downloaded Files After downloading, verify that the datasets have been correctly extracted: ls -lh ./data/i2p-dataset/sample/ ls -lh ./data/quick-canvas-dataset/sample/","title":"Downloading data and models."},{"location":"algorithms/concept_ablation/#run-train","text":"","title":"Run Train"},{"location":"algorithms/concept_ablation/#example-command","text":"python -m mu.algorithms.concept_ablation.scripts.train \\ --config_path mu/algorithms/concept_ablation/configs/train_config.yaml \\ --prompts mu/algorithms/concept_ablation/data/anchor_prompts/finetune_prompts/sd_prompt_Architectures_sample.txt","title":"Example Command"},{"location":"algorithms/concept_ablation/#running-the-training-script-in-offline-mode","text":"WANDB_MODE=offline python -m mu.algorithms.concept_ablation.scripts.train \\ --config_path mu/algorithms/concept_ablation/configs/train_config.yaml \\ --prompts /home/ubuntu/Projects/Palistha/msu_unlearningalgorithm/mu/algorithms/concept_ablation/data/anchor_prompts/finetune_prompts/sd_prompt_Architectures_sample.txt","title":"Running the Training Script in Offline Mode"},{"location":"algorithms/concept_ablation/#overriding-configuration-via-command-line","text":"You can override configuration parameters by passing them directly as arguments during runtime. Example Usage with Command-Line Arguments: python -m mu.algorithms.concept_ablation.scripts.train \\ --config_path mu/algorithms/concept_ablation/configs/train_config.yaml \\ --batch_size 8 \\ --base_lr 1e-5 \\ --devices 0,1 \\ --output_dir outputs/experiment_2 Explanation: * --config_path : Specifies the YAML configuration file. * --batch_size : Overrides the batch size to 8. * --base_lr : Updates the base learning rate to 1e-5. * --devices : Specifies the GPUs (e.g., device 0 and 1). * --output_dir : Sets a custom output directory for the experiment.","title":"Overriding Configuration via Command Line"},{"location":"algorithms/concept_ablation/#directory-structure","text":"algorithm.py : Core implementation of the Concept Ablation Algorithm. configs/ : Configuration files for training and generation. data_handler.py : Data handling and preprocessing. scripts/train.py : Script to train the Concept Ablation Algorithm. callbacks/ : Custom callbacks for logging and monitoring training. utils.py : Utility functions.","title":"Directory Structure"},{"location":"algorithms/concept_ablation/#how-it-works","text":"Default Configuration: Loads values from the specified YAML file ( --config_path ). Command-Line Overrides: Updates the configuration with values provided as command-line arguments. Training Execution: Initializes the ConceptAblationAlgorithm and trains the model using the provided dataset, model checkpoint, and configuration. Output: Saves the fine-tuned model and logs training metrics in the specified output directory.","title":"How It Works"},{"location":"algorithms/concept_ablation/#notes","text":"Ensure all dependencies are installed as per the environment file. The training process generates logs in the logs/ directory for easy monitoring. Use appropriate CUDA devices for optimal performance during training. Regularly verify dataset and model configurations to avoid errors during execution.","title":"Notes"},{"location":"algorithms/concept_ablation/#configuration-file-train_configyaml","text":"","title":"Configuration File (train_config.yaml)"},{"location":"algorithms/concept_ablation/#training-parameters","text":"seed: Random seed for reproducibility. Type: int Example: 23 scale_lr: Whether to scale the base learning rate. Type: bool Example: True caption_target: Target style to remove. Type: str Example: \"Abstractionism Style\" regularization: Adds regularization loss during training. Type: bool Example: True n_samples: Number of batch sizes for image generation. Type: int Example: 10 train_size: Number of generated images for training. Type: int Example: 1000 base_lr: Learning rate for the optimizer. Type: float Example: 2.0e-06","title":"Training Parameters"},{"location":"algorithms/concept_ablation/#model-configuration","text":"model_config_path: Path to the Stable Diffusion model configuration YAML file. Type: str Example: \"/path/to/model_config.yaml\" ckpt_path: Path to the Stable Diffusion model checkpoint. Type: str Example: \"/path/to/compvis.ckpt\"","title":"Model Configuration"},{"location":"algorithms/concept_ablation/#dataset-directories","text":"raw_dataset_dir: Directory containing the raw dataset categorized by themes or classes. Type: str Example: \"/path/to/raw_dataset\" processed_dataset_dir: Directory to save the processed dataset. Type: str Example: \"/path/to/processed_dataset\" dataset_type: Specifies the dataset type for training. Choices: [\"unlearncanvas\", \"i2p\"] Example: \"unlearncanvas\" template: Type of template to use during training. Choices: [\"object\", \"style\", \"i2p\"] Example: \"style\" template_name: Name of the concept or style to erase. Choices: [\"self-harm\", \"Abstractionism\"] Example: \"Abstractionism\"","title":"Dataset Directories"},{"location":"algorithms/concept_ablation/#output-configurations","text":"output_dir: Directory to save fine-tuned models and results. Type: str Example: \"outputs/concept_ablation/finetuned_models\"","title":"Output Configurations"},{"location":"algorithms/concept_ablation/#device-configuration","text":"devices: CUDA devices for training (comma-separated). Type: str Example: \"0\"","title":"Device Configuration"},{"location":"algorithms/concept_ablation/#concept-ablation-evaluation-framework","text":"This section provides instructions for running the evaluation framework for the Concept ablation algorithm on Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying machine unlearning.","title":"Concept ablation Evaluation Framework"},{"location":"algorithms/concept_ablation/#running-the-evaluation-framework","text":"You can run the evaluation framework using the evaluate.py script located in the mu/algorithms/concept_ablation/scripts/ directory.","title":"Running the Evaluation Framework"},{"location":"algorithms/concept_ablation/#basic-command-to-run-evaluation","text":"conda activate <env_name> python -m mu.algorithms.concept_ablation.scripts.evaluate \\ --config_path mu/algorithms/concept_ablation/configs/evaluation_config.yaml Running in Offline Mode: WANDB_MODE=offline python -m mu.algorithms.concept_ablation.scripts.evaluate \\ --config_path mu/algorithms/concept_ablation/configs/evaluation_config.yaml Example with CLI Overrides: python -m mu.algorithms.concept_ablation.scripts.evaluate \\ --config_path mu/algorithms/concept_ablation/configs/evaluation_config.yaml \\ --devices \"0\" \\ --seed 123 \\ --cfg_text 8.5 \\ --batch_size 16","title":"Basic Command to Run Evaluation:"},{"location":"algorithms/concept_ablation/#description-of-parameters-in-evaluation_configyaml","text":"The evaluation_config.yaml file contains the necessary parameters for running the Concept ablation evaluation framework. Below is a detailed description of each parameter along with examples.","title":"Description of parameters in evaluation_config.yaml"},{"location":"algorithms/concept_ablation/#model-configuration_1","text":"model_config : Path to the YAML file specifying the model architecture and settings. Type: str Example: \"mu/algorithms/concept_ablation/configs/model_config.yaml\" ckpt_path : Path to the finetuned Stable Diffusion checkpoint file to be evaluated. Type: str Example: \"outputs/concept_ablation/finetuned_models/concept_ablation_Abstractionism_model.pth\" classification_model : Specifies the classification model used for evaluating the generated outputs. Type: str Example: \"vit_large_patch16_224\" model_ckpt_path: Path to pretrained Stable Diffusion model. Type : str Example : models/compvis/style50/compvis.ckpt","title":"Model Configuration:"},{"location":"algorithms/concept_ablation/#training-and-sampling-parameters","text":"theme : Specifies the theme or concept being evaluated for removal from the model's outputs. Type: str Example: \"Bricks\" devices : CUDA device IDs to be used for the evaluation process. Type: str Example: \"0\" cfg_text : Classifier-free guidance scale value for image generation. Higher values increase the strength of the conditioning prompt. Type: float Example: 9.0 seed : Random seed for reproducibility of results. Type: int Example: 188 ddim_steps : Number of steps for the DDIM (Denoising Diffusion Implicit Models) sampling process. Type: int Example: 100 ddim_eta : DDIM eta value for controlling the amount of randomness during sampling. Set to 0 for deterministic sampling. Type: float Example: 0.0 image_height : Height of the generated images in pixels. Type: int Example: 512 image_width : Width of the generated images in pixels. Type: int Example: 512","title":"Training and Sampling Parameters:"},{"location":"algorithms/concept_ablation/#output-and-logging-parameters","text":"sampler_output_dir : Directory where generated images will be saved during evaluation. Type: str Example: \"outputs/eval_results/mu_results/concept_ablation/\" eval_output_dir : Directory where evaluation metrics and results will be stored. Type: str Example: \"outputs/eval_results/mu_results/concept_ablation/\" reference_dir : Directory containing original images for comparison during evaluation. Type: str Example: \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\"","title":"Output and Logging Parameters:"},{"location":"algorithms/concept_ablation/#performance-and-efficiency-parameters","text":"multiprocessing : Enables multiprocessing for faster evaluation for FID score. Recommended for large datasets. Type: bool Example: False batch_size : Batch size used during FID computation and evaluation. Type: int Example: 16","title":"Performance and Efficiency Parameters:"},{"location":"algorithms/concept_ablation/#optimization-parameters","text":"forget_theme : Concept or style intended for removal in the evaluation process. Type: str Example: \"Bricks\" seed_list : List of random seeds for performing multiple evaluations with different randomness levels. Type: list Example: [\"188\"]","title":"Optimization Parameters:"},{"location":"algorithms/contributing/","text":"Contributing to Unlearn Diff Thank you for your interest in contributing to Unlearn ! This document outlines the steps and best practices for adding new algorithms, making pull requests, filing issues, and more. Table of Contents Introduction Code of Conduct Project Structure How to Contribute Reporting Issues Suggesting Enhancements Submitting Pull Requests Adding a New Algorithm Folder Structure Creating an Environment Documentation Code Style Contact Introduction Unlearn is an open-source Python package designed for unlearning algorithms in diffusion models. We welcome contributions from the community, whether it\u2019s a new feature, bug fix, or an entirely new unlearning algorithm. Code of Conduct Please note that we expect all contributors to abide by our code of conduct. Be respectful and considerate of others, and help keep our community healthy and inclusive. Project Structure A quick overview of relevant folders: mu/algorithms/ This is where all algorithms reside. Each algorithm has its own subfolder containing: algorithm.py (core logic) data_handler.py (if data-processing is needed) datasets/ (special dataset classes) environment.yaml (conda environment details) evaluator.py (evaluation routines) model.py (model definitions) sampler.py (sampling methods) scripts/ (training, evaluation scripts, etc.) trainer.py (training loop, optimization) utils.py (auxiliary functions) Readme.md (algorithm-specific documentation) mu/core/ Contains abstract/base classes ( base_algorithm.py , base_trainer.py , etc.) that algorithms can extend or override. mu/datasets/ Common dataset classes and utilities used by multiple algorithms. helpers/ Shared utility functions (e.g., logging, path setup, config loading). How to Contribute Reporting Issues Check the to see if your bug or feature request has already been reported. If not, open a new issue. Provide a clear title and description, including steps to reproduce if it\u2019s a bug. Label your issue appropriately (e.g., \u201cbug\u201d, \u201cenhancement\u201d, \u201cdocumentation\u201d). Suggesting Enhancements If you have an idea for a new feature or improvement, open a GitHub issue labeled \u201cenhancement\u201d or \u201cfeature request.\u201d Include any relevant use-cases, examples, or background information to help the community understand your proposal. Submitting Pull Requests Fork the repository and create your feature branch from main (or the appropriate branch if instructed otherwise): bash git checkout -b feature/my-awesome-feature Make your changes, including tests if applicable. Commit and push your changes: bash git push origin feature/my-awesome-feature Open a Pull Request from your fork into the main repo. Provide a clear description of what your PR does, referencing any related issues. Please ensure your PR follows the Code Style guidelines and includes updated or new tests if needed. Adding a New Algorithm One of the core goals of Unlearn Diff is to make it easy to add and benchmark new unlearning algorithms. Follow these steps when introducing a new method. Folder Structure Create a new subfolder in mu/algorithms/ with a clear, descriptive name (e.g., my_new_algo ). Inside this subfolder, include the following (at minimum): algorithm.py trainer.py scripts/train.py (and optionally scripts/evaluate.py if your training and evaluation are separate) configs/ containing YAML config files for training, evaluation, etc. environment.yaml specifying dependencies for conda Readme.md explaining how to install, run, and configure your new algorithm (Optional) data_handler.py , datasets/ , model.py , sampler.py , utils.py , etc., depending on your algorithm\u2019s design. Extend or import from base classes in mu/core/ if your algorithm logic aligns with any existing abstract classes (e.g., BaseAlgorithm , BaseTrainer , etc.). This ensures code consistency and easier maintenance. Creating an Environment To keep dependencies organized, each algorithm has its own environment.yaml . Contributors are encouraged to: Specify all required packages (e.g., torch , torchvision , diffusers , etc.). Name the environment after the algorithm, e.g., mu_<algorithm_name> . Update instructions in your algorithm\u2019s Readme.md on how to install and activate this environment. Documentation Each algorithm folder must contain a Readme.md that documents: High-level description of the algorithm (goals, references to papers, etc.). How to install any special dependencies (if not covered by environment.yaml ). How to run training and evaluation (with example commands). Explanation of the config files in configs/ . Any unique parameters or hyperparameters. If you introduce new metrics or logs, please outline them in your algorithm\u2019s README and update any relevant docs in the main docs/ folder as needed. Code Style We recommend following PEP 8 for Python code. Use descriptive variable names, docstrings, and type hints where possible. Keep functions and methods short and focused\u2014avoid large, monolithic methods. Use Python\u2019s built-in logging or the logging utilities in helpers/logger.py for consistent log outputs. Use wandb global wandb logger if required. Additionally, we prefer: - 4 spaces for indentation (no tabs). - Meaningful commit messages: \u201cFix dataset loader bug for large images\u201d vs. \u201cFix stuff.\u201d Contact If you have any questions, feel free to open an issue or reach out to us via email. We appreciate your contributions and look forward to collaborating with you on Unlearn Diff ! Thank you again for your interest in contributing. We\u2019re excited to see your ideas and improvements!","title":"Contributing"},{"location":"algorithms/contributing/#contributing-to-unlearn-diff","text":"Thank you for your interest in contributing to Unlearn ! This document outlines the steps and best practices for adding new algorithms, making pull requests, filing issues, and more.","title":"Contributing to Unlearn Diff"},{"location":"algorithms/contributing/#table-of-contents","text":"Introduction Code of Conduct Project Structure How to Contribute Reporting Issues Suggesting Enhancements Submitting Pull Requests Adding a New Algorithm Folder Structure Creating an Environment Documentation Code Style Contact","title":"Table of Contents"},{"location":"algorithms/contributing/#introduction","text":"Unlearn is an open-source Python package designed for unlearning algorithms in diffusion models. We welcome contributions from the community, whether it\u2019s a new feature, bug fix, or an entirely new unlearning algorithm.","title":"Introduction"},{"location":"algorithms/contributing/#code-of-conduct","text":"Please note that we expect all contributors to abide by our code of conduct. Be respectful and considerate of others, and help keep our community healthy and inclusive.","title":"Code of Conduct"},{"location":"algorithms/contributing/#project-structure","text":"A quick overview of relevant folders: mu/algorithms/ This is where all algorithms reside. Each algorithm has its own subfolder containing: algorithm.py (core logic) data_handler.py (if data-processing is needed) datasets/ (special dataset classes) environment.yaml (conda environment details) evaluator.py (evaluation routines) model.py (model definitions) sampler.py (sampling methods) scripts/ (training, evaluation scripts, etc.) trainer.py (training loop, optimization) utils.py (auxiliary functions) Readme.md (algorithm-specific documentation) mu/core/ Contains abstract/base classes ( base_algorithm.py , base_trainer.py , etc.) that algorithms can extend or override. mu/datasets/ Common dataset classes and utilities used by multiple algorithms. helpers/ Shared utility functions (e.g., logging, path setup, config loading).","title":"Project Structure"},{"location":"algorithms/contributing/#how-to-contribute","text":"","title":"How to Contribute"},{"location":"algorithms/contributing/#reporting-issues","text":"Check the to see if your bug or feature request has already been reported. If not, open a new issue. Provide a clear title and description, including steps to reproduce if it\u2019s a bug. Label your issue appropriately (e.g., \u201cbug\u201d, \u201cenhancement\u201d, \u201cdocumentation\u201d).","title":"Reporting Issues"},{"location":"algorithms/contributing/#suggesting-enhancements","text":"If you have an idea for a new feature or improvement, open a GitHub issue labeled \u201cenhancement\u201d or \u201cfeature request.\u201d Include any relevant use-cases, examples, or background information to help the community understand your proposal.","title":"Suggesting Enhancements"},{"location":"algorithms/contributing/#submitting-pull-requests","text":"Fork the repository and create your feature branch from main (or the appropriate branch if instructed otherwise): bash git checkout -b feature/my-awesome-feature Make your changes, including tests if applicable. Commit and push your changes: bash git push origin feature/my-awesome-feature Open a Pull Request from your fork into the main repo. Provide a clear description of what your PR does, referencing any related issues. Please ensure your PR follows the Code Style guidelines and includes updated or new tests if needed.","title":"Submitting Pull Requests"},{"location":"algorithms/contributing/#adding-a-new-algorithm","text":"One of the core goals of Unlearn Diff is to make it easy to add and benchmark new unlearning algorithms. Follow these steps when introducing a new method.","title":"Adding a New Algorithm"},{"location":"algorithms/contributing/#folder-structure","text":"Create a new subfolder in mu/algorithms/ with a clear, descriptive name (e.g., my_new_algo ). Inside this subfolder, include the following (at minimum): algorithm.py trainer.py scripts/train.py (and optionally scripts/evaluate.py if your training and evaluation are separate) configs/ containing YAML config files for training, evaluation, etc. environment.yaml specifying dependencies for conda Readme.md explaining how to install, run, and configure your new algorithm (Optional) data_handler.py , datasets/ , model.py , sampler.py , utils.py , etc., depending on your algorithm\u2019s design. Extend or import from base classes in mu/core/ if your algorithm logic aligns with any existing abstract classes (e.g., BaseAlgorithm , BaseTrainer , etc.). This ensures code consistency and easier maintenance.","title":"Folder Structure"},{"location":"algorithms/contributing/#creating-an-environment","text":"To keep dependencies organized, each algorithm has its own environment.yaml . Contributors are encouraged to: Specify all required packages (e.g., torch , torchvision , diffusers , etc.). Name the environment after the algorithm, e.g., mu_<algorithm_name> . Update instructions in your algorithm\u2019s Readme.md on how to install and activate this environment.","title":"Creating an Environment"},{"location":"algorithms/contributing/#documentation","text":"Each algorithm folder must contain a Readme.md that documents: High-level description of the algorithm (goals, references to papers, etc.). How to install any special dependencies (if not covered by environment.yaml ). How to run training and evaluation (with example commands). Explanation of the config files in configs/ . Any unique parameters or hyperparameters. If you introduce new metrics or logs, please outline them in your algorithm\u2019s README and update any relevant docs in the main docs/ folder as needed.","title":"Documentation"},{"location":"algorithms/contributing/#code-style","text":"We recommend following PEP 8 for Python code. Use descriptive variable names, docstrings, and type hints where possible. Keep functions and methods short and focused\u2014avoid large, monolithic methods. Use Python\u2019s built-in logging or the logging utilities in helpers/logger.py for consistent log outputs. Use wandb global wandb logger if required. Additionally, we prefer: - 4 spaces for indentation (no tabs). - Meaningful commit messages: \u201cFix dataset loader bug for large images\u201d vs. \u201cFix stuff.\u201d","title":"Code Style"},{"location":"algorithms/contributing/#contact","text":"If you have any questions, feel free to open an issue or reach out to us via email. We appreciate your contributions and look forward to collaborating with you on Unlearn Diff ! Thank you again for your interest in contributing. We\u2019re excited to see your ideas and improvements!","title":"Contact"},{"location":"algorithms/erase_diff/","text":"EraseDiff Algorithm for Machine Unlearning This repository provides an implementation of the erase diff algorithm for machine unlearning in Stable Diffusion models. The erasediff algorithm allows you to remove specific concepts or styles from a pre-trained model without retraining it from scratch. Installation pip install unlearn_diff Prerequisities Ensure conda is installed on your system. You can install Miniconda or Anaconda: Miniconda (recommended): https://docs.conda.io/en/latest/miniconda.html Anaconda : https://www.anaconda.com/products/distribution After installing conda , ensure it is available in your PATH by running. You may require to restart the terminal session: conda --version Create environment: create_env <algorithm_name> eg: create_env erase_diff Activate environment: conda activate <environment_name> eg: conda activate erase_diff The has to be one of the folders in the mu/algorithms folder. Downloading data and models. After you install the package, you can use the following commands to download. Dataset : i2p : Sample : download_data sample i2p Full : download_data full i2p quick_canvas : Sample : download_data sample quick_canvas Full : download_data full quick_canvas Model : compvis : download_model compvis diffuser : download_model diffuser Verify the Downloaded Files After downloading, verify that the datasets have been correctly extracted: ls -lh ./data/i2p-dataset/sample/ ls -lh ./data/quick-canvas-dataset/sample/ Run Train Example Command python -m mu.algorithms.erase_diff.scripts.train \\ --config_path mu/algorithms/erase_diff/configs/train_config.yaml Running the Training Script in Offline Mode WANDB_MODE=offline python -m mu.algorithms.erase_diff.scripts.train \\ --config_path mu/algorithms/erase_diff/configs/train_config.yaml Passing Arguments via the Command Line The train.py script allows you to override configuration parameters specified in the train_config.yaml file by passing them directly as arguments during runtime. This can be useful for quick experimentation without modifying the configuration file. Example Usage with Command-Line Arguments python -m mu.algorithms.erase_diff.scripts.train \\ --config_path mu/algorithms/erase_diff/configs/train_config.yaml \\ --train_method xattn \\ --alpha 0.2 \\ --epochs 10 \\ --lr 1e-4 \\ --devices 0,1 \\ --raw_dataset_dir /path/to/raw_dataset \\ --output_dir outputs/experiment_1 Explanation of the Example --config_path: Specifies the YAML configuration file to load default values. --train_method: Overrides the training method (\"xattn\"). --alpha: Sets the guidance strength for the starting image to 0.2. --epochs: Increases the number of training epochs to 10. --lr: Updates the learning rate to 1e-4. --devices: Specifies the GPUs (e.g., device 0 and 1) for training. --raw_dataset_dir: Changes the raw dataset directory. --output_dir: Sets a custom output directory for this run. How It Works * Default Values: The script first loads default values from the YAML file specified by --config_path. Command-Line Overrides: Any arguments passed on the command line will override the corresponding keys in the YAML configuration file. Final Configuration: The script merges the YAML file and command-line arguments into a single configuration dictionary and uses it for training. Directory Structure algorithm.py : Implementation of the EraseDiffAlgorithm class. configs/ : Contains configuration files for training and generation. model.py : Implementation of the EraseDiffModel class. scripts/train.py : Script to train the EraseDiff algorithm. trainer.py : Implementation of the EraseDiffTrainer class. utils.py : Utility functions used in the project. data_handler.py : Implementation of DataHandler class Description of Arguments in train_config.yaml Training Parameters train_method: Specifies the method of training for concept erasure. Choices: [\"noxattn\", \"selfattn\", \"xattn\", \"full\", \"notime\", \"xlayer\", \"selflayer\"] Example: \"xattn\" alpha: Guidance strength for the starting image during training. Type: float Example: 0.1 epochs: Number of epochs to train the model. Type: int Example: 1 K_steps: Number of K optimization steps during training. Type: int Example: 2 lr: Learning rate used for the optimizer during training. Type: float Example: 5e-5 Model Configuration model_config_path: File path to the Stable Diffusion model configuration YAML file. type: str Example: \"/path/to/model_config.yaml\" ckpt_path: File path to the checkpoint of the Stable Diffusion model. Type: str Example: \"/path/to/model_checkpoint.ckpt\" Dataset Directories raw_dataset_dir: Directory containing the raw dataset categorized by themes or classes. Type: str Example: \"/path/to/raw_dataset\" processed_dataset_dir: Directory to save the processed dataset. Type: str Example: \"/path/to/processed_dataset\" dataset_type: Specifies the dataset type for the training process. Choices: [\"unlearncanvas\", \"i2p\"] Example: \"unlearncanvas\" template: Type of template to use during training. Choices: [\"object\", \"style\", \"i2p\"] Example: \"style\" template_name: Name of the specific concept or style to be erased. Choices: [\"self-harm\", \"Abstractionism\"] Example: \"Abstractionism\" Output Configurations output_dir: Directory where the fine-tuned models and results will be saved. Type: str Example: \"outputs/erase_diff/finetuned_models\" separator: String separator used to train multiple words separately, if applicable. Type: str or null Example: null Sampling and Image Configurations image_size: Size of the training images (height and width in pixels). Type: int Example: 512 interpolation: Interpolation method used for image resizing. Choices: [\"bilinear\", \"bicubic\", \"lanczos\"] Example: \"bicubic\" ddim_steps: Number of DDIM inference steps during training. Type: int Example: 50 ddim_eta: DDIM eta parameter for stochasticity during sampling. Type: float Example: 0.0 Device Configuration devices: Specifies the CUDA devices to be used for training (comma-separated). Type: str Example: \"0\" Additional Flags use_sample: Flag to indicate whether a sample dataset should be used for training. Type: bool Example: True num_workers: Number of worker threads for data loading. Type: int Example: 4 pin_memory: Flag to enable pinning memory during data loading for faster GPU transfers. Type: bool Example: true Erase Diff Evaluation Framework This section provides instructions for running the evaluation framework for the Erase Diff algorithm on Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying machine unlearning. Running the Evaluation Framework You can run the evaluation framework using the evaluate.py script located in the mu/algorithms/erase_diff/scripts/ directory. Basic Command to Run Evaluation: conda activate <env_name> python -m mu.algorithms.erase_diff.scripts.evaluate \\ --config_path mu/algorithms/erase_diff/configs/evaluation_config.yaml Running in Offline Mode: WANDB_MODE=offline python -m mu.algorithms.erase_diff.scripts.evaluate \\ --config_path mu/algorithms/erase_diff/configs/evaluation_config.yaml Example with CLI Overrides: python -m mu.algorithms.erase_diff.scripts.evaluate \\ --config_path mu/algorithms/erase_diff/configs/evaluation_config.yaml \\ --devices \"0\" \\ --seed 123 \\ --cfg_text 8.5 \\ --batch_size 16 Description of parameters in evaluation_config.yaml The evaluation_config.yaml file contains the necessary parameters for running the Erase Diff evaluation framework. Below is a detailed description of each parameter along with examples. Model Configuration: model_config : Path to the YAML file specifying the model architecture and settings. Type: str Example: \"mu/algorithms/erase_diff/configs/model_config.yaml\" ckpt_path : Path to the finetuned Stable Diffusion checkpoint file to be evaluated. Type: str Example: \"outputs/erase_diff/finetuned_models/erase_diff_Abstractionism_model.pth\" classification_model : Specifies the classification model used for evaluating the generated outputs. Type: str Example: \"vit_large_patch16_224\" model_ckpt_path: Path to pretrained Stable Diffusion model. Type : str Example : models/compvis/style50/compvis.ckpt Training and Sampling Parameters: theme : Specifies the theme or concept being evaluated for removal from the model's outputs. Type: str Example: \"Bricks\" devices : CUDA device IDs to be used for the evaluation process. Type: str Example: \"0\" cfg_text : Classifier-free guidance scale value for image generation. Higher values increase the strength of the conditioning prompt. Type: float Example: 9.0 seed : Random seed for reproducibility of results. Type: int Example: 188 ddim_steps : Number of steps for the DDIM (Denoising Diffusion Implicit Models) sampling process. Type: int Example: 100 ddim_eta : DDIM eta value for controlling the amount of randomness during sampling. Set to 0 for deterministic sampling. Type: float Example: 0.0 image_height : Height of the generated images in pixels. Type: int Example: 512 image_width : Width of the generated images in pixels. Type: int Example: 512 Output and Logging Parameters: sampler_output_dir : Directory where generated images will be saved during evaluation. Type: str Example: \"outputs/eval_results/mu_results/erase_diff/\" eval_output_dir : Directory where evaluation metrics and results will be stored. Type: str Example: \"outputs/eval_results/mu_results/erase_diff/\" reference_dir : Directory containing original images for comparison during evaluation. Type: str Example: \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\" Performance and Efficiency Parameters: multiprocessing : Enables multiprocessing for faster evaluation for FID score. Recommended for large datasets. Type: bool Example: False batch_size : Batch size used during FID computation and evaluation. Type: int Example: 16 Optimization Parameters: forget_theme : Concept or style intended for removal in the evaluation process. Type: str Example: \"Bricks\" seed_list : List of random seeds for performing multiple evaluations with different randomness levels. Type: list Example: [\"188\"]","title":"Erase Diff"},{"location":"algorithms/erase_diff/#erasediff-algorithm-for-machine-unlearning","text":"This repository provides an implementation of the erase diff algorithm for machine unlearning in Stable Diffusion models. The erasediff algorithm allows you to remove specific concepts or styles from a pre-trained model without retraining it from scratch.","title":"EraseDiff Algorithm for Machine Unlearning"},{"location":"algorithms/erase_diff/#installation","text":"pip install unlearn_diff","title":"Installation"},{"location":"algorithms/erase_diff/#prerequisities","text":"Ensure conda is installed on your system. You can install Miniconda or Anaconda: Miniconda (recommended): https://docs.conda.io/en/latest/miniconda.html Anaconda : https://www.anaconda.com/products/distribution After installing conda , ensure it is available in your PATH by running. You may require to restart the terminal session: conda --version","title":"Prerequisities"},{"location":"algorithms/erase_diff/#create-environment","text":"create_env <algorithm_name> eg: create_env erase_diff","title":"Create environment:"},{"location":"algorithms/erase_diff/#activate-environment","text":"conda activate <environment_name> eg: conda activate erase_diff The has to be one of the folders in the mu/algorithms folder.","title":"Activate environment:"},{"location":"algorithms/erase_diff/#downloading-data-and-models","text":"After you install the package, you can use the following commands to download. Dataset : i2p : Sample : download_data sample i2p Full : download_data full i2p quick_canvas : Sample : download_data sample quick_canvas Full : download_data full quick_canvas Model : compvis : download_model compvis diffuser : download_model diffuser Verify the Downloaded Files After downloading, verify that the datasets have been correctly extracted: ls -lh ./data/i2p-dataset/sample/ ls -lh ./data/quick-canvas-dataset/sample/","title":"Downloading data and models."},{"location":"algorithms/erase_diff/#run-train","text":"Example Command python -m mu.algorithms.erase_diff.scripts.train \\ --config_path mu/algorithms/erase_diff/configs/train_config.yaml Running the Training Script in Offline Mode WANDB_MODE=offline python -m mu.algorithms.erase_diff.scripts.train \\ --config_path mu/algorithms/erase_diff/configs/train_config.yaml Passing Arguments via the Command Line The train.py script allows you to override configuration parameters specified in the train_config.yaml file by passing them directly as arguments during runtime. This can be useful for quick experimentation without modifying the configuration file. Example Usage with Command-Line Arguments python -m mu.algorithms.erase_diff.scripts.train \\ --config_path mu/algorithms/erase_diff/configs/train_config.yaml \\ --train_method xattn \\ --alpha 0.2 \\ --epochs 10 \\ --lr 1e-4 \\ --devices 0,1 \\ --raw_dataset_dir /path/to/raw_dataset \\ --output_dir outputs/experiment_1 Explanation of the Example --config_path: Specifies the YAML configuration file to load default values. --train_method: Overrides the training method (\"xattn\"). --alpha: Sets the guidance strength for the starting image to 0.2. --epochs: Increases the number of training epochs to 10. --lr: Updates the learning rate to 1e-4. --devices: Specifies the GPUs (e.g., device 0 and 1) for training. --raw_dataset_dir: Changes the raw dataset directory. --output_dir: Sets a custom output directory for this run. How It Works * Default Values: The script first loads default values from the YAML file specified by --config_path. Command-Line Overrides: Any arguments passed on the command line will override the corresponding keys in the YAML configuration file. Final Configuration: The script merges the YAML file and command-line arguments into a single configuration dictionary and uses it for training.","title":"Run Train"},{"location":"algorithms/erase_diff/#directory-structure","text":"algorithm.py : Implementation of the EraseDiffAlgorithm class. configs/ : Contains configuration files for training and generation. model.py : Implementation of the EraseDiffModel class. scripts/train.py : Script to train the EraseDiff algorithm. trainer.py : Implementation of the EraseDiffTrainer class. utils.py : Utility functions used in the project. data_handler.py : Implementation of DataHandler class","title":"Directory Structure"},{"location":"algorithms/erase_diff/#description-of-arguments-in-train_configyaml","text":"Training Parameters train_method: Specifies the method of training for concept erasure. Choices: [\"noxattn\", \"selfattn\", \"xattn\", \"full\", \"notime\", \"xlayer\", \"selflayer\"] Example: \"xattn\" alpha: Guidance strength for the starting image during training. Type: float Example: 0.1 epochs: Number of epochs to train the model. Type: int Example: 1 K_steps: Number of K optimization steps during training. Type: int Example: 2 lr: Learning rate used for the optimizer during training. Type: float Example: 5e-5 Model Configuration model_config_path: File path to the Stable Diffusion model configuration YAML file. type: str Example: \"/path/to/model_config.yaml\" ckpt_path: File path to the checkpoint of the Stable Diffusion model. Type: str Example: \"/path/to/model_checkpoint.ckpt\" Dataset Directories raw_dataset_dir: Directory containing the raw dataset categorized by themes or classes. Type: str Example: \"/path/to/raw_dataset\" processed_dataset_dir: Directory to save the processed dataset. Type: str Example: \"/path/to/processed_dataset\" dataset_type: Specifies the dataset type for the training process. Choices: [\"unlearncanvas\", \"i2p\"] Example: \"unlearncanvas\" template: Type of template to use during training. Choices: [\"object\", \"style\", \"i2p\"] Example: \"style\" template_name: Name of the specific concept or style to be erased. Choices: [\"self-harm\", \"Abstractionism\"] Example: \"Abstractionism\" Output Configurations output_dir: Directory where the fine-tuned models and results will be saved. Type: str Example: \"outputs/erase_diff/finetuned_models\" separator: String separator used to train multiple words separately, if applicable. Type: str or null Example: null Sampling and Image Configurations image_size: Size of the training images (height and width in pixels). Type: int Example: 512 interpolation: Interpolation method used for image resizing. Choices: [\"bilinear\", \"bicubic\", \"lanczos\"] Example: \"bicubic\" ddim_steps: Number of DDIM inference steps during training. Type: int Example: 50 ddim_eta: DDIM eta parameter for stochasticity during sampling. Type: float Example: 0.0 Device Configuration devices: Specifies the CUDA devices to be used for training (comma-separated). Type: str Example: \"0\" Additional Flags use_sample: Flag to indicate whether a sample dataset should be used for training. Type: bool Example: True num_workers: Number of worker threads for data loading. Type: int Example: 4 pin_memory: Flag to enable pinning memory during data loading for faster GPU transfers. Type: bool Example: true","title":"Description of Arguments in train_config.yaml"},{"location":"algorithms/erase_diff/#erase-diff-evaluation-framework","text":"This section provides instructions for running the evaluation framework for the Erase Diff algorithm on Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying machine unlearning.","title":"Erase Diff Evaluation Framework"},{"location":"algorithms/erase_diff/#running-the-evaluation-framework","text":"You can run the evaluation framework using the evaluate.py script located in the mu/algorithms/erase_diff/scripts/ directory.","title":"Running the Evaluation Framework"},{"location":"algorithms/erase_diff/#basic-command-to-run-evaluation","text":"conda activate <env_name> python -m mu.algorithms.erase_diff.scripts.evaluate \\ --config_path mu/algorithms/erase_diff/configs/evaluation_config.yaml Running in Offline Mode: WANDB_MODE=offline python -m mu.algorithms.erase_diff.scripts.evaluate \\ --config_path mu/algorithms/erase_diff/configs/evaluation_config.yaml Example with CLI Overrides: python -m mu.algorithms.erase_diff.scripts.evaluate \\ --config_path mu/algorithms/erase_diff/configs/evaluation_config.yaml \\ --devices \"0\" \\ --seed 123 \\ --cfg_text 8.5 \\ --batch_size 16","title":"Basic Command to Run Evaluation:"},{"location":"algorithms/erase_diff/#description-of-parameters-in-evaluation_configyaml","text":"The evaluation_config.yaml file contains the necessary parameters for running the Erase Diff evaluation framework. Below is a detailed description of each parameter along with examples.","title":"Description of parameters in evaluation_config.yaml"},{"location":"algorithms/erase_diff/#model-configuration","text":"model_config : Path to the YAML file specifying the model architecture and settings. Type: str Example: \"mu/algorithms/erase_diff/configs/model_config.yaml\" ckpt_path : Path to the finetuned Stable Diffusion checkpoint file to be evaluated. Type: str Example: \"outputs/erase_diff/finetuned_models/erase_diff_Abstractionism_model.pth\" classification_model : Specifies the classification model used for evaluating the generated outputs. Type: str Example: \"vit_large_patch16_224\" model_ckpt_path: Path to pretrained Stable Diffusion model. Type : str Example : models/compvis/style50/compvis.ckpt","title":"Model Configuration:"},{"location":"algorithms/erase_diff/#training-and-sampling-parameters","text":"theme : Specifies the theme or concept being evaluated for removal from the model's outputs. Type: str Example: \"Bricks\" devices : CUDA device IDs to be used for the evaluation process. Type: str Example: \"0\" cfg_text : Classifier-free guidance scale value for image generation. Higher values increase the strength of the conditioning prompt. Type: float Example: 9.0 seed : Random seed for reproducibility of results. Type: int Example: 188 ddim_steps : Number of steps for the DDIM (Denoising Diffusion Implicit Models) sampling process. Type: int Example: 100 ddim_eta : DDIM eta value for controlling the amount of randomness during sampling. Set to 0 for deterministic sampling. Type: float Example: 0.0 image_height : Height of the generated images in pixels. Type: int Example: 512 image_width : Width of the generated images in pixels. Type: int Example: 512","title":"Training and Sampling Parameters:"},{"location":"algorithms/erase_diff/#output-and-logging-parameters","text":"sampler_output_dir : Directory where generated images will be saved during evaluation. Type: str Example: \"outputs/eval_results/mu_results/erase_diff/\" eval_output_dir : Directory where evaluation metrics and results will be stored. Type: str Example: \"outputs/eval_results/mu_results/erase_diff/\" reference_dir : Directory containing original images for comparison during evaluation. Type: str Example: \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\"","title":"Output and Logging Parameters:"},{"location":"algorithms/erase_diff/#performance-and-efficiency-parameters","text":"multiprocessing : Enables multiprocessing for faster evaluation for FID score. Recommended for large datasets. Type: bool Example: False batch_size : Batch size used during FID computation and evaluation. Type: int Example: 16","title":"Performance and Efficiency Parameters:"},{"location":"algorithms/erase_diff/#optimization-parameters","text":"forget_theme : Concept or style intended for removal in the evaluation process. Type: str Example: \"Bricks\" seed_list : List of random seeds for performing multiple evaluations with different randomness levels. Type: list Example: [\"188\"]","title":"Optimization Parameters:"},{"location":"algorithms/esd/","text":"ESD Algorithm for Machine Unlearning This repository provides an implementation of the ESD algorithm for machine unlearning in Stable Diffusion models. The ESD algorithm allows you to remove specific concepts or styles from a pre-trained model without retraining it from scratch. Installation pip install unlearn_diff Prerequisities Ensure conda is installed on your system. You can install Miniconda or Anaconda: Miniconda (recommended): https://docs.conda.io/en/latest/miniconda.html Anaconda : https://www.anaconda.com/products/distribution After installing conda , ensure it is available in your PATH by running. You may require to restart the terminal session: conda --version Create environment: create_env <algorithm_name> eg: create_env esd Activate environment: conda activate <environment_name> eg: conda activate esd The has to be one of the folders in the mu/algorithms folder. Downloading data and models. After you install the package, you can use the following commands to download. Dataset : i2p : Sample : download_data sample i2p Full : download_data full i2p quick_canvas : Sample : download_data sample quick_canvas Full : download_data full quick_canvas Model : compvis : download_model compvis diffuser : download_model diffuser Verify the Downloaded Files After downloading, verify that the datasets have been correctly extracted: ls -lh ./data/i2p-dataset/sample/ ls -lh ./data/quick-canvas-dataset/sample/ Example Command python -m mu.algorithms.esd.scripts.train \\ --config_path mu/algorithms/esd/configs/train_config.yaml Running the Script in Offline Mode WANDB_MODE=offline python -m mu.algorithms.esd.scripts.train \\ --config_path mu/algorithms/esd/configs/train_config.yaml Passing Arguments via the Command Line The train.py script allows you to override configuration parameters specified in the train_config.yaml file by passing them directly as arguments during runtime. This can be useful for quick experimentation without modifying the configuration file. python mu/algorithms/esd/scripts/train.py \\ --config_path train_config.yaml \\ --train_method \"xattn\" \\ --start_guidance 0.1 \\ --negative_guidance 0.0 \\ --iterations 1000 \\ --lr 5e-5 Explanation of the Example train_method: Specifies which model layers to update during training. start_guidance: Guidance scale for generating initial images. negative_guidance: Guidance scale for erasing the target concept. iterations: Number of training iterations (epochs). lr: Learning rate for the optimizer. How It Works * Default Values: The script first loads default values from the YAML file specified by --config_path. Command-Line Overrides: Any arguments passed on the command line will override the corresponding keys in the YAML configuration file. Final Configuration: The script merges the YAML file and command-line arguments into a single configuration dictionary and uses it for training. Directory Structure algorithm.py : Implementation of the ESDAlgorithm class. configs/ : Contains configuration files for training and generation. constants/const.py : Constants used throughout the project. model.py : Implementation of the ESDModel class. scripts/train.py : Script to train the ESD algorithm. trainer.py : Implementation of the ESDTrainer class. utils.py : Utility functions used in the project. Description of arguments being used in train_config.yaml The config/train_config.yaml file is a configuration file for training a Stable Diffusion model using the ESD (Erase Stable Diffusion) method. It defines various parameters related to training, model setup, dataset handling, and output configuration. Below is a detailed description of each section and parameter: Training Parameters These parameters control the fine-tuning process, including the method of training, guidance scales, learning rate, and iteration settings. train_method: Specifies the method of training to decide which parts of the model to update. Type: str Choices: noxattn, selfattn, xattn, full, notime, xlayer, selflayer Example: xattn start_guidance: Guidance scale for generating initial images during training. Affects the diversity of the training set. Type: float Example: 0.1 negative_guidance: Guidance scale for erasing the target concept during training. Type: float Example: 0.0 iterations: Number of training iterations (similar to epochs). Type: int Example: 1 lr: Learning rate used by the optimizer for fine-tuning. Type: float Example: 5e-5 image_size: Size of images used during training and sampling (in pixels). Type: int Example: 512 ddim_steps: Number of diffusion steps used in the DDIM sampling process. Type: int Example: 50 Model Configuration These parameters specify the Stable Diffusion model checkpoint and configuration file. model_config_path: Path to the YAML file defining the model architecture and parameters. Type: str Example: mu/algorithms/esd/configs/model_config.yaml ckpt_path: Path to the finetuned Stable Diffusion model checkpoint. Type: str Example: '../models/compvis/style50/compvis.ckpt' Dataset Configuration These parameters define the dataset type and template for training, specifying whether to focus on objects, styles, or inappropriate content. dataset_type: Type of dataset used for training. Type: str Choices: unlearncanvas, i2p Example: unlearncanvas template: Type of concept or style to erase during training. Type: str Choices: object, style, i2p Example: style template_name: Specific name of the object or style to erase (e.g., \"Abstractionism\"). Type: str Example Choices: Abstractionism, self-harm Example: Abstractionism Output Configuration These parameters control where the outputs of the training process, such as fine-tuned models, are stored. output_dir: Directory where the fine-tuned model and training results will be saved. Type: str Example: outputs/esd/finetuned_models separator: Separator character used to handle multiple prompts during training. If set to null, no special handling occurs. Type: str or null Example: null Device Configuration These parameters define the compute resources for training. devices: Specifies the CUDA devices used for training. Provide a comma-separated list of device IDs. Type: str Example: 0,1 use_sample: Boolean flag indicating whether to use a sample dataset for testing or debugging. Type: bool Example: True ESD Evaluation Framework This section provides instructions for running the evaluation framework for the ESD algorithm on Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying machine unlearning. Running the Evaluation Framework You can run the evaluation framework using the evaluate.py script located in the mu/algorithms/esd/scripts/ directory. Basic Command to Run Evaluation: conda activate <env_name> python -m mu.algorithms.esd.scripts.evaluate \\ --config_path mu/algorithms/esd/configs/evaluation_config.yaml Running in Offline Mode: WANDB_MODE=offline python -m mu.algorithms.esd.scripts.evaluate \\ --config_path mu/algorithms/esd/configs/evaluation_config.yaml Example with CLI Overrides: python -m mu.algorithms.esd.scripts.evaluate \\ --config_path mu/algorithms/esd/configs/evaluation_config.yaml \\ --devices \"0\" \\ --seed 123 \\ --cfg_text 8.5 \\ --batch_size 16 Description of parameters in evaluation_config.yaml The evaluation_config.yaml file contains the necessary parameters for running the ESD evaluation framework. Below is a detailed description of each parameter along with examples. Model Configuration: model_config : Path to the YAML file specifying the model architecture and settings. Type: str Example: \"mu/algorithms/esd/configs/model_config.yaml\" ckpt_path : Path to the finetuned Stable Diffusion checkpoint file to be evaluated. Type: str Example: \"outputs/esd/finetuned_models/esd_Abstractionism_model.pth\" classification_model : Specifies the classification model used for evaluating the generated outputs. Type: str Example: \"vit_large_patch16_224\" model_ckpt_path: Path to pretrained Stable Diffusion model. Type : str Example : models/compvis/style50/compvis.ckpt Training and Sampling Parameters: theme : Specifies the theme or concept being evaluated for removal from the model's outputs. Type: str Example: \"Bricks\" devices : CUDA device IDs to be used for the evaluation process. Type: str Example: \"0\" cfg_text : Classifier-free guidance scale value for image generation. Higher values increase the strength of the conditioning prompt. Type: float Example: 9.0 seed : Random seed for reproducibility of results. Type: int Example: 188 ddim_steps : Number of steps for the DDIM (Denoising Diffusion Implicit Models) sampling process. Type: int Example: 100 ddim_eta : DDIM eta value for controlling the amount of randomness during sampling. Set to 0 for deterministic sampling. Type: float Example: 0.0 image_height : Height of the generated images in pixels. Type: int Example: 512 image_width : Width of the generated images in pixels. Type: int Example: 512 Output and Logging Parameters: sampler_output_dir : Directory where generated images will be saved during evaluation. Type: str Example: \"outputs/eval_results/mu_results/esd/\" eval_output_dir : Directory where evaluation metrics and results will be stored. Type: str Example: \"outputs/eval_results/mu_results/esd/\" reference_dir : Directory containing original images for comparison during evaluation. Type: str Example: \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\" Performance and Efficiency Parameters: multiprocessing : Enables multiprocessing for faster evaluation for FID score. Recommended for large datasets. Type: bool Example: False batch_size : Batch size used during FID computation and evaluation. Type: int Example: 16 Optimization Parameters: forget_theme : Concept or style intended for removal in the evaluation process. Type: str Example: \"Bricks\" seed_list : List of random seeds for performing multiple evaluations with different randomness levels. Type: list Example: [\"188\"]","title":"ESD"},{"location":"algorithms/esd/#esd-algorithm-for-machine-unlearning","text":"This repository provides an implementation of the ESD algorithm for machine unlearning in Stable Diffusion models. The ESD algorithm allows you to remove specific concepts or styles from a pre-trained model without retraining it from scratch.","title":"ESD Algorithm for Machine Unlearning"},{"location":"algorithms/esd/#installation","text":"pip install unlearn_diff","title":"Installation"},{"location":"algorithms/esd/#prerequisities","text":"Ensure conda is installed on your system. You can install Miniconda or Anaconda: Miniconda (recommended): https://docs.conda.io/en/latest/miniconda.html Anaconda : https://www.anaconda.com/products/distribution After installing conda , ensure it is available in your PATH by running. You may require to restart the terminal session: conda --version","title":"Prerequisities"},{"location":"algorithms/esd/#create-environment","text":"create_env <algorithm_name> eg: create_env esd","title":"Create environment:"},{"location":"algorithms/esd/#activate-environment","text":"conda activate <environment_name> eg: conda activate esd The has to be one of the folders in the mu/algorithms folder.","title":"Activate environment:"},{"location":"algorithms/esd/#downloading-data-and-models","text":"After you install the package, you can use the following commands to download. Dataset : i2p : Sample : download_data sample i2p Full : download_data full i2p quick_canvas : Sample : download_data sample quick_canvas Full : download_data full quick_canvas Model : compvis : download_model compvis diffuser : download_model diffuser Verify the Downloaded Files After downloading, verify that the datasets have been correctly extracted: ls -lh ./data/i2p-dataset/sample/ ls -lh ./data/quick-canvas-dataset/sample/","title":"Downloading data and models."},{"location":"algorithms/esd/#example-command","text":"python -m mu.algorithms.esd.scripts.train \\ --config_path mu/algorithms/esd/configs/train_config.yaml Running the Script in Offline Mode WANDB_MODE=offline python -m mu.algorithms.esd.scripts.train \\ --config_path mu/algorithms/esd/configs/train_config.yaml Passing Arguments via the Command Line The train.py script allows you to override configuration parameters specified in the train_config.yaml file by passing them directly as arguments during runtime. This can be useful for quick experimentation without modifying the configuration file. python mu/algorithms/esd/scripts/train.py \\ --config_path train_config.yaml \\ --train_method \"xattn\" \\ --start_guidance 0.1 \\ --negative_guidance 0.0 \\ --iterations 1000 \\ --lr 5e-5 Explanation of the Example train_method: Specifies which model layers to update during training. start_guidance: Guidance scale for generating initial images. negative_guidance: Guidance scale for erasing the target concept. iterations: Number of training iterations (epochs). lr: Learning rate for the optimizer. How It Works * Default Values: The script first loads default values from the YAML file specified by --config_path. Command-Line Overrides: Any arguments passed on the command line will override the corresponding keys in the YAML configuration file. Final Configuration: The script merges the YAML file and command-line arguments into a single configuration dictionary and uses it for training.","title":"Example Command"},{"location":"algorithms/esd/#directory-structure","text":"algorithm.py : Implementation of the ESDAlgorithm class. configs/ : Contains configuration files for training and generation. constants/const.py : Constants used throughout the project. model.py : Implementation of the ESDModel class. scripts/train.py : Script to train the ESD algorithm. trainer.py : Implementation of the ESDTrainer class. utils.py : Utility functions used in the project.","title":"Directory Structure"},{"location":"algorithms/esd/#description-of-arguments-being-used-in-train_configyaml","text":"The config/train_config.yaml file is a configuration file for training a Stable Diffusion model using the ESD (Erase Stable Diffusion) method. It defines various parameters related to training, model setup, dataset handling, and output configuration. Below is a detailed description of each section and parameter: Training Parameters These parameters control the fine-tuning process, including the method of training, guidance scales, learning rate, and iteration settings. train_method: Specifies the method of training to decide which parts of the model to update. Type: str Choices: noxattn, selfattn, xattn, full, notime, xlayer, selflayer Example: xattn start_guidance: Guidance scale for generating initial images during training. Affects the diversity of the training set. Type: float Example: 0.1 negative_guidance: Guidance scale for erasing the target concept during training. Type: float Example: 0.0 iterations: Number of training iterations (similar to epochs). Type: int Example: 1 lr: Learning rate used by the optimizer for fine-tuning. Type: float Example: 5e-5 image_size: Size of images used during training and sampling (in pixels). Type: int Example: 512 ddim_steps: Number of diffusion steps used in the DDIM sampling process. Type: int Example: 50 Model Configuration These parameters specify the Stable Diffusion model checkpoint and configuration file. model_config_path: Path to the YAML file defining the model architecture and parameters. Type: str Example: mu/algorithms/esd/configs/model_config.yaml ckpt_path: Path to the finetuned Stable Diffusion model checkpoint. Type: str Example: '../models/compvis/style50/compvis.ckpt' Dataset Configuration These parameters define the dataset type and template for training, specifying whether to focus on objects, styles, or inappropriate content. dataset_type: Type of dataset used for training. Type: str Choices: unlearncanvas, i2p Example: unlearncanvas template: Type of concept or style to erase during training. Type: str Choices: object, style, i2p Example: style template_name: Specific name of the object or style to erase (e.g., \"Abstractionism\"). Type: str Example Choices: Abstractionism, self-harm Example: Abstractionism Output Configuration These parameters control where the outputs of the training process, such as fine-tuned models, are stored. output_dir: Directory where the fine-tuned model and training results will be saved. Type: str Example: outputs/esd/finetuned_models separator: Separator character used to handle multiple prompts during training. If set to null, no special handling occurs. Type: str or null Example: null Device Configuration These parameters define the compute resources for training. devices: Specifies the CUDA devices used for training. Provide a comma-separated list of device IDs. Type: str Example: 0,1 use_sample: Boolean flag indicating whether to use a sample dataset for testing or debugging. Type: bool Example: True","title":"Description of arguments being used in train_config.yaml"},{"location":"algorithms/esd/#esd-evaluation-framework","text":"This section provides instructions for running the evaluation framework for the ESD algorithm on Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying machine unlearning.","title":"ESD Evaluation Framework"},{"location":"algorithms/esd/#running-the-evaluation-framework","text":"You can run the evaluation framework using the evaluate.py script located in the mu/algorithms/esd/scripts/ directory.","title":"Running the Evaluation Framework"},{"location":"algorithms/esd/#basic-command-to-run-evaluation","text":"conda activate <env_name> python -m mu.algorithms.esd.scripts.evaluate \\ --config_path mu/algorithms/esd/configs/evaluation_config.yaml Running in Offline Mode: WANDB_MODE=offline python -m mu.algorithms.esd.scripts.evaluate \\ --config_path mu/algorithms/esd/configs/evaluation_config.yaml Example with CLI Overrides: python -m mu.algorithms.esd.scripts.evaluate \\ --config_path mu/algorithms/esd/configs/evaluation_config.yaml \\ --devices \"0\" \\ --seed 123 \\ --cfg_text 8.5 \\ --batch_size 16","title":"Basic Command to Run Evaluation:"},{"location":"algorithms/esd/#description-of-parameters-in-evaluation_configyaml","text":"The evaluation_config.yaml file contains the necessary parameters for running the ESD evaluation framework. Below is a detailed description of each parameter along with examples.","title":"Description of parameters in evaluation_config.yaml"},{"location":"algorithms/esd/#model-configuration","text":"model_config : Path to the YAML file specifying the model architecture and settings. Type: str Example: \"mu/algorithms/esd/configs/model_config.yaml\" ckpt_path : Path to the finetuned Stable Diffusion checkpoint file to be evaluated. Type: str Example: \"outputs/esd/finetuned_models/esd_Abstractionism_model.pth\" classification_model : Specifies the classification model used for evaluating the generated outputs. Type: str Example: \"vit_large_patch16_224\" model_ckpt_path: Path to pretrained Stable Diffusion model. Type : str Example : models/compvis/style50/compvis.ckpt","title":"Model Configuration:"},{"location":"algorithms/esd/#training-and-sampling-parameters","text":"theme : Specifies the theme or concept being evaluated for removal from the model's outputs. Type: str Example: \"Bricks\" devices : CUDA device IDs to be used for the evaluation process. Type: str Example: \"0\" cfg_text : Classifier-free guidance scale value for image generation. Higher values increase the strength of the conditioning prompt. Type: float Example: 9.0 seed : Random seed for reproducibility of results. Type: int Example: 188 ddim_steps : Number of steps for the DDIM (Denoising Diffusion Implicit Models) sampling process. Type: int Example: 100 ddim_eta : DDIM eta value for controlling the amount of randomness during sampling. Set to 0 for deterministic sampling. Type: float Example: 0.0 image_height : Height of the generated images in pixels. Type: int Example: 512 image_width : Width of the generated images in pixels. Type: int Example: 512","title":"Training and Sampling Parameters:"},{"location":"algorithms/esd/#output-and-logging-parameters","text":"sampler_output_dir : Directory where generated images will be saved during evaluation. Type: str Example: \"outputs/eval_results/mu_results/esd/\" eval_output_dir : Directory where evaluation metrics and results will be stored. Type: str Example: \"outputs/eval_results/mu_results/esd/\" reference_dir : Directory containing original images for comparison during evaluation. Type: str Example: \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\"","title":"Output and Logging Parameters:"},{"location":"algorithms/esd/#performance-and-efficiency-parameters","text":"multiprocessing : Enables multiprocessing for faster evaluation for FID score. Recommended for large datasets. Type: bool Example: False batch_size : Batch size used during FID computation and evaluation. Type: int Example: 16","title":"Performance and Efficiency Parameters:"},{"location":"algorithms/esd/#optimization-parameters","text":"forget_theme : Concept or style intended for removal in the evaluation process. Type: str Example: \"Bricks\" seed_list : List of random seeds for performing multiple evaluations with different randomness levels. Type: list Example: [\"188\"]","title":"Optimization Parameters:"},{"location":"algorithms/forget_me_not/","text":"Forget Me Not Algorithm for Machine Unlearning This repository provides an implementation of the erase diff algorithm for machine unlearning in Stable Diffusion models. The Forget Me Not algorithm allows you to remove specific concepts or styles from a pre-trained model without retraining it from scratch. Installation pip install unlearn_diff Prerequisities Ensure conda is installed on your system. You can install Miniconda or Anaconda: Miniconda (recommended): https://docs.conda.io/en/latest/miniconda.html Anaconda : https://www.anaconda.com/products/distribution After installing conda , ensure it is available in your PATH by running. You may require to restart the terminal session: conda --version Create environment: create_env <algorithm_name> eg: create_env forget_me_not Activate environment: conda activate <environment_name> eg: conda activate forget_me_not The has to be one of the folders in the mu/algorithms folder. Downloading data and models. After you install the package, you can use the following commands to download. Dataset : i2p : Sample : download_data sample i2p Full : download_data full i2p quick_canvas : Sample : download_data sample quick_canvas Full : download_data full quick_canvas Model : compvis : download_model compvis diffuser : download_model diffuser Verify the Downloaded Files After downloading, verify that the datasets have been correctly extracted: ls -lh ./data/i2p-dataset/sample/ ls -lh ./data/quick-canvas-dataset/sample/ Example Command Train a Text Inversion python -m mu.algorithms.forget_me_not.scripts.train_ti \\ --config_path mu/algorithms/forget_me_not/config/train_ti_config.yaml Running the Script in Offlikne Mode WANDB_MODE=offline python -m mu.algorithms.forget_me_not.scripts.train_ti \\ --config_path mu/algorithms/forget_me_not/config/train_ti_config.yaml Perform Unlearning Before running the train_attn.py script, update the ti_weights_path parameter in the configuration file to point to the output generated from the Text Inversion (train_ti.py) stage. Example: ti_weights_path: \"outputs/forget_me_not/ti_models/step_inv_10.safetensors\" Run unlearning script: python -m mu.algorithms.forget_me_not.scripts.train_attn \\ --config_path mu/algorithms/forget_me_not/config/train_attn_config.yaml Running the Script in Offline Mode WANDB_MODE=offline python -m mu.algorithms.forget_me_not.scripts.train_attn \\ --config_path mu/algorithms/forget_me_not/config/train_attn_config.yaml Passing Arguments via the Command Line The train_ti.py and train_attn.py script allows you to override configuration parameters specified in the train_ti_config.yaml and train_attn_config.yaml files by passing them directly as arguments during runtime. This can be useful for quick experimentation without modifying the configuration file. Example Usage with Command-Line Arguments python -m mu.algorithms.forget_me_not.scripts.train_ti \\ --config_path mu/algorithms/forget_me_not/config/train_ti_config.yaml \\ --ckpt_path /path/to/style50 \\ --raw_dataset_dir /path/to/raw_dataset \\ Explanation of the Example * --config_path: Path to the pretrained model's checkpoint file for Stable Diffusion. * --raw_dataset_dir: Directory containing the original dataset organized by themes and classes. How It Works * Default Values: The script first loads default values from the YAML file specified by --config_path. Command-Line Overrides: Any arguments passed on the command line will override the corresponding keys in the YAML configuration file. Final Configuration: The script merges the YAML file and command-line arguments into a single configuration dictionary and uses it for training. Directory Structure algorithm.py : Implementation of the Forget Me NotAlgorithm class. configs/ : Contains configuration files for training and generation. model.py : Implementation of the Forget Me NotModel class. scripts/train.py : Script to train the Forget Me Not algorithm. trainer.py : Implementation of the Forget Me NotTrainer class. utils.py : Utility functions used in the project. data_handler.py : Implementation of DataHandler class This method involves two stages: Train a Text Inversion : The first stage involves training a Text Inversion. Refer to the script train_ti.py for details and implementation. It uses train_ti_config.yaml as config file. Perform Unlearning : The second stage uses the outputs from the first stage to perform unlearning. Refer to the script train_attn.py for details and implementation. It uses train_attn_config.yaml as config file. Description of Arguments in train_ti_config.yaml Pretrained Model ckpt_path : File path to the pretrained model's checkpoint file. Dataset raw_dataset_dir : Directory containing the original dataset organized by themes and classes. processed_dataset_dir : Directory where the processed datasets will be saved. dataset_type : Type of dataset to use (e.g., unlearncanvas ). template : Type of template to use (e.g., style ). template_name : Name of the template, defining the style or theme (e.g., Abstractionism ). use_sample : Boolean indicating whether to use the sample dataset for training. Training Configuration initializer_tokens : Tokens used to initialize the training process, referencing the template name. steps : Number of training steps. lr : Learning rate for the training optimizer. weight_decay_ti : Weight decay for Text Inversion training. seed : Random seed for reproducibility. placeholder_tokens : Tokens used as placeholders during training. placeholder_token_at_data : Placeholders used in the dataset for Text Inversion training. gradient_checkpointing : Boolean to enable or disable gradient checkpointing. scale_lr : Boolean indicating whether to scale the learning rate based on batch size. gradient_accumulation_steps : Number of steps to accumulate gradients before updating weights. train_batch_size : Batch size for training. lr_warmup_steps : Number of steps for linear warmup of the learning rate. Output Configuration output_dir : Directory path to save training results, including models and logs. Device Configuration devices : CUDA devices to train on (comma-separated). Description of Arguments in train_attn_config.yaml Key Parameters Pretrained Model ckpt_path : File path to the pretrained model's checkpoint file. Dataset raw_dataset_dir : Directory containing the original dataset organized by themes and classes. processed_dataset_dir : Directory where the processed datasets will be saved. dataset_type : Type of dataset to use (e.g., unlearncanvas ). template : Type of template to use (e.g., style ). template_name : Name of the template, defining the style or theme (e.g., Abstractionism ). use_sample : Boolean indicating whether to use the sample dataset for training. Text Inversion use_ti : Boolean indicating whether to use Text Inversion weights. ti_weights_path : File path to the Text Inversion model weights. Tokens initializer_tokens : Tokens used to initialize the training process, referencing the template name. placeholder_tokens : Tokens used as placeholders during training. Training Configuration mixed_precision : Precision type to use during training (e.g., fp16 or fp32 ). gradient_accumulation_steps : Number of steps to accumulate gradients before updating weights. train_text_encoder : Boolean to enable or disable training of the text encoder. enable_xformers_memory_efficient_attention : Boolean to enable memory-efficient attention mechanisms. gradient_checkpointing : Boolean to enable or disable gradient checkpointing. allow_tf32 : Boolean to allow TensorFloat-32 computation for faster training. scale_lr : Boolean indicating whether to scale the learning rate based on batch size. train_batch_size : Batch size for training. use_8bit_adam : Boolean to enable or disable 8-bit Adam optimizer. adam_beta1 : Beta1 parameter for the Adam optimizer. adam_beta2 : Beta2 parameter for the Adam optimizer. adam_weight_decay : Weight decay for the Adam optimizer. adam_epsilon : Epsilon value for the Adam optimizer. size : Image resolution size for training. with_prior_preservation : Boolean indicating whether to use prior preservation during training. num_train_epochs : Number of training epochs. lr_warmup_steps : Number of steps for linear warmup of the learning rate. lr_num_cycles : Number of cycles for learning rate scheduling. lr_power : Exponent to control the shape of the learning rate curve. max-steps : Maximum number of training steps. no_real_image : Boolean to skip using real images in training. max_grad_norm : Maximum norm for gradient clipping. checkpointing_steps : Number of steps between model checkpoints. set_grads_to_none : Boolean to set gradients to None instead of zeroing them out. lr : Learning rate for the training optimizer. Output Configuration output_dir : Directory path to save training results, including models and logs. Device Configuration devices : CUDA devices to train on (comma-separated). Miscellaneous only-xa : Boolean to enable additional configurations specific to the XA pipeline. forget_me_not Evaluation Framework This section provides instructions for running the evaluation framework for the forget_me_not algorithm on Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying machine unlearning. Running the Evaluation Framework You can run the evaluation framework using the evaluate.py script located in the mu/algorithms/forget_me_not/scripts/ directory. Basic Command to Run Evaluation: conda activate <env_name> python -m mu.algorithms.forget_me_not.scripts.evaluate \\ --config_path mu/algorithms/forget_me_not/configs/evaluation_config.yaml Running in Offline Mode: WANDB_MODE=offline python -m mu.algorithms.forget_me_not.scripts.evaluate \\ --config_path mu/algorithms/forget_me_not/configs/evaluation_config.yaml Example with CLI Overrides: python -m mu.algorithms.forget_me_not.scripts.evaluate \\ --config_path mu/algorithms/forget_me_not/configs/evaluation_config.yaml \\ --devices \"0\" \\ --seed 123 \\ --cfg_text 8.5 \\ --batch_size 16 Description of parameters in evaluation_config.yaml The evaluation_config.yaml file contains the necessary parameters for running the forget_me_not evaluation framework. Below is a detailed description of each parameter along with examples. Model Configuration: ckpt_path : Path to the finetuned Stable Diffusion checkpoint file to be evaluated. Type: str Example: \"outputs/forget_me_not/finetuned_models/forget_me_not_Abstractionism_model.pth\" classification_model : Specifies the classification model used for evaluating the generated outputs. Type: str Example: \"vit_large_patch16_224\" model_ckpt_path: Path to pretrained Stable Diffusion model. Type : str Example : models/diffuser/style50 Training and Sampling Parameters: theme : Specifies the theme or concept being evaluated for removal from the model's outputs. Type: str Example: \"Bricks\" devices : CUDA device IDs to be used for the evaluation process. Type: str Example: \"0\" cfg_text_list : Classifier-free guidance scale value for image generation. Higher values increase the strength of the conditioning prompt. Type: list Example: [9.0] seed : Random seed for reproducibility of results. Type: int Example: 188 ddim_steps : Number of steps for the DDIM (Denoising Diffusion Implicit Models) sampling process. Type: int Example: 100 ddim_eta : DDIM eta value for controlling the amount of randomness during sampling. Set to 0 for deterministic sampling. Type: float Example: 0.0 image_height : Height of the generated images in pixels. Type: int Example: 512 image_width : Width of the generated images in pixels. Type: int Example: 512 Output and Logging Parameters: sampler_output_dir : Directory where generated images will be saved during evaluation. Type: str Example: \"outputs/eval_results/mu_results/forget_me_not/\" eval_output_dir : Directory where evaluation metrics and results will be stored. Type: str Example: \"outputs/eval_results/mu_results/forget_me_not/\" reference_dir : Directory containing original images for comparison during evaluation. Type: str Example: \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\" Performance and Efficiency Parameters: multiprocessing : Enables multiprocessing for faster evaluation for FID score. Recommended for large datasets. Type: bool Example: False batch_size : Batch size used during FID computation and evaluation. Type: int Example: 16 Optimization Parameters: forget_theme : Concept or style intended for removal in the evaluation process. Type: str Example: \"Bricks\" seed_list : List of random seeds for performing multiple evaluations with different randomness levels. Type: list Example: [\"188\"]","title":"Forget Me Not"},{"location":"algorithms/forget_me_not/#forget-me-not-algorithm-for-machine-unlearning","text":"This repository provides an implementation of the erase diff algorithm for machine unlearning in Stable Diffusion models. The Forget Me Not algorithm allows you to remove specific concepts or styles from a pre-trained model without retraining it from scratch.","title":"Forget Me Not Algorithm for Machine Unlearning"},{"location":"algorithms/forget_me_not/#installation","text":"pip install unlearn_diff","title":"Installation"},{"location":"algorithms/forget_me_not/#prerequisities","text":"Ensure conda is installed on your system. You can install Miniconda or Anaconda: Miniconda (recommended): https://docs.conda.io/en/latest/miniconda.html Anaconda : https://www.anaconda.com/products/distribution After installing conda , ensure it is available in your PATH by running. You may require to restart the terminal session: conda --version","title":"Prerequisities"},{"location":"algorithms/forget_me_not/#create-environment","text":"create_env <algorithm_name> eg: create_env forget_me_not","title":"Create environment:"},{"location":"algorithms/forget_me_not/#activate-environment","text":"conda activate <environment_name> eg: conda activate forget_me_not The has to be one of the folders in the mu/algorithms folder.","title":"Activate environment:"},{"location":"algorithms/forget_me_not/#downloading-data-and-models","text":"After you install the package, you can use the following commands to download. Dataset : i2p : Sample : download_data sample i2p Full : download_data full i2p quick_canvas : Sample : download_data sample quick_canvas Full : download_data full quick_canvas Model : compvis : download_model compvis diffuser : download_model diffuser Verify the Downloaded Files After downloading, verify that the datasets have been correctly extracted: ls -lh ./data/i2p-dataset/sample/ ls -lh ./data/quick-canvas-dataset/sample/","title":"Downloading data and models."},{"location":"algorithms/forget_me_not/#example-command","text":"Train a Text Inversion python -m mu.algorithms.forget_me_not.scripts.train_ti \\ --config_path mu/algorithms/forget_me_not/config/train_ti_config.yaml Running the Script in Offlikne Mode WANDB_MODE=offline python -m mu.algorithms.forget_me_not.scripts.train_ti \\ --config_path mu/algorithms/forget_me_not/config/train_ti_config.yaml Perform Unlearning Before running the train_attn.py script, update the ti_weights_path parameter in the configuration file to point to the output generated from the Text Inversion (train_ti.py) stage. Example: ti_weights_path: \"outputs/forget_me_not/ti_models/step_inv_10.safetensors\" Run unlearning script: python -m mu.algorithms.forget_me_not.scripts.train_attn \\ --config_path mu/algorithms/forget_me_not/config/train_attn_config.yaml Running the Script in Offline Mode WANDB_MODE=offline python -m mu.algorithms.forget_me_not.scripts.train_attn \\ --config_path mu/algorithms/forget_me_not/config/train_attn_config.yaml Passing Arguments via the Command Line The train_ti.py and train_attn.py script allows you to override configuration parameters specified in the train_ti_config.yaml and train_attn_config.yaml files by passing them directly as arguments during runtime. This can be useful for quick experimentation without modifying the configuration file. Example Usage with Command-Line Arguments python -m mu.algorithms.forget_me_not.scripts.train_ti \\ --config_path mu/algorithms/forget_me_not/config/train_ti_config.yaml \\ --ckpt_path /path/to/style50 \\ --raw_dataset_dir /path/to/raw_dataset \\ Explanation of the Example * --config_path: Path to the pretrained model's checkpoint file for Stable Diffusion. * --raw_dataset_dir: Directory containing the original dataset organized by themes and classes. How It Works * Default Values: The script first loads default values from the YAML file specified by --config_path. Command-Line Overrides: Any arguments passed on the command line will override the corresponding keys in the YAML configuration file. Final Configuration: The script merges the YAML file and command-line arguments into a single configuration dictionary and uses it for training.","title":"Example Command"},{"location":"algorithms/forget_me_not/#directory-structure","text":"algorithm.py : Implementation of the Forget Me NotAlgorithm class. configs/ : Contains configuration files for training and generation. model.py : Implementation of the Forget Me NotModel class. scripts/train.py : Script to train the Forget Me Not algorithm. trainer.py : Implementation of the Forget Me NotTrainer class. utils.py : Utility functions used in the project. data_handler.py : Implementation of DataHandler class This method involves two stages: Train a Text Inversion : The first stage involves training a Text Inversion. Refer to the script train_ti.py for details and implementation. It uses train_ti_config.yaml as config file. Perform Unlearning : The second stage uses the outputs from the first stage to perform unlearning. Refer to the script train_attn.py for details and implementation. It uses train_attn_config.yaml as config file.","title":"Directory Structure"},{"location":"algorithms/forget_me_not/#description-of-arguments-in-train_ti_configyaml","text":"Pretrained Model ckpt_path : File path to the pretrained model's checkpoint file. Dataset raw_dataset_dir : Directory containing the original dataset organized by themes and classes. processed_dataset_dir : Directory where the processed datasets will be saved. dataset_type : Type of dataset to use (e.g., unlearncanvas ). template : Type of template to use (e.g., style ). template_name : Name of the template, defining the style or theme (e.g., Abstractionism ). use_sample : Boolean indicating whether to use the sample dataset for training. Training Configuration initializer_tokens : Tokens used to initialize the training process, referencing the template name. steps : Number of training steps. lr : Learning rate for the training optimizer. weight_decay_ti : Weight decay for Text Inversion training. seed : Random seed for reproducibility. placeholder_tokens : Tokens used as placeholders during training. placeholder_token_at_data : Placeholders used in the dataset for Text Inversion training. gradient_checkpointing : Boolean to enable or disable gradient checkpointing. scale_lr : Boolean indicating whether to scale the learning rate based on batch size. gradient_accumulation_steps : Number of steps to accumulate gradients before updating weights. train_batch_size : Batch size for training. lr_warmup_steps : Number of steps for linear warmup of the learning rate. Output Configuration output_dir : Directory path to save training results, including models and logs. Device Configuration devices : CUDA devices to train on (comma-separated).","title":"Description of Arguments in train_ti_config.yaml"},{"location":"algorithms/forget_me_not/#description-of-arguments-in-train_attn_configyaml","text":"","title":"Description of Arguments in train_attn_config.yaml"},{"location":"algorithms/forget_me_not/#key-parameters","text":"Pretrained Model ckpt_path : File path to the pretrained model's checkpoint file. Dataset raw_dataset_dir : Directory containing the original dataset organized by themes and classes. processed_dataset_dir : Directory where the processed datasets will be saved. dataset_type : Type of dataset to use (e.g., unlearncanvas ). template : Type of template to use (e.g., style ). template_name : Name of the template, defining the style or theme (e.g., Abstractionism ). use_sample : Boolean indicating whether to use the sample dataset for training. Text Inversion use_ti : Boolean indicating whether to use Text Inversion weights. ti_weights_path : File path to the Text Inversion model weights. Tokens initializer_tokens : Tokens used to initialize the training process, referencing the template name. placeholder_tokens : Tokens used as placeholders during training. Training Configuration mixed_precision : Precision type to use during training (e.g., fp16 or fp32 ). gradient_accumulation_steps : Number of steps to accumulate gradients before updating weights. train_text_encoder : Boolean to enable or disable training of the text encoder. enable_xformers_memory_efficient_attention : Boolean to enable memory-efficient attention mechanisms. gradient_checkpointing : Boolean to enable or disable gradient checkpointing. allow_tf32 : Boolean to allow TensorFloat-32 computation for faster training. scale_lr : Boolean indicating whether to scale the learning rate based on batch size. train_batch_size : Batch size for training. use_8bit_adam : Boolean to enable or disable 8-bit Adam optimizer. adam_beta1 : Beta1 parameter for the Adam optimizer. adam_beta2 : Beta2 parameter for the Adam optimizer. adam_weight_decay : Weight decay for the Adam optimizer. adam_epsilon : Epsilon value for the Adam optimizer. size : Image resolution size for training. with_prior_preservation : Boolean indicating whether to use prior preservation during training. num_train_epochs : Number of training epochs. lr_warmup_steps : Number of steps for linear warmup of the learning rate. lr_num_cycles : Number of cycles for learning rate scheduling. lr_power : Exponent to control the shape of the learning rate curve. max-steps : Maximum number of training steps. no_real_image : Boolean to skip using real images in training. max_grad_norm : Maximum norm for gradient clipping. checkpointing_steps : Number of steps between model checkpoints. set_grads_to_none : Boolean to set gradients to None instead of zeroing them out. lr : Learning rate for the training optimizer. Output Configuration output_dir : Directory path to save training results, including models and logs. Device Configuration devices : CUDA devices to train on (comma-separated). Miscellaneous only-xa : Boolean to enable additional configurations specific to the XA pipeline.","title":"Key Parameters"},{"location":"algorithms/forget_me_not/#forget_me_not-evaluation-framework","text":"This section provides instructions for running the evaluation framework for the forget_me_not algorithm on Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying machine unlearning.","title":"forget_me_not Evaluation Framework"},{"location":"algorithms/forget_me_not/#running-the-evaluation-framework","text":"You can run the evaluation framework using the evaluate.py script located in the mu/algorithms/forget_me_not/scripts/ directory.","title":"Running the Evaluation Framework"},{"location":"algorithms/forget_me_not/#basic-command-to-run-evaluation","text":"conda activate <env_name> python -m mu.algorithms.forget_me_not.scripts.evaluate \\ --config_path mu/algorithms/forget_me_not/configs/evaluation_config.yaml Running in Offline Mode: WANDB_MODE=offline python -m mu.algorithms.forget_me_not.scripts.evaluate \\ --config_path mu/algorithms/forget_me_not/configs/evaluation_config.yaml Example with CLI Overrides: python -m mu.algorithms.forget_me_not.scripts.evaluate \\ --config_path mu/algorithms/forget_me_not/configs/evaluation_config.yaml \\ --devices \"0\" \\ --seed 123 \\ --cfg_text 8.5 \\ --batch_size 16","title":"Basic Command to Run Evaluation:"},{"location":"algorithms/forget_me_not/#description-of-parameters-in-evaluation_configyaml","text":"The evaluation_config.yaml file contains the necessary parameters for running the forget_me_not evaluation framework. Below is a detailed description of each parameter along with examples.","title":"Description of parameters in evaluation_config.yaml"},{"location":"algorithms/forget_me_not/#model-configuration","text":"ckpt_path : Path to the finetuned Stable Diffusion checkpoint file to be evaluated. Type: str Example: \"outputs/forget_me_not/finetuned_models/forget_me_not_Abstractionism_model.pth\" classification_model : Specifies the classification model used for evaluating the generated outputs. Type: str Example: \"vit_large_patch16_224\" model_ckpt_path: Path to pretrained Stable Diffusion model. Type : str Example : models/diffuser/style50","title":"Model Configuration:"},{"location":"algorithms/forget_me_not/#training-and-sampling-parameters","text":"theme : Specifies the theme or concept being evaluated for removal from the model's outputs. Type: str Example: \"Bricks\" devices : CUDA device IDs to be used for the evaluation process. Type: str Example: \"0\" cfg_text_list : Classifier-free guidance scale value for image generation. Higher values increase the strength of the conditioning prompt. Type: list Example: [9.0] seed : Random seed for reproducibility of results. Type: int Example: 188 ddim_steps : Number of steps for the DDIM (Denoising Diffusion Implicit Models) sampling process. Type: int Example: 100 ddim_eta : DDIM eta value for controlling the amount of randomness during sampling. Set to 0 for deterministic sampling. Type: float Example: 0.0 image_height : Height of the generated images in pixels. Type: int Example: 512 image_width : Width of the generated images in pixels. Type: int Example: 512","title":"Training and Sampling Parameters:"},{"location":"algorithms/forget_me_not/#output-and-logging-parameters","text":"sampler_output_dir : Directory where generated images will be saved during evaluation. Type: str Example: \"outputs/eval_results/mu_results/forget_me_not/\" eval_output_dir : Directory where evaluation metrics and results will be stored. Type: str Example: \"outputs/eval_results/mu_results/forget_me_not/\" reference_dir : Directory containing original images for comparison during evaluation. Type: str Example: \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\"","title":"Output and Logging Parameters:"},{"location":"algorithms/forget_me_not/#performance-and-efficiency-parameters","text":"multiprocessing : Enables multiprocessing for faster evaluation for FID score. Recommended for large datasets. Type: bool Example: False batch_size : Batch size used during FID computation and evaluation. Type: int Example: 16","title":"Performance and Efficiency Parameters:"},{"location":"algorithms/forget_me_not/#optimization-parameters","text":"forget_theme : Concept or style intended for removal in the evaluation process. Type: str Example: \"Bricks\" seed_list : List of random seeds for performing multiple evaluations with different randomness levels. Type: list Example: [\"188\"]","title":"Optimization Parameters:"},{"location":"algorithms/saliency/","text":"Saliency Unlearning Algorithm for Machine Unlearning This repository provides an implementation of the Saliency Unlearning algorithm for machine unlearning in Stable Diffusion models. The Saliency Unlearning algorithm allows you to remove specific concepts or styles from a pre-trained model without retraining it from scratch. Installation pip install unlearn_diff Prerequisities Ensure conda is installed on your system. You can install Miniconda or Anaconda: Miniconda (recommended): https://docs.conda.io/en/latest/miniconda.html Anaconda : https://www.anaconda.com/products/distribution After installing conda , ensure it is available in your PATH by running. You may require to restart the terminal session: conda --version Create environment: create_env <algorithm_name> eg: create_env saliency_unlearning Activate environment: conda activate <environment_name> eg: conda activate saliency_unlearning The has to be one of the folders in the mu/algorithms folder. Downloading data and models. After you install the package, you can use the following commands to download. Dataset : i2p : Sample : download_data sample i2p Full : download_data full i2p quick_canvas : Sample : download_data sample quick_canvas Full : download_data full quick_canvas Model : compvis : download_model compvis diffuser : download_model diffuser Verify the Downloaded Files After downloading, verify that the datasets have been correctly extracted: ls -lh ./data/i2p-dataset/sample/ ls -lh ./data/quick-canvas-dataset/sample/ Usage To train the saliency unlearning algorithm to unlearn a specific concept or style from the Stable Diffusion model, use the train.py script located in the scripts directory. Step 1: Generate mask python -m mu.algorithms.saliency_unlearning.scripts.generate_mask \\ --config_path mu/algorithms/saliency_unlearning/configs/mask_config.yaml Running the Script in Offline Mode WANDB_MODE=offline python -m mu.algorithms.saliency_unlearning.scripts.generate_mask \\ --config_path mu/algorithms/saliency_unlearning/configs/mask_config.yaml Step 2: Unlearn the weights Add the generated mask path to the train_config.yaml file or you can override it by passing them directly as arguments during runtime. Run the script: python -m mu.algorithms.saliency_unlearning.scripts.train \\ --config_path mu/algorithms/saliency_unlearning/configs/train_config.yaml Running the Script in Offline Mode WANDB_MODE=offline python -m mu.algorithms.saliency_unlearning.scripts.train \\ --config_path mu/algorithms/saliency_unlearning/configs/train_config.yaml Passing Arguments via the Command Line The train.py script allows you to override configuration parameters specified in the train_config.yaml file by passing them directly as arguments during runtime. This can be useful for quick experimentation without modifying the configuration file. Example Usage with Command-Line Arguments python -m mu.algorithms.saliency_unlearning.scripts.train \\ --config_path mu/algorithms/saliency_unlearning/configs/train_config.yaml \\ --mask_path /path/to/mask.pt \\ --alpha 0.1 \\ --epochs 10 \\ --raw_dataset_dir /path/to/raw_dataset \\ --output_dir outputs/experiment_1 Explanation of the Example --config_path: Specifies the YAML configuration file to load default values. --mask_path: Path of the generated mask. --alpha: Sets the guidance strength for the starting image to 0.2. --epochs: Increases the number of training epochs to 10. --lr: Updates the learning rate to 1e-4. --raw_dataset_dir: Changes the raw dataset directory. --output_dir: Sets a custom output directory for this run. Similarly, you can pass arguments during runtime to generate mask. How It Works * Default Values: The script first loads default values from the YAML file specified by --config_path. Command-Line Overrides: Any arguments passed on the command line will override the corresponding keys in the YAML configuration file. Final Configuration: The script merges the YAML file and command-line arguments into a single configuration dictionary and uses it for training. Directory Structure algorithm.py : Implementation of the SaliencyUnlearnAlgorithm class. configs/ : Contains configuration files for training and generation. model.py : Implementation of the SaliencyUnlearnModel class. scripts/train.py : Script to train the SaliencyUnlearn algorithm. trainer.py : Implementation of the SaliencyUnlearnTrainer class. utils.py : Utility functions used in the project. data_handler.py : Implementation of DataHandler class The unlearning has two stages: Generate the mask Unlearn the weights. Description of Arguments in mask_config.yaml The config/mask_config.yaml file is a configuration file for generating saliency masks using the scripts/generate_mask.py script. It defines various parameters related to the model, dataset, output, and training. Below is a detailed description of each section and parameter: Model Configuration These parameters specify settings for the Stable Diffusion model and guidance configurations. c_guidance: Guidance scale used during loss computation in the model. Higher values may emphasize certain features in mask generation. Type: float Example: 7.5 batch_size: Number of images processed in a single batch. Type: int Example: 4 ckpt_path: Path to the model checkpoint file for Stable Diffusion. Type: str Example: /path/to/compvis.ckpt model_config_path: Path to the model configuration YAML file for Stable Diffusion. Type: str Example: /path/to/model_config.yaml num_timesteps: Number of timesteps used in the diffusion process. Type: int Example: 1000 image_size: Size of the input images used for training and mask generation (in pixels). Type: int Example: 512 Dataset Configuration These parameters define the dataset paths and settings for mask generation. raw_dataset_dir: Path to the directory containing the original dataset, organized by themes and classes. Type: str Example: /path/to/raw/dataset processed_dataset_dir: Path to the directory where processed datasets will be saved after mask generation. Type: str Example: /path/to/processed/dataset dataset_type: Type of dataset being used. Choices: unlearncanvas, i2p Type: str Example: i2p template: Type of template for mask generation. Choices: object, style, i2p Type: str Example: style template_name: Specific template name for the mask generation process. Example Choices: self-harm, Abstractionism Type: str Example: Abstractionism threshold: Threshold value for mask generation to filter salient regions. Type: float Example: 0.5 Output Configuration These parameters specify the directory where the results are saved. output_dir: Directory where the generated masks will be saved. Type: str Example: outputs/saliency_unlearning/masks Training Configuration These parameters control the training process for mask generation. lr: Learning rate used for training the masking algorithm. Type: float Example: 0.00001 devices: CUDA devices used for training, specified as a comma-separated list. Type: str Example: 0 use_sample: Flag indicating whether to use a sample dataset for training and mask generation. Type: bool Example: True Description of Arguments train_config.yaml The scripts/train.py script is used to fine-tune the Stable Diffusion model to perform saliency-based unlearning. This script relies on a configuration file ( config/train_config.yaml ) and supports additional runtime arguments for further customization. Below is a detailed description of each argument: General Arguments alpha: Guidance scale used to balance the loss components during training. Type: float Example: 0.1 epochs: Number of epochs to train the model. Type: int Example: 5 train_method: Specifies the training method or strategy to be used. Choices: noxattn, selfattn, xattn, full, notime, xlayer, selflayer Type: str Example: noxattn model_config_path: Path to the model configuration YAML file for Stable Diffusion. Type: str Example: 'mu/algorithms/saliency_unlearning/configs/model_config.yaml' Dataset Arguments raw_dataset_dir: Path to the directory containing the raw dataset, organized by themes and classes. Type: str Example: 'path/raw_dataset/' processed_dataset_dir: Path to the directory where the processed dataset will be saved. Type: str Example: 'path/processed_dataset_dir' dataset_type: Specifies the type of dataset to use for training. Choices: unlearncanvas, i2p Type: str Example: i2p template: Specifies the template type for training. Choices: object, style, i2p Type: str Example: style template_name: Name of the specific template used for training. Example Choices: self-harm, Abstractionism Type: str Example: Abstractionism Output Arguments output_dir: Directory where the fine-tuned model and training outputs will be saved. Type: str Example: 'output/folder_name' mask_path: Path to the saliency mask file used during training. Type: str Example: Saliency Unlearning Evaluation Framework This section provides instructions for running the evaluation framework for the Saliency Unlearning algorithm on Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying machine unlearning. Running the Evaluation Framework You can run the evaluation framework using the evaluate.py script located in the mu/algorithms/saliency_unlearning/scripts/ directory. Basic Command to Run Evaluation: conda activate <env_name> python -m mu.algorithms.saliency_unlearning.scripts.evaluate \\ --config_path mu/algorithms/saliency_unlearning/configs/evaluation_config.yaml Running in Offline Mode: WANDB_MODE=offline python -m mu.algorithms.saliency_unlearning.scripts.evaluate \\ --config_path mu/algorithms/saliency_unlearning/configs/evaluation_config.yaml Example with CLI Overrides: python -m mu.algorithms.saliency_unlearning.scripts.evaluate \\ --config_path mu/algorithms/saliency_unlearning/configs/evaluation_config.yaml \\ --devices \"0\" \\ --seed 123 \\ --cfg_text 8.5 \\ --batch_size 16 Description of parameters in evaluation_config.yaml The evaluation_config.yaml file contains the necessary parameters for running the Saliency Unlearning evaluation framework. Below is a detailed description of each parameter along with examples. Model Configuration: model_config : Path to the YAML file specifying the model architecture and settings. Type: str Example: \"mu/algorithms/saliency_unlearning/configs/model_config.yaml\" ckpt_path : Path to the finetuned Stable Diffusion checkpoint file to be evaluated. Type: str Example: \"outputs/saliency_unlearning/finetuned_models/saliency_unlearning_Abstractionism_model.pth\" classification_model : Specifies the classification model used for evaluating the generated outputs. Type: str Example: \"vit_large_patch16_224\" model_ckpt_path: Path to pretrained Stable Diffusion model. Type : str Example : models/compvis/style50/compvis.ckpt Training and Sampling Parameters: theme : Specifies the theme or concept being evaluated for removal from the model's outputs. Type: str Example: \"Bricks\" devices : CUDA device IDs to be used for the evaluation process. Type: str Example: \"0\" cfg_text : Classifier-free guidance scale value for image generation. Higher values increase the strength of the conditioning prompt. Type: float Example: 9.0 seed : Random seed for reproducibility of results. Type: int Example: 188 ddim_steps : Number of steps for the DDIM (Denoising Diffusion Implicit Models) sampling process. Type: int Example: 100 ddim_eta : DDIM eta value for controlling the amount of randomness during sampling. Set to 0 for deterministic sampling. Type: float Example: 0.0 image_height : Height of the generated images in pixels. Type: int Example: 512 image_width : Width of the generated images in pixels. Type: int Example: 512 Output and Logging Parameters: sampler_output_dir : Directory where generated images will be saved during evaluation. Type: str Example: \"outputs/eval_results/mu_results/saliency_unlearning/\" eval_output_dir : Directory where evaluation metrics and results will be stored. Type: str Example: \"outputs/eval_results/mu_results/saliency_unlearning/\" reference_dir : Directory containing original images for comparison during evaluation. Type: str Example: \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\" Performance and Efficiency Parameters: multiprocessing : Enables multiprocessing for faster evaluation for FID score. Recommended for large datasets. Type: bool Example: False batch_size : Batch size used during FID computation and evaluation. Type: int Example: 16 Optimization Parameters: forget_theme : Concept or style intended for removal in the evaluation process. Type: str Example: \"Bricks\" seed_list : List of random seeds for performing multiple evaluations with different randomness levels. Type: list Example: [\"188\"]","title":"Saliency Unlearning"},{"location":"algorithms/saliency/#saliency-unlearning-algorithm-for-machine-unlearning","text":"This repository provides an implementation of the Saliency Unlearning algorithm for machine unlearning in Stable Diffusion models. The Saliency Unlearning algorithm allows you to remove specific concepts or styles from a pre-trained model without retraining it from scratch.","title":"Saliency Unlearning Algorithm for Machine Unlearning"},{"location":"algorithms/saliency/#installation","text":"pip install unlearn_diff","title":"Installation"},{"location":"algorithms/saliency/#prerequisities","text":"Ensure conda is installed on your system. You can install Miniconda or Anaconda: Miniconda (recommended): https://docs.conda.io/en/latest/miniconda.html Anaconda : https://www.anaconda.com/products/distribution After installing conda , ensure it is available in your PATH by running. You may require to restart the terminal session: conda --version","title":"Prerequisities"},{"location":"algorithms/saliency/#create-environment","text":"create_env <algorithm_name> eg: create_env saliency_unlearning","title":"Create environment:"},{"location":"algorithms/saliency/#activate-environment","text":"conda activate <environment_name> eg: conda activate saliency_unlearning The has to be one of the folders in the mu/algorithms folder.","title":"Activate environment:"},{"location":"algorithms/saliency/#downloading-data-and-models","text":"After you install the package, you can use the following commands to download. Dataset : i2p : Sample : download_data sample i2p Full : download_data full i2p quick_canvas : Sample : download_data sample quick_canvas Full : download_data full quick_canvas Model : compvis : download_model compvis diffuser : download_model diffuser Verify the Downloaded Files After downloading, verify that the datasets have been correctly extracted: ls -lh ./data/i2p-dataset/sample/ ls -lh ./data/quick-canvas-dataset/sample/","title":"Downloading data and models."},{"location":"algorithms/saliency/#usage","text":"To train the saliency unlearning algorithm to unlearn a specific concept or style from the Stable Diffusion model, use the train.py script located in the scripts directory. Step 1: Generate mask python -m mu.algorithms.saliency_unlearning.scripts.generate_mask \\ --config_path mu/algorithms/saliency_unlearning/configs/mask_config.yaml Running the Script in Offline Mode WANDB_MODE=offline python -m mu.algorithms.saliency_unlearning.scripts.generate_mask \\ --config_path mu/algorithms/saliency_unlearning/configs/mask_config.yaml Step 2: Unlearn the weights Add the generated mask path to the train_config.yaml file or you can override it by passing them directly as arguments during runtime. Run the script: python -m mu.algorithms.saliency_unlearning.scripts.train \\ --config_path mu/algorithms/saliency_unlearning/configs/train_config.yaml Running the Script in Offline Mode WANDB_MODE=offline python -m mu.algorithms.saliency_unlearning.scripts.train \\ --config_path mu/algorithms/saliency_unlearning/configs/train_config.yaml Passing Arguments via the Command Line The train.py script allows you to override configuration parameters specified in the train_config.yaml file by passing them directly as arguments during runtime. This can be useful for quick experimentation without modifying the configuration file. Example Usage with Command-Line Arguments python -m mu.algorithms.saliency_unlearning.scripts.train \\ --config_path mu/algorithms/saliency_unlearning/configs/train_config.yaml \\ --mask_path /path/to/mask.pt \\ --alpha 0.1 \\ --epochs 10 \\ --raw_dataset_dir /path/to/raw_dataset \\ --output_dir outputs/experiment_1 Explanation of the Example --config_path: Specifies the YAML configuration file to load default values. --mask_path: Path of the generated mask. --alpha: Sets the guidance strength for the starting image to 0.2. --epochs: Increases the number of training epochs to 10. --lr: Updates the learning rate to 1e-4. --raw_dataset_dir: Changes the raw dataset directory. --output_dir: Sets a custom output directory for this run. Similarly, you can pass arguments during runtime to generate mask. How It Works * Default Values: The script first loads default values from the YAML file specified by --config_path. Command-Line Overrides: Any arguments passed on the command line will override the corresponding keys in the YAML configuration file. Final Configuration: The script merges the YAML file and command-line arguments into a single configuration dictionary and uses it for training.","title":"Usage"},{"location":"algorithms/saliency/#directory-structure","text":"algorithm.py : Implementation of the SaliencyUnlearnAlgorithm class. configs/ : Contains configuration files for training and generation. model.py : Implementation of the SaliencyUnlearnModel class. scripts/train.py : Script to train the SaliencyUnlearn algorithm. trainer.py : Implementation of the SaliencyUnlearnTrainer class. utils.py : Utility functions used in the project. data_handler.py : Implementation of DataHandler class The unlearning has two stages: Generate the mask Unlearn the weights.","title":"Directory Structure"},{"location":"algorithms/saliency/#description-of-arguments-in-mask_configyaml","text":"The config/mask_config.yaml file is a configuration file for generating saliency masks using the scripts/generate_mask.py script. It defines various parameters related to the model, dataset, output, and training. Below is a detailed description of each section and parameter: Model Configuration These parameters specify settings for the Stable Diffusion model and guidance configurations. c_guidance: Guidance scale used during loss computation in the model. Higher values may emphasize certain features in mask generation. Type: float Example: 7.5 batch_size: Number of images processed in a single batch. Type: int Example: 4 ckpt_path: Path to the model checkpoint file for Stable Diffusion. Type: str Example: /path/to/compvis.ckpt model_config_path: Path to the model configuration YAML file for Stable Diffusion. Type: str Example: /path/to/model_config.yaml num_timesteps: Number of timesteps used in the diffusion process. Type: int Example: 1000 image_size: Size of the input images used for training and mask generation (in pixels). Type: int Example: 512 Dataset Configuration These parameters define the dataset paths and settings for mask generation. raw_dataset_dir: Path to the directory containing the original dataset, organized by themes and classes. Type: str Example: /path/to/raw/dataset processed_dataset_dir: Path to the directory where processed datasets will be saved after mask generation. Type: str Example: /path/to/processed/dataset dataset_type: Type of dataset being used. Choices: unlearncanvas, i2p Type: str Example: i2p template: Type of template for mask generation. Choices: object, style, i2p Type: str Example: style template_name: Specific template name for the mask generation process. Example Choices: self-harm, Abstractionism Type: str Example: Abstractionism threshold: Threshold value for mask generation to filter salient regions. Type: float Example: 0.5 Output Configuration These parameters specify the directory where the results are saved. output_dir: Directory where the generated masks will be saved. Type: str Example: outputs/saliency_unlearning/masks Training Configuration These parameters control the training process for mask generation. lr: Learning rate used for training the masking algorithm. Type: float Example: 0.00001 devices: CUDA devices used for training, specified as a comma-separated list. Type: str Example: 0 use_sample: Flag indicating whether to use a sample dataset for training and mask generation. Type: bool Example: True","title":"Description of Arguments in mask_config.yaml"},{"location":"algorithms/saliency/#description-of-arguments-train_configyaml","text":"The scripts/train.py script is used to fine-tune the Stable Diffusion model to perform saliency-based unlearning. This script relies on a configuration file ( config/train_config.yaml ) and supports additional runtime arguments for further customization. Below is a detailed description of each argument: General Arguments alpha: Guidance scale used to balance the loss components during training. Type: float Example: 0.1 epochs: Number of epochs to train the model. Type: int Example: 5 train_method: Specifies the training method or strategy to be used. Choices: noxattn, selfattn, xattn, full, notime, xlayer, selflayer Type: str Example: noxattn model_config_path: Path to the model configuration YAML file for Stable Diffusion. Type: str Example: 'mu/algorithms/saliency_unlearning/configs/model_config.yaml' Dataset Arguments raw_dataset_dir: Path to the directory containing the raw dataset, organized by themes and classes. Type: str Example: 'path/raw_dataset/' processed_dataset_dir: Path to the directory where the processed dataset will be saved. Type: str Example: 'path/processed_dataset_dir' dataset_type: Specifies the type of dataset to use for training. Choices: unlearncanvas, i2p Type: str Example: i2p template: Specifies the template type for training. Choices: object, style, i2p Type: str Example: style template_name: Name of the specific template used for training. Example Choices: self-harm, Abstractionism Type: str Example: Abstractionism Output Arguments output_dir: Directory where the fine-tuned model and training outputs will be saved. Type: str Example: 'output/folder_name' mask_path: Path to the saliency mask file used during training. Type: str Example:","title":"Description of Arguments train_config.yaml"},{"location":"algorithms/saliency/#saliency-unlearning-evaluation-framework","text":"This section provides instructions for running the evaluation framework for the Saliency Unlearning algorithm on Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying machine unlearning.","title":"Saliency Unlearning Evaluation Framework"},{"location":"algorithms/saliency/#running-the-evaluation-framework","text":"You can run the evaluation framework using the evaluate.py script located in the mu/algorithms/saliency_unlearning/scripts/ directory.","title":"Running the Evaluation Framework"},{"location":"algorithms/saliency/#basic-command-to-run-evaluation","text":"conda activate <env_name> python -m mu.algorithms.saliency_unlearning.scripts.evaluate \\ --config_path mu/algorithms/saliency_unlearning/configs/evaluation_config.yaml Running in Offline Mode: WANDB_MODE=offline python -m mu.algorithms.saliency_unlearning.scripts.evaluate \\ --config_path mu/algorithms/saliency_unlearning/configs/evaluation_config.yaml Example with CLI Overrides: python -m mu.algorithms.saliency_unlearning.scripts.evaluate \\ --config_path mu/algorithms/saliency_unlearning/configs/evaluation_config.yaml \\ --devices \"0\" \\ --seed 123 \\ --cfg_text 8.5 \\ --batch_size 16","title":"Basic Command to Run Evaluation:"},{"location":"algorithms/saliency/#description-of-parameters-in-evaluation_configyaml","text":"The evaluation_config.yaml file contains the necessary parameters for running the Saliency Unlearning evaluation framework. Below is a detailed description of each parameter along with examples.","title":"Description of parameters in evaluation_config.yaml"},{"location":"algorithms/saliency/#model-configuration","text":"model_config : Path to the YAML file specifying the model architecture and settings. Type: str Example: \"mu/algorithms/saliency_unlearning/configs/model_config.yaml\" ckpt_path : Path to the finetuned Stable Diffusion checkpoint file to be evaluated. Type: str Example: \"outputs/saliency_unlearning/finetuned_models/saliency_unlearning_Abstractionism_model.pth\" classification_model : Specifies the classification model used for evaluating the generated outputs. Type: str Example: \"vit_large_patch16_224\" model_ckpt_path: Path to pretrained Stable Diffusion model. Type : str Example : models/compvis/style50/compvis.ckpt","title":"Model Configuration:"},{"location":"algorithms/saliency/#training-and-sampling-parameters","text":"theme : Specifies the theme or concept being evaluated for removal from the model's outputs. Type: str Example: \"Bricks\" devices : CUDA device IDs to be used for the evaluation process. Type: str Example: \"0\" cfg_text : Classifier-free guidance scale value for image generation. Higher values increase the strength of the conditioning prompt. Type: float Example: 9.0 seed : Random seed for reproducibility of results. Type: int Example: 188 ddim_steps : Number of steps for the DDIM (Denoising Diffusion Implicit Models) sampling process. Type: int Example: 100 ddim_eta : DDIM eta value for controlling the amount of randomness during sampling. Set to 0 for deterministic sampling. Type: float Example: 0.0 image_height : Height of the generated images in pixels. Type: int Example: 512 image_width : Width of the generated images in pixels. Type: int Example: 512","title":"Training and Sampling Parameters:"},{"location":"algorithms/saliency/#output-and-logging-parameters","text":"sampler_output_dir : Directory where generated images will be saved during evaluation. Type: str Example: \"outputs/eval_results/mu_results/saliency_unlearning/\" eval_output_dir : Directory where evaluation metrics and results will be stored. Type: str Example: \"outputs/eval_results/mu_results/saliency_unlearning/\" reference_dir : Directory containing original images for comparison during evaluation. Type: str Example: \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\"","title":"Output and Logging Parameters:"},{"location":"algorithms/saliency/#performance-and-efficiency-parameters","text":"multiprocessing : Enables multiprocessing for faster evaluation for FID score. Recommended for large datasets. Type: bool Example: False batch_size : Batch size used during FID computation and evaluation. Type: int Example: 16","title":"Performance and Efficiency Parameters:"},{"location":"algorithms/saliency/#optimization-parameters","text":"forget_theme : Concept or style intended for removal in the evaluation process. Type: str Example: \"Bricks\" seed_list : List of random seeds for performing multiple evaluations with different randomness levels. Type: list Example: [\"188\"]","title":"Optimization Parameters:"},{"location":"algorithms/scissorhands/","text":"ScissorHands Algorithm for Machine Unlearning This repository provides an implementation of the scissor hands algorithm for machine unlearning in Stable Diffusion models. The scissor hands algorithm allows you to remove specific concepts or styles from a pre-trained model without retraining it from scratch. Installation pip install unlearn_diff Prerequisities Ensure conda is installed on your system. You can install Miniconda or Anaconda: Miniconda (recommended): https://docs.conda.io/en/latest/miniconda.html Anaconda : https://www.anaconda.com/products/distribution After installing conda , ensure it is available in your PATH by running. You may require to restart the terminal session: conda --version Create environment: create_env <algorithm_name> eg: create_env scissorhands Activate environment: conda activate <environment_name> eg: conda activate scissorhands The has to be one of the folders in the mu/algorithms folder. Downloading data and models. After you install the package, you can use the following commands to download. Dataset : i2p : Sample : download_data sample i2p Full : download_data full i2p quick_canvas : Sample : download_data sample quick_canvas Full : download_data full quick_canvas Model : compvis : download_model compvis diffuser : download_model diffuser Verify the Downloaded Files After downloading, verify that the datasets have been correctly extracted: ls -lh ./data/i2p-dataset/sample/ ls -lh ./data/quick-canvas-dataset/sample/ Usage To train the ScissorHands algorithm to unlearn a specific concept or style from the Stable Diffusion model, use the train.py script located in the scripts directory. Example Command python -m mu.algorithms.scissorhands.scripts.train \\ --config_path mu/algorithms/scissorhands/configs/train_config.yaml Running the Training Script in Offline Mode WANDB_MODE=offline python -m mu.algorithms.scissorhands.scripts.train \\ --config_path mu/algorithms/scissorhands/configs/train_config.yaml Passing Arguments via the Command Line The train.py script allows you to override configuration parameters specified in the train_config.yaml file by passing them directly as arguments during runtime. This can be useful for quick experimentation without modifying the configuration file. Example Usage with Command-Line Arguments python -m mu.algorithms.scissorhands.scripts.train \\ --config_path mu/algorithms/scissorhands/configs/train_config.yaml \\ --train_method xattn \\ --alpha 0.2 \\ --devices 0,1 \\ --raw_dataset_dir /path/to/raw_dataset \\ --output_dir outputs/experiment_1 Explanation of the Example --config_path: Specifies the YAML configuration file to load default values. --train_method: Overrides the training method (\"xattn\"). --alpha: Sets the guidance strength for the starting image to 0.2. --devices: Specifies the GPUs (e.g., device 0 and 1) for training. --raw_dataset_dir: Changes the raw dataset directory. --output_dir: Sets a custom output directory for this run. How It Works * Default Values: The script first loads default values from the YAML file specified by --config_path. Command-Line Overrides: Any arguments passed on the command line will override the corresponding keys in the YAML configuration file. Final Configuration: The script merges the YAML file and command-line arguments into a single configuration dictionary and uses it for training. Directory Structure algorithm.py : Implementation of the ScissorHandsAlgorithm class. configs/ : Contains configuration files for training and generation. model.py : Implementation of the ScissorHandsModel class. scripts/train.py : Script to train the ScissorHands algorithm. trainer.py : Implementation of the ScissorHandsTrainer class. utils.py : Utility functions used in the project. data_handler.py : Implementation of DataHandler class Description of Arguments in train_config.yaml Training Parameters train_method: Specifies the method of training for concept erasure. Choices: [\"noxattn\", \"selfattn\", \"xattn\", \"full\", \"notime\", \"xlayer\", \"selflayer\"] Example: \"xattn\" alpha: Guidance strength for the starting image during training. Type: float Example: 0.1 epochs: Number of epochs to train the model. Type: int Example: 1 Model Configuration model_config_path: File path to the Stable Diffusion model configuration YAML file. type: str Example: \"/path/to/model_config.yaml\" ckpt_path: File path to the checkpoint of the Stable Diffusion model. Type: str Example: \"/path/to/model_checkpoint.ckpt\" Dataset Directories raw_dataset_dir: Directory containing the raw dataset categorized by themes or classes. Type: str Example: \"/path/to/raw_dataset\" processed_dataset_dir: Directory to save the processed dataset. Type: str Example: \"/path/to/processed_dataset\" dataset_type: Specifies the dataset type for the training process. Choices: [\"unlearncanvas\", \"i2p\"] Example: \"unlearncanvas\" template: Type of template to use during training. Choices: [\"object\", \"style\", \"i2p\"] Example: \"style\" template_name: Name of the specific concept or style to be erased. Choices: [\"self-harm\", \"Abstractionism\"] Example: \"Abstractionism\" Output Configurations output_dir: Directory where the fine-tuned models and results will be saved. Type: str Example: \"outputs/erase_diff/finetuned_models\" Sampling and Image Configurations sparsity: Threshold for mask. Type: float Example: 0.99 project: Type: bool Example: false memory_num: Type: Int Example: 1 prune_num: Type: Int Example: 1 Device Configuration devices: Specifies the CUDA devices to be used for training (comma-separated). Type: str (Comma separated) Example: \"0, 1\" Additional Flags use_sample: Flag to indicate whether a sample dataset should be used for training. Type: bool Example: True Scissorshands Evaluation Framework This section provides instructions for running the evaluation framework for the Scissorshands algorithm on Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying machine unlearning. Running the Evaluation Framework You can run the evaluation framework using the evaluate.py script located in the mu/algorithms/scissorshands/scripts/ directory. Basic Command to Run Evaluation: conda activate <env_name> python -m mu.algorithms.scissorhands.scripts.evaluate \\ --config_path mu/algorithms/scissorshands/configs/evaluation_config.yaml Running in Offline Mode: WANDB_MODE=offline python -m mu.algorithms.scissorhands.scripts.evaluate \\ --config_path mu/algorithms/scissorshands/configs/evaluation_config.yaml Example with CLI Overrides: python -m mu.algorithms.scissorshands.scripts.evaluate \\ --config_path mu/algorithms/scissorshands/configs/evaluation_config.yaml \\ --devices \"0\" \\ --seed 123 \\ --cfg_text 8.5 \\ --batch_size 16 Description of parameters in evaluation_config.yaml The evaluation_config.yaml file contains the necessary parameters for running the Scissorshands evaluation framework. Below is a detailed description of each parameter along with examples. Model Configuration: model_config : Path to the YAML file specifying the model architecture and settings. Type: str Example: \"mu/algorithms/scissorshands/configs/model_config.yaml\" ckpt_path : Path to the finetuned Stable Diffusion checkpoint file to be evaluated. Type: str Example: \"outputs/scissorshands/finetuned_models/scissorshands_Abstractionism_model.pth\" classification_model : Specifies the classification model used for evaluating the generated outputs. Type: str Example: \"vit_large_patch16_224\" Training and Sampling Parameters: theme : Specifies the theme or concept being evaluated for removal from the model's outputs. Type: str Example: \"Bricks\" devices : CUDA device IDs to be used for the evaluation process. Type: str Example: \"0\" cfg_text : Classifier-free guidance scale value for image generation. Higher values increase the strength of the conditioning prompt. Type: float Example: 9.0 seed : Random seed for reproducibility of results. Type: int Example: 188 ddim_steps : Number of steps for the DDIM (Denoising Diffusion Implicit Models) sampling process. Type: int Example: 100 ddim_eta : DDIM eta value for controlling the amount of randomness during sampling. Set to 0 for deterministic sampling. Type: float Example: 0.0 image_height : Height of the generated images in pixels. Type: int Example: 512 image_width : Width of the generated images in pixels. Type: int Example: 512 Output and Logging Parameters: sampler_output_dir : Directory where generated images will be saved during evaluation. Type: str Example: \"outputs/eval_results/mu_results/scissorshands/\" eval_output_dir : Directory where evaluation metrics and results will be stored. Type: str Example: \"outputs/eval_results/mu_results/scissorshands/\" reference_dir : Directory containing original images for comparison during evaluation. Type: str Example: \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\" Performance and Efficiency Parameters: multiprocessing : Enables multiprocessing for faster evaluation for FID score. Recommended for large datasets. Type: bool Example: False batch_size : Batch size used during FID computation and evaluation. Type: int Example: 16 Optimization Parameters: forget_theme : Concept or style intended for removal in the evaluation process. Type: str Example: \"Bricks\" seed_list : List of random seeds for performing multiple evaluations with different randomness levels. Type: list Example: [\"188\"]","title":"Scissor Hands"},{"location":"algorithms/scissorhands/#scissorhands-algorithm-for-machine-unlearning","text":"This repository provides an implementation of the scissor hands algorithm for machine unlearning in Stable Diffusion models. The scissor hands algorithm allows you to remove specific concepts or styles from a pre-trained model without retraining it from scratch.","title":"ScissorHands Algorithm for Machine Unlearning"},{"location":"algorithms/scissorhands/#installation","text":"pip install unlearn_diff","title":"Installation"},{"location":"algorithms/scissorhands/#prerequisities","text":"Ensure conda is installed on your system. You can install Miniconda or Anaconda: Miniconda (recommended): https://docs.conda.io/en/latest/miniconda.html Anaconda : https://www.anaconda.com/products/distribution After installing conda , ensure it is available in your PATH by running. You may require to restart the terminal session: conda --version","title":"Prerequisities"},{"location":"algorithms/scissorhands/#create-environment","text":"create_env <algorithm_name> eg: create_env scissorhands","title":"Create environment:"},{"location":"algorithms/scissorhands/#activate-environment","text":"conda activate <environment_name> eg: conda activate scissorhands The has to be one of the folders in the mu/algorithms folder.","title":"Activate environment:"},{"location":"algorithms/scissorhands/#downloading-data-and-models","text":"After you install the package, you can use the following commands to download. Dataset : i2p : Sample : download_data sample i2p Full : download_data full i2p quick_canvas : Sample : download_data sample quick_canvas Full : download_data full quick_canvas Model : compvis : download_model compvis diffuser : download_model diffuser Verify the Downloaded Files After downloading, verify that the datasets have been correctly extracted: ls -lh ./data/i2p-dataset/sample/ ls -lh ./data/quick-canvas-dataset/sample/","title":"Downloading data and models."},{"location":"algorithms/scissorhands/#usage","text":"To train the ScissorHands algorithm to unlearn a specific concept or style from the Stable Diffusion model, use the train.py script located in the scripts directory. Example Command python -m mu.algorithms.scissorhands.scripts.train \\ --config_path mu/algorithms/scissorhands/configs/train_config.yaml Running the Training Script in Offline Mode WANDB_MODE=offline python -m mu.algorithms.scissorhands.scripts.train \\ --config_path mu/algorithms/scissorhands/configs/train_config.yaml Passing Arguments via the Command Line The train.py script allows you to override configuration parameters specified in the train_config.yaml file by passing them directly as arguments during runtime. This can be useful for quick experimentation without modifying the configuration file. Example Usage with Command-Line Arguments python -m mu.algorithms.scissorhands.scripts.train \\ --config_path mu/algorithms/scissorhands/configs/train_config.yaml \\ --train_method xattn \\ --alpha 0.2 \\ --devices 0,1 \\ --raw_dataset_dir /path/to/raw_dataset \\ --output_dir outputs/experiment_1 Explanation of the Example --config_path: Specifies the YAML configuration file to load default values. --train_method: Overrides the training method (\"xattn\"). --alpha: Sets the guidance strength for the starting image to 0.2. --devices: Specifies the GPUs (e.g., device 0 and 1) for training. --raw_dataset_dir: Changes the raw dataset directory. --output_dir: Sets a custom output directory for this run. How It Works * Default Values: The script first loads default values from the YAML file specified by --config_path. Command-Line Overrides: Any arguments passed on the command line will override the corresponding keys in the YAML configuration file. Final Configuration: The script merges the YAML file and command-line arguments into a single configuration dictionary and uses it for training.","title":"Usage"},{"location":"algorithms/scissorhands/#directory-structure","text":"algorithm.py : Implementation of the ScissorHandsAlgorithm class. configs/ : Contains configuration files for training and generation. model.py : Implementation of the ScissorHandsModel class. scripts/train.py : Script to train the ScissorHands algorithm. trainer.py : Implementation of the ScissorHandsTrainer class. utils.py : Utility functions used in the project. data_handler.py : Implementation of DataHandler class","title":"Directory Structure"},{"location":"algorithms/scissorhands/#description-of-arguments-in-train_configyaml","text":"Training Parameters train_method: Specifies the method of training for concept erasure. Choices: [\"noxattn\", \"selfattn\", \"xattn\", \"full\", \"notime\", \"xlayer\", \"selflayer\"] Example: \"xattn\" alpha: Guidance strength for the starting image during training. Type: float Example: 0.1 epochs: Number of epochs to train the model. Type: int Example: 1 Model Configuration model_config_path: File path to the Stable Diffusion model configuration YAML file. type: str Example: \"/path/to/model_config.yaml\" ckpt_path: File path to the checkpoint of the Stable Diffusion model. Type: str Example: \"/path/to/model_checkpoint.ckpt\" Dataset Directories raw_dataset_dir: Directory containing the raw dataset categorized by themes or classes. Type: str Example: \"/path/to/raw_dataset\" processed_dataset_dir: Directory to save the processed dataset. Type: str Example: \"/path/to/processed_dataset\" dataset_type: Specifies the dataset type for the training process. Choices: [\"unlearncanvas\", \"i2p\"] Example: \"unlearncanvas\" template: Type of template to use during training. Choices: [\"object\", \"style\", \"i2p\"] Example: \"style\" template_name: Name of the specific concept or style to be erased. Choices: [\"self-harm\", \"Abstractionism\"] Example: \"Abstractionism\" Output Configurations output_dir: Directory where the fine-tuned models and results will be saved. Type: str Example: \"outputs/erase_diff/finetuned_models\" Sampling and Image Configurations sparsity: Threshold for mask. Type: float Example: 0.99 project: Type: bool Example: false memory_num: Type: Int Example: 1 prune_num: Type: Int Example: 1 Device Configuration devices: Specifies the CUDA devices to be used for training (comma-separated). Type: str (Comma separated) Example: \"0, 1\" Additional Flags use_sample: Flag to indicate whether a sample dataset should be used for training. Type: bool Example: True","title":"Description of Arguments in train_config.yaml"},{"location":"algorithms/scissorhands/#scissorshands-evaluation-framework","text":"This section provides instructions for running the evaluation framework for the Scissorshands algorithm on Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying machine unlearning.","title":"Scissorshands Evaluation Framework"},{"location":"algorithms/scissorhands/#running-the-evaluation-framework","text":"You can run the evaluation framework using the evaluate.py script located in the mu/algorithms/scissorshands/scripts/ directory.","title":"Running the Evaluation Framework"},{"location":"algorithms/scissorhands/#basic-command-to-run-evaluation","text":"conda activate <env_name> python -m mu.algorithms.scissorhands.scripts.evaluate \\ --config_path mu/algorithms/scissorshands/configs/evaluation_config.yaml Running in Offline Mode: WANDB_MODE=offline python -m mu.algorithms.scissorhands.scripts.evaluate \\ --config_path mu/algorithms/scissorshands/configs/evaluation_config.yaml Example with CLI Overrides: python -m mu.algorithms.scissorshands.scripts.evaluate \\ --config_path mu/algorithms/scissorshands/configs/evaluation_config.yaml \\ --devices \"0\" \\ --seed 123 \\ --cfg_text 8.5 \\ --batch_size 16","title":"Basic Command to Run Evaluation:"},{"location":"algorithms/scissorhands/#description-of-parameters-in-evaluation_configyaml","text":"The evaluation_config.yaml file contains the necessary parameters for running the Scissorshands evaluation framework. Below is a detailed description of each parameter along with examples.","title":"Description of parameters in evaluation_config.yaml"},{"location":"algorithms/scissorhands/#model-configuration","text":"model_config : Path to the YAML file specifying the model architecture and settings. Type: str Example: \"mu/algorithms/scissorshands/configs/model_config.yaml\" ckpt_path : Path to the finetuned Stable Diffusion checkpoint file to be evaluated. Type: str Example: \"outputs/scissorshands/finetuned_models/scissorshands_Abstractionism_model.pth\" classification_model : Specifies the classification model used for evaluating the generated outputs. Type: str Example: \"vit_large_patch16_224\"","title":"Model Configuration:"},{"location":"algorithms/scissorhands/#training-and-sampling-parameters","text":"theme : Specifies the theme or concept being evaluated for removal from the model's outputs. Type: str Example: \"Bricks\" devices : CUDA device IDs to be used for the evaluation process. Type: str Example: \"0\" cfg_text : Classifier-free guidance scale value for image generation. Higher values increase the strength of the conditioning prompt. Type: float Example: 9.0 seed : Random seed for reproducibility of results. Type: int Example: 188 ddim_steps : Number of steps for the DDIM (Denoising Diffusion Implicit Models) sampling process. Type: int Example: 100 ddim_eta : DDIM eta value for controlling the amount of randomness during sampling. Set to 0 for deterministic sampling. Type: float Example: 0.0 image_height : Height of the generated images in pixels. Type: int Example: 512 image_width : Width of the generated images in pixels. Type: int Example: 512","title":"Training and Sampling Parameters:"},{"location":"algorithms/scissorhands/#output-and-logging-parameters","text":"sampler_output_dir : Directory where generated images will be saved during evaluation. Type: str Example: \"outputs/eval_results/mu_results/scissorshands/\" eval_output_dir : Directory where evaluation metrics and results will be stored. Type: str Example: \"outputs/eval_results/mu_results/scissorshands/\" reference_dir : Directory containing original images for comparison during evaluation. Type: str Example: \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\"","title":"Output and Logging Parameters:"},{"location":"algorithms/scissorhands/#performance-and-efficiency-parameters","text":"multiprocessing : Enables multiprocessing for faster evaluation for FID score. Recommended for large datasets. Type: bool Example: False batch_size : Batch size used during FID computation and evaluation. Type: int Example: 16","title":"Performance and Efficiency Parameters:"},{"location":"algorithms/scissorhands/#optimization-parameters","text":"forget_theme : Concept or style intended for removal in the evaluation process. Type: str Example: \"Bricks\" seed_list : List of random seeds for performing multiple evaluations with different randomness levels. Type: list Example: [\"188\"]","title":"Optimization Parameters:"},{"location":"algorithms/selective_amnesia/","text":"Selective Amnesia Algorithm for Machine Unlearning This repository provides an implementation of the Selective Amnesia algorithm for machine unlearning in Stable Diffusion models. The Selective Amnesia algorithm focuses on removing specific concepts or styles from a pre-trained model while retaining the rest of the knowledge. Installation pip install unlearn_diff Prerequisities Ensure conda is installed on your system. You can install Miniconda or Anaconda: Miniconda (recommended): https://docs.conda.io/en/latest/miniconda.html Anaconda : https://www.anaconda.com/products/distribution After installing conda , ensure it is available in your PATH by running. You may require to restart the terminal session: conda --version Create environment: create_env <algorithm_name> eg: create_env selective_amnesia Activate environment: conda activate <environment_name> eg: conda activate selective_amnesia The has to be one of the folders in the mu/algorithms folder. Downloading data and models. After you install the package, you can use the following commands to download. Dataset : i2p : Sample : download_data sample i2p Full : download_data full i2p quick_canvas : Sample : download_data sample quick_canvas Full : download_data full quick_canvas Model : compvis : download_model compvis diffuser : download_model diffuser Verify the Downloaded Files After downloading, verify that the datasets have been correctly extracted: ls -lh ./data/i2p-dataset/sample/ ls -lh ./data/quick-canvas-dataset/sample/ Usage To train the Selective Amnesia algorithm to remove specific concepts or styles from the Stable Diffusion model, use the train.py script located in the scripts directory. Example Command First download the full_fisher_dict.pkl file. wget https://huggingface.co/ajrheng/selective-amnesia/resolve/main/full_fisher_dict.pkl Run the script python -m mu.algorithms.selective_amnesia.scripts.train --config_path mu/algorithms/selective_amnesia/configs/train_config.yaml --full_fisher_dict_pkl_path /path/full_fisher_dict.pkl Running the Training Script in Offline Mode WANDB_MODE=offline python -m mu.algorithms.selective_amnesia.scripts.train --config_path mu/algorithms/selective_amnesia/configs/train_config.yaml --full_fisher_dict_pkl_path /path/full_fisher_dict.pkl Overriding Configuration via Command Line You can override configuration parameters by passing them directly as arguments during runtime. Example Usage with Command-Line Arguments: python -m mu.algorithms.selective_amnesia.scripts.train \\ --config_path mu/algorithms/selective_amnesia/configs/train_config.yaml \\ --train_batch_size 8 \\ --max_epochs 100 \\ --devices 0,1 \\ --output_dir outputs/experiment_2 Explanation: * --config_path : Specifies the YAML configuration file. * --train_batch_size : Overrides the training batch size to 8. * --max_epochs : Updates the maximum number of training epochs to 100. * --devices : Specifies the GPUs (e.g., device 0 and 1). * --output_dir : Sets a custom output directory for the experiment. Directory Structure algorithm.py : Core implementation of the Selective Amnesia Algorithm. configs/ : Configuration files for training and generation. data_handler.py : Data handling and preprocessing. scripts/train.py : Script to train the Selective Amnesia Algorithm. callbacks/ : Custom callbacks for logging and monitoring training. utils.py : Utility functions. How It Works Default Configuration: Loads values from the specified YAML file ( --config_path ). Command-Line Overrides: Updates the configuration with values provided as command-line arguments. Training Execution: Initializes the SelectiveAmnesiaAlgorithm and trains the model using the provided dataset, model checkpoint, and configuration. Output: Saves the fine-tuned model and logs training metrics in the specified output directory. Notes Ensure all dependencies are installed as per the environment file. The training process generates logs in the logs/ directory for easy monitoring. Use appropriate CUDA devices for optimal performance during training. Regularly verify dataset and model configurations to avoid errors during execution. Configuration File ( train_config.yaml ) Training Parameters seed: Random seed for reproducibility. Type: int Example: 23 scale_lr: Whether to scale the base learning rate. Type: bool Example: True Model Configuration model_config_path: Path to the Stable Diffusion model configuration YAML file. Type: str Example: \"/path/to/model_config.yaml\" ckpt_path: Path to the Stable Diffusion model checkpoint. Type: str Example: \"/path/to/compvis.ckpt\" full_fisher_dict_pkl_path: Path to the full fisher dict pkl file Type: str Example: \"full_fisher_dict.pkl\" Dataset Directories raw_dataset_dir: Directory containing the raw dataset categorized by themes or classes. Type: str Example: \"/path/to/raw_dataset\" processed_dataset_dir: Directory to save the processed dataset. Type: str Example: \"/path/to/processed_dataset\" dataset_type: Specifies the dataset type for training. Choices: [\"unlearncanvas\", \"i2p\"] Example: \"unlearncanvas\" template: Type of template to use during training. Choices: [\"object\", \"style\", \"i2p\"] Example: \"style\" template_name: Name of the concept or style to erase. Choices: [\"self-harm\", \"Abstractionism\"] Example: \"Abstractionism\" Output Configurations output_dir: Directory to save fine-tuned models and results. Type: str Example: \"outputs/selective_amnesia/finetuned_models\" Device Configuration devices: CUDA devices for training (comma-separated). Type: str Example: \"0\" Data Parameters train_batch_size: Batch size for training. Type: int Example: 4 val_batch_size: Batch size for validation. Type: int Example: 6 num_workers: Number of worker threads for data loading. Type: int Example: 4 forget_prompt: Prompt to specify the style or concept to forget. Type: str Example: \"An image in Artist_Sketch style\" Lightning Configuration max_epochs: Maximum number of epochs for training. Type: int Example: 50 callbacks: batch_frequency: Frequency for logging image batches. Type: int Example: 1 max_images: Maximum number of images to log. Type: int Example: 999","title":"Selective Amnesia"},{"location":"algorithms/selective_amnesia/#selective-amnesia-algorithm-for-machine-unlearning","text":"This repository provides an implementation of the Selective Amnesia algorithm for machine unlearning in Stable Diffusion models. The Selective Amnesia algorithm focuses on removing specific concepts or styles from a pre-trained model while retaining the rest of the knowledge.","title":"Selective Amnesia Algorithm for Machine Unlearning"},{"location":"algorithms/selective_amnesia/#installation","text":"pip install unlearn_diff","title":"Installation"},{"location":"algorithms/selective_amnesia/#prerequisities","text":"Ensure conda is installed on your system. You can install Miniconda or Anaconda: Miniconda (recommended): https://docs.conda.io/en/latest/miniconda.html Anaconda : https://www.anaconda.com/products/distribution After installing conda , ensure it is available in your PATH by running. You may require to restart the terminal session: conda --version","title":"Prerequisities"},{"location":"algorithms/selective_amnesia/#create-environment","text":"create_env <algorithm_name> eg: create_env selective_amnesia","title":"Create environment:"},{"location":"algorithms/selective_amnesia/#activate-environment","text":"conda activate <environment_name> eg: conda activate selective_amnesia The has to be one of the folders in the mu/algorithms folder.","title":"Activate environment:"},{"location":"algorithms/selective_amnesia/#downloading-data-and-models","text":"After you install the package, you can use the following commands to download. Dataset : i2p : Sample : download_data sample i2p Full : download_data full i2p quick_canvas : Sample : download_data sample quick_canvas Full : download_data full quick_canvas Model : compvis : download_model compvis diffuser : download_model diffuser Verify the Downloaded Files After downloading, verify that the datasets have been correctly extracted: ls -lh ./data/i2p-dataset/sample/ ls -lh ./data/quick-canvas-dataset/sample/","title":"Downloading data and models."},{"location":"algorithms/selective_amnesia/#usage","text":"To train the Selective Amnesia algorithm to remove specific concepts or styles from the Stable Diffusion model, use the train.py script located in the scripts directory.","title":"Usage"},{"location":"algorithms/selective_amnesia/#example-command","text":"First download the full_fisher_dict.pkl file. wget https://huggingface.co/ajrheng/selective-amnesia/resolve/main/full_fisher_dict.pkl Run the script python -m mu.algorithms.selective_amnesia.scripts.train --config_path mu/algorithms/selective_amnesia/configs/train_config.yaml --full_fisher_dict_pkl_path /path/full_fisher_dict.pkl","title":"Example Command"},{"location":"algorithms/selective_amnesia/#running-the-training-script-in-offline-mode","text":"WANDB_MODE=offline python -m mu.algorithms.selective_amnesia.scripts.train --config_path mu/algorithms/selective_amnesia/configs/train_config.yaml --full_fisher_dict_pkl_path /path/full_fisher_dict.pkl","title":"Running the Training Script in Offline Mode"},{"location":"algorithms/selective_amnesia/#overriding-configuration-via-command-line","text":"You can override configuration parameters by passing them directly as arguments during runtime. Example Usage with Command-Line Arguments: python -m mu.algorithms.selective_amnesia.scripts.train \\ --config_path mu/algorithms/selective_amnesia/configs/train_config.yaml \\ --train_batch_size 8 \\ --max_epochs 100 \\ --devices 0,1 \\ --output_dir outputs/experiment_2 Explanation: * --config_path : Specifies the YAML configuration file. * --train_batch_size : Overrides the training batch size to 8. * --max_epochs : Updates the maximum number of training epochs to 100. * --devices : Specifies the GPUs (e.g., device 0 and 1). * --output_dir : Sets a custom output directory for the experiment.","title":"Overriding Configuration via Command Line"},{"location":"algorithms/selective_amnesia/#directory-structure","text":"algorithm.py : Core implementation of the Selective Amnesia Algorithm. configs/ : Configuration files for training and generation. data_handler.py : Data handling and preprocessing. scripts/train.py : Script to train the Selective Amnesia Algorithm. callbacks/ : Custom callbacks for logging and monitoring training. utils.py : Utility functions.","title":"Directory Structure"},{"location":"algorithms/selective_amnesia/#how-it-works","text":"Default Configuration: Loads values from the specified YAML file ( --config_path ). Command-Line Overrides: Updates the configuration with values provided as command-line arguments. Training Execution: Initializes the SelectiveAmnesiaAlgorithm and trains the model using the provided dataset, model checkpoint, and configuration. Output: Saves the fine-tuned model and logs training metrics in the specified output directory.","title":"How It Works"},{"location":"algorithms/selective_amnesia/#notes","text":"Ensure all dependencies are installed as per the environment file. The training process generates logs in the logs/ directory for easy monitoring. Use appropriate CUDA devices for optimal performance during training. Regularly verify dataset and model configurations to avoid errors during execution.","title":"Notes"},{"location":"algorithms/selective_amnesia/#configuration-file-train_configyaml","text":"","title":"Configuration File (train_config.yaml)"},{"location":"algorithms/selective_amnesia/#training-parameters","text":"seed: Random seed for reproducibility. Type: int Example: 23 scale_lr: Whether to scale the base learning rate. Type: bool Example: True","title":"Training Parameters"},{"location":"algorithms/selective_amnesia/#model-configuration","text":"model_config_path: Path to the Stable Diffusion model configuration YAML file. Type: str Example: \"/path/to/model_config.yaml\" ckpt_path: Path to the Stable Diffusion model checkpoint. Type: str Example: \"/path/to/compvis.ckpt\" full_fisher_dict_pkl_path: Path to the full fisher dict pkl file Type: str Example: \"full_fisher_dict.pkl\"","title":"Model Configuration"},{"location":"algorithms/selective_amnesia/#dataset-directories","text":"raw_dataset_dir: Directory containing the raw dataset categorized by themes or classes. Type: str Example: \"/path/to/raw_dataset\" processed_dataset_dir: Directory to save the processed dataset. Type: str Example: \"/path/to/processed_dataset\" dataset_type: Specifies the dataset type for training. Choices: [\"unlearncanvas\", \"i2p\"] Example: \"unlearncanvas\" template: Type of template to use during training. Choices: [\"object\", \"style\", \"i2p\"] Example: \"style\" template_name: Name of the concept or style to erase. Choices: [\"self-harm\", \"Abstractionism\"] Example: \"Abstractionism\"","title":"Dataset Directories"},{"location":"algorithms/selective_amnesia/#output-configurations","text":"output_dir: Directory to save fine-tuned models and results. Type: str Example: \"outputs/selective_amnesia/finetuned_models\"","title":"Output Configurations"},{"location":"algorithms/selective_amnesia/#device-configuration","text":"devices: CUDA devices for training (comma-separated). Type: str Example: \"0\"","title":"Device Configuration"},{"location":"algorithms/selective_amnesia/#data-parameters","text":"train_batch_size: Batch size for training. Type: int Example: 4 val_batch_size: Batch size for validation. Type: int Example: 6 num_workers: Number of worker threads for data loading. Type: int Example: 4 forget_prompt: Prompt to specify the style or concept to forget. Type: str Example: \"An image in Artist_Sketch style\"","title":"Data Parameters"},{"location":"algorithms/selective_amnesia/#lightning-configuration","text":"max_epochs: Maximum number of epochs for training. Type: int Example: 50 callbacks: batch_frequency: Frequency for logging image batches. Type: int Example: 1 max_images: Maximum number of images to log. Type: int Example: 999","title":"Lightning Configuration"},{"location":"algorithms/semi_permeable/","text":"Semi Permeable Membrane Algorithm for Machine Unlearning This repository provides an implementation of the semipermeable membrane algorithm for machine unlearning in Stable Diffusion models. The semipermeable membrane algorithm allows you to remove specific concepts or styles from a pre-trained model without retraining it from scratch. Installation pip install unlearn_diff Prerequisities Ensure conda is installed on your system. You can install Miniconda or Anaconda: Miniconda (recommended): https://docs.conda.io/en/latest/miniconda.html Anaconda : https://www.anaconda.com/products/distribution After installing conda , ensure it is available in your PATH by running. You may require to restart the terminal session: conda --version Create environment: create_env <algorithm_name> eg: create_env semipermeable_membrane Activate environment: conda activate <environment_name> eg: conda activate semipermeable_membrane The has to be one of the folders in the mu/algorithms folder. Downloading data and models. After you install the package, you can use the following commands to download. Dataset : i2p : Sample : download_data sample i2p Full : download_data full i2p quick_canvas : Sample : download_data sample quick_canvas Full : download_data full quick_canvas Model : compvis : download_model compvis diffuser : download_model diffuser Verify the Downloaded Files After downloading, verify that the datasets have been correctly extracted: ls -lh ./data/i2p-dataset/sample/ ls -lh ./data/quick-canvas-dataset/sample/ Usage To train the Semi Permeable Membrane algorithm to unlearn a specific concept or style from the Stable Diffusion model, use the train.py script located in the scripts directory. Example Command python -m mu.algorithms.semipermeable_membrane.scripts.train \\ --config_path mu/algorithms/semipermeable_membrane/config/train_config.yaml Running the Script in Offline Mode WANDB_MODE=offline python -m mu.algorithms.semipermeable_membrane.scripts.train \\ --config_path mu/algorithms/semipermeable_membrane/config/train_config.yaml Passing Arguments via the Command Line The train.py script allows you to override configuration parameters specified in the train_config.yaml file by passing them directly as arguments during runtime. This can be useful for quick experimentation without modifying the configuration file. Example Usage with Command-Line Arguments python -m mu.algorithms.semipermeable_membrane.scripts.train \\ --config_path mu/algorithms/semipermeable_membrane/config/train_config.yaml \\ --dataset_type unlearncanvas \\ --template object \\ --template_name self-harm \\ --devices 0,1 \\ --output_dir outputs/experiment_1 \\ --use_sample Explanation of the Example --config_path: Specifies the YAML configuration file to load default values. --dataset_type: Defines the dataset type (e.g., unlearncanvas or i2p). --template: Specifies the template to use (e.g., object, style, or i2p). --template_name: The specific name for the template being used (e.g., self-harm, Abstractionism). --devices: Comma-separated list of CUDA devices to use for training (e.g., 0,1 for using GPU 0 and GPU 1). --output_dir: Sets a custom output directory for the results of this run. --use_sample: Specifies whether to use a sample dataset for training. How It Works * Default Values: The script first loads default values from the YAML file specified by --config_path. Command-Line Overrides: Any arguments passed on the command line will override the corresponding keys in the YAML configuration file. Final Configuration: The script merges the YAML file and command-line arguments into a single configuration dictionary and uses it for training. Directory Structure algorithm.py : Implementation of the Semi Permeable MembraneAlgorithm class. configs/ : Contains configuration files for training and generation. model.py : Implementation of the Semi Permeable MembraneModel class. scripts/train.py : Script to train the Semi Permeable Membrane algorithm. trainer.py : Implementation of the Semi Permeable MembraneTrainer class. utils.py : Utility functions used in the project. data_handler.py : Implementation of DataHandler class Description of Arguments in train_config.yaml pretrained_model ckpt_path: File path to the pretrained model's checkpoint file. v2: Boolean indicating whether the pretrained model is version 2 or not. v_pred: Boolean to enable/disable \"v-prediction\" mode for diffusion models. clip_skip: Number of CLIP layers to skip during inference. network rank: Rank of the low-rank adaptation network. alpha: Scaling factor for the network during training. train precision: Numerical precision to use during training (e.g., float32 or float16). noise_scheduler: Type of noise scheduler to use in the training loop (e.g., ddim). iterations: Number of training iterations. batch_size: Batch size for training. lr: Learning rate for the training optimizer. unet_lr: Learning rate for the U-Net model. text_encoder_lr: Learning rate for the text encoder. optimizer_type: Optimizer to use for training (e.g., AdamW8bit). lr_scheduler: Learning rate scheduler to apply during training. lr_warmup_steps: Number of steps for linear warmup of the learning rate. lr_scheduler_num_cycles: Number of cycles for a cosine-with-restarts scheduler. max_denoising_steps: Maximum denoising steps to use during training. save per_steps: Frequency of saving the model (in steps). precision: Numerical precision for saved model weights other use_xformers: Boolean to enable xformers memory-efficient attention. wandb_project and wandb_run Configuration for tracking the training progress using Weights & Biases. wandb_project: Project name in W&B. wandb_run: Specific run name in the W&B dashboard. use_sample Boolean to indicate whether to use the sample dataset for training. dataset_type Type of dataset to use, options are unlearncanvas or i2p. template * Specifies the template type, choices are: * object: Focus on specific objects. * style: Focus on artistic styles. * i2p: Intermediate style processing. template_name Name of the template, choices are: self-harm Abstractionism prompt target: Target template or concept to guide training (references template_name). positive: Positive prompt based on the template. unconditional: Unconditional prompt text. neutral: Neutral prompt text. action: Specifies the action applied to the prompt (e.g., erase_with_la). guidance_scale: Guidance scale for classifier-free guidance. resolution: Image resolution for training. batch_size: Batch size for generating prompts. dynamic_resolution: Boolean to allow dynamic resolution. la_strength: Strength of local adaptation. sampling_batch_size: Batch size for sampling images. devices CUDA devices to use for training (specified as a comma-separated list, e.g., \"0,1\"). output_dir Directory to save the fine-tuned model and other outputs. verbose Boolean flag for verbose logging during training. Semipermeable membrane Evaluation Framework This section provides instructions for running the evaluation framework for the Semipermeable membrane algorithm on Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying machine unlearning. Running the Evaluation Framework You can run the evaluation framework using the evaluate.py script located in the mu/algorithms/semipermeable_membrane/scripts/ directory. Basic Command to Run Evaluation: conda activate <env_name> python -m mu.algorithms.semipermeable_membrane.scripts.evaluate \\ --config_path mu/algorithms/semipermeable_membrane/configs/evaluation_config.yaml Running in Offline Mode: WANDB_MODE=offline python -m mu.algorithms.semipermeable_membrane.scripts.evaluate \\ --config_path mu/algorithms/semipermeable_membrane/configs/evaluation_config.yaml Example with CLI Overrides: python -m mu.algorithms.semipermeable_membrane.scripts.evaluate \\ --config_path mu/algorithms/semipermeable_membrane/configs/evaluation_config.yaml \\ --devices \"0\" \\ --seed 123 \\ --cfg_text 8.5 \\ --batch_size 16 Description of parameters in evaluation_config.yaml The evaluation_config.yaml file contains the necessary parameters for running the Semipermeable membrane evaluation framework. Below is a detailed description of each parameter along with examples. Model Configuration Parameters: spm_path: paths to finetuned model checkpoint. Type: list Example: outputs/semipermeable_membrane/finetuned_models/semipermeable_membrane_Abstractionism_last.safetensors base_model : Path to the pre-trained base model used for image generation. Type: str Example: \"path/to/base/model.pth\" precision : Specifies the numerical precision for model computation. Type: str Options: \"fp32\" , \"fp16\" , \"bf16\" Example: \"fp32\" spm_multiplier: Specifies the multiplier for Semipermeable Membrane (SPM) model. Type: float Example: 1.0 v2 : Specifies whether to use version 2.x of the model. Type: bool Example: false matching_metric : Metric used for evaluating the similarity between generated prompts and erased concepts. Type: str Options: \"clipcos\" , \"clipcos_tokenuni\" , \"tokenuni\" Example: \"clipcos_tokenuni\" model_config : Path to the model configuration YAML file. Type: str Example: \"mu/algorithms/semipermeable_membrane/config\" model_ckpt_path: Path to pretrained Stable Diffusion model. Type : str Example : models/diffuser/style50 Sampling Parameters: theme : Specifies the theme for the evaluation process. Type: str Example: \"Bricks\" seed : Random seed for reproducibility of the evaluation process. Type: int Example: 188 devices : Specifies the CUDA devices for running the model. Type: str (Comma-separated for multiple devices) Example: \"0\" task : Specifies the task type for the evaluation process. Type: str Options: \"class\" , \"style\" Example: \"class\" Output Parameters: sampler_output_dir : Directory where generated images will be saved during the sampling process. Type: str Example: \"outputs/eval_results/mu_results/semipermeable_membrane/\" eval_output_dir : Directory where evaluation metrics and results will be stored. Type: str Example: \"outputs/eval_results/mu_results/semipermeable_membrane/\" Dataset and Classification Parameters: reference_dir : Path to the reference dataset used for evaluation and comparison. Type: str Example: \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\" classification_model : Specifies the classification model used for the evaluation. Type: str Example: \"vit_large_patch16_224\" forget_theme : Specifies the theme to be forgotten during the unlearning process. Type: str Example: \"Bricks\" Performance Parameters: multiprocessing : Enables multiprocessing for faster evaluation. Type: bool Example: false seed_list : List of random seeds for multiple evaluation trials. Type: list Example: [\"188\"]","title":"Semi Permeable Membrane"},{"location":"algorithms/semi_permeable/#semi-permeable-membrane-algorithm-for-machine-unlearning","text":"This repository provides an implementation of the semipermeable membrane algorithm for machine unlearning in Stable Diffusion models. The semipermeable membrane algorithm allows you to remove specific concepts or styles from a pre-trained model without retraining it from scratch.","title":"Semi Permeable Membrane Algorithm for Machine Unlearning"},{"location":"algorithms/semi_permeable/#installation","text":"pip install unlearn_diff","title":"Installation"},{"location":"algorithms/semi_permeable/#prerequisities","text":"Ensure conda is installed on your system. You can install Miniconda or Anaconda: Miniconda (recommended): https://docs.conda.io/en/latest/miniconda.html Anaconda : https://www.anaconda.com/products/distribution After installing conda , ensure it is available in your PATH by running. You may require to restart the terminal session: conda --version","title":"Prerequisities"},{"location":"algorithms/semi_permeable/#create-environment","text":"create_env <algorithm_name> eg: create_env semipermeable_membrane","title":"Create environment:"},{"location":"algorithms/semi_permeable/#activate-environment","text":"conda activate <environment_name> eg: conda activate semipermeable_membrane The has to be one of the folders in the mu/algorithms folder.","title":"Activate environment:"},{"location":"algorithms/semi_permeable/#downloading-data-and-models","text":"After you install the package, you can use the following commands to download. Dataset : i2p : Sample : download_data sample i2p Full : download_data full i2p quick_canvas : Sample : download_data sample quick_canvas Full : download_data full quick_canvas Model : compvis : download_model compvis diffuser : download_model diffuser Verify the Downloaded Files After downloading, verify that the datasets have been correctly extracted: ls -lh ./data/i2p-dataset/sample/ ls -lh ./data/quick-canvas-dataset/sample/","title":"Downloading data and models."},{"location":"algorithms/semi_permeable/#usage","text":"To train the Semi Permeable Membrane algorithm to unlearn a specific concept or style from the Stable Diffusion model, use the train.py script located in the scripts directory.","title":"Usage"},{"location":"algorithms/semi_permeable/#example-command","text":"python -m mu.algorithms.semipermeable_membrane.scripts.train \\ --config_path mu/algorithms/semipermeable_membrane/config/train_config.yaml Running the Script in Offline Mode WANDB_MODE=offline python -m mu.algorithms.semipermeable_membrane.scripts.train \\ --config_path mu/algorithms/semipermeable_membrane/config/train_config.yaml Passing Arguments via the Command Line The train.py script allows you to override configuration parameters specified in the train_config.yaml file by passing them directly as arguments during runtime. This can be useful for quick experimentation without modifying the configuration file. Example Usage with Command-Line Arguments python -m mu.algorithms.semipermeable_membrane.scripts.train \\ --config_path mu/algorithms/semipermeable_membrane/config/train_config.yaml \\ --dataset_type unlearncanvas \\ --template object \\ --template_name self-harm \\ --devices 0,1 \\ --output_dir outputs/experiment_1 \\ --use_sample Explanation of the Example --config_path: Specifies the YAML configuration file to load default values. --dataset_type: Defines the dataset type (e.g., unlearncanvas or i2p). --template: Specifies the template to use (e.g., object, style, or i2p). --template_name: The specific name for the template being used (e.g., self-harm, Abstractionism). --devices: Comma-separated list of CUDA devices to use for training (e.g., 0,1 for using GPU 0 and GPU 1). --output_dir: Sets a custom output directory for the results of this run. --use_sample: Specifies whether to use a sample dataset for training. How It Works * Default Values: The script first loads default values from the YAML file specified by --config_path. Command-Line Overrides: Any arguments passed on the command line will override the corresponding keys in the YAML configuration file. Final Configuration: The script merges the YAML file and command-line arguments into a single configuration dictionary and uses it for training.","title":"Example Command"},{"location":"algorithms/semi_permeable/#directory-structure","text":"algorithm.py : Implementation of the Semi Permeable MembraneAlgorithm class. configs/ : Contains configuration files for training and generation. model.py : Implementation of the Semi Permeable MembraneModel class. scripts/train.py : Script to train the Semi Permeable Membrane algorithm. trainer.py : Implementation of the Semi Permeable MembraneTrainer class. utils.py : Utility functions used in the project. data_handler.py : Implementation of DataHandler class","title":"Directory Structure"},{"location":"algorithms/semi_permeable/#description-of-arguments-in-train_configyaml","text":"pretrained_model ckpt_path: File path to the pretrained model's checkpoint file. v2: Boolean indicating whether the pretrained model is version 2 or not. v_pred: Boolean to enable/disable \"v-prediction\" mode for diffusion models. clip_skip: Number of CLIP layers to skip during inference. network rank: Rank of the low-rank adaptation network. alpha: Scaling factor for the network during training. train precision: Numerical precision to use during training (e.g., float32 or float16). noise_scheduler: Type of noise scheduler to use in the training loop (e.g., ddim). iterations: Number of training iterations. batch_size: Batch size for training. lr: Learning rate for the training optimizer. unet_lr: Learning rate for the U-Net model. text_encoder_lr: Learning rate for the text encoder. optimizer_type: Optimizer to use for training (e.g., AdamW8bit). lr_scheduler: Learning rate scheduler to apply during training. lr_warmup_steps: Number of steps for linear warmup of the learning rate. lr_scheduler_num_cycles: Number of cycles for a cosine-with-restarts scheduler. max_denoising_steps: Maximum denoising steps to use during training. save per_steps: Frequency of saving the model (in steps). precision: Numerical precision for saved model weights other use_xformers: Boolean to enable xformers memory-efficient attention. wandb_project and wandb_run Configuration for tracking the training progress using Weights & Biases. wandb_project: Project name in W&B. wandb_run: Specific run name in the W&B dashboard. use_sample Boolean to indicate whether to use the sample dataset for training. dataset_type Type of dataset to use, options are unlearncanvas or i2p. template * Specifies the template type, choices are: * object: Focus on specific objects. * style: Focus on artistic styles. * i2p: Intermediate style processing. template_name Name of the template, choices are: self-harm Abstractionism prompt target: Target template or concept to guide training (references template_name). positive: Positive prompt based on the template. unconditional: Unconditional prompt text. neutral: Neutral prompt text. action: Specifies the action applied to the prompt (e.g., erase_with_la). guidance_scale: Guidance scale for classifier-free guidance. resolution: Image resolution for training. batch_size: Batch size for generating prompts. dynamic_resolution: Boolean to allow dynamic resolution. la_strength: Strength of local adaptation. sampling_batch_size: Batch size for sampling images. devices CUDA devices to use for training (specified as a comma-separated list, e.g., \"0,1\"). output_dir Directory to save the fine-tuned model and other outputs. verbose Boolean flag for verbose logging during training.","title":"Description of Arguments in train_config.yaml"},{"location":"algorithms/semi_permeable/#semipermeable-membrane-evaluation-framework","text":"This section provides instructions for running the evaluation framework for the Semipermeable membrane algorithm on Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying machine unlearning.","title":"Semipermeable membrane Evaluation Framework"},{"location":"algorithms/semi_permeable/#running-the-evaluation-framework","text":"You can run the evaluation framework using the evaluate.py script located in the mu/algorithms/semipermeable_membrane/scripts/ directory.","title":"Running the Evaluation Framework"},{"location":"algorithms/semi_permeable/#basic-command-to-run-evaluation","text":"conda activate <env_name> python -m mu.algorithms.semipermeable_membrane.scripts.evaluate \\ --config_path mu/algorithms/semipermeable_membrane/configs/evaluation_config.yaml Running in Offline Mode: WANDB_MODE=offline python -m mu.algorithms.semipermeable_membrane.scripts.evaluate \\ --config_path mu/algorithms/semipermeable_membrane/configs/evaluation_config.yaml Example with CLI Overrides: python -m mu.algorithms.semipermeable_membrane.scripts.evaluate \\ --config_path mu/algorithms/semipermeable_membrane/configs/evaluation_config.yaml \\ --devices \"0\" \\ --seed 123 \\ --cfg_text 8.5 \\ --batch_size 16","title":"Basic Command to Run Evaluation:"},{"location":"algorithms/semi_permeable/#description-of-parameters-in-evaluation_configyaml","text":"The evaluation_config.yaml file contains the necessary parameters for running the Semipermeable membrane evaluation framework. Below is a detailed description of each parameter along with examples.","title":"Description of parameters in evaluation_config.yaml"},{"location":"algorithms/semi_permeable/#model-configuration-parameters","text":"spm_path: paths to finetuned model checkpoint. Type: list Example: outputs/semipermeable_membrane/finetuned_models/semipermeable_membrane_Abstractionism_last.safetensors base_model : Path to the pre-trained base model used for image generation. Type: str Example: \"path/to/base/model.pth\" precision : Specifies the numerical precision for model computation. Type: str Options: \"fp32\" , \"fp16\" , \"bf16\" Example: \"fp32\" spm_multiplier: Specifies the multiplier for Semipermeable Membrane (SPM) model. Type: float Example: 1.0 v2 : Specifies whether to use version 2.x of the model. Type: bool Example: false matching_metric : Metric used for evaluating the similarity between generated prompts and erased concepts. Type: str Options: \"clipcos\" , \"clipcos_tokenuni\" , \"tokenuni\" Example: \"clipcos_tokenuni\" model_config : Path to the model configuration YAML file. Type: str Example: \"mu/algorithms/semipermeable_membrane/config\" model_ckpt_path: Path to pretrained Stable Diffusion model. Type : str Example : models/diffuser/style50","title":"Model Configuration Parameters:"},{"location":"algorithms/semi_permeable/#sampling-parameters","text":"theme : Specifies the theme for the evaluation process. Type: str Example: \"Bricks\" seed : Random seed for reproducibility of the evaluation process. Type: int Example: 188 devices : Specifies the CUDA devices for running the model. Type: str (Comma-separated for multiple devices) Example: \"0\" task : Specifies the task type for the evaluation process. Type: str Options: \"class\" , \"style\" Example: \"class\"","title":"Sampling Parameters:"},{"location":"algorithms/semi_permeable/#output-parameters","text":"sampler_output_dir : Directory where generated images will be saved during the sampling process. Type: str Example: \"outputs/eval_results/mu_results/semipermeable_membrane/\" eval_output_dir : Directory where evaluation metrics and results will be stored. Type: str Example: \"outputs/eval_results/mu_results/semipermeable_membrane/\"","title":"Output Parameters:"},{"location":"algorithms/semi_permeable/#dataset-and-classification-parameters","text":"reference_dir : Path to the reference dataset used for evaluation and comparison. Type: str Example: \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\" classification_model : Specifies the classification model used for the evaluation. Type: str Example: \"vit_large_patch16_224\" forget_theme : Specifies the theme to be forgotten during the unlearning process. Type: str Example: \"Bricks\"","title":"Dataset and Classification Parameters:"},{"location":"algorithms/semi_permeable/#performance-parameters","text":"multiprocessing : Enables multiprocessing for faster evaluation. Type: bool Example: false seed_list : List of random seeds for multiple evaluation trials. Type: list Example: [\"188\"]","title":"Performance Parameters:"},{"location":"algorithms/uce/","text":"Unified Concept Editing Algorithm for Machine Unlearning This repository provides an implementation of the unified concept editing algorithm for machine unlearning in Stable Diffusion models. The unified concept editing algorithm allows you to remove specific concepts or styles from a pre-trained model without retraining it from scratch. Installation pip install unlearn_diff Prerequisities Ensure conda is installed on your system. You can install Miniconda or Anaconda: Miniconda (recommended): https://docs.conda.io/en/latest/miniconda.html Anaconda : https://www.anaconda.com/products/distribution After installing conda , ensure it is available in your PATH by running. You may require to restart the terminal session: conda --version Create environment: create_env <algorithm_name> eg: create_env unified_concept_editing Activate environment: conda activate <environment_name> eg: conda activate unified_concept_editing The has to be one of the folders in the mu/algorithms folder. Downloading data and models. After you install the package, you can use the following commands to download. Dataset : i2p : Sample : download_data sample i2p Full : download_data full i2p quick_canvas : Sample : download_data sample quick_canvas Full : download_data full quick_canvas Model : compvis : download_model compvis diffuser : download_model diffuser Verify the Downloaded Files After downloading, verify that the datasets have been correctly extracted: ls -lh ./data/i2p-dataset/sample/ ls -lh ./data/quick-canvas-dataset/sample/ Usage To train the Unified Concept Editing algorithm to unlearn a specific concept or style from the Stable Diffusion model, use the train.py script located in the scripts directory. Example Command python -m mu.algorithms.unified_concept_editing.scripts.train \\ --config_path mu/algorithms/unified_concept_editing/configs/train_config.yaml Running the Training Script in Offline Mode WANDB_MODE=offline python -m mu.algorithms.unified_concept_editing.scripts.train \\ --config_path mu/algorithms/unified_concept_editing/configs/train_config.yaml Passing Arguments via the Command Line The train.py script allows you to override configuration parameters specified in the train_config.yaml file by passing them directly as arguments during runtime. This can be useful for quick experimentation without modifying the configuration file. Example Usage with Command-Line Arguments python -m mu.algorithms.unified_concept_editing.scripts.train \\ --config_path mu/algorithms/unified_concept_editing/configs/train_config.yaml \\ --train_method xattn \\ --alpha 0.2 \\ --devices 0,1 \\ --output_dir outputs/experiment_1 Explanation of the Example --config_path: Specifies the YAML configuration file to load default values. --train_method: Overrides the training method (\"xattn\"). --alpha: Sets the guidance strength for the starting image to 0.2. --devices: Specifies the GPUs (e.g., device 0 and 1) for training. --output_dir: Sets a custom output directory for this run. How It Works * Default Values: The script first loads default values from the YAML file specified by --config_path. Command-Line Overrides: Any arguments passed on the command line will override the corresponding keys in the YAML configuration file. Final Configuration: The script merges the YAML file and command-line arguments into a single configuration dictionary and uses it for training. Directory Structure algorithm.py : Implementation of the ScissorHandsAlgorithm class. configs/ : Contains configuration files for training and generation. model.py : Implementation of the ScissorHandsModel class. scripts/train.py : Script to train the ScissorHands algorithm. trainer.py : Implementation of the ScissorHandsTrainer class. utils.py : Utility functions used in the project. data_handler.py : Implementation of DataHandler class Description of Arguments in train_config.yaml Training Parameters train_method : Specifies the method of training for concept erasure. Choices: [\"full\", \"partial\"] Example: \"full\" alpha : Guidance strength for the starting image during training. Type: float Example: 0.1 epochs : Number of epochs to train the model. Type: int Example: 10 lr : Learning rate used for the optimizer during training. Type: float Example: 5e-5 Model Configuration * ckpt_path : File path to the checkpoint of the Stable Diffusion model. * Type: str * Example: \"/path/to/model_checkpoint.ckpt\" config_path : File path to the Stable Diffusion model configuration YAML file. Type: str Example: \"/path/to/config.yaml\" Dataset Directories dataset_type : Specifies the dataset type for the training process. Choices: [\"unlearncanvas\", \"i2p\"] Example: \"unlearncanvas\" template : Type of template to use during training. Choices: [\"object\", \"style\", \"i2p\"] Example: \"style\" template_name : Name of the specific concept or style to be erased. Choices: [\"self-harm\", \"Abstractionism\"] Example: \"Abstractionism\" Output Configurations output_dir : Directory where the fine-tuned models and results will be saved. Type: str Example: \"outputs/erase_diff/finetuned_models\" Sampling and Image Configurations use_sample : Flag to indicate whether a sample dataset should be used for training. Type: bool Example: True guided_concepts : Concepts to guide the editing process. Type: str Example: \"Nature, Abstract\" technique : Specifies the editing technique. Choices: [\"replace\", \"tensor\"] Example: \"replace\" preserve_scale : Scale for preservation during the editing process. Type: float Example: 0.5 preserve_number : Number of items to preserve during editing. Type: int Example: 10 erase_scale : Scale for erasure during the editing process. Type: float Example: 0.8 lamb : Lambda parameter for controlling balance during editing. Type: float Example: 0.01 add_prompts : Flag to indicate whether additional prompts should be used. Type: bool Example: True Device Configuration devices : Specifies the CUDA devices to be used for training (comma-separated). Type: str (Comma-separated) Example: \"0,1\" unified_concept_editing Evaluation Framework This section provides instructions for running the evaluation framework for the unified_concept_editing algorithm on Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying machine unlearning. Running the Evaluation Framework You can run the evaluation framework using the evaluate.py script located in the mu/algorithms/unified_concept_editing/scripts/ directory. Basic Command to Run Evaluation: conda activate <env_name> python -m mu.algorithms.unified_concept_editing.scripts.evaluate \\ --config_path mu/algorithms/unified_concept_editing/configs/evaluation_config.yaml Running in Offline Mode: WANDB_MODE=offline python -m mu.algorithms.unified_concept_editing.scripts.evaluate \\ --config_path mu/algorithms/unified_concept_editing/configs/evaluation_config.yaml Example with CLI Overrides: python -m mu.algorithms.unified_concept_editing.scripts.evaluate \\ --config_path mu/algorithms/unified_concept_editing/configs/evaluation_config.yaml \\ --devices \"0\" \\ --seed 123 \\ --cfg_text 8.5 \\ --batch_size 16 Description of parameters in evaluation_config.yaml The evaluation_config.yaml file contains the necessary parameters for running the unified_concept_editing evaluation framework. Below is a detailed description of each parameter along with examples. Model Configuration: pipeline_path: path to pipeline. Type : str Example : ckpts/sd_model/diffuser/style50/step19999/ ckpt_path : Path to the finetuned Stable Diffusion checkpoint file to be evaluated. Type: str Example: \"outputs/unified_concept_editing/finetuned_models/unified_concept_editing_Abstractionism_model.pth\" classification_model : Specifies the classification model used for evaluating the generated outputs. Type: str Example: \"vit_large_patch16_224\" model_ckpt_path: Path to pretrained Stable Diffusion model. Type : str Example : models/diffuser/style50 Training and Sampling Parameters: theme : Specifies the theme or concept being evaluated for removal from the model's outputs. Type: str Example: \"Bricks\" devices : CUDA device IDs to be used for the evaluation process. Type: str Example: \"0\" cfg_text : Classifier-free guidance scale value for image generation. Higher values increase the strength of the conditioning prompt. Type: float Example: 9.0 seed : Random seed for reproducibility of results. Type: int Example: 188 ddim_steps : Number of steps for the DDIM (Denoising Diffusion Implicit Models) sampling process. Type: int Example: 100 ddim_eta : DDIM eta value for controlling the amount of randomness during sampling. Set to 0 for deterministic sampling. Type: float Example: 0.0 image_height : Height of the generated images in pixels. Type: int Example: 512 image_width : Width of the generated images in pixels. Type: int Example: 512 Output and Logging Parameters: sampler_output_dir : Directory where generated images will be saved during evaluation. Type: str Example: \"outputs/eval_results/mu_results/unified_concept_editing/\" eval_output_dir : Directory where evaluation metrics and results will be stored. Type: str Example: \"outputs/eval_results/mu_results/unified_concept_editing/\" reference_dir : Directory containing original images for comparison during evaluation. Type: str Example: \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\" Performance and Efficiency Parameters: multiprocessing : Enables multiprocessing for faster evaluation for FID score. Recommended for large datasets. Type: bool Example: False batch_size : Batch size used during FID computation and evaluation. Type: int Example: 16 Optimization Parameters: forget_theme : Concept or style intended for removal in the evaluation process. Type: str Example: \"Bricks\" seed_list : List of random seeds for performing multiple evaluations with different randomness levels. Type: list Example: [\"188\"]","title":"Unified Concept Editing"},{"location":"algorithms/uce/#unified-concept-editing-algorithm-for-machine-unlearning","text":"This repository provides an implementation of the unified concept editing algorithm for machine unlearning in Stable Diffusion models. The unified concept editing algorithm allows you to remove specific concepts or styles from a pre-trained model without retraining it from scratch.","title":"Unified Concept Editing Algorithm for Machine Unlearning"},{"location":"algorithms/uce/#installation","text":"pip install unlearn_diff","title":"Installation"},{"location":"algorithms/uce/#prerequisities","text":"Ensure conda is installed on your system. You can install Miniconda or Anaconda: Miniconda (recommended): https://docs.conda.io/en/latest/miniconda.html Anaconda : https://www.anaconda.com/products/distribution After installing conda , ensure it is available in your PATH by running. You may require to restart the terminal session: conda --version","title":"Prerequisities"},{"location":"algorithms/uce/#create-environment","text":"create_env <algorithm_name> eg: create_env unified_concept_editing","title":"Create environment:"},{"location":"algorithms/uce/#activate-environment","text":"conda activate <environment_name> eg: conda activate unified_concept_editing The has to be one of the folders in the mu/algorithms folder.","title":"Activate environment:"},{"location":"algorithms/uce/#downloading-data-and-models","text":"After you install the package, you can use the following commands to download. Dataset : i2p : Sample : download_data sample i2p Full : download_data full i2p quick_canvas : Sample : download_data sample quick_canvas Full : download_data full quick_canvas Model : compvis : download_model compvis diffuser : download_model diffuser Verify the Downloaded Files After downloading, verify that the datasets have been correctly extracted: ls -lh ./data/i2p-dataset/sample/ ls -lh ./data/quick-canvas-dataset/sample/","title":"Downloading data and models."},{"location":"algorithms/uce/#usage","text":"To train the Unified Concept Editing algorithm to unlearn a specific concept or style from the Stable Diffusion model, use the train.py script located in the scripts directory. Example Command python -m mu.algorithms.unified_concept_editing.scripts.train \\ --config_path mu/algorithms/unified_concept_editing/configs/train_config.yaml Running the Training Script in Offline Mode WANDB_MODE=offline python -m mu.algorithms.unified_concept_editing.scripts.train \\ --config_path mu/algorithms/unified_concept_editing/configs/train_config.yaml Passing Arguments via the Command Line The train.py script allows you to override configuration parameters specified in the train_config.yaml file by passing them directly as arguments during runtime. This can be useful for quick experimentation without modifying the configuration file. Example Usage with Command-Line Arguments python -m mu.algorithms.unified_concept_editing.scripts.train \\ --config_path mu/algorithms/unified_concept_editing/configs/train_config.yaml \\ --train_method xattn \\ --alpha 0.2 \\ --devices 0,1 \\ --output_dir outputs/experiment_1 Explanation of the Example --config_path: Specifies the YAML configuration file to load default values. --train_method: Overrides the training method (\"xattn\"). --alpha: Sets the guidance strength for the starting image to 0.2. --devices: Specifies the GPUs (e.g., device 0 and 1) for training. --output_dir: Sets a custom output directory for this run. How It Works * Default Values: The script first loads default values from the YAML file specified by --config_path. Command-Line Overrides: Any arguments passed on the command line will override the corresponding keys in the YAML configuration file. Final Configuration: The script merges the YAML file and command-line arguments into a single configuration dictionary and uses it for training.","title":"Usage"},{"location":"algorithms/uce/#directory-structure","text":"algorithm.py : Implementation of the ScissorHandsAlgorithm class. configs/ : Contains configuration files for training and generation. model.py : Implementation of the ScissorHandsModel class. scripts/train.py : Script to train the ScissorHands algorithm. trainer.py : Implementation of the ScissorHandsTrainer class. utils.py : Utility functions used in the project. data_handler.py : Implementation of DataHandler class","title":"Directory Structure"},{"location":"algorithms/uce/#description-of-arguments-in-train_configyaml","text":"Training Parameters train_method : Specifies the method of training for concept erasure. Choices: [\"full\", \"partial\"] Example: \"full\" alpha : Guidance strength for the starting image during training. Type: float Example: 0.1 epochs : Number of epochs to train the model. Type: int Example: 10 lr : Learning rate used for the optimizer during training. Type: float Example: 5e-5 Model Configuration * ckpt_path : File path to the checkpoint of the Stable Diffusion model. * Type: str * Example: \"/path/to/model_checkpoint.ckpt\" config_path : File path to the Stable Diffusion model configuration YAML file. Type: str Example: \"/path/to/config.yaml\" Dataset Directories dataset_type : Specifies the dataset type for the training process. Choices: [\"unlearncanvas\", \"i2p\"] Example: \"unlearncanvas\" template : Type of template to use during training. Choices: [\"object\", \"style\", \"i2p\"] Example: \"style\" template_name : Name of the specific concept or style to be erased. Choices: [\"self-harm\", \"Abstractionism\"] Example: \"Abstractionism\" Output Configurations output_dir : Directory where the fine-tuned models and results will be saved. Type: str Example: \"outputs/erase_diff/finetuned_models\" Sampling and Image Configurations use_sample : Flag to indicate whether a sample dataset should be used for training. Type: bool Example: True guided_concepts : Concepts to guide the editing process. Type: str Example: \"Nature, Abstract\" technique : Specifies the editing technique. Choices: [\"replace\", \"tensor\"] Example: \"replace\" preserve_scale : Scale for preservation during the editing process. Type: float Example: 0.5 preserve_number : Number of items to preserve during editing. Type: int Example: 10 erase_scale : Scale for erasure during the editing process. Type: float Example: 0.8 lamb : Lambda parameter for controlling balance during editing. Type: float Example: 0.01 add_prompts : Flag to indicate whether additional prompts should be used. Type: bool Example: True Device Configuration devices : Specifies the CUDA devices to be used for training (comma-separated). Type: str (Comma-separated) Example: \"0,1\"","title":"Description of Arguments in train_config.yaml"},{"location":"algorithms/uce/#unified_concept_editing-evaluation-framework","text":"This section provides instructions for running the evaluation framework for the unified_concept_editing algorithm on Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying machine unlearning.","title":"unified_concept_editing Evaluation Framework"},{"location":"algorithms/uce/#running-the-evaluation-framework","text":"You can run the evaluation framework using the evaluate.py script located in the mu/algorithms/unified_concept_editing/scripts/ directory.","title":"Running the Evaluation Framework"},{"location":"algorithms/uce/#basic-command-to-run-evaluation","text":"conda activate <env_name> python -m mu.algorithms.unified_concept_editing.scripts.evaluate \\ --config_path mu/algorithms/unified_concept_editing/configs/evaluation_config.yaml Running in Offline Mode: WANDB_MODE=offline python -m mu.algorithms.unified_concept_editing.scripts.evaluate \\ --config_path mu/algorithms/unified_concept_editing/configs/evaluation_config.yaml Example with CLI Overrides: python -m mu.algorithms.unified_concept_editing.scripts.evaluate \\ --config_path mu/algorithms/unified_concept_editing/configs/evaluation_config.yaml \\ --devices \"0\" \\ --seed 123 \\ --cfg_text 8.5 \\ --batch_size 16","title":"Basic Command to Run Evaluation:"},{"location":"algorithms/uce/#description-of-parameters-in-evaluation_configyaml","text":"The evaluation_config.yaml file contains the necessary parameters for running the unified_concept_editing evaluation framework. Below is a detailed description of each parameter along with examples.","title":"Description of parameters in evaluation_config.yaml"},{"location":"algorithms/uce/#model-configuration","text":"pipeline_path: path to pipeline. Type : str Example : ckpts/sd_model/diffuser/style50/step19999/ ckpt_path : Path to the finetuned Stable Diffusion checkpoint file to be evaluated. Type: str Example: \"outputs/unified_concept_editing/finetuned_models/unified_concept_editing_Abstractionism_model.pth\" classification_model : Specifies the classification model used for evaluating the generated outputs. Type: str Example: \"vit_large_patch16_224\" model_ckpt_path: Path to pretrained Stable Diffusion model. Type : str Example : models/diffuser/style50","title":"Model Configuration:"},{"location":"algorithms/uce/#training-and-sampling-parameters","text":"theme : Specifies the theme or concept being evaluated for removal from the model's outputs. Type: str Example: \"Bricks\" devices : CUDA device IDs to be used for the evaluation process. Type: str Example: \"0\" cfg_text : Classifier-free guidance scale value for image generation. Higher values increase the strength of the conditioning prompt. Type: float Example: 9.0 seed : Random seed for reproducibility of results. Type: int Example: 188 ddim_steps : Number of steps for the DDIM (Denoising Diffusion Implicit Models) sampling process. Type: int Example: 100 ddim_eta : DDIM eta value for controlling the amount of randomness during sampling. Set to 0 for deterministic sampling. Type: float Example: 0.0 image_height : Height of the generated images in pixels. Type: int Example: 512 image_width : Width of the generated images in pixels. Type: int Example: 512","title":"Training and Sampling Parameters:"},{"location":"algorithms/uce/#output-and-logging-parameters","text":"sampler_output_dir : Directory where generated images will be saved during evaluation. Type: str Example: \"outputs/eval_results/mu_results/unified_concept_editing/\" eval_output_dir : Directory where evaluation metrics and results will be stored. Type: str Example: \"outputs/eval_results/mu_results/unified_concept_editing/\" reference_dir : Directory containing original images for comparison during evaluation. Type: str Example: \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\"","title":"Output and Logging Parameters:"},{"location":"algorithms/uce/#performance-and-efficiency-parameters","text":"multiprocessing : Enables multiprocessing for faster evaluation for FID score. Recommended for large datasets. Type: bool Example: False batch_size : Batch size used during FID computation and evaluation. Type: int Example: 16","title":"Performance and Efficiency Parameters:"},{"location":"algorithms/uce/#optimization-parameters","text":"forget_theme : Concept or style intended for removal in the evaluation process. Type: str Example: \"Bricks\" seed_list : List of random seeds for performing multiple evaluations with different randomness levels. Type: list Example: [\"188\"]","title":"Optimization Parameters:"}]}