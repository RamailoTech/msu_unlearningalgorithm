{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Unlearn Diff Unlearn Diff is an open-source Python package designed to streamline the development of unlearning algorithms and establish a standardized evaluation pipeline for diffusion models. It provides researchers and practitioners with tools to implement, evaluate, and extend unlearning algorithms effectively. Documentation You can find the full documentation for this project at the url given below. https://ramailotech.github.io/msu_unlearningalgorithm/ Features Comprehensive Algorithm Support : Includes commonly used concept erasing and machine unlearning algorithms tailored for diffusion models. Each algorithm is encapsulated and standardized in terms of input-output formats. Automated Evaluation : Supports automatic evaluation on datasets like UnlearnCanvas or IP2P. Performs standard and adversarial evaluations, outputting metrics as detailed in UnlearnCanvas and UnlearnDiffAtk. Extensibility : Designed for easy integration of new unlearning algorithms, attack methods, defense mechanisms, and datasets with minimal modifications. Supported Algorithms The initial version includes established methods benchmarked in UnlearnCanvas and defensive unlearning techniques: CA (Concept Ablation) ED (Erase Diff) ESD (Efficient Substitution Distillation) FMN (Forget Me Not) SU (Saliency Unlearning) SH (ScissorHands) SA (Selective Amnesia) SPM (Semi Permeable Membrane) UCE (Unified Concept Editing) For detailed information on each algorithm, please refer to the respective README.md files located inside mu/algorithms . Project Architecture The project is organized to facilitate scalability and maintainability. data/ : Stores data-related files. i2p-dataset/ : contains i2p-dataset sample/ : Sample dataset full/ : Full dataset quick-canvas-dataset/ : contains quick canvas dataset sample/ : Sample dataset full/ : Full dataset docs/ : Documentation, including API references and user guides. outputs/ : Outputs of the trained algorithms. examples/ : Sample code and notebooks demonstrating usage. logs/ : Log files for debugging and auditing. models/ : Repository of trained models and checkpoints. mu/ : Core source code for machine unlearning. algorithms/ : Implementation of various algorithms. Each algorithm has its own subdirectory containing code and a README.md with detailed documentation. esd/ : ESD algorithm components. README.md : Documentation specific to the ESD algorithm. algorithm.py : Core implementation of ESD. configs/ : Configuration files for training and generation tasks. constants/const.py : Constant values used across the ESD algorithm. environment.yaml : Environment setup for ESD. model.py : Model architectures specific to ESD. sampler.py : Sampling methods used during training or inference. evaluator.py : Evaluation of ESD algorithm. scripts/train.py : Training script for ESD. trainer.py : Training routines and optimization strategies. utils.py : Utility functions and helpers. ca/ : Components for the CA algorithm. README.md : Documentation specific to the CA algorithm. ...and so on for other algorithms core/ : Foundational classes and utilities. base_algorithm.py : Abstract base class for algorithm implementations. base_data_handler.py : Base class for data handling. base_model.py : Base class for model definitions. base_sampler.py : Base class for sampling methods. base_trainer.py : Base class for training routines. base_config.py : Base class for config class. datasets/ : Dataset management and utilities. __init__.py : Initializes the dataset package. dataset.py : Dataset classes and methods. helpers/ : Helper functions for data processing. unlearning_canvas_dataset.py : Specific dataset class for unlearning tasks. helpers/ : Utility functions and helpers. helper.py : General-purpose helper functions. logger.py : Logging utilities to standardize logging practices. path_setup.py : Path configurations and environment setup. mu_attack/ : Core source code for attack. attackers/ : Implementation of each of the attacks. hard_prompt.py : Implementation for hard prompt attack. no_attack.py : Implementation for no attack. random.py : Implementation for random attack. seed_search.py : Implementation for seed search attack. soft_prompt.py : Implementation for soft_prompt attack. text_grad.py : Implementation for text_grad attack. configs : Configuration files for attackers and evaluation tasks. adv_unlearn : Config class for adversial unlearning. evaluation : Config class for evaluation. illegal : Config class for illegal. nudity : Config class for nudity. object : Config class for object. style : Config class for style. violence : Config class for violence. core/ : Foundational classes and utilities. base_attacker.py : Abstract base class for attack implementations. base_config.py : Base class for config class. base_stable_diffusion.py : Base class for stable diffusion pipeline. base_task.py : Base class for tasks. datasets/ : Script to generate dataset. exces/ : Implemenation of attacks. adv_attack.py : Implementation of advattack. attack.py : Main orchestration class that implements attacks. helpers/ : Utility functions and helpers. utils.py : Contains utility functions used in mu_attack. loggers/ : Logging utilities to standardize logging practices. tasks/ : Contains tasks for attacks. utils/ : Utilities for tasks. classifier.py : Implementation of classifier task. p4d.py : Implementation of p4d task. transfer.py : Implementation of transfer task. sd_compvis.py : Base stable diffusion pipeline implementation for compvis model. sd_diffuser.py : Base stable diffusion pipeline implementation for diffuser model. environment.yaml : Environment setup for attack. mu_defense/ : Core source code for defense i.e adversial unlearning. algorithms/ : Algorithms for defense. adv_unlearn/ : Implementation of defense algorithm (adv_unlearn). configs/ : Configuration files for AdvUnlearn, evaluation and image generation tasks. adv_unlearn_config.py : Configuration class for AdvUnlearn. evaluation_config.py : Configuration class for evaluation for AdvUnlearn. example_img_generator_config.py : Configuration class for image generation for AdvUnlearn evaluation. algorithm.py : Core implementation of AdvUnlearn. trainer.py : Core training routines and optimization strategies. compvis_trainer.py : Compvis specific training routines and optimization strategies. diffuser_trainer.py : Diffuser specific training routines and optimization strategies. generate_example_image.py : Implementation of image generation for evaluation for both compvis and diffuser. evaluator.py : Implementation of evaluation framework for adv_unlearn. model.py : Script to load Compvis and diffuser model. utils.py : Utility function used in AdvUnlearn. README.md : Docs for AdvUnlearn. core/ : Foundational classes and utilities. base_algorithm.py : Abstract base class for algorithm implementations. base_data_handler.py : Base class for data handling. base_model.py : Base class for model definitions. base_config.py : Base class for config class. base_trainer.py : Base class for training routines. environment.yaml : Environment setup for defense. tests/ : Test suites for ensuring code reliability. stable_diffusion/ : Components for stable diffusion. lora_diffusion/ : Components for the LoRA Diffusion. Datasets We use the Quick Canvas benchmark dataset, available here . Currently, the algorithms are trained using 5 images belonging to the themes of Abstractionism and Architectures . Usage This section contains the usage guide for the package. Installation pip install unlearn_diff Prerequisities Ensure conda is installed on your system. You can install Miniconda or Anaconda: Miniconda (recommended): https://docs.conda.io/en/latest/miniconda.html Anaconda : https://www.anaconda.com/products/distribution After installing conda , ensure it is available in your PATH by running. You may require to restart the terminal session: conda --version Create environment (Default): create_env Optional(Create environment for algorithm): create_env erase_diff Activate environment: conda activate <environment_name> eg: conda activate UnlearnDiff The has to be one of the folders in the mu/algorithms folder. Downloading data and models. After you install the package, you can use the following commands to download. Dataset : i2p : Sample : download_data sample i2p Full : download_data full i2p quick_canvas : Sample : download_data sample quick_canvas Full : download_data full quick_canvas Model : compvis : download_model compvis diffuser : download_model diffuser Run Train Each algorithm has their own script to run the algorithm, Some also have different process all together. Follow usage section in readme for the algorithm you want to run with the help of the github repository. You will need to run the code snippet provided in usage section with necessary configuration passed.","title":"Home"},{"location":"#unlearn-diff","text":"Unlearn Diff is an open-source Python package designed to streamline the development of unlearning algorithms and establish a standardized evaluation pipeline for diffusion models. It provides researchers and practitioners with tools to implement, evaluate, and extend unlearning algorithms effectively.","title":"Unlearn Diff"},{"location":"#documentation","text":"You can find the full documentation for this project at the url given below. https://ramailotech.github.io/msu_unlearningalgorithm/","title":"Documentation"},{"location":"#features","text":"Comprehensive Algorithm Support : Includes commonly used concept erasing and machine unlearning algorithms tailored for diffusion models. Each algorithm is encapsulated and standardized in terms of input-output formats. Automated Evaluation : Supports automatic evaluation on datasets like UnlearnCanvas or IP2P. Performs standard and adversarial evaluations, outputting metrics as detailed in UnlearnCanvas and UnlearnDiffAtk. Extensibility : Designed for easy integration of new unlearning algorithms, attack methods, defense mechanisms, and datasets with minimal modifications.","title":"Features"},{"location":"#supported-algorithms","text":"The initial version includes established methods benchmarked in UnlearnCanvas and defensive unlearning techniques: CA (Concept Ablation) ED (Erase Diff) ESD (Efficient Substitution Distillation) FMN (Forget Me Not) SU (Saliency Unlearning) SH (ScissorHands) SA (Selective Amnesia) SPM (Semi Permeable Membrane) UCE (Unified Concept Editing) For detailed information on each algorithm, please refer to the respective README.md files located inside mu/algorithms .","title":"Supported Algorithms"},{"location":"#project-architecture","text":"The project is organized to facilitate scalability and maintainability. data/ : Stores data-related files. i2p-dataset/ : contains i2p-dataset sample/ : Sample dataset full/ : Full dataset quick-canvas-dataset/ : contains quick canvas dataset sample/ : Sample dataset full/ : Full dataset docs/ : Documentation, including API references and user guides. outputs/ : Outputs of the trained algorithms. examples/ : Sample code and notebooks demonstrating usage. logs/ : Log files for debugging and auditing. models/ : Repository of trained models and checkpoints. mu/ : Core source code for machine unlearning. algorithms/ : Implementation of various algorithms. Each algorithm has its own subdirectory containing code and a README.md with detailed documentation. esd/ : ESD algorithm components. README.md : Documentation specific to the ESD algorithm. algorithm.py : Core implementation of ESD. configs/ : Configuration files for training and generation tasks. constants/const.py : Constant values used across the ESD algorithm. environment.yaml : Environment setup for ESD. model.py : Model architectures specific to ESD. sampler.py : Sampling methods used during training or inference. evaluator.py : Evaluation of ESD algorithm. scripts/train.py : Training script for ESD. trainer.py : Training routines and optimization strategies. utils.py : Utility functions and helpers. ca/ : Components for the CA algorithm. README.md : Documentation specific to the CA algorithm. ...and so on for other algorithms core/ : Foundational classes and utilities. base_algorithm.py : Abstract base class for algorithm implementations. base_data_handler.py : Base class for data handling. base_model.py : Base class for model definitions. base_sampler.py : Base class for sampling methods. base_trainer.py : Base class for training routines. base_config.py : Base class for config class. datasets/ : Dataset management and utilities. __init__.py : Initializes the dataset package. dataset.py : Dataset classes and methods. helpers/ : Helper functions for data processing. unlearning_canvas_dataset.py : Specific dataset class for unlearning tasks. helpers/ : Utility functions and helpers. helper.py : General-purpose helper functions. logger.py : Logging utilities to standardize logging practices. path_setup.py : Path configurations and environment setup. mu_attack/ : Core source code for attack. attackers/ : Implementation of each of the attacks. hard_prompt.py : Implementation for hard prompt attack. no_attack.py : Implementation for no attack. random.py : Implementation for random attack. seed_search.py : Implementation for seed search attack. soft_prompt.py : Implementation for soft_prompt attack. text_grad.py : Implementation for text_grad attack. configs : Configuration files for attackers and evaluation tasks. adv_unlearn : Config class for adversial unlearning. evaluation : Config class for evaluation. illegal : Config class for illegal. nudity : Config class for nudity. object : Config class for object. style : Config class for style. violence : Config class for violence. core/ : Foundational classes and utilities. base_attacker.py : Abstract base class for attack implementations. base_config.py : Base class for config class. base_stable_diffusion.py : Base class for stable diffusion pipeline. base_task.py : Base class for tasks. datasets/ : Script to generate dataset. exces/ : Implemenation of attacks. adv_attack.py : Implementation of advattack. attack.py : Main orchestration class that implements attacks. helpers/ : Utility functions and helpers. utils.py : Contains utility functions used in mu_attack. loggers/ : Logging utilities to standardize logging practices. tasks/ : Contains tasks for attacks. utils/ : Utilities for tasks. classifier.py : Implementation of classifier task. p4d.py : Implementation of p4d task. transfer.py : Implementation of transfer task. sd_compvis.py : Base stable diffusion pipeline implementation for compvis model. sd_diffuser.py : Base stable diffusion pipeline implementation for diffuser model. environment.yaml : Environment setup for attack. mu_defense/ : Core source code for defense i.e adversial unlearning. algorithms/ : Algorithms for defense. adv_unlearn/ : Implementation of defense algorithm (adv_unlearn). configs/ : Configuration files for AdvUnlearn, evaluation and image generation tasks. adv_unlearn_config.py : Configuration class for AdvUnlearn. evaluation_config.py : Configuration class for evaluation for AdvUnlearn. example_img_generator_config.py : Configuration class for image generation for AdvUnlearn evaluation. algorithm.py : Core implementation of AdvUnlearn. trainer.py : Core training routines and optimization strategies. compvis_trainer.py : Compvis specific training routines and optimization strategies. diffuser_trainer.py : Diffuser specific training routines and optimization strategies. generate_example_image.py : Implementation of image generation for evaluation for both compvis and diffuser. evaluator.py : Implementation of evaluation framework for adv_unlearn. model.py : Script to load Compvis and diffuser model. utils.py : Utility function used in AdvUnlearn. README.md : Docs for AdvUnlearn. core/ : Foundational classes and utilities. base_algorithm.py : Abstract base class for algorithm implementations. base_data_handler.py : Base class for data handling. base_model.py : Base class for model definitions. base_config.py : Base class for config class. base_trainer.py : Base class for training routines. environment.yaml : Environment setup for defense. tests/ : Test suites for ensuring code reliability. stable_diffusion/ : Components for stable diffusion. lora_diffusion/ : Components for the LoRA Diffusion.","title":"Project Architecture"},{"location":"#datasets","text":"We use the Quick Canvas benchmark dataset, available here . Currently, the algorithms are trained using 5 images belonging to the themes of Abstractionism and Architectures .","title":"Datasets"},{"location":"#usage","text":"This section contains the usage guide for the package.","title":"Usage"},{"location":"#installation","text":"pip install unlearn_diff","title":"Installation"},{"location":"#prerequisities","text":"Ensure conda is installed on your system. You can install Miniconda or Anaconda: Miniconda (recommended): https://docs.conda.io/en/latest/miniconda.html Anaconda : https://www.anaconda.com/products/distribution After installing conda , ensure it is available in your PATH by running. You may require to restart the terminal session: conda --version","title":"Prerequisities"},{"location":"#create-environment-default","text":"create_env","title":"Create environment (Default):"},{"location":"#optionalcreate-environment-for-algorithm","text":"create_env erase_diff","title":"Optional(Create environment for algorithm):"},{"location":"#activate-environment","text":"conda activate <environment_name> eg: conda activate UnlearnDiff The has to be one of the folders in the mu/algorithms folder.","title":"Activate environment:"},{"location":"#downloading-data-and-models","text":"After you install the package, you can use the following commands to download. Dataset : i2p : Sample : download_data sample i2p Full : download_data full i2p quick_canvas : Sample : download_data sample quick_canvas Full : download_data full quick_canvas Model : compvis : download_model compvis diffuser : download_model diffuser","title":"Downloading data and models."},{"location":"#run-train","text":"Each algorithm has their own script to run the algorithm, Some also have different process all together. Follow usage section in readme for the algorithm you want to run with the help of the github repository. You will need to run the code snippet provided in usage section with necessary configuration passed.","title":"Run Train"},{"location":"evaluation/","text":"","title":"Index"},{"location":"evaluation/concept_ablation/","text":"Concept ablation Evaluation Framework This section provides instructions for running the evaluation framework for the Concept ablation algorithm on Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying machine unlearning. Running the Evaluation Framework Create a file, eg, evaluate.py and use examples and modify your configs to run the file. Example Code from mu.algorithms.concept_ablation import ConceptAblationEvaluator from mu.algorithms.concept_ablation.configs import ( concept_ablation_evaluation_config ) evaluator = ConceptAblationEvaluator( concept_ablation_evaluation_config, ckpt_path=\"/home/ubuntu/Projects/dipesh/unlearn_diff/outputs/concept_ablation/finetuned_models/checkpoints/last.ckpt\", classifier_ckpt_path = \"/home/ubuntu/Projects/models/classifier_ckpt_path/style50_cls.pth\", reference_dir= \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\" ) evaluator.run() Running the Training Script in Offline Mode WANDB_MODE=offline python evaluate.py How It Works * Default Values: The script first loads default values from the evluation config file as in configs section. Parameter Overrides: Any parameters passed directly to the algorithm, overrides these configs. Final Configuration: The script merges the configs and convert them into dictionary to proceed with the evaluation. Description of parameters in evaluation_config The evaluation_config contains the necessary parameters for running the Concept ablation evaluation framework. Below is a detailed description of each parameter along with examples. Model Configuration: model_config : Path to the YAML file specifying the model architecture and settings. Type: str Example: \"mu/algorithms/concept_ablation/configs/model_config.yaml\" ckpt_path : Path to the finetuned Stable Diffusion checkpoint file to be evaluated. Type: str Example: \"outputs/concept_ablation/finetuned_models/concept_ablation_Abstractionism_model.pth\" classification_model : Specifies the classification model used for evaluating the generated outputs. Type: str Example: \"vit_large_patch16_224\" classifier_ckpt_path: Path to classifer checkpoint. Type : str Example : models/classifier_ckpt_path/style50_cls.pth Training and Sampling Parameters: forget_theme : Concept or style intended for removal in the evaluation process. Type: str Example: \"Bricks\" devices : CUDA device IDs to be used for the evaluation process. Type: str Example: \"0\" cfg_text : Classifier-free guidance scale value for image generation. Higher values increase the strength of the conditioning prompt. Type: float Example: 9.0 seed : Random seed for reproducibility of results. Type: int Example: 188 ddim_steps : Number of steps for the DDIM (Denoising Diffusion Implicit Models) sampling process. Type: int Example: 100 ddim_eta : DDIM eta value for controlling the amount of randomness during sampling. Set to 0 for deterministic sampling. Type: float Example: 0.0 image_height : Height of the generated images in pixels. Type: int Example: 512 image_width : Width of the generated images in pixels. Type: int Example: 512 Output and Logging Parameters: sampler_output_dir : Directory where generated images will be saved during evaluation. Type: str Example: \"outputs/eval_results/mu_results/concept_ablation/\" eval_output_dir : Directory where evaluation metrics and results will be stored. Type: str Example: \"outputs/eval_results/mu_results/concept_ablation/\" reference_dir : Directory containing original images for comparison during evaluation. Type: str Example: \"data/quick-canvas-dataset/sample/\" Performance and Efficiency Parameters: multiprocessing : Enables multiprocessing for faster evaluation for FID score. Recommended for large datasets. Type: bool Example: False batch_size : Batch size used during FID computation and evaluation. Type: int Example: 16 Optimization Parameters: seed_list : List of random seeds for performing multiple evaluations with different randomness levels. Type: list Example: [\"188\"]","title":"Evaluation"},{"location":"evaluation/concept_ablation/#concept-ablation-evaluation-framework","text":"This section provides instructions for running the evaluation framework for the Concept ablation algorithm on Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying machine unlearning.","title":"Concept ablation Evaluation Framework"},{"location":"evaluation/concept_ablation/#running-the-evaluation-framework","text":"Create a file, eg, evaluate.py and use examples and modify your configs to run the file. Example Code from mu.algorithms.concept_ablation import ConceptAblationEvaluator from mu.algorithms.concept_ablation.configs import ( concept_ablation_evaluation_config ) evaluator = ConceptAblationEvaluator( concept_ablation_evaluation_config, ckpt_path=\"/home/ubuntu/Projects/dipesh/unlearn_diff/outputs/concept_ablation/finetuned_models/checkpoints/last.ckpt\", classifier_ckpt_path = \"/home/ubuntu/Projects/models/classifier_ckpt_path/style50_cls.pth\", reference_dir= \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\" ) evaluator.run() Running the Training Script in Offline Mode WANDB_MODE=offline python evaluate.py How It Works * Default Values: The script first loads default values from the evluation config file as in configs section. Parameter Overrides: Any parameters passed directly to the algorithm, overrides these configs. Final Configuration: The script merges the configs and convert them into dictionary to proceed with the evaluation.","title":"Running the Evaluation Framework"},{"location":"evaluation/concept_ablation/#description-of-parameters-in-evaluation_config","text":"The evaluation_config contains the necessary parameters for running the Concept ablation evaluation framework. Below is a detailed description of each parameter along with examples.","title":"Description of parameters in evaluation_config"},{"location":"evaluation/concept_ablation/#model-configuration","text":"model_config : Path to the YAML file specifying the model architecture and settings. Type: str Example: \"mu/algorithms/concept_ablation/configs/model_config.yaml\" ckpt_path : Path to the finetuned Stable Diffusion checkpoint file to be evaluated. Type: str Example: \"outputs/concept_ablation/finetuned_models/concept_ablation_Abstractionism_model.pth\" classification_model : Specifies the classification model used for evaluating the generated outputs. Type: str Example: \"vit_large_patch16_224\" classifier_ckpt_path: Path to classifer checkpoint. Type : str Example : models/classifier_ckpt_path/style50_cls.pth","title":"Model Configuration:"},{"location":"evaluation/concept_ablation/#training-and-sampling-parameters","text":"forget_theme : Concept or style intended for removal in the evaluation process. Type: str Example: \"Bricks\" devices : CUDA device IDs to be used for the evaluation process. Type: str Example: \"0\" cfg_text : Classifier-free guidance scale value for image generation. Higher values increase the strength of the conditioning prompt. Type: float Example: 9.0 seed : Random seed for reproducibility of results. Type: int Example: 188 ddim_steps : Number of steps for the DDIM (Denoising Diffusion Implicit Models) sampling process. Type: int Example: 100 ddim_eta : DDIM eta value for controlling the amount of randomness during sampling. Set to 0 for deterministic sampling. Type: float Example: 0.0 image_height : Height of the generated images in pixels. Type: int Example: 512 image_width : Width of the generated images in pixels. Type: int Example: 512","title":"Training and Sampling Parameters:"},{"location":"evaluation/concept_ablation/#output-and-logging-parameters","text":"sampler_output_dir : Directory where generated images will be saved during evaluation. Type: str Example: \"outputs/eval_results/mu_results/concept_ablation/\" eval_output_dir : Directory where evaluation metrics and results will be stored. Type: str Example: \"outputs/eval_results/mu_results/concept_ablation/\" reference_dir : Directory containing original images for comparison during evaluation. Type: str Example: \"data/quick-canvas-dataset/sample/\"","title":"Output and Logging Parameters:"},{"location":"evaluation/concept_ablation/#performance-and-efficiency-parameters","text":"multiprocessing : Enables multiprocessing for faster evaluation for FID score. Recommended for large datasets. Type: bool Example: False batch_size : Batch size used during FID computation and evaluation. Type: int Example: 16","title":"Performance and Efficiency Parameters:"},{"location":"evaluation/concept_ablation/#optimization-parameters","text":"seed_list : List of random seeds for performing multiple evaluations with different randomness levels. Type: list Example: [\"188\"]","title":"Optimization Parameters:"},{"location":"evaluation/contributing/","text":"Contributing to Unlearn Diff Thank you for your interest in contributing to Unlearn ! This document outlines the steps and best practices for adding new algorithms, making pull requests, filing issues, and more. Table of Contents Introduction Code of Conduct Project Structure How to Contribute Reporting Issues Suggesting Enhancements Submitting Pull Requests Adding a New Algorithm Folder Structure Creating an Environment Documentation Code Style Contact Introduction Unlearn is an open-source Python package designed for unlearning algorithms in diffusion models. We welcome contributions from the community, whether it\u2019s a new feature, bug fix, or an entirely new unlearning algorithm. Code of Conduct Please note that we expect all contributors to abide by our code of conduct. Be respectful and considerate of others, and help keep our community healthy and inclusive. Project Structure A quick overview of relevant folders: mu/algorithms/ This is where all algorithms reside. Each algorithm has its own subfolder containing: algorithm.py (core logic) data_handler.py (if data-processing is needed) datasets/ (special dataset classes) environment.yaml (conda environment details) evaluator.py (evaluation routines) model.py (model definitions) sampler.py (sampling methods) scripts/ (training, evaluation scripts, etc.) trainer.py (training loop, optimization) utils.py (auxiliary functions) Readme.md (algorithm-specific documentation) mu/core/ Contains abstract/base classes ( base_algorithm.py , base_trainer.py , etc.) that algorithms can extend or override. mu/datasets/ Common dataset classes and utilities used by multiple algorithms. helpers/ Shared utility functions (e.g., logging, path setup, config loading). How to Contribute Reporting Issues Check the to see if your bug or feature request has already been reported. If not, open a new issue. Provide a clear title and description, including steps to reproduce if it\u2019s a bug. Label your issue appropriately (e.g., \u201cbug\u201d, \u201cenhancement\u201d, \u201cdocumentation\u201d). Suggesting Enhancements If you have an idea for a new feature or improvement, open a GitHub issue labeled \u201cenhancement\u201d or \u201cfeature request.\u201d Include any relevant use-cases, examples, or background information to help the community understand your proposal. Submitting Pull Requests Fork the repository and create your feature branch from main (or the appropriate branch if instructed otherwise): bash git checkout -b feature/my-awesome-feature Make your changes, including tests if applicable. Commit and push your changes: bash git push origin feature/my-awesome-feature Open a Pull Request from your fork into the main repo. Provide a clear description of what your PR does, referencing any related issues. Please ensure your PR follows the Code Style guidelines and includes updated or new tests if needed. Adding a New Algorithm One of the core goals of Unlearn Diff is to make it easy to add and benchmark new unlearning algorithms. Follow these steps when introducing a new method. Folder Structure Create a new subfolder in mu/algorithms/ with a clear, descriptive name (e.g., my_new_algo ). Inside this subfolder, include the following (at minimum): algorithm.py trainer.py scripts/train.py (and optionally scripts/evaluate.py if your training and evaluation are separate) configs/ containing YAML config files for training, evaluation, etc. environment.yaml specifying dependencies for conda Readme.md explaining how to install, run, and configure your new algorithm (Optional) data_handler.py , datasets/ , model.py , sampler.py , utils.py , etc., depending on your algorithm\u2019s design. Extend or import from base classes in mu/core/ if your algorithm logic aligns with any existing abstract classes (e.g., BaseAlgorithm , BaseTrainer , etc.). This ensures code consistency and easier maintenance. Creating an Environment To keep dependencies organized, each algorithm has its own environment.yaml . Contributors are encouraged to: Specify all required packages (e.g., torch , torchvision , diffusers , etc.). Name the environment after the algorithm, e.g., mu_<algorithm_name> . Update instructions in your algorithm\u2019s Readme.md on how to install and activate this environment. Documentation Each algorithm folder must contain a Readme.md that documents: High-level description of the algorithm (goals, references to papers, etc.). How to install any special dependencies (if not covered by environment.yaml ). How to run training and evaluation (with example commands). Explanation of the config files in configs/ . Any unique parameters or hyperparameters. If you introduce new metrics or logs, please outline them in your algorithm\u2019s README and update any relevant docs in the main docs/ folder as needed. Code Style We recommend following PEP 8 for Python code. Use descriptive variable names, docstrings, and type hints where possible. Keep functions and methods short and focused\u2014avoid large, monolithic methods. Use Python\u2019s built-in logging or the logging utilities in helpers/logger.py for consistent log outputs. Use wandb global wandb logger if required. Additionally, we prefer: - 4 spaces for indentation (no tabs). - Meaningful commit messages: \u201cFix dataset loader bug for large images\u201d vs. \u201cFix stuff.\u201d Contact If you have any questions, feel free to open an issue or reach out to us via email. We appreciate your contributions and look forward to collaborating with you on Unlearn Diff ! Thank you again for your interest in contributing. We\u2019re excited to see your ideas and improvements!","title":"Contributing to Unlearn Diff"},{"location":"evaluation/contributing/#contributing-to-unlearn-diff","text":"Thank you for your interest in contributing to Unlearn ! This document outlines the steps and best practices for adding new algorithms, making pull requests, filing issues, and more.","title":"Contributing to Unlearn Diff"},{"location":"evaluation/contributing/#table-of-contents","text":"Introduction Code of Conduct Project Structure How to Contribute Reporting Issues Suggesting Enhancements Submitting Pull Requests Adding a New Algorithm Folder Structure Creating an Environment Documentation Code Style Contact","title":"Table of Contents"},{"location":"evaluation/contributing/#introduction","text":"Unlearn is an open-source Python package designed for unlearning algorithms in diffusion models. We welcome contributions from the community, whether it\u2019s a new feature, bug fix, or an entirely new unlearning algorithm.","title":"Introduction"},{"location":"evaluation/contributing/#code-of-conduct","text":"Please note that we expect all contributors to abide by our code of conduct. Be respectful and considerate of others, and help keep our community healthy and inclusive.","title":"Code of Conduct"},{"location":"evaluation/contributing/#project-structure","text":"A quick overview of relevant folders: mu/algorithms/ This is where all algorithms reside. Each algorithm has its own subfolder containing: algorithm.py (core logic) data_handler.py (if data-processing is needed) datasets/ (special dataset classes) environment.yaml (conda environment details) evaluator.py (evaluation routines) model.py (model definitions) sampler.py (sampling methods) scripts/ (training, evaluation scripts, etc.) trainer.py (training loop, optimization) utils.py (auxiliary functions) Readme.md (algorithm-specific documentation) mu/core/ Contains abstract/base classes ( base_algorithm.py , base_trainer.py , etc.) that algorithms can extend or override. mu/datasets/ Common dataset classes and utilities used by multiple algorithms. helpers/ Shared utility functions (e.g., logging, path setup, config loading).","title":"Project Structure"},{"location":"evaluation/contributing/#how-to-contribute","text":"","title":"How to Contribute"},{"location":"evaluation/contributing/#reporting-issues","text":"Check the to see if your bug or feature request has already been reported. If not, open a new issue. Provide a clear title and description, including steps to reproduce if it\u2019s a bug. Label your issue appropriately (e.g., \u201cbug\u201d, \u201cenhancement\u201d, \u201cdocumentation\u201d).","title":"Reporting Issues"},{"location":"evaluation/contributing/#suggesting-enhancements","text":"If you have an idea for a new feature or improvement, open a GitHub issue labeled \u201cenhancement\u201d or \u201cfeature request.\u201d Include any relevant use-cases, examples, or background information to help the community understand your proposal.","title":"Suggesting Enhancements"},{"location":"evaluation/contributing/#submitting-pull-requests","text":"Fork the repository and create your feature branch from main (or the appropriate branch if instructed otherwise): bash git checkout -b feature/my-awesome-feature Make your changes, including tests if applicable. Commit and push your changes: bash git push origin feature/my-awesome-feature Open a Pull Request from your fork into the main repo. Provide a clear description of what your PR does, referencing any related issues. Please ensure your PR follows the Code Style guidelines and includes updated or new tests if needed.","title":"Submitting Pull Requests"},{"location":"evaluation/contributing/#adding-a-new-algorithm","text":"One of the core goals of Unlearn Diff is to make it easy to add and benchmark new unlearning algorithms. Follow these steps when introducing a new method.","title":"Adding a New Algorithm"},{"location":"evaluation/contributing/#folder-structure","text":"Create a new subfolder in mu/algorithms/ with a clear, descriptive name (e.g., my_new_algo ). Inside this subfolder, include the following (at minimum): algorithm.py trainer.py scripts/train.py (and optionally scripts/evaluate.py if your training and evaluation are separate) configs/ containing YAML config files for training, evaluation, etc. environment.yaml specifying dependencies for conda Readme.md explaining how to install, run, and configure your new algorithm (Optional) data_handler.py , datasets/ , model.py , sampler.py , utils.py , etc., depending on your algorithm\u2019s design. Extend or import from base classes in mu/core/ if your algorithm logic aligns with any existing abstract classes (e.g., BaseAlgorithm , BaseTrainer , etc.). This ensures code consistency and easier maintenance.","title":"Folder Structure"},{"location":"evaluation/contributing/#creating-an-environment","text":"To keep dependencies organized, each algorithm has its own environment.yaml . Contributors are encouraged to: Specify all required packages (e.g., torch , torchvision , diffusers , etc.). Name the environment after the algorithm, e.g., mu_<algorithm_name> . Update instructions in your algorithm\u2019s Readme.md on how to install and activate this environment.","title":"Creating an Environment"},{"location":"evaluation/contributing/#documentation","text":"Each algorithm folder must contain a Readme.md that documents: High-level description of the algorithm (goals, references to papers, etc.). How to install any special dependencies (if not covered by environment.yaml ). How to run training and evaluation (with example commands). Explanation of the config files in configs/ . Any unique parameters or hyperparameters. If you introduce new metrics or logs, please outline them in your algorithm\u2019s README and update any relevant docs in the main docs/ folder as needed.","title":"Documentation"},{"location":"evaluation/contributing/#code-style","text":"We recommend following PEP 8 for Python code. Use descriptive variable names, docstrings, and type hints where possible. Keep functions and methods short and focused\u2014avoid large, monolithic methods. Use Python\u2019s built-in logging or the logging utilities in helpers/logger.py for consistent log outputs. Use wandb global wandb logger if required. Additionally, we prefer: - 4 spaces for indentation (no tabs). - Meaningful commit messages: \u201cFix dataset loader bug for large images\u201d vs. \u201cFix stuff.\u201d","title":"Code Style"},{"location":"evaluation/contributing/#contact","text":"If you have any questions, feel free to open an issue or reach out to us via email. We appreciate your contributions and look forward to collaborating with you on Unlearn Diff ! Thank you again for your interest in contributing. We\u2019re excited to see your ideas and improvements!","title":"Contact"},{"location":"evaluation/erase_diff/","text":"Erase Diff Evaluation Framework This section provides instructions for running the evaluation framework for the Erase Diff algorithm on Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying machine unlearning. Running the Evaluation Framework You can run the evaluation framework using the evaluate.py script located in the mu/algorithms/erase_diff/scripts/ directory. Basic Command to Run Evaluation: conda activate <env_name> Add the following code to evaluate.py . from mu.algorithms.erase_diff import EraseDiffEvaluator from mu.algorithms.erase_diff.configs import ( erase_diff_evaluation_config ) evaluator = EraseDiffEvaluator( erase_diff_evaluation_config, ckpt_path=\"/home/ubuntu/Projects/dipesh/unlearn_diff/outputs/erase_diff/erase_diff_Abstractionism_model.pth\", classifier_ckpt_path = \"/home/ubuntu/Projects/models/classifier_ckpt_path/style50_cls.pth\", reference_dir= \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\" ) evaluator.run() Run the script python evaluate.py Description of parameters in evaluation_config.yaml The evaluation_config.yaml file contains the necessary parameters for running the Erase Diff evaluation framework. Below is a detailed description of each parameter along with examples. Model Configuration: model_config : Path to the YAML file specifying the model architecture and settings. Type: str Example: \"mu/algorithms/erase_diff/configs/model_config.yaml\" ckpt_path : Path to the finetuned Stable Diffusion checkpoint file to be evaluated. Type: str Example: \"outputs/erase_diff/finetuned_models/erase_diff_Abstractionism_model.pth\" classification_model : Specifies the classification model used for evaluating the generated outputs. Type: str Example: \"vit_large_patch16_224\" model_ckpt_path: Path to pretrained Stable Diffusion model. Type : str Example : models/compvis/style50/compvis.ckpt Training and Sampling Parameters: theme : Specifies the theme or concept being evaluated for removal from the model's outputs. Type: str Example: \"Bricks\" devices : CUDA device IDs to be used for the evaluation process. Type: str Example: \"0\" cfg_text : Classifier-free guidance scale value for image generation. Higher values increase the strength of the conditioning prompt. Type: float Example: 9.0 seed : Random seed for reproducibility of results. Type: int Example: 188 ddim_steps : Number of steps for the DDIM (Denoising Diffusion Implicit Models) sampling process. Type: int Example: 100 ddim_eta : DDIM eta value for controlling the amount of randomness during sampling. Set to 0 for deterministic sampling. Type: float Example: 0.0 image_height : Height of the generated images in pixels. Type: int Example: 512 image_width : Width of the generated images in pixels. Type: int Example: 512 Output and Logging Parameters: sampler_output_dir : Directory where generated images will be saved during evaluation. Type: str Example: \"outputs/eval_results/mu_results/erase_diff/\" eval_output_dir : Directory where evaluation metrics and results will be stored. Type: str Example: \"outputs/eval_results/mu_results/erase_diff/\" reference_dir : Directory containing original images for comparison during evaluation. Type: str Example: \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\" Performance and Efficiency Parameters: multiprocessing : Enables multiprocessing for faster evaluation for FID score. Recommended for large datasets. Type: bool Example: False batch_size : Batch size used during FID computation and evaluation. Type: int Example: 16 Optimization Parameters: forget_theme : Concept or style intended for removal in the evaluation process. Type: str Example: \"Bricks\" seed_list : List of random seeds for performing multiple evaluations with different randomness levels. Type: list Example: [\"188\"]","title":"Evaluation"},{"location":"evaluation/erase_diff/#erase-diff-evaluation-framework","text":"This section provides instructions for running the evaluation framework for the Erase Diff algorithm on Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying machine unlearning.","title":"Erase Diff Evaluation Framework"},{"location":"evaluation/erase_diff/#running-the-evaluation-framework","text":"You can run the evaluation framework using the evaluate.py script located in the mu/algorithms/erase_diff/scripts/ directory.","title":"Running the Evaluation Framework"},{"location":"evaluation/erase_diff/#basic-command-to-run-evaluation","text":"conda activate <env_name> Add the following code to evaluate.py . from mu.algorithms.erase_diff import EraseDiffEvaluator from mu.algorithms.erase_diff.configs import ( erase_diff_evaluation_config ) evaluator = EraseDiffEvaluator( erase_diff_evaluation_config, ckpt_path=\"/home/ubuntu/Projects/dipesh/unlearn_diff/outputs/erase_diff/erase_diff_Abstractionism_model.pth\", classifier_ckpt_path = \"/home/ubuntu/Projects/models/classifier_ckpt_path/style50_cls.pth\", reference_dir= \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\" ) evaluator.run() Run the script python evaluate.py","title":"Basic Command to Run Evaluation:"},{"location":"evaluation/erase_diff/#description-of-parameters-in-evaluation_configyaml","text":"The evaluation_config.yaml file contains the necessary parameters for running the Erase Diff evaluation framework. Below is a detailed description of each parameter along with examples.","title":"Description of parameters in evaluation_config.yaml"},{"location":"evaluation/erase_diff/#model-configuration","text":"model_config : Path to the YAML file specifying the model architecture and settings. Type: str Example: \"mu/algorithms/erase_diff/configs/model_config.yaml\" ckpt_path : Path to the finetuned Stable Diffusion checkpoint file to be evaluated. Type: str Example: \"outputs/erase_diff/finetuned_models/erase_diff_Abstractionism_model.pth\" classification_model : Specifies the classification model used for evaluating the generated outputs. Type: str Example: \"vit_large_patch16_224\" model_ckpt_path: Path to pretrained Stable Diffusion model. Type : str Example : models/compvis/style50/compvis.ckpt","title":"Model Configuration:"},{"location":"evaluation/erase_diff/#training-and-sampling-parameters","text":"theme : Specifies the theme or concept being evaluated for removal from the model's outputs. Type: str Example: \"Bricks\" devices : CUDA device IDs to be used for the evaluation process. Type: str Example: \"0\" cfg_text : Classifier-free guidance scale value for image generation. Higher values increase the strength of the conditioning prompt. Type: float Example: 9.0 seed : Random seed for reproducibility of results. Type: int Example: 188 ddim_steps : Number of steps for the DDIM (Denoising Diffusion Implicit Models) sampling process. Type: int Example: 100 ddim_eta : DDIM eta value for controlling the amount of randomness during sampling. Set to 0 for deterministic sampling. Type: float Example: 0.0 image_height : Height of the generated images in pixels. Type: int Example: 512 image_width : Width of the generated images in pixels. Type: int Example: 512","title":"Training and Sampling Parameters:"},{"location":"evaluation/erase_diff/#output-and-logging-parameters","text":"sampler_output_dir : Directory where generated images will be saved during evaluation. Type: str Example: \"outputs/eval_results/mu_results/erase_diff/\" eval_output_dir : Directory where evaluation metrics and results will be stored. Type: str Example: \"outputs/eval_results/mu_results/erase_diff/\" reference_dir : Directory containing original images for comparison during evaluation. Type: str Example: \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\"","title":"Output and Logging Parameters:"},{"location":"evaluation/erase_diff/#performance-and-efficiency-parameters","text":"multiprocessing : Enables multiprocessing for faster evaluation for FID score. Recommended for large datasets. Type: bool Example: False batch_size : Batch size used during FID computation and evaluation. Type: int Example: 16","title":"Performance and Efficiency Parameters:"},{"location":"evaluation/erase_diff/#optimization-parameters","text":"forget_theme : Concept or style intended for removal in the evaluation process. Type: str Example: \"Bricks\" seed_list : List of random seeds for performing multiple evaluations with different randomness levels. Type: list Example: [\"188\"]","title":"Optimization Parameters:"},{"location":"evaluation/esd/","text":"ESD Evaluation Framework This section provides instructions for running the evaluation framework for the ESD algorithm on Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying machine unlearning. Running the Evaluation Framework You can run the evaluation framework using the evaluate.py script located in the mu/algorithms/esd/scripts/ directory. Basic Command to Run Evaluation: conda activate <env_name> Add the following code to evaluate.py . from mu.algorithms.esd import ESDEvaluator from mu.algorithms.esd.configs import ( esd_evaluation_config ) evaluator = ESDEvaluator( esd_evaluation_config, ckpt_path=\"/home/ubuntu/Projects/dipesh/unlearn_diff/outputs/esd/esd_Abstractionism_model.pth\", classifier_ckpt_path = \"/home/ubuntu/Projects/models/classifier_ckpt_path/style50_cls.pth\", reference_dir= \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\" ) evaluator.run() Run the script python evaluate.py Description of parameters in evaluation_config.yaml The evaluation_config.yaml file contains the necessary parameters for running the ESD evaluation framework. Below is a detailed description of each parameter along with examples. Model Configuration: model_config : Path to the YAML file specifying the model architecture and settings. Type: str Example: \"mu/algorithms/esd/configs/model_config.yaml\" ckpt_path : Path to the finetuned Stable Diffusion checkpoint file to be evaluated. Type: str Example: \"outputs/esd/finetuned_models/esd_Abstractionism_model.pth\" classification_model : Specifies the classification model used for evaluating the generated outputs. Type: str Example: \"vit_large_patch16_224\" model_ckpt_path: Path to pretrained Stable Diffusion model. Type : str Example : models/compvis/style50/compvis.ckpt Training and Sampling Parameters: theme : Specifies the theme or concept being evaluated for removal from the model's outputs. Type: str Example: \"Bricks\" devices : CUDA device IDs to be used for the evaluation process. Type: str Example: \"0\" cfg_text : Classifier-free guidance scale value for image generation. Higher values increase the strength of the conditioning prompt. Type: float Example: 9.0 seed : Random seed for reproducibility of results. Type: int Example: 188 ddim_steps : Number of steps for the DDIM (Denoising Diffusion Implicit Models) sampling process. Type: int Example: 100 ddim_eta : DDIM eta value for controlling the amount of randomness during sampling. Set to 0 for deterministic sampling. Type: float Example: 0.0 image_height : Height of the generated images in pixels. Type: int Example: 512 image_width : Width of the generated images in pixels. Type: int Example: 512 Output and Logging Parameters: sampler_output_dir : Directory where generated images will be saved during evaluation. Type: str Example: \"outputs/eval_results/mu_results/esd/\" eval_output_dir : Directory where evaluation metrics and results will be stored. Type: str Example: \"outputs/eval_results/mu_results/esd/\" reference_dir : Directory containing original images for comparison during evaluation. Type: str Example: \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\" Performance and Efficiency Parameters: multiprocessing : Enables multiprocessing for faster evaluation for FID score. Recommended for large datasets. Type: bool Example: False batch_size : Batch size used during FID computation and evaluation. Type: int Example: 16 Optimization Parameters: forget_theme : Concept or style intended for removal in the evaluation process. Type: str Example: \"Bricks\" seed_list : List of random seeds for performing multiple evaluations with different randomness levels. Type: list Example: [\"188\"]","title":"Evaluation"},{"location":"evaluation/esd/#esd-evaluation-framework","text":"This section provides instructions for running the evaluation framework for the ESD algorithm on Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying machine unlearning.","title":"ESD Evaluation Framework"},{"location":"evaluation/esd/#running-the-evaluation-framework","text":"You can run the evaluation framework using the evaluate.py script located in the mu/algorithms/esd/scripts/ directory.","title":"Running the Evaluation Framework"},{"location":"evaluation/esd/#basic-command-to-run-evaluation","text":"conda activate <env_name> Add the following code to evaluate.py . from mu.algorithms.esd import ESDEvaluator from mu.algorithms.esd.configs import ( esd_evaluation_config ) evaluator = ESDEvaluator( esd_evaluation_config, ckpt_path=\"/home/ubuntu/Projects/dipesh/unlearn_diff/outputs/esd/esd_Abstractionism_model.pth\", classifier_ckpt_path = \"/home/ubuntu/Projects/models/classifier_ckpt_path/style50_cls.pth\", reference_dir= \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\" ) evaluator.run() Run the script python evaluate.py","title":"Basic Command to Run Evaluation:"},{"location":"evaluation/esd/#description-of-parameters-in-evaluation_configyaml","text":"The evaluation_config.yaml file contains the necessary parameters for running the ESD evaluation framework. Below is a detailed description of each parameter along with examples.","title":"Description of parameters in evaluation_config.yaml"},{"location":"evaluation/esd/#model-configuration","text":"model_config : Path to the YAML file specifying the model architecture and settings. Type: str Example: \"mu/algorithms/esd/configs/model_config.yaml\" ckpt_path : Path to the finetuned Stable Diffusion checkpoint file to be evaluated. Type: str Example: \"outputs/esd/finetuned_models/esd_Abstractionism_model.pth\" classification_model : Specifies the classification model used for evaluating the generated outputs. Type: str Example: \"vit_large_patch16_224\" model_ckpt_path: Path to pretrained Stable Diffusion model. Type : str Example : models/compvis/style50/compvis.ckpt","title":"Model Configuration:"},{"location":"evaluation/esd/#training-and-sampling-parameters","text":"theme : Specifies the theme or concept being evaluated for removal from the model's outputs. Type: str Example: \"Bricks\" devices : CUDA device IDs to be used for the evaluation process. Type: str Example: \"0\" cfg_text : Classifier-free guidance scale value for image generation. Higher values increase the strength of the conditioning prompt. Type: float Example: 9.0 seed : Random seed for reproducibility of results. Type: int Example: 188 ddim_steps : Number of steps for the DDIM (Denoising Diffusion Implicit Models) sampling process. Type: int Example: 100 ddim_eta : DDIM eta value for controlling the amount of randomness during sampling. Set to 0 for deterministic sampling. Type: float Example: 0.0 image_height : Height of the generated images in pixels. Type: int Example: 512 image_width : Width of the generated images in pixels. Type: int Example: 512","title":"Training and Sampling Parameters:"},{"location":"evaluation/esd/#output-and-logging-parameters","text":"sampler_output_dir : Directory where generated images will be saved during evaluation. Type: str Example: \"outputs/eval_results/mu_results/esd/\" eval_output_dir : Directory where evaluation metrics and results will be stored. Type: str Example: \"outputs/eval_results/mu_results/esd/\" reference_dir : Directory containing original images for comparison during evaluation. Type: str Example: \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\"","title":"Output and Logging Parameters:"},{"location":"evaluation/esd/#performance-and-efficiency-parameters","text":"multiprocessing : Enables multiprocessing for faster evaluation for FID score. Recommended for large datasets. Type: bool Example: False batch_size : Batch size used during FID computation and evaluation. Type: int Example: 16","title":"Performance and Efficiency Parameters:"},{"location":"evaluation/esd/#optimization-parameters","text":"forget_theme : Concept or style intended for removal in the evaluation process. Type: str Example: \"Bricks\" seed_list : List of random seeds for performing multiple evaluations with different randomness levels. Type: list Example: [\"188\"]","title":"Optimization Parameters:"},{"location":"evaluation/forget_me_not/","text":"forget_me_not Evaluation Framework This section provides instructions for running the evaluation framework for the forget_me_not algorithm on Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying machine unlearning. Running the Evaluation Framework You can run the evaluation framework using the evaluate.py script located in the mu/algorithms/forget_me_not/scripts/ directory. Basic Command to Run Evaluation: conda activate <env_name> Add the following code to evaluate.py from mu.algorithms.forget_me_not import ForgetMeNotEvaluator from mu.algorithms.forget_me_not.configs import ( forget_me_not_evaluation_config ) evaluator = ForgetMeNotEvaluator( forget_me_not_evaluation_config, ckpt_path=\"/home/ubuntu/Projects/dipesh/unlearn_diff/outputs/forget_me_not/finetuned_models/Abstractionism\", classifier_ckpt_path = \"/home/ubuntu/Projects/models/classifier_ckpt_path/style50_cls.pth\", reference_dir= \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\" ) evaluator.run() Run the script python evaluate.py Description of parameters in evaluation_config.yaml The evaluation_config.yaml file contains the necessary parameters for running the forget_me_not evaluation framework. Below is a detailed description of each parameter along with examples. Model Configuration: ckpt_path : Path to the finetuned Stable Diffusion checkpoint file to be evaluated. Type: str Example: \"outputs/forget_me_not/finetuned_models/forget_me_not_Abstractionism_model.pth\" classification_model : Specifies the classification model used for evaluating the generated outputs. Type: str Example: \"vit_large_patch16_224\" model_ckpt_path: Path to pretrained Stable Diffusion model. Type : str Example : models/diffuser/style50 Training and Sampling Parameters: theme : Specifies the theme or concept being evaluated for removal from the model's outputs. Type: str Example: \"Bricks\" devices : CUDA device IDs to be used for the evaluation process. Type: str Example: \"0\" cfg_text_list : Classifier-free guidance scale value for image generation. Higher values increase the strength of the conditioning prompt. Type: list Example: [9.0] seed : Random seed for reproducibility of results. Type: int Example: 188 ddim_steps : Number of steps for the DDIM (Denoising Diffusion Implicit Models) sampling process. Type: int Example: 100 ddim_eta : DDIM eta value for controlling the amount of randomness during sampling. Set to 0 for deterministic sampling. Type: float Example: 0.0 image_height : Height of the generated images in pixels. Type: int Example: 512 image_width : Width of the generated images in pixels. Type: int Example: 512 Output and Logging Parameters: sampler_output_dir : Directory where generated images will be saved during evaluation. Type: str Example: \"outputs/eval_results/mu_results/forget_me_not/\" eval_output_dir : Directory where evaluation metrics and results will be stored. Type: str Example: \"outputs/eval_results/mu_results/forget_me_not/\" reference_dir : Directory containing original images for comparison during evaluation. Type: str Example: \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\" Performance and Efficiency Parameters: multiprocessing : Enables multiprocessing for faster evaluation for FID score. Recommended for large datasets. Type: bool Example: False batch_size : Batch size used during FID computation and evaluation. Type: int Example: 16 Optimization Parameters: forget_theme : Concept or style intended for removal in the evaluation process. Type: str Example: \"Bricks\" seed_list : List of random seeds for performing multiple evaluations with different randomness levels. Type: list Example: [\"188\"]","title":"Evaluation"},{"location":"evaluation/forget_me_not/#forget_me_not-evaluation-framework","text":"This section provides instructions for running the evaluation framework for the forget_me_not algorithm on Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying machine unlearning.","title":"forget_me_not Evaluation Framework"},{"location":"evaluation/forget_me_not/#running-the-evaluation-framework","text":"You can run the evaluation framework using the evaluate.py script located in the mu/algorithms/forget_me_not/scripts/ directory.","title":"Running the Evaluation Framework"},{"location":"evaluation/forget_me_not/#basic-command-to-run-evaluation","text":"conda activate <env_name> Add the following code to evaluate.py from mu.algorithms.forget_me_not import ForgetMeNotEvaluator from mu.algorithms.forget_me_not.configs import ( forget_me_not_evaluation_config ) evaluator = ForgetMeNotEvaluator( forget_me_not_evaluation_config, ckpt_path=\"/home/ubuntu/Projects/dipesh/unlearn_diff/outputs/forget_me_not/finetuned_models/Abstractionism\", classifier_ckpt_path = \"/home/ubuntu/Projects/models/classifier_ckpt_path/style50_cls.pth\", reference_dir= \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\" ) evaluator.run() Run the script python evaluate.py","title":"Basic Command to Run Evaluation:"},{"location":"evaluation/forget_me_not/#description-of-parameters-in-evaluation_configyaml","text":"The evaluation_config.yaml file contains the necessary parameters for running the forget_me_not evaluation framework. Below is a detailed description of each parameter along with examples.","title":"Description of parameters in evaluation_config.yaml"},{"location":"evaluation/forget_me_not/#model-configuration","text":"ckpt_path : Path to the finetuned Stable Diffusion checkpoint file to be evaluated. Type: str Example: \"outputs/forget_me_not/finetuned_models/forget_me_not_Abstractionism_model.pth\" classification_model : Specifies the classification model used for evaluating the generated outputs. Type: str Example: \"vit_large_patch16_224\" model_ckpt_path: Path to pretrained Stable Diffusion model. Type : str Example : models/diffuser/style50","title":"Model Configuration:"},{"location":"evaluation/forget_me_not/#training-and-sampling-parameters","text":"theme : Specifies the theme or concept being evaluated for removal from the model's outputs. Type: str Example: \"Bricks\" devices : CUDA device IDs to be used for the evaluation process. Type: str Example: \"0\" cfg_text_list : Classifier-free guidance scale value for image generation. Higher values increase the strength of the conditioning prompt. Type: list Example: [9.0] seed : Random seed for reproducibility of results. Type: int Example: 188 ddim_steps : Number of steps for the DDIM (Denoising Diffusion Implicit Models) sampling process. Type: int Example: 100 ddim_eta : DDIM eta value for controlling the amount of randomness during sampling. Set to 0 for deterministic sampling. Type: float Example: 0.0 image_height : Height of the generated images in pixels. Type: int Example: 512 image_width : Width of the generated images in pixels. Type: int Example: 512","title":"Training and Sampling Parameters:"},{"location":"evaluation/forget_me_not/#output-and-logging-parameters","text":"sampler_output_dir : Directory where generated images will be saved during evaluation. Type: str Example: \"outputs/eval_results/mu_results/forget_me_not/\" eval_output_dir : Directory where evaluation metrics and results will be stored. Type: str Example: \"outputs/eval_results/mu_results/forget_me_not/\" reference_dir : Directory containing original images for comparison during evaluation. Type: str Example: \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\"","title":"Output and Logging Parameters:"},{"location":"evaluation/forget_me_not/#performance-and-efficiency-parameters","text":"multiprocessing : Enables multiprocessing for faster evaluation for FID score. Recommended for large datasets. Type: bool Example: False batch_size : Batch size used during FID computation and evaluation. Type: int Example: 16","title":"Performance and Efficiency Parameters:"},{"location":"evaluation/forget_me_not/#optimization-parameters","text":"forget_theme : Concept or style intended for removal in the evaluation process. Type: str Example: \"Bricks\" seed_list : List of random seeds for performing multiple evaluations with different randomness levels. Type: list Example: [\"188\"]","title":"Optimization Parameters:"},{"location":"evaluation/saliency/","text":"Saliency Unlearning Evaluation Framework This section provides instructions for running the evaluation framework for the Saliency Unlearning algorithm on Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying machine unlearning. Running the Evaluation Framework You can run the evaluation framework using the evaluate.py script located in the mu/algorithms/saliency_unlearning/scripts/ directory. Basic Command to Run Evaluation: conda activate <env_name> Add the following code to evaluate.py from mu.algorithms.saliency_unlearning import SaliencyUnlearningEvaluator from mu.algorithms.saliency_unlearning.configs import ( saliency_unlearning_evaluation_config ) evaluator = SaliencyUnlearningEvaluator( saliency_unlearning_evaluation_config, ckpt_path=\"/home/ubuntu/Projects/dipesh/unlearn_diff/outputs/saliency_unlearning/saliency_unlearning_Abstractionism_model.pth\", classifier_ckpt_path = \"/home/ubuntu/Projects/models/classifier_ckpt_path/style50_cls.pth\", reference_dir= \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\" ) evaluator.run() Run the script python evaluate.py Description of parameters in evaluation_config.yaml The evaluation_config.yaml file contains the necessary parameters for running the Saliency Unlearning evaluation framework. Below is a detailed description of each parameter along with examples. Model Configuration: model_config : Path to the YAML file specifying the model architecture and settings. Type: str Example: \"mu/algorithms/saliency_unlearning/configs/model_config.yaml\" ckpt_path : Path to the finetuned Stable Diffusion checkpoint file to be evaluated. Type: str Example: \"outputs/saliency_unlearning/finetuned_models/saliency_unlearning_Abstractionism_model.pth\" classification_model : Specifies the classification model used for evaluating the generated outputs. Type: str Example: \"vit_large_patch16_224\" model_ckpt_path: Path to pretrained Stable Diffusion model. Type : str Example : models/compvis/style50/compvis.ckpt Training and Sampling Parameters: theme : Specifies the theme or concept being evaluated for removal from the model's outputs. Type: str Example: \"Bricks\" devices : CUDA device IDs to be used for the evaluation process. Type: str Example: \"0\" cfg_text : Classifier-free guidance scale value for image generation. Higher values increase the strength of the conditioning prompt. Type: float Example: 9.0 seed : Random seed for reproducibility of results. Type: int Example: 188 ddim_steps : Number of steps for the DDIM (Denoising Diffusion Implicit Models) sampling process. Type: int Example: 100 ddim_eta : DDIM eta value for controlling the amount of randomness during sampling. Set to 0 for deterministic sampling. Type: float Example: 0.0 image_height : Height of the generated images in pixels. Type: int Example: 512 image_width : Width of the generated images in pixels. Type: int Example: 512 Output and Logging Parameters: sampler_output_dir : Directory where generated images will be saved during evaluation. Type: str Example: \"outputs/eval_results/mu_results/saliency_unlearning/\" eval_output_dir : Directory where evaluation metrics and results will be stored. Type: str Example: \"outputs/eval_results/mu_results/saliency_unlearning/\" reference_dir : Directory containing original images for comparison during evaluation. Type: str Example: \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\" Performance and Efficiency Parameters: multiprocessing : Enables multiprocessing for faster evaluation for FID score. Recommended for large datasets. Type: bool Example: False batch_size : Batch size used during FID computation and evaluation. Type: int Example: 16 Optimization Parameters: forget_theme : Concept or style intended for removal in the evaluation process. Type: str Example: \"Bricks\" seed_list : List of random seeds for performing multiple evaluations with different randomness levels. Type: list Example: [\"188\"]","title":"Evaluation"},{"location":"evaluation/saliency/#saliency-unlearning-evaluation-framework","text":"This section provides instructions for running the evaluation framework for the Saliency Unlearning algorithm on Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying machine unlearning.","title":"Saliency Unlearning Evaluation Framework"},{"location":"evaluation/saliency/#running-the-evaluation-framework","text":"You can run the evaluation framework using the evaluate.py script located in the mu/algorithms/saliency_unlearning/scripts/ directory.","title":"Running the Evaluation Framework"},{"location":"evaluation/saliency/#basic-command-to-run-evaluation","text":"conda activate <env_name> Add the following code to evaluate.py from mu.algorithms.saliency_unlearning import SaliencyUnlearningEvaluator from mu.algorithms.saliency_unlearning.configs import ( saliency_unlearning_evaluation_config ) evaluator = SaliencyUnlearningEvaluator( saliency_unlearning_evaluation_config, ckpt_path=\"/home/ubuntu/Projects/dipesh/unlearn_diff/outputs/saliency_unlearning/saliency_unlearning_Abstractionism_model.pth\", classifier_ckpt_path = \"/home/ubuntu/Projects/models/classifier_ckpt_path/style50_cls.pth\", reference_dir= \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\" ) evaluator.run() Run the script python evaluate.py","title":"Basic Command to Run Evaluation:"},{"location":"evaluation/saliency/#description-of-parameters-in-evaluation_configyaml","text":"The evaluation_config.yaml file contains the necessary parameters for running the Saliency Unlearning evaluation framework. Below is a detailed description of each parameter along with examples.","title":"Description of parameters in evaluation_config.yaml"},{"location":"evaluation/saliency/#model-configuration","text":"model_config : Path to the YAML file specifying the model architecture and settings. Type: str Example: \"mu/algorithms/saliency_unlearning/configs/model_config.yaml\" ckpt_path : Path to the finetuned Stable Diffusion checkpoint file to be evaluated. Type: str Example: \"outputs/saliency_unlearning/finetuned_models/saliency_unlearning_Abstractionism_model.pth\" classification_model : Specifies the classification model used for evaluating the generated outputs. Type: str Example: \"vit_large_patch16_224\" model_ckpt_path: Path to pretrained Stable Diffusion model. Type : str Example : models/compvis/style50/compvis.ckpt","title":"Model Configuration:"},{"location":"evaluation/saliency/#training-and-sampling-parameters","text":"theme : Specifies the theme or concept being evaluated for removal from the model's outputs. Type: str Example: \"Bricks\" devices : CUDA device IDs to be used for the evaluation process. Type: str Example: \"0\" cfg_text : Classifier-free guidance scale value for image generation. Higher values increase the strength of the conditioning prompt. Type: float Example: 9.0 seed : Random seed for reproducibility of results. Type: int Example: 188 ddim_steps : Number of steps for the DDIM (Denoising Diffusion Implicit Models) sampling process. Type: int Example: 100 ddim_eta : DDIM eta value for controlling the amount of randomness during sampling. Set to 0 for deterministic sampling. Type: float Example: 0.0 image_height : Height of the generated images in pixels. Type: int Example: 512 image_width : Width of the generated images in pixels. Type: int Example: 512","title":"Training and Sampling Parameters:"},{"location":"evaluation/saliency/#output-and-logging-parameters","text":"sampler_output_dir : Directory where generated images will be saved during evaluation. Type: str Example: \"outputs/eval_results/mu_results/saliency_unlearning/\" eval_output_dir : Directory where evaluation metrics and results will be stored. Type: str Example: \"outputs/eval_results/mu_results/saliency_unlearning/\" reference_dir : Directory containing original images for comparison during evaluation. Type: str Example: \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\"","title":"Output and Logging Parameters:"},{"location":"evaluation/saliency/#performance-and-efficiency-parameters","text":"multiprocessing : Enables multiprocessing for faster evaluation for FID score. Recommended for large datasets. Type: bool Example: False batch_size : Batch size used during FID computation and evaluation. Type: int Example: 16","title":"Performance and Efficiency Parameters:"},{"location":"evaluation/saliency/#optimization-parameters","text":"forget_theme : Concept or style intended for removal in the evaluation process. Type: str Example: \"Bricks\" seed_list : List of random seeds for performing multiple evaluations with different randomness levels. Type: list Example: [\"188\"]","title":"Optimization Parameters:"},{"location":"evaluation/scissorhands/","text":"Scissorshands Evaluation Framework This section provides instructions for running the evaluation framework for the Scissorshands algorithm on Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying machine unlearning. Running the Evaluation Framework You can run the evaluation framework using the evaluate.py script located in the mu/algorithms/scissorshands/scripts/ directory. Basic Command to Run Evaluation: conda activate <env_name> Add the following code to evaluate.py from mu.algorithms.scissorhands import ScissorHandsEvaluator from mu.algorithms.scissorhands.configs import ( scissorhands_evaluation_config ) evaluator = ScissorHandsEvaluator( scissorhands_evaluation_config, ckpt_path=\"/home/ubuntu/Projects/dipesh/unlearn_diff/outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", classifier_ckpt_path = \"/home/ubuntu/Projects/models/classifier_ckpt_path/style50_cls.pth\", reference_dir= \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\" ) evaluator.run() Run the script python evaluate.py Running in Offline Mode: WANDB_MODE=offline python -m mu.algorithms.scissorhands.scripts.evaluate \\ --config_path mu/algorithms/scissorshands/configs/evaluation_config.yaml Example with CLI Overrides: python -m mu.algorithms.scissorshands.scripts.evaluate \\ --config_path mu/algorithms/scissorshands/configs/evaluation_config.yaml \\ --devices \"0\" \\ --seed 123 \\ --cfg_text 8.5 \\ --batch_size 16 Description of parameters in evaluation_config.yaml The evaluation_config.yaml file contains the necessary parameters for running the Scissorshands evaluation framework. Below is a detailed description of each parameter along with examples. Model Configuration: model_config : Path to the YAML file specifying the model architecture and settings. Type: str Example: \"mu/algorithms/scissorshands/configs/model_config.yaml\" ckpt_path : Path to the finetuned Stable Diffusion checkpoint file to be evaluated. Type: str Example: \"outputs/scissorshands/finetuned_models/scissorshands_Abstractionism_model.pth\" classification_model : Specifies the classification model used for evaluating the generated outputs. Type: str Example: \"vit_large_patch16_224\" Training and Sampling Parameters: theme : Specifies the theme or concept being evaluated for removal from the model's outputs. Type: str Example: \"Bricks\" devices : CUDA device IDs to be used for the evaluation process. Type: str Example: \"0\" cfg_text : Classifier-free guidance scale value for image generation. Higher values increase the strength of the conditioning prompt. Type: float Example: 9.0 seed : Random seed for reproducibility of results. Type: int Example: 188 ddim_steps : Number of steps for the DDIM (Denoising Diffusion Implicit Models) sampling process. Type: int Example: 100 ddim_eta : DDIM eta value for controlling the amount of randomness during sampling. Set to 0 for deterministic sampling. Type: float Example: 0.0 image_height : Height of the generated images in pixels. Type: int Example: 512 image_width : Width of the generated images in pixels. Type: int Example: 512 Output and Logging Parameters: sampler_output_dir : Directory where generated images will be saved during evaluation. Type: str Example: \"outputs/eval_results/mu_results/scissorshands/\" eval_output_dir : Directory where evaluation metrics and results will be stored. Type: str Example: \"outputs/eval_results/mu_results/scissorshands/\" reference_dir : Directory containing original images for comparison during evaluation. Type: str Example: \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\" Performance and Efficiency Parameters: multiprocessing : Enables multiprocessing for faster evaluation for FID score. Recommended for large datasets. Type: bool Example: False batch_size : Batch size used during FID computation and evaluation. Type: int Example: 16 Optimization Parameters: forget_theme : Concept or style intended for removal in the evaluation process. Type: str Example: \"Bricks\" seed_list : List of random seeds for performing multiple evaluations with different randomness levels. Type: list Example: [\"188\"]","title":"Evaluation"},{"location":"evaluation/scissorhands/#scissorshands-evaluation-framework","text":"This section provides instructions for running the evaluation framework for the Scissorshands algorithm on Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying machine unlearning.","title":"Scissorshands Evaluation Framework"},{"location":"evaluation/scissorhands/#running-the-evaluation-framework","text":"You can run the evaluation framework using the evaluate.py script located in the mu/algorithms/scissorshands/scripts/ directory.","title":"Running the Evaluation Framework"},{"location":"evaluation/scissorhands/#basic-command-to-run-evaluation","text":"conda activate <env_name> Add the following code to evaluate.py from mu.algorithms.scissorhands import ScissorHandsEvaluator from mu.algorithms.scissorhands.configs import ( scissorhands_evaluation_config ) evaluator = ScissorHandsEvaluator( scissorhands_evaluation_config, ckpt_path=\"/home/ubuntu/Projects/dipesh/unlearn_diff/outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", classifier_ckpt_path = \"/home/ubuntu/Projects/models/classifier_ckpt_path/style50_cls.pth\", reference_dir= \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\" ) evaluator.run() Run the script python evaluate.py Running in Offline Mode: WANDB_MODE=offline python -m mu.algorithms.scissorhands.scripts.evaluate \\ --config_path mu/algorithms/scissorshands/configs/evaluation_config.yaml Example with CLI Overrides: python -m mu.algorithms.scissorshands.scripts.evaluate \\ --config_path mu/algorithms/scissorshands/configs/evaluation_config.yaml \\ --devices \"0\" \\ --seed 123 \\ --cfg_text 8.5 \\ --batch_size 16","title":"Basic Command to Run Evaluation:"},{"location":"evaluation/scissorhands/#description-of-parameters-in-evaluation_configyaml","text":"The evaluation_config.yaml file contains the necessary parameters for running the Scissorshands evaluation framework. Below is a detailed description of each parameter along with examples.","title":"Description of parameters in evaluation_config.yaml"},{"location":"evaluation/scissorhands/#model-configuration","text":"model_config : Path to the YAML file specifying the model architecture and settings. Type: str Example: \"mu/algorithms/scissorshands/configs/model_config.yaml\" ckpt_path : Path to the finetuned Stable Diffusion checkpoint file to be evaluated. Type: str Example: \"outputs/scissorshands/finetuned_models/scissorshands_Abstractionism_model.pth\" classification_model : Specifies the classification model used for evaluating the generated outputs. Type: str Example: \"vit_large_patch16_224\"","title":"Model Configuration:"},{"location":"evaluation/scissorhands/#training-and-sampling-parameters","text":"theme : Specifies the theme or concept being evaluated for removal from the model's outputs. Type: str Example: \"Bricks\" devices : CUDA device IDs to be used for the evaluation process. Type: str Example: \"0\" cfg_text : Classifier-free guidance scale value for image generation. Higher values increase the strength of the conditioning prompt. Type: float Example: 9.0 seed : Random seed for reproducibility of results. Type: int Example: 188 ddim_steps : Number of steps for the DDIM (Denoising Diffusion Implicit Models) sampling process. Type: int Example: 100 ddim_eta : DDIM eta value for controlling the amount of randomness during sampling. Set to 0 for deterministic sampling. Type: float Example: 0.0 image_height : Height of the generated images in pixels. Type: int Example: 512 image_width : Width of the generated images in pixels. Type: int Example: 512","title":"Training and Sampling Parameters:"},{"location":"evaluation/scissorhands/#output-and-logging-parameters","text":"sampler_output_dir : Directory where generated images will be saved during evaluation. Type: str Example: \"outputs/eval_results/mu_results/scissorshands/\" eval_output_dir : Directory where evaluation metrics and results will be stored. Type: str Example: \"outputs/eval_results/mu_results/scissorshands/\" reference_dir : Directory containing original images for comparison during evaluation. Type: str Example: \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\"","title":"Output and Logging Parameters:"},{"location":"evaluation/scissorhands/#performance-and-efficiency-parameters","text":"multiprocessing : Enables multiprocessing for faster evaluation for FID score. Recommended for large datasets. Type: bool Example: False batch_size : Batch size used during FID computation and evaluation. Type: int Example: 16","title":"Performance and Efficiency Parameters:"},{"location":"evaluation/scissorhands/#optimization-parameters","text":"forget_theme : Concept or style intended for removal in the evaluation process. Type: str Example: \"Bricks\" seed_list : List of random seeds for performing multiple evaluations with different randomness levels. Type: list Example: [\"188\"]","title":"Optimization Parameters:"},{"location":"evaluation/semipermeable_membrane/","text":"Semipermeable membrane Evaluation Framework This section provides instructions for running the evaluation framework for the Semipermeable membrane algorithm on Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying machine unlearning. Running the Evaluation Framework You can run the evaluation framework using the evaluate.py script located in the mu/algorithms/semipermeable_membrane/scripts/ directory. Basic Command to Run Evaluation: conda activate <env_name> Add the following code to evaluate.py from mu.algorithms.semipermeable_membrane import SemipermeableMembraneEvaluator from mu.algorithms.semipermeable_membrane.configs import ( semipermeable_membrane_eval_config ) evaluator = SemipermeableMembraneEvaluator( semipermeable_membrane_eval_config, ckpt_path=\"/home/ubuntu/Projects/dipesh/unlearn_diff/outputs/semipermeable_membrane/finetuned_models/semipermeable_membrane_Abstractionism_last.safetensors\", spm_path = [\"/home/ubuntu/Projects/dipesh/unlearn_diff/outputs/semipermeable_membrane/finetuned_models/semipermeable_membrane_Abstractionism_last.safetensors\"], classifier_ckpt_path = \"/home/ubuntu/Projects/models/classifier_ckpt_path/style50_cls.pth\", reference_dir= \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\", model_config = \"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/mu_semipermeable_membrane_spm/configs\" ) evaluator.run() Run the script python evaluate.py Description of parameters in evaluation_config.yaml The evaluation_config.yaml file contains the necessary parameters for running the Semipermeable membrane evaluation framework. Below is a detailed description of each parameter along with examples. Model Configuration Parameters: spm_path: paths to finetuned model checkpoint. Type: list Example: outputs/semipermeable_membrane/finetuned_models/semipermeable_membrane_Abstractionism_last.safetensors base_model : Path to the pre-trained base model used for image generation. Type: str Example: \"path/to/base/model.pth\" precision : Specifies the numerical precision for model computation. Type: str Options: \"fp32\" , \"fp16\" , \"bf16\" Example: \"fp32\" spm_multiplier: Specifies the multiplier for Semipermeable Membrane (SPM) model. Type: float Example: 1.0 v2 : Specifies whether to use version 2.x of the model. Type: bool Example: false matching_metric : Metric used for evaluating the similarity between generated prompts and erased concepts. Type: str Options: \"clipcos\" , \"clipcos_tokenuni\" , \"tokenuni\" Example: \"clipcos_tokenuni\" model_config : Path to the model configuration YAML file. Type: str Example: \"mu/algorithms/semipermeable_membrane/config\" model_ckpt_path: Path to pretrained Stable Diffusion model. Type : str Example : models/diffuser/style50 Sampling Parameters: theme : Specifies the theme for the evaluation process. Type: str Example: \"Bricks\" seed : Random seed for reproducibility of the evaluation process. Type: int Example: 188 devices : Specifies the CUDA devices for running the model. Type: str (Comma-separated for multiple devices) Example: \"0\" task : Specifies the task type for the evaluation process. Type: str Options: \"class\" , \"style\" Example: \"class\" Output Parameters: sampler_output_dir : Directory where generated images will be saved during the sampling process. Type: str Example: \"outputs/eval_results/mu_results/semipermeable_membrane/\" eval_output_dir : Directory where evaluation metrics and results will be stored. Type: str Example: \"outputs/eval_results/mu_results/semipermeable_membrane/\" Dataset and Classification Parameters: reference_dir : Path to the reference dataset used for evaluation and comparison. Type: str Example: \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\" classification_model : Specifies the classification model used for the evaluation. Type: str Example: \"vit_large_patch16_224\" forget_theme : Specifies the theme to be forgotten during the unlearning process. Type: str Example: \"Bricks\" Performance Parameters: multiprocessing : Enables multiprocessing for faster evaluation. Type: bool Example: false seed_list : List of random seeds for multiple evaluation trials. Type: list Example: [\"188\"]","title":"Evaluation"},{"location":"evaluation/semipermeable_membrane/#semipermeable-membrane-evaluation-framework","text":"This section provides instructions for running the evaluation framework for the Semipermeable membrane algorithm on Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying machine unlearning.","title":"Semipermeable membrane Evaluation Framework"},{"location":"evaluation/semipermeable_membrane/#running-the-evaluation-framework","text":"You can run the evaluation framework using the evaluate.py script located in the mu/algorithms/semipermeable_membrane/scripts/ directory.","title":"Running the Evaluation Framework"},{"location":"evaluation/semipermeable_membrane/#basic-command-to-run-evaluation","text":"conda activate <env_name> Add the following code to evaluate.py from mu.algorithms.semipermeable_membrane import SemipermeableMembraneEvaluator from mu.algorithms.semipermeable_membrane.configs import ( semipermeable_membrane_eval_config ) evaluator = SemipermeableMembraneEvaluator( semipermeable_membrane_eval_config, ckpt_path=\"/home/ubuntu/Projects/dipesh/unlearn_diff/outputs/semipermeable_membrane/finetuned_models/semipermeable_membrane_Abstractionism_last.safetensors\", spm_path = [\"/home/ubuntu/Projects/dipesh/unlearn_diff/outputs/semipermeable_membrane/finetuned_models/semipermeable_membrane_Abstractionism_last.safetensors\"], classifier_ckpt_path = \"/home/ubuntu/Projects/models/classifier_ckpt_path/style50_cls.pth\", reference_dir= \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\", model_config = \"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/mu_semipermeable_membrane_spm/configs\" ) evaluator.run() Run the script python evaluate.py","title":"Basic Command to Run Evaluation:"},{"location":"evaluation/semipermeable_membrane/#description-of-parameters-in-evaluation_configyaml","text":"The evaluation_config.yaml file contains the necessary parameters for running the Semipermeable membrane evaluation framework. Below is a detailed description of each parameter along with examples.","title":"Description of parameters in evaluation_config.yaml"},{"location":"evaluation/semipermeable_membrane/#model-configuration-parameters","text":"spm_path: paths to finetuned model checkpoint. Type: list Example: outputs/semipermeable_membrane/finetuned_models/semipermeable_membrane_Abstractionism_last.safetensors base_model : Path to the pre-trained base model used for image generation. Type: str Example: \"path/to/base/model.pth\" precision : Specifies the numerical precision for model computation. Type: str Options: \"fp32\" , \"fp16\" , \"bf16\" Example: \"fp32\" spm_multiplier: Specifies the multiplier for Semipermeable Membrane (SPM) model. Type: float Example: 1.0 v2 : Specifies whether to use version 2.x of the model. Type: bool Example: false matching_metric : Metric used for evaluating the similarity between generated prompts and erased concepts. Type: str Options: \"clipcos\" , \"clipcos_tokenuni\" , \"tokenuni\" Example: \"clipcos_tokenuni\" model_config : Path to the model configuration YAML file. Type: str Example: \"mu/algorithms/semipermeable_membrane/config\" model_ckpt_path: Path to pretrained Stable Diffusion model. Type : str Example : models/diffuser/style50","title":"Model Configuration Parameters:"},{"location":"evaluation/semipermeable_membrane/#sampling-parameters","text":"theme : Specifies the theme for the evaluation process. Type: str Example: \"Bricks\" seed : Random seed for reproducibility of the evaluation process. Type: int Example: 188 devices : Specifies the CUDA devices for running the model. Type: str (Comma-separated for multiple devices) Example: \"0\" task : Specifies the task type for the evaluation process. Type: str Options: \"class\" , \"style\" Example: \"class\"","title":"Sampling Parameters:"},{"location":"evaluation/semipermeable_membrane/#output-parameters","text":"sampler_output_dir : Directory where generated images will be saved during the sampling process. Type: str Example: \"outputs/eval_results/mu_results/semipermeable_membrane/\" eval_output_dir : Directory where evaluation metrics and results will be stored. Type: str Example: \"outputs/eval_results/mu_results/semipermeable_membrane/\"","title":"Output Parameters:"},{"location":"evaluation/semipermeable_membrane/#dataset-and-classification-parameters","text":"reference_dir : Path to the reference dataset used for evaluation and comparison. Type: str Example: \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\" classification_model : Specifies the classification model used for the evaluation. Type: str Example: \"vit_large_patch16_224\" forget_theme : Specifies the theme to be forgotten during the unlearning process. Type: str Example: \"Bricks\"","title":"Dataset and Classification Parameters:"},{"location":"evaluation/semipermeable_membrane/#performance-parameters","text":"multiprocessing : Enables multiprocessing for faster evaluation. Type: bool Example: false seed_list : List of random seeds for multiple evaluation trials. Type: list Example: [\"188\"]","title":"Performance Parameters:"},{"location":"evaluation/uce/","text":"unified_concept_editing Evaluation Framework This section provides instructions for running the evaluation framework for the unified_concept_editing algorithm on Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying machine unlearning. Running the Evaluation Framework You can run the evaluation framework using the evaluate.py script located in the mu/algorithms/unified_concept_editing/scripts/ directory. Basic Command to Run Evaluation: conda activate <env_name> Add the following code to evaluate.py from mu.algorithms.unified_concept_editing import UnifiedConceptEditingEvaluator from mu.algorithms.unified_concept_editing.configs import ( uce_evaluation_config ) evaluator = UnifiedConceptEditingEvaluator( uce_evaluation_config, ckpt_path=\"/home/ubuntu/Projects/dipesh/unlearn_diff/outputs/uce/finetuned_models/uce_Abstractionism_model\", classifier_ckpt_path = \"/home/ubuntu/Projects/models/classifier_ckpt_path/style50_cls.pth\", reference_dir= \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\", pipeline_path = \"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/diffuser/style50\" ) evaluator.run() python evaluate.py Running in Offline Mode: WANDB_MODE=offline python -m mu.algorithms.unified_concept_editing.scripts.evaluate \\ --config_path mu/algorithms/unified_concept_editing/configs/evaluation_config.yaml Example with CLI Overrides: python -m mu.algorithms.unified_concept_editing.scripts.evaluate \\ --config_path mu/algorithms/unified_concept_editing/configs/evaluation_config.yaml \\ --devices \"0\" \\ --seed 123 \\ --cfg_text 8.5 \\ --batch_size 16 Description of parameters in evaluation_config.yaml The evaluation_config.yaml file contains the necessary parameters for running the unified_concept_editing evaluation framework. Below is a detailed description of each parameter along with examples. Model Configuration: pipeline_path: path to pipeline. Type : str Example : ckpts/sd_model/diffuser/style50/step19999/ ckpt_path : Path to the finetuned Stable Diffusion checkpoint file to be evaluated. Type: str Example: \"outputs/unified_concept_editing/finetuned_models/unified_concept_editing_Abstractionism_model.pth\" classification_model : Specifies the classification model used for evaluating the generated outputs. Type: str Example: \"vit_large_patch16_224\" model_ckpt_path: Path to pretrained Stable Diffusion model. Type : str Example : models/diffuser/style50 Training and Sampling Parameters: theme : Specifies the theme or concept being evaluated for removal from the model's outputs. Type: str Example: \"Bricks\" devices : CUDA device IDs to be used for the evaluation process. Type: str Example: \"0\" cfg_text : Classifier-free guidance scale value for image generation. Higher values increase the strength of the conditioning prompt. Type: float Example: 9.0 seed : Random seed for reproducibility of results. Type: int Example: 188 ddim_steps : Number of steps for the DDIM (Denoising Diffusion Implicit Models) sampling process. Type: int Example: 100 ddim_eta : DDIM eta value for controlling the amount of randomness during sampling. Set to 0 for deterministic sampling. Type: float Example: 0.0 image_height : Height of the generated images in pixels. Type: int Example: 512 image_width : Width of the generated images in pixels. Type: int Example: 512 Output and Logging Parameters: sampler_output_dir : Directory where generated images will be saved during evaluation. Type: str Example: \"outputs/eval_results/mu_results/unified_concept_editing/\" eval_output_dir : Directory where evaluation metrics and results will be stored. Type: str Example: \"outputs/eval_results/mu_results/unified_concept_editing/\" reference_dir : Directory containing original images for comparison during evaluation. Type: str Example: \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\" Performance and Efficiency Parameters: multiprocessing : Enables multiprocessing for faster evaluation for FID score. Recommended for large datasets. Type: bool Example: False batch_size : Batch size used during FID computation and evaluation. Type: int Example: 16 Optimization Parameters: forget_theme : Concept or style intended for removal in the evaluation process. Type: str Example: \"Bricks\" seed_list : List of random seeds for performing multiple evaluations with different randomness levels. Type: list Example: [\"188\"]","title":"Evaluation"},{"location":"evaluation/uce/#unified_concept_editing-evaluation-framework","text":"This section provides instructions for running the evaluation framework for the unified_concept_editing algorithm on Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying machine unlearning.","title":"unified_concept_editing Evaluation Framework"},{"location":"evaluation/uce/#running-the-evaluation-framework","text":"You can run the evaluation framework using the evaluate.py script located in the mu/algorithms/unified_concept_editing/scripts/ directory.","title":"Running the Evaluation Framework"},{"location":"evaluation/uce/#basic-command-to-run-evaluation","text":"conda activate <env_name> Add the following code to evaluate.py from mu.algorithms.unified_concept_editing import UnifiedConceptEditingEvaluator from mu.algorithms.unified_concept_editing.configs import ( uce_evaluation_config ) evaluator = UnifiedConceptEditingEvaluator( uce_evaluation_config, ckpt_path=\"/home/ubuntu/Projects/dipesh/unlearn_diff/outputs/uce/finetuned_models/uce_Abstractionism_model\", classifier_ckpt_path = \"/home/ubuntu/Projects/models/classifier_ckpt_path/style50_cls.pth\", reference_dir= \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\", pipeline_path = \"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/diffuser/style50\" ) evaluator.run() python evaluate.py Running in Offline Mode: WANDB_MODE=offline python -m mu.algorithms.unified_concept_editing.scripts.evaluate \\ --config_path mu/algorithms/unified_concept_editing/configs/evaluation_config.yaml Example with CLI Overrides: python -m mu.algorithms.unified_concept_editing.scripts.evaluate \\ --config_path mu/algorithms/unified_concept_editing/configs/evaluation_config.yaml \\ --devices \"0\" \\ --seed 123 \\ --cfg_text 8.5 \\ --batch_size 16","title":"Basic Command to Run Evaluation:"},{"location":"evaluation/uce/#description-of-parameters-in-evaluation_configyaml","text":"The evaluation_config.yaml file contains the necessary parameters for running the unified_concept_editing evaluation framework. Below is a detailed description of each parameter along with examples.","title":"Description of parameters in evaluation_config.yaml"},{"location":"evaluation/uce/#model-configuration","text":"pipeline_path: path to pipeline. Type : str Example : ckpts/sd_model/diffuser/style50/step19999/ ckpt_path : Path to the finetuned Stable Diffusion checkpoint file to be evaluated. Type: str Example: \"outputs/unified_concept_editing/finetuned_models/unified_concept_editing_Abstractionism_model.pth\" classification_model : Specifies the classification model used for evaluating the generated outputs. Type: str Example: \"vit_large_patch16_224\" model_ckpt_path: Path to pretrained Stable Diffusion model. Type : str Example : models/diffuser/style50","title":"Model Configuration:"},{"location":"evaluation/uce/#training-and-sampling-parameters","text":"theme : Specifies the theme or concept being evaluated for removal from the model's outputs. Type: str Example: \"Bricks\" devices : CUDA device IDs to be used for the evaluation process. Type: str Example: \"0\" cfg_text : Classifier-free guidance scale value for image generation. Higher values increase the strength of the conditioning prompt. Type: float Example: 9.0 seed : Random seed for reproducibility of results. Type: int Example: 188 ddim_steps : Number of steps for the DDIM (Denoising Diffusion Implicit Models) sampling process. Type: int Example: 100 ddim_eta : DDIM eta value for controlling the amount of randomness during sampling. Set to 0 for deterministic sampling. Type: float Example: 0.0 image_height : Height of the generated images in pixels. Type: int Example: 512 image_width : Width of the generated images in pixels. Type: int Example: 512","title":"Training and Sampling Parameters:"},{"location":"evaluation/uce/#output-and-logging-parameters","text":"sampler_output_dir : Directory where generated images will be saved during evaluation. Type: str Example: \"outputs/eval_results/mu_results/unified_concept_editing/\" eval_output_dir : Directory where evaluation metrics and results will be stored. Type: str Example: \"outputs/eval_results/mu_results/unified_concept_editing/\" reference_dir : Directory containing original images for comparison during evaluation. Type: str Example: \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\"","title":"Output and Logging Parameters:"},{"location":"evaluation/uce/#performance-and-efficiency-parameters","text":"multiprocessing : Enables multiprocessing for faster evaluation for FID score. Recommended for large datasets. Type: bool Example: False batch_size : Batch size used during FID computation and evaluation. Type: int Example: 16","title":"Performance and Efficiency Parameters:"},{"location":"evaluation/uce/#optimization-parameters","text":"forget_theme : Concept or style intended for removal in the evaluation process. Type: str Example: \"Bricks\" seed_list : List of random seeds for performing multiple evaluations with different randomness levels. Type: list Example: [\"188\"]","title":"Optimization Parameters:"},{"location":"mu_attack/Readme_MU_Attack/","text":"UnlearnDiffAttak This repository contains the implementation of UnlearnDiffAtk, a framework for evaluating the robustness of safety-driven unlearned Diffusion Models using adversarial prompts. Create Environment conda env create -f environment.yaml Generate Dataset python -m scripts.generate_dataset --prompts_path data/prompts/prompts.csv --concept i2p_nude --save_path outputs/dataset Description of fields in config json file for diffuser overall This section defines the high-level configuration for the attack. task : The name of the task being performed. Type: str Example: P4D, classifer attacker: Specifies the attack type. Type: str Example: hard_prompt, no_attack logger: Defines the logging mechanism. Type: str Example: JSON resume: Option to resume from previous checkpoint. task concept: The concept targeted by the attack. Type: str Example: nudity, harm diffusers_model_name_or_path: Path to the pre-trained checkpoint of the diffuser model. Type: str Example: \"outputs/semipermeable_membrane/finetuned_models/\" target_ckpt: Path to the target model checkpoint used in the attack. Type: str Example: \"files/pretrained/SD-1-4/ESD_ckpt/Nudity-ESDx1-UNET-SD.pt\" cache_path: Directory to cache intermediate results. Type: str Example: \".cache\" dataset_path: Path to the dataset used for the attack. Type: str Example: \"outputs/dataset/i2p_nude\" criterion: The loss function or criterion used during the attack. Type: str Example: \"l2\" classifier_dir: Directory for the classifier, if applicable. null if not used. Type: str Example: \"/path/classifier_dir\" sampling_step_num: Number of sampling steps during the attack. Type: int Example: 1 sld: Strength of latent disentanglement. Type: str Example: \"weak\" sld_concept: Concept tied to latent disentanglement. Type: str Example: \"nudity\" negative_prompt: The negative prompt used to steer the generation. Type: str Example: \"\" backend: Specifies the backend model i.e \"diffuser\". attacker insertion_location: The point of insertion for the prompt. Type: str Example: \"prefix_k\" k: The value of k for the prompt insertion point. Type: int Example: 5 iteration: Number of iterations for the attack. Type: int Example: 1 seed_iteration: Random seed for the iterative process. Type: int Example: 1 attack_idx: Index of the attack for evaluation purposes. Type: int Example: 0 eval_seed: Seed value used for evaluation. Type: int Example: 0 universal: Whether the attack is universal (true or false). Type: bool Example: false sequential: Whether the attack is applied sequentially. Type: bool Example: true lr: Learning rate for the attack optimization process. Type: float Example: 0.01 weight_decay: Weight decay applied during optimization. Type: float Example: 0.1 logger json: Logging configuration. root: Path to the directory where logs will be saved. Type: str Example: \"results/hard_prompt_esd_nudity_P4D\" name: Name for the log file or experiment. - Type: str - Example: \"P4D\" Description of fields in config json file for compvis overall This section defines the high-level configuration for the attack. task : The name of the task being performed. Type: str Example: P4D, classifer attacker: Specifies the attack type. Type: str Example: hard_prompt, no_attack logger: Defines the logging mechanism. Type: str Example: JSON resume: Option to resume from previous checkpoint. task concept: The concept targeted by the attack. Type: str Example: nudity, harm compvis_ckpt_path: Path to the pre-trained checkpoint of the CompVis model. Type: str Example: \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\" compvis_config_path: Path to the configuration file for the CompVis model. Type: str Example: \"configs/scissorhands/model_config.yaml\" cache_path: Directory to cache intermediate results. Type: str Example: \".cache\" dataset_path: Path to the dataset used for the attack. Type: str Example: \"outputs/dataset/i2p_nude\" criterion: The loss function or criterion used during the attack. Type: str Example: \"l2\" classifier_dir: Directory for the classifier, if applicable. null if not used. Type: str Example: \"/path/classifier_dir\" sampling_step_num: Number of sampling steps during the attack. Type: int Example: 1 sld: Strength of latent disentanglement. Type: str Example: \"weak\" sld_concept: Concept tied to latent disentanglement. Type: str Example: \"nudity\" negative_prompt: The negative prompt used to steer the generation. Type: str Example: \"\" backend: Specifies the backend model i.e \"compvis\". attacker insertion_location: The point of insertion for the prompt. Type: str Example: \"prefix_k\" k: The value of k for the prompt insertion point. Type: int Example: 5 iteration: Number of iterations for the attack. Type: int Example: 1 seed_iteration: Random seed for the iterative process. Type: int Example: 1 attack_idx: Index of the attack for evaluation purposes. Type: int Example: 0 eval_seed: Seed value used for evaluation. Type: int Example: 0 universal: Whether the attack is universal (true or false). Type: bool Example: false sequential: Whether the attack is applied sequentially. Type: bool Example: true lr: Learning rate for the attack optimization process. Type: float Example: 0.01 weight_decay: Weight decay applied during optimization. Type: float Example: 0.1 logger json: Logging configuration. root: Path to the directory where logs will be saved. Type: str Example: \"results/hard_prompt_esd_nudity_P4D\" name: Name for the log file or experiment. - Type: str - Example: \"P4D\" Run Attack Hard Prompt Attack compvis python -m mu_attack.execs.attack --config_path mu_attack/configs/nudity/hard_prompt_esd_nudity_P4D_compvis.json diffuser python -m mu_attack.execs.attack --config_path mu_attack/configs/nudity/hard_prompt_esd_nudity_P4D_diffuser.json Random Attack compvis python -m mu_attack.execs.attack --config_path mu_attack/configs/nudity/random_esd_nudity_compvis.json diffuser python -m mu_attack.execs.attack --config_path mu_attack/configs/nudity/random_esd_nudity_diffuser.json Seed Search compvis python -m mu_attack.execs.attack --config_path mu_attack/configs/nudity/seed_search_esd_nudity_classifier_compvis.json diffusers python -m mu_attack.execs.attack --config_path mu_attack/configs/nudity/seed_search_esd_nudity_classifier_diffuser.json Text Grad compvis python -m mu_attack.execs.attack --config_path mu_attack/configs/nudity/text_grad_esd_nudity_classifier_compvis.json diffusers python -m mu_attack.execs.attack --config_path mu_attack/configs/nudity/text_grad_esd_nudity_classifier_diffuser.json Run No Attack compvis python -m mu_attack.execs.attack --config_path mu_attack/configs/nudity/no_attack_esd_nudity_classifier_compvis.json diffusers python -m mu_attack.execs.attack --config_path mu_attack/configs/nudity/no_attack_esd_nudity_classifier_diffuser.json Mass Attack Attack using all attack ids, for a specific attack method. Run the following command to make the script executable: chmod +x scripts/mass_attack.sh Run the Script Execute the mass attack script using: scripts/mass_attack.sh mu_attack/configs/nudity/hard_prompt_esd_nudity_P4D.json Evaluation: In this section, we assess the performance and robustness of the results generated by the attack algorithms Running the Evaluation Framework Create a file, eg, evaluate.py and use examples and modify your configs to run the file. Example Code from mu_attack.evaluators.asr import ASREvaluator from mu_attack.evaluators.clip_score import ClipScoreEvaluator from mu_attack.evaluators.fid import FIDEvaluator from mu_attack.configs.evaluation import attack_evaluation_config def main(): # Initialize the configuration config = attack_evaluation_config config.asr.root = \"/home/ubuntu/Projects/Palistha/unlearn_diff_attack/results/hard_prompt_esd_nudity_P4D_concept_ablation/P4d\" config.asr.root_no_attack = \"/home/ubuntu/Projects/Palistha/unlearn_diff_attack/results/no_attack_esd_nudity/NoAttackEsdNudity\" config.clip.devices = \"0\" config.clip.image_path = \"/home/ubuntu/Projects/Palistha/unlearn_diff_attack/results/hard_prompt_esd_nudity_P4D_concept_ablation/P4d/images\" config.clip.log_path = \"/home/ubuntu/Projects/Palistha/unlearn_diff_attack/results/hard_prompt_esd_nudity_P4D_concept_ablation/P4d/log.json\" config.fid.ref_batch_path = \"/home/ubuntu/Projects/Palistha/unlearn_diff_attack/results/hard_prompt_esd_nudity_P4D_concept_ablation/P4d/images\" config.fid.sample_batch_path = \"/home/ubuntu/Projects/balaram/unlearn_diff_attack/outputs/dataset/i2p_nude/imgs\" # Common output path config.output_path = \"/home/ubuntu/Projects/Palistha/unlearn_diff_attack/results/evaluation/results.json\" # Initialize and run the ASR evaluator asr_evaluator = ASREvaluator( config = attack_evaluation_config, root=config.asr.root, root_no_attack=config.asr.root_no_attack, output_path=config.output_path ) print(\"Running ASR Evaluator...\") asr_evaluator.run() # Initialize and run the CLIP Score evaluator clip_evaluator = ClipScoreEvaluator( config = attack_evaluation_config, image_path=config.clip.image_path, log_path=config.clip.log_path, output_path=config.output_path, devices = config.clip.devices ) print(\"Running CLIP Score Evaluator...\") clip_evaluator.run() # Initialize and run the FID evaluator fid_evaluator = FIDEvaluator( config = attack_evaluation_config, ref_batch_path=config.fid.ref_batch_path, sample_batch_path=config.fid.sample_batch_path, output_path=config.output_path ) print(\"Running FID Evaluator...\") fid_evaluator.run() if __name__ == \"__main__\": main() ] Running the Training Script in Offline Mode WANDB_MODE=offline python evaluate.py How It Works * Default Values: The script first loads default values from the evluation config file as in configs section. Parameter Overrides: Any parameters passed directly to the algorithm, overrides these configs. Final Configuration: The script merges the configs and convert them into dictionary to proceed with the evaluation. Evaluation Metrics: Attack Succes Rate (ASR) Fr\u00e9chet inception distance(FID): evaluate distributional quality of image generations, lower is better. CLIP score : measure contextual alignment with prompt descriptions, higher is better. Configuration File Structure for Evaluator ASR Evaluator Configuration root: Directory containing results with attack. root-no-attack: Directory containing results without attack. Clip Evaluator Configuration image_path: Path to the directory containing generated images to evaluate. devices: Device ID(s) to use for evaluation. Example: \"0\" for the first GPU or \"0,1\" for multiple GPUs. log_path: Path to the log file containing prompt for the generated images. model_name_or_path: Path or model name for the pre-trained CLIP model. Default is \"openai/clip-vit-base-patch32\". FID Evaluator Configuration ref_batch_path: Path to the directory containing reference images. sample_batch_path: Path to the directory containing generated/sample images. Global Configuration output_path: Path to save the evaluation results as a JSON file.","title":"Readme MU Attack"},{"location":"mu_attack/Readme_MU_Attack/#unlearndiffattak","text":"This repository contains the implementation of UnlearnDiffAtk, a framework for evaluating the robustness of safety-driven unlearned Diffusion Models using adversarial prompts.","title":"UnlearnDiffAttak"},{"location":"mu_attack/Readme_MU_Attack/#create-environment","text":"conda env create -f environment.yaml","title":"Create Environment"},{"location":"mu_attack/Readme_MU_Attack/#generate-dataset","text":"python -m scripts.generate_dataset --prompts_path data/prompts/prompts.csv --concept i2p_nude --save_path outputs/dataset","title":"Generate Dataset"},{"location":"mu_attack/Readme_MU_Attack/#description-of-fields-in-config-json-file-for-diffuser","text":"overall This section defines the high-level configuration for the attack. task : The name of the task being performed. Type: str Example: P4D, classifer attacker: Specifies the attack type. Type: str Example: hard_prompt, no_attack logger: Defines the logging mechanism. Type: str Example: JSON resume: Option to resume from previous checkpoint. task concept: The concept targeted by the attack. Type: str Example: nudity, harm diffusers_model_name_or_path: Path to the pre-trained checkpoint of the diffuser model. Type: str Example: \"outputs/semipermeable_membrane/finetuned_models/\" target_ckpt: Path to the target model checkpoint used in the attack. Type: str Example: \"files/pretrained/SD-1-4/ESD_ckpt/Nudity-ESDx1-UNET-SD.pt\" cache_path: Directory to cache intermediate results. Type: str Example: \".cache\" dataset_path: Path to the dataset used for the attack. Type: str Example: \"outputs/dataset/i2p_nude\" criterion: The loss function or criterion used during the attack. Type: str Example: \"l2\" classifier_dir: Directory for the classifier, if applicable. null if not used. Type: str Example: \"/path/classifier_dir\" sampling_step_num: Number of sampling steps during the attack. Type: int Example: 1 sld: Strength of latent disentanglement. Type: str Example: \"weak\" sld_concept: Concept tied to latent disentanglement. Type: str Example: \"nudity\" negative_prompt: The negative prompt used to steer the generation. Type: str Example: \"\" backend: Specifies the backend model i.e \"diffuser\". attacker insertion_location: The point of insertion for the prompt. Type: str Example: \"prefix_k\" k: The value of k for the prompt insertion point. Type: int Example: 5 iteration: Number of iterations for the attack. Type: int Example: 1 seed_iteration: Random seed for the iterative process. Type: int Example: 1 attack_idx: Index of the attack for evaluation purposes. Type: int Example: 0 eval_seed: Seed value used for evaluation. Type: int Example: 0 universal: Whether the attack is universal (true or false). Type: bool Example: false sequential: Whether the attack is applied sequentially. Type: bool Example: true lr: Learning rate for the attack optimization process. Type: float Example: 0.01 weight_decay: Weight decay applied during optimization. Type: float Example: 0.1 logger json: Logging configuration. root: Path to the directory where logs will be saved. Type: str Example: \"results/hard_prompt_esd_nudity_P4D\" name: Name for the log file or experiment. - Type: str - Example: \"P4D\"","title":"Description of fields in config json file for diffuser"},{"location":"mu_attack/Readme_MU_Attack/#description-of-fields-in-config-json-file-for-compvis","text":"overall This section defines the high-level configuration for the attack. task : The name of the task being performed. Type: str Example: P4D, classifer attacker: Specifies the attack type. Type: str Example: hard_prompt, no_attack logger: Defines the logging mechanism. Type: str Example: JSON resume: Option to resume from previous checkpoint. task concept: The concept targeted by the attack. Type: str Example: nudity, harm compvis_ckpt_path: Path to the pre-trained checkpoint of the CompVis model. Type: str Example: \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\" compvis_config_path: Path to the configuration file for the CompVis model. Type: str Example: \"configs/scissorhands/model_config.yaml\" cache_path: Directory to cache intermediate results. Type: str Example: \".cache\" dataset_path: Path to the dataset used for the attack. Type: str Example: \"outputs/dataset/i2p_nude\" criterion: The loss function or criterion used during the attack. Type: str Example: \"l2\" classifier_dir: Directory for the classifier, if applicable. null if not used. Type: str Example: \"/path/classifier_dir\" sampling_step_num: Number of sampling steps during the attack. Type: int Example: 1 sld: Strength of latent disentanglement. Type: str Example: \"weak\" sld_concept: Concept tied to latent disentanglement. Type: str Example: \"nudity\" negative_prompt: The negative prompt used to steer the generation. Type: str Example: \"\" backend: Specifies the backend model i.e \"compvis\". attacker insertion_location: The point of insertion for the prompt. Type: str Example: \"prefix_k\" k: The value of k for the prompt insertion point. Type: int Example: 5 iteration: Number of iterations for the attack. Type: int Example: 1 seed_iteration: Random seed for the iterative process. Type: int Example: 1 attack_idx: Index of the attack for evaluation purposes. Type: int Example: 0 eval_seed: Seed value used for evaluation. Type: int Example: 0 universal: Whether the attack is universal (true or false). Type: bool Example: false sequential: Whether the attack is applied sequentially. Type: bool Example: true lr: Learning rate for the attack optimization process. Type: float Example: 0.01 weight_decay: Weight decay applied during optimization. Type: float Example: 0.1 logger json: Logging configuration. root: Path to the directory where logs will be saved. Type: str Example: \"results/hard_prompt_esd_nudity_P4D\" name: Name for the log file or experiment. - Type: str - Example: \"P4D\"","title":"Description of fields in config json file for compvis"},{"location":"mu_attack/Readme_MU_Attack/#run-attack","text":"Hard Prompt Attack compvis python -m mu_attack.execs.attack --config_path mu_attack/configs/nudity/hard_prompt_esd_nudity_P4D_compvis.json diffuser python -m mu_attack.execs.attack --config_path mu_attack/configs/nudity/hard_prompt_esd_nudity_P4D_diffuser.json Random Attack compvis python -m mu_attack.execs.attack --config_path mu_attack/configs/nudity/random_esd_nudity_compvis.json diffuser python -m mu_attack.execs.attack --config_path mu_attack/configs/nudity/random_esd_nudity_diffuser.json Seed Search compvis python -m mu_attack.execs.attack --config_path mu_attack/configs/nudity/seed_search_esd_nudity_classifier_compvis.json diffusers python -m mu_attack.execs.attack --config_path mu_attack/configs/nudity/seed_search_esd_nudity_classifier_diffuser.json Text Grad compvis python -m mu_attack.execs.attack --config_path mu_attack/configs/nudity/text_grad_esd_nudity_classifier_compvis.json diffusers python -m mu_attack.execs.attack --config_path mu_attack/configs/nudity/text_grad_esd_nudity_classifier_diffuser.json","title":"Run Attack"},{"location":"mu_attack/Readme_MU_Attack/#run-no-attack","text":"compvis python -m mu_attack.execs.attack --config_path mu_attack/configs/nudity/no_attack_esd_nudity_classifier_compvis.json diffusers python -m mu_attack.execs.attack --config_path mu_attack/configs/nudity/no_attack_esd_nudity_classifier_diffuser.json","title":"Run No Attack"},{"location":"mu_attack/Readme_MU_Attack/#mass-attack","text":"Attack using all attack ids, for a specific attack method. Run the following command to make the script executable: chmod +x scripts/mass_attack.sh Run the Script Execute the mass attack script using: scripts/mass_attack.sh mu_attack/configs/nudity/hard_prompt_esd_nudity_P4D.json","title":"Mass Attack"},{"location":"mu_attack/Readme_MU_Attack/#evaluation","text":"In this section, we assess the performance and robustness of the results generated by the attack algorithms","title":"Evaluation:"},{"location":"mu_attack/Readme_MU_Attack/#running-the-evaluation-framework","text":"Create a file, eg, evaluate.py and use examples and modify your configs to run the file. Example Code from mu_attack.evaluators.asr import ASREvaluator from mu_attack.evaluators.clip_score import ClipScoreEvaluator from mu_attack.evaluators.fid import FIDEvaluator from mu_attack.configs.evaluation import attack_evaluation_config def main(): # Initialize the configuration config = attack_evaluation_config config.asr.root = \"/home/ubuntu/Projects/Palistha/unlearn_diff_attack/results/hard_prompt_esd_nudity_P4D_concept_ablation/P4d\" config.asr.root_no_attack = \"/home/ubuntu/Projects/Palistha/unlearn_diff_attack/results/no_attack_esd_nudity/NoAttackEsdNudity\" config.clip.devices = \"0\" config.clip.image_path = \"/home/ubuntu/Projects/Palistha/unlearn_diff_attack/results/hard_prompt_esd_nudity_P4D_concept_ablation/P4d/images\" config.clip.log_path = \"/home/ubuntu/Projects/Palistha/unlearn_diff_attack/results/hard_prompt_esd_nudity_P4D_concept_ablation/P4d/log.json\" config.fid.ref_batch_path = \"/home/ubuntu/Projects/Palistha/unlearn_diff_attack/results/hard_prompt_esd_nudity_P4D_concept_ablation/P4d/images\" config.fid.sample_batch_path = \"/home/ubuntu/Projects/balaram/unlearn_diff_attack/outputs/dataset/i2p_nude/imgs\" # Common output path config.output_path = \"/home/ubuntu/Projects/Palistha/unlearn_diff_attack/results/evaluation/results.json\" # Initialize and run the ASR evaluator asr_evaluator = ASREvaluator( config = attack_evaluation_config, root=config.asr.root, root_no_attack=config.asr.root_no_attack, output_path=config.output_path ) print(\"Running ASR Evaluator...\") asr_evaluator.run() # Initialize and run the CLIP Score evaluator clip_evaluator = ClipScoreEvaluator( config = attack_evaluation_config, image_path=config.clip.image_path, log_path=config.clip.log_path, output_path=config.output_path, devices = config.clip.devices ) print(\"Running CLIP Score Evaluator...\") clip_evaluator.run() # Initialize and run the FID evaluator fid_evaluator = FIDEvaluator( config = attack_evaluation_config, ref_batch_path=config.fid.ref_batch_path, sample_batch_path=config.fid.sample_batch_path, output_path=config.output_path ) print(\"Running FID Evaluator...\") fid_evaluator.run() if __name__ == \"__main__\": main() ] Running the Training Script in Offline Mode WANDB_MODE=offline python evaluate.py How It Works * Default Values: The script first loads default values from the evluation config file as in configs section. Parameter Overrides: Any parameters passed directly to the algorithm, overrides these configs. Final Configuration: The script merges the configs and convert them into dictionary to proceed with the evaluation. Evaluation Metrics: Attack Succes Rate (ASR) Fr\u00e9chet inception distance(FID): evaluate distributional quality of image generations, lower is better. CLIP score : measure contextual alignment with prompt descriptions, higher is better. Configuration File Structure for Evaluator ASR Evaluator Configuration root: Directory containing results with attack. root-no-attack: Directory containing results without attack. Clip Evaluator Configuration image_path: Path to the directory containing generated images to evaluate. devices: Device ID(s) to use for evaluation. Example: \"0\" for the first GPU or \"0,1\" for multiple GPUs. log_path: Path to the log file containing prompt for the generated images. model_name_or_path: Path or model name for the pre-trained CLIP model. Default is \"openai/clip-vit-base-patch32\". FID Evaluator Configuration ref_batch_path: Path to the directory containing reference images. sample_batch_path: Path to the directory containing generated/sample images. Global Configuration output_path: Path to save the evaluation results as a JSON file.","title":"Running the Evaluation Framework"},{"location":"mu_attack/mu_attack_evaluation/","text":"Evaluation: In this section, we assess the performance and robustness of the results generated by the attack algorithms Activate Environment You can either use default environment or mu_attack specific environment. Example create_env mu_attack Running the Evaluation Framework Create a file, eg, evaluate.py and use examples and modify your configs to run the file. Example Code from mu_attack.configs.evaluation import attack_evaluation_config from mu_attack.execs.evaluator import MuAttackEvaluator def main(): config = attack_evaluation_config config = attack_evaluation_config config.asr.root = \"results/hard_prompt_esd_nudity_P4D_abstractionism/P4d\" config.asr.root_no_attack = \"results/hard_prompt_esd_nudity_P4D_abstrc/NoAttackEsdNudity\" config.clip.devices = \"0\" config.clip.image_path = \"results/hard_prompt_esd_nudity_P4D_abstractionism/P4d/images\" config.clip.log_path = \"results/hard_prompt_esd_nudity_P4D_abstractionism/P4d/log.json\" config.fid.ref_batch_path = \"results/hard_prompt_esd_nudity_P4D_abstractionism/P4d/images\" config.fid.sample_batch_path = \"data/i2p/nude\" # Common output path config.output_path = \"results/evaluation/results.json\" evaluator = MuAttackEvaluator(config) # Run the evaluation (this will run ASR, CLIP, and FID evaluators) results = evaluator.run() print(\"Evaluation Results:\",results) if __name__ == \"__main__\": main() Running the Training Script in Offline Mode WANDB_MODE=offline python evaluate.py How It Works * Default Values: The script first loads default values from the evluation config file as in configs section. Parameter Overrides: Any parameters passed directly to the algorithm, overrides these configs. Final Configuration: The script merges the configs and convert them into dictionary to proceed with the evaluation. Evaluation Metrics: Attack Succes Rate (ASR) Fr\u00e9chet inception distance(FID): evaluate distributional quality of image generations, lower is better. CLIP score : measure contextual alignment with prompt descriptions, higher is better. Configuration File Structure for Evaluator ASR Evaluator Configuration root: Directory containing results with attack. root-no-attack: Directory containing results without attack. Clip Evaluator Configuration image_path: Path to the directory containing generated images to evaluate. devices: Device ID(s) to use for evaluation. Example: \"0\" for the first GPU or \"0,1\" for multiple GPUs. log_path: Path to the log file containing prompt for the generated images. model_name_or_path: Path or model name for the pre-trained CLIP model. Default is \"openai/clip-vit-base-patch32\". FID Evaluator Configuration ref_batch_path: Path to the directory containing reference images. sample_batch_path: Path to the directory containing generated/sample images. Global Configuration output_path: Path to save the evaluation results as a JSON file.","title":"Usage"},{"location":"mu_attack/mu_attack_evaluation/#evaluation","text":"In this section, we assess the performance and robustness of the results generated by the attack algorithms","title":"Evaluation:"},{"location":"mu_attack/mu_attack_evaluation/#activate-environment","text":"You can either use default environment or mu_attack specific environment. Example create_env mu_attack","title":"Activate Environment"},{"location":"mu_attack/mu_attack_evaluation/#running-the-evaluation-framework","text":"Create a file, eg, evaluate.py and use examples and modify your configs to run the file. Example Code from mu_attack.configs.evaluation import attack_evaluation_config from mu_attack.execs.evaluator import MuAttackEvaluator def main(): config = attack_evaluation_config config = attack_evaluation_config config.asr.root = \"results/hard_prompt_esd_nudity_P4D_abstractionism/P4d\" config.asr.root_no_attack = \"results/hard_prompt_esd_nudity_P4D_abstrc/NoAttackEsdNudity\" config.clip.devices = \"0\" config.clip.image_path = \"results/hard_prompt_esd_nudity_P4D_abstractionism/P4d/images\" config.clip.log_path = \"results/hard_prompt_esd_nudity_P4D_abstractionism/P4d/log.json\" config.fid.ref_batch_path = \"results/hard_prompt_esd_nudity_P4D_abstractionism/P4d/images\" config.fid.sample_batch_path = \"data/i2p/nude\" # Common output path config.output_path = \"results/evaluation/results.json\" evaluator = MuAttackEvaluator(config) # Run the evaluation (this will run ASR, CLIP, and FID evaluators) results = evaluator.run() print(\"Evaluation Results:\",results) if __name__ == \"__main__\": main() Running the Training Script in Offline Mode WANDB_MODE=offline python evaluate.py How It Works * Default Values: The script first loads default values from the evluation config file as in configs section. Parameter Overrides: Any parameters passed directly to the algorithm, overrides these configs. Final Configuration: The script merges the configs and convert them into dictionary to proceed with the evaluation. Evaluation Metrics: Attack Succes Rate (ASR) Fr\u00e9chet inception distance(FID): evaluate distributional quality of image generations, lower is better. CLIP score : measure contextual alignment with prompt descriptions, higher is better. Configuration File Structure for Evaluator ASR Evaluator Configuration root: Directory containing results with attack. root-no-attack: Directory containing results without attack. Clip Evaluator Configuration image_path: Path to the directory containing generated images to evaluate. devices: Device ID(s) to use for evaluation. Example: \"0\" for the first GPU or \"0,1\" for multiple GPUs. log_path: Path to the log file containing prompt for the generated images. model_name_or_path: Path or model name for the pre-trained CLIP model. Default is \"openai/clip-vit-base-patch32\". FID Evaluator Configuration ref_batch_path: Path to the directory containing reference images. sample_batch_path: Path to the directory containing generated/sample images. Global Configuration output_path: Path to save the evaluation results as a JSON file.","title":"Running the Evaluation Framework"},{"location":"mu_attack/attack/hard_prompt/","text":"UnlearnDiffAttak This repository contains the implementation of UnlearnDiffAttack for hard prompt attack, a framework for evaluating the robustness of safety-driven unlearned Models using adversarial prompts. Create Environment If you prefer not to use the default environment, you can create and activate a dedicated environment for mu_attack. create_env <algorithm_name> Example create_env mu_attack Generate Dataset python -m scripts.generate_dataset --prompts_path data/prompts/prompts.csv --concept i2p_nude --save_path outputs/dataset Run Attack Hard Prompt Attack - compvis Use the following code if you wish to run the hard prompt attack using the CompVis model directly (without converting it into Diffusers format): from mu_attack.configs.nudity import hard_prompt_esd_nudity_P4D_compvis_config from mu_attack.execs.attack import MUAttack from mu.algorithms.scissorhands.configs import scissorhands_train_mu def run_attack_for_nudity(): overridable_params = { \"task.compvis_ckpt_path\":\"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\": scissorhands_train_mu.model_config_path, \"task.dataset_path\":\"outputs/dataset/i2p_nude\", \"logger.json.root\":\"results/hard_prompt_esd_nudity_P4D_scissorhands\", } MUAttack( config=hard_prompt_esd_nudity_P4D_compvis_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() Hard Prompt Attack \u2013 CompVis to Diffusers Conversion If you want to convert the CompVis model into the Diffusers format before running the attack, use the following code. Note: For the conversion to take place, set task.save_diffuser to True and to use the converted model task.sld should be set to None. from mu_attack.configs.nudity import hard_prompt_esd_nudity_P4D_compvis_config from mu_attack.execs.attack import MUAttack from mu.algorithms.scissorhands.configs import scissorhands_train_mu def run_attack_for_nudity(): overridable_params = { \"task.compvis_ckpt_path\":\"/home/ubuntu/Projects/dipesh/unlearn_diff/outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\": scissorhands_train_mu.model_config_path, \"task.dataset_path\":\"/home/ubuntu/Projects/Palistha/unlearn_diff_attack/outputs/dataset/i2p_nude\", \"logger.json.root\":\"results/hard_prompt_esd_nudity_P4D_scissorhands\", \"task.save_diffuser\": True, # This flag triggers conversion \"task.sld\": None, # Set sld to None for conversion \"task.model_name\": \"SD-v1-4\" } MUAttack( config=hard_prompt_esd_nudity_P4D_compvis_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() For Conversion: When converting a CompVis model to the Diffusers format, ensure that task.save_diffuser is set to True and task.sld is set to None. This instructs the pipeline to perform the conversion during initialization and then load the converted checkpoint. Code Explanation & Important Notes from mu_attack.configs.nudity import hard_prompt_esd_nudity_P4D_compvis_config \u2192 This imports the predefined Hard Prompt Attack configuration for nudity unlearning in the CompVis model. It sets up the attack parameters and methodologies. from mu.algorithms.scissorhands.configs import scissorhands_train_mu \u2192 Imports the Scissorhands model configuration, required to set the task.compvis_config_path parameter correctly. Overriding Parameters in JSON Configuration The overridable_params dictionary allows dynamic modification of parameters defined in the JSON configuration. This enables users to override default values by passing them as arguments. Example usage overridable_params = { \"task.compvis_ckpt_path\": \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\": scissorhands_train_mu.model_config_path, # Overrides model config \"task.dataset_path\": \"outputs/dataset/i2p_nude\", # Overrides dataset path \"logger.json.root\": \"results/hard_prompt_esd_nudity_P4D_scissorhands\", # Overrides logging path \"attacker.k\" = 3, } Hard Prompt Attack - diffuser from mu_attack.configs.nudity import hard_prompt_esd_nudity_P4D_diffusers_config from mu_attack.execs.attack import MUAttack def run_attack_for_nudity(): overridable_params = { \"task.diffusers_model_name_or_path\" : \"outputs/forget_me_not/finetuned_models/Abstractionism\", \"task.dataset_path\" : \"outputs/dataset/i2p_nude\", \"logger.json.root\" :\"results/hard_prompt_esd_nudity_P4D_abstractionism\" } MUAttack( config=hard_prompt_esd_nudity_P4D_diffusers_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() Code Explanation & Important Notes from mu_attack.configs.nudity import hard_prompt_esd_nudity_P4D_diffusers_config \u2192 This imports the predefined Hard Prompt Attack configuration for nudity unlearning in the diffusers model. It sets up the attack parameters and methodologies. Description of fields in config file overall This section defines the high-level configuration for the attack. task : The name of the task being performed. Type: str Example: classifer attacker: Specifies the attack type. Type: str Example: no_attack logger: Defines the logging mechanism. Type: str Example: JSON resume: Option to resume from previous checkpoint. task concept: The concept targeted by the attack. Type: str Example: nudity, harm diffusers_model_name_or_path: Path to the pre-trained checkpoint of the diffuser model. (For diffuser) Type: str Example: \"outputs/semipermeable_membrane/finetuned_models/\" target_ckpt: Path to the target model checkpoint used in the attack. (For diffuser) Type: str Example: \"files/pretrained/SD-1-4/ESD_ckpt/Nudity-ESDx1-UNET-SD.pt\" compvis_ckpt_path: Path to the pre-trained checkpoint of the CompVis model. (For compvis) Type: str Example: \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\" compvis_config_path: Path to the configuration file for the CompVis model. (For compvis) Type: str Example: \"configs/scissorhands/model_config.yaml\" cache_path: Directory to cache intermediate results. Type: str Example: \".cache\" dataset_path: Path to the dataset used for the attack. Type: str Example: \"outputs/dataset/i2p_nude\" criterion: The loss function or criterion used during the attack. Type: str Example: \"l2\" classifier_dir: Directory for the classifier, if applicable. null if not used. Type: str Example: \"/path/classifier_dir\" sampling_step_num: Number of sampling steps during the attack. Type: int Example: 1 sld: Strength of latent disentanglement. Type: str Example: \"weak\" sld_concept: Concept tied to latent disentanglement. Type: str Example: \"nudity\" negative_prompt: The negative prompt used to steer the generation. Type: str Example: \"\" model_name: Name of the model. The model_name parameter determines which base Stable Diffusion model is used by the pipeline. Type: str Example: \"SD-v1-4\" Choices: \"SD-v1-4\", \"SD-V2\", \"SD-V2-1\" save_diffuser: A Boolean flag that determines whether the CompVis model should be converted into the Diffusers format before being used. Type: str Example: True Behavior: * If set to True, the pipeline will perform a conversion of the CompVis model into the Diffusers format and then load the converted checkpoint. If set to False, the conversion is skipped and the model remains in its original CompVis format for use and uses compvis based implementation. converted_model_folder_path: Folder path to save the converted compvis model to diffuser. Type: str Example: \"outputs\" backend: Specifies the backend model i.e \"diffusers\". Type: str Options: \"diffusers\" or \"compvis\" attacker insertion_location: The point of insertion for the prompt. Type: str Example: \"prefix_k\" k: The value of k for the prompt insertion point. Type: int Example: 5 iteration: Number of iterations for the attack. Type: int Example: 1 seed_iteration: Random seed for the iterative process. Type: int Example: 1 attack_idx: Index of the attack for evaluation purposes. Type: int Example: 0 eval_seed: Seed value used for evaluation. Type: int Example: 0 universal: Whether the attack is universal (true or false). Type: bool Example: false sequential: Whether the attack is applied sequentially. Type: bool Example: true lr: Learning rate for the attack optimization process. Type: float Example: 0.01 weight_decay: Weight decay applied during optimization. Type: float Example: 0.1 logger json: Logging configuration. root: Path to the directory where logs will be saved. Type: str Example: \"results/hard_prompt_esd_nudity_P4D\" name: Name for the log file or experiment. Type: str Example: \"P4D\" Example usage: { \"json\": { \"root\": \"results/no_attack_esd_nudity_esd\", \"name\": \"P4D\" } }","title":"Usage"},{"location":"mu_attack/attack/hard_prompt/#unlearndiffattak","text":"This repository contains the implementation of UnlearnDiffAttack for hard prompt attack, a framework for evaluating the robustness of safety-driven unlearned Models using adversarial prompts.","title":"UnlearnDiffAttak"},{"location":"mu_attack/attack/hard_prompt/#create-environment","text":"If you prefer not to use the default environment, you can create and activate a dedicated environment for mu_attack. create_env <algorithm_name> Example create_env mu_attack","title":"Create Environment"},{"location":"mu_attack/attack/hard_prompt/#generate-dataset","text":"python -m scripts.generate_dataset --prompts_path data/prompts/prompts.csv --concept i2p_nude --save_path outputs/dataset","title":"Generate Dataset"},{"location":"mu_attack/attack/hard_prompt/#run-attack","text":"Hard Prompt Attack - compvis Use the following code if you wish to run the hard prompt attack using the CompVis model directly (without converting it into Diffusers format): from mu_attack.configs.nudity import hard_prompt_esd_nudity_P4D_compvis_config from mu_attack.execs.attack import MUAttack from mu.algorithms.scissorhands.configs import scissorhands_train_mu def run_attack_for_nudity(): overridable_params = { \"task.compvis_ckpt_path\":\"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\": scissorhands_train_mu.model_config_path, \"task.dataset_path\":\"outputs/dataset/i2p_nude\", \"logger.json.root\":\"results/hard_prompt_esd_nudity_P4D_scissorhands\", } MUAttack( config=hard_prompt_esd_nudity_P4D_compvis_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() Hard Prompt Attack \u2013 CompVis to Diffusers Conversion If you want to convert the CompVis model into the Diffusers format before running the attack, use the following code. Note: For the conversion to take place, set task.save_diffuser to True and to use the converted model task.sld should be set to None. from mu_attack.configs.nudity import hard_prompt_esd_nudity_P4D_compvis_config from mu_attack.execs.attack import MUAttack from mu.algorithms.scissorhands.configs import scissorhands_train_mu def run_attack_for_nudity(): overridable_params = { \"task.compvis_ckpt_path\":\"/home/ubuntu/Projects/dipesh/unlearn_diff/outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\": scissorhands_train_mu.model_config_path, \"task.dataset_path\":\"/home/ubuntu/Projects/Palistha/unlearn_diff_attack/outputs/dataset/i2p_nude\", \"logger.json.root\":\"results/hard_prompt_esd_nudity_P4D_scissorhands\", \"task.save_diffuser\": True, # This flag triggers conversion \"task.sld\": None, # Set sld to None for conversion \"task.model_name\": \"SD-v1-4\" } MUAttack( config=hard_prompt_esd_nudity_P4D_compvis_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() For Conversion: When converting a CompVis model to the Diffusers format, ensure that task.save_diffuser is set to True and task.sld is set to None. This instructs the pipeline to perform the conversion during initialization and then load the converted checkpoint. Code Explanation & Important Notes from mu_attack.configs.nudity import hard_prompt_esd_nudity_P4D_compvis_config \u2192 This imports the predefined Hard Prompt Attack configuration for nudity unlearning in the CompVis model. It sets up the attack parameters and methodologies. from mu.algorithms.scissorhands.configs import scissorhands_train_mu \u2192 Imports the Scissorhands model configuration, required to set the task.compvis_config_path parameter correctly. Overriding Parameters in JSON Configuration The overridable_params dictionary allows dynamic modification of parameters defined in the JSON configuration. This enables users to override default values by passing them as arguments. Example usage overridable_params = { \"task.compvis_ckpt_path\": \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\": scissorhands_train_mu.model_config_path, # Overrides model config \"task.dataset_path\": \"outputs/dataset/i2p_nude\", # Overrides dataset path \"logger.json.root\": \"results/hard_prompt_esd_nudity_P4D_scissorhands\", # Overrides logging path \"attacker.k\" = 3, } Hard Prompt Attack - diffuser from mu_attack.configs.nudity import hard_prompt_esd_nudity_P4D_diffusers_config from mu_attack.execs.attack import MUAttack def run_attack_for_nudity(): overridable_params = { \"task.diffusers_model_name_or_path\" : \"outputs/forget_me_not/finetuned_models/Abstractionism\", \"task.dataset_path\" : \"outputs/dataset/i2p_nude\", \"logger.json.root\" :\"results/hard_prompt_esd_nudity_P4D_abstractionism\" } MUAttack( config=hard_prompt_esd_nudity_P4D_diffusers_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() Code Explanation & Important Notes from mu_attack.configs.nudity import hard_prompt_esd_nudity_P4D_diffusers_config \u2192 This imports the predefined Hard Prompt Attack configuration for nudity unlearning in the diffusers model. It sets up the attack parameters and methodologies.","title":"Run Attack"},{"location":"mu_attack/attack/hard_prompt/#description-of-fields-in-config-file","text":"overall This section defines the high-level configuration for the attack. task : The name of the task being performed. Type: str Example: classifer attacker: Specifies the attack type. Type: str Example: no_attack logger: Defines the logging mechanism. Type: str Example: JSON resume: Option to resume from previous checkpoint. task concept: The concept targeted by the attack. Type: str Example: nudity, harm diffusers_model_name_or_path: Path to the pre-trained checkpoint of the diffuser model. (For diffuser) Type: str Example: \"outputs/semipermeable_membrane/finetuned_models/\" target_ckpt: Path to the target model checkpoint used in the attack. (For diffuser) Type: str Example: \"files/pretrained/SD-1-4/ESD_ckpt/Nudity-ESDx1-UNET-SD.pt\" compvis_ckpt_path: Path to the pre-trained checkpoint of the CompVis model. (For compvis) Type: str Example: \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\" compvis_config_path: Path to the configuration file for the CompVis model. (For compvis) Type: str Example: \"configs/scissorhands/model_config.yaml\" cache_path: Directory to cache intermediate results. Type: str Example: \".cache\" dataset_path: Path to the dataset used for the attack. Type: str Example: \"outputs/dataset/i2p_nude\" criterion: The loss function or criterion used during the attack. Type: str Example: \"l2\" classifier_dir: Directory for the classifier, if applicable. null if not used. Type: str Example: \"/path/classifier_dir\" sampling_step_num: Number of sampling steps during the attack. Type: int Example: 1 sld: Strength of latent disentanglement. Type: str Example: \"weak\" sld_concept: Concept tied to latent disentanglement. Type: str Example: \"nudity\" negative_prompt: The negative prompt used to steer the generation. Type: str Example: \"\" model_name: Name of the model. The model_name parameter determines which base Stable Diffusion model is used by the pipeline. Type: str Example: \"SD-v1-4\" Choices: \"SD-v1-4\", \"SD-V2\", \"SD-V2-1\" save_diffuser: A Boolean flag that determines whether the CompVis model should be converted into the Diffusers format before being used. Type: str Example: True Behavior: * If set to True, the pipeline will perform a conversion of the CompVis model into the Diffusers format and then load the converted checkpoint. If set to False, the conversion is skipped and the model remains in its original CompVis format for use and uses compvis based implementation. converted_model_folder_path: Folder path to save the converted compvis model to diffuser. Type: str Example: \"outputs\" backend: Specifies the backend model i.e \"diffusers\". Type: str Options: \"diffusers\" or \"compvis\" attacker insertion_location: The point of insertion for the prompt. Type: str Example: \"prefix_k\" k: The value of k for the prompt insertion point. Type: int Example: 5 iteration: Number of iterations for the attack. Type: int Example: 1 seed_iteration: Random seed for the iterative process. Type: int Example: 1 attack_idx: Index of the attack for evaluation purposes. Type: int Example: 0 eval_seed: Seed value used for evaluation. Type: int Example: 0 universal: Whether the attack is universal (true or false). Type: bool Example: false sequential: Whether the attack is applied sequentially. Type: bool Example: true lr: Learning rate for the attack optimization process. Type: float Example: 0.01 weight_decay: Weight decay applied during optimization. Type: float Example: 0.1 logger json: Logging configuration. root: Path to the directory where logs will be saved. Type: str Example: \"results/hard_prompt_esd_nudity_P4D\" name: Name for the log file or experiment. Type: str Example: \"P4D\" Example usage: { \"json\": { \"root\": \"results/no_attack_esd_nudity_esd\", \"name\": \"P4D\" } }","title":"Description of fields in config file"},{"location":"mu_attack/attack/no_attack/","text":"UnlearnDiffAttak This repository contains the implementation of UnlearnDiffAttack for No-attack, a framework for evaluating the robustness of safety-driven unlearned Models using adversarial prompts. Create Environment If you prefer not to use the default environment, you can create and activate a dedicated environment for mu_attack. create_env <algorithm_name> Example create_env mu_attack Generate Dataset python -m scripts.generate_dataset --prompts_path data/prompts/prompts.csv --concept i2p_nude --save_path outputs/dataset Run Attack No Attack - compvis Use the following code if you wish to run the hard prompt attack using the CompVis model directly (without converting it into Diffusers format): from mu_attack.configs.nudity import no_attack_esd_nudity_classifier_compvis_config from mu_attack.execs.attack import MUAttack from mu.algorithms.scissorhands.configs import scissorhands_train_mu def run_attack_for_nudity(): overridable_params = { \"task.compvis_ckpt_path\" : \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\" : scissorhands_train_mu.model_config_path, \"task.dataset_path\" : \"outputs/dataset/i2p_nude\", \"logger.json.root\" : \"results/hard_prompt_esd_nudity_P4D_scissorhands\", \"attacker.no_attack.dataset_path\" : \"outputs/dataset/i2p_nude\" } MUAttack( config=no_attack_esd_nudity_classifier_compvis_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() No Attack \u2013 CompVis to Diffusers Conversion If you want to convert the CompVis model into the Diffusers format before running the attack, use the following code. Note: For the conversion to take place, set task.save_diffuser to True and to use the converted model task.sld should be set to None. from mu_attack.configs.nudity import no_attack_esd_nudity_classifier_compvis_config from mu_attack.execs.attack import MUAttack from mu.algorithms.scissorhands.configs import scissorhands_train_mu def run_attack_for_nudity(): overridable_params = { \"task.compvis_ckpt_path\" : \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\" : scissorhands_train_mu.model_config_path, \"task.dataset_path\" : \"outputs/dataset/i2p_nude\", \"logger.json.root\" : \"results/hard_prompt_esd_nudity_P4D_scissorhands\", \"attacker.no_attack.dataset_path\" : \"outputs/dataset/i2p_nude\", \"task.save_diffuser\": True, # This flag triggers conversion \"task.sld\": None, # Set sld to None for conversion \"task.model_name\": \"SD-v1-4\" } MUAttack( config=no_attack_esd_nudity_classifier_compvis_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() For Conversion: When converting a CompVis model to the Diffusers format, ensure that task.save_diffuser is set to True and task.sld is set to None. This instructs the pipeline to perform the conversion during initialization and then load the converted checkpoint. Code Explanation & Important Notes from mu_attack.configs.nudity import hard_prompt_esd_nudity_P4D_compvis_config \u2192 This imports the predefined Hard Prompt Attack configuration for nudity unlearning in the CompVis model. It sets up the attack parameters and methodologies. from mu.algorithms.scissorhands.configs import scissorhands_train_mu \u2192 Imports the Scissorhands model configuration, required to set the task.compvis_config_path parameter correctly. Overriding Parameters in JSON Configuration The overridable_params dictionary allows dynamic modification of parameters defined in the JSON configuration. This enables users to override default values by passing them as arguments. Example usage overridable_params = { \"task.compvis_ckpt_path\": \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\": scissorhands_train_mu.model_config_path, # Overrides model config \"task.dataset_path\": \"outputs/dataset/i2p_nude\", # Overrides dataset path \"logger.json.root\": \"results/hard_prompt_esd_nudity_P4D_scissorhands\", # Overrides logging path \"attacker.k\" = 3, \"attacker.no_attack.dataset_path\" = \"path/to/dataset\" #overrides the datset path for no attack } No Attack - diffuser from mu_attack.configs.nudity import no_attack_esd_nudity_classifier_diffusers_config from mu_attack.execs.attack import MUAttack def run_no_attack_for_nudity(): overridable_params = { \"task.diffusers_model_name_or_path\" :\"outputs/forget_me_not/finetuned_models/Abstractionism\", \"task.dataset_path\" : \"outputs/dataset/i2p_nude\", \"logger.json.root\" : \"results/hard_prompt_esd_nudity_P4D_abstrc\", \"attacker.no_attack.dataset_path\" : \"outputs/dataset/i2p_nude\" } MUAttack( config=no_attack_esd_nudity_classifier_diffusers_config, **overridable_params ) if __name__ == \"__main__\": run_no_attack_for_nudity() Code Explanation & Important Notes from mu_attack.configs.nudity import no_attack_esd_nudity_P4D_diffusers_config \u2192 This imports the predefined no attack Attack configuration for nudity unlearning in the diffusers model. It sets up the attack parameters and methodologies. Description of fields in config json file overall This section defines the high-level configuration for the attack. task : The name of the task being performed. Type: str Example: classifer attacker: Specifies the attack type. Type: str Example: no_attack logger: Defines the logging mechanism. Type: str Example: JSON resume: Option to resume from previous checkpoint. task concept: The concept targeted by the attack. Type: str Example: nudity, harm diffusers_model_name_or_path: Path to the pre-trained checkpoint of the diffuser model. (For diffuser) Type: str Example: \"outputs/semipermeable_membrane/finetuned_models/\" target_ckpt: Path to the target model checkpoint used in the attack. (For diffuser) Type: str Example: \"files/pretrained/SD-1-4/ESD_ckpt/Nudity-ESDx1-UNET-SD.pt\" compvis_ckpt_path: Path to the pre-trained checkpoint of the CompVis model. (For compvis) Type: str Example: \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\" compvis_config_path: Path to the configuration file for the CompVis model. (For compvis) Type: str Example: \"configs/scissorhands/model_config.yaml\" cache_path: Directory to cache intermediate results. Type: str Example: \".cache\" dataset_path: Path to the dataset used for the attack. Type: str Example: \"outputs/dataset/i2p_nude\" criterion: The loss function or criterion used during the attack. Type: str Example: \"l2\" classifier_dir: Directory for the classifier, if applicable. null if not used. Type: str Example: \"/path/classifier_dir\" sampling_step_num: Number of sampling steps during the attack. Type: int Example: 1 sld: Strength of latent disentanglement. Type: str Example: \"weak\" sld_concept: Concept tied to latent disentanglement. Type: str Example: \"nudity\" negative_prompt: The negative prompt used to steer the generation. Type: str Example: \"sth\" model_name: Name of the model. The model_name parameter determines which base Stable Diffusion model is used by the pipeline. Type: str Example: \"SD-v1-4\" Choices: \"SD-v1-4\", \"SD-V2\", \"SD-V2-1\" save_diffuser: A Boolean flag that determines whether the CompVis model should be converted into the Diffusers format before being used. Type: str Example: True Behavior: * If set to True, the pipeline will perform a conversion of the CompVis model into the Diffusers format and then load the converted checkpoint. If set to False, the conversion is skipped and the model remains in its original CompVis format for use and uses compvis based implementation. converted_model_folder_path: Folder path to save the converted compvis model to diffuser. Type: str Example: \"outputs\" backend: Specifies the backend model i.e \"diffusers\". Type: str Options: \"diffusers\" or \"compvis\" attacker insertion_location: The point of insertion for the prompt. Type: str Example: \"prefix_k\" k: The value of k for the prompt insertion point. Type: int Example: 5 iteration: Number of iterations for the attack. Type: int Example: 1 seed_iteration: Random seed for the iterative process. Type: int Example: 1 attack_idx: Index of the attack for evaluation purposes. Type: int Example: 0 eval_seed: Seed value used for evaluation. Type: int Example: 0 universal: Whether the attack is universal (true or false). Type: bool Example: false sequential: Whether the attack is applied sequentially. Type: bool Example: true lr: Learning rate for the attack optimization process. Type: float Example: 0.01 weight_decay: Weight decay applied during optimization. Type: float Example: 0.1 logger json: Logging configuration. root: Path to the directory where logs will be saved. Type: str Example: \"results/hard_prompt_esd_nudity_P4D\" name: Name for the log file or experiment. Type: str Example: \"NoAttackEsdNudity\" Example usage: \"json\": { \"root\": \"results/no_attack_esd_nudity_esd\", \"name\": \"NoAttackEsdNudity\" }","title":"Usage"},{"location":"mu_attack/attack/no_attack/#unlearndiffattak","text":"This repository contains the implementation of UnlearnDiffAttack for No-attack, a framework for evaluating the robustness of safety-driven unlearned Models using adversarial prompts.","title":"UnlearnDiffAttak"},{"location":"mu_attack/attack/no_attack/#create-environment","text":"If you prefer not to use the default environment, you can create and activate a dedicated environment for mu_attack. create_env <algorithm_name> Example create_env mu_attack","title":"Create Environment"},{"location":"mu_attack/attack/no_attack/#generate-dataset","text":"python -m scripts.generate_dataset --prompts_path data/prompts/prompts.csv --concept i2p_nude --save_path outputs/dataset","title":"Generate Dataset"},{"location":"mu_attack/attack/no_attack/#run-attack","text":"No Attack - compvis Use the following code if you wish to run the hard prompt attack using the CompVis model directly (without converting it into Diffusers format): from mu_attack.configs.nudity import no_attack_esd_nudity_classifier_compvis_config from mu_attack.execs.attack import MUAttack from mu.algorithms.scissorhands.configs import scissorhands_train_mu def run_attack_for_nudity(): overridable_params = { \"task.compvis_ckpt_path\" : \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\" : scissorhands_train_mu.model_config_path, \"task.dataset_path\" : \"outputs/dataset/i2p_nude\", \"logger.json.root\" : \"results/hard_prompt_esd_nudity_P4D_scissorhands\", \"attacker.no_attack.dataset_path\" : \"outputs/dataset/i2p_nude\" } MUAttack( config=no_attack_esd_nudity_classifier_compvis_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() No Attack \u2013 CompVis to Diffusers Conversion If you want to convert the CompVis model into the Diffusers format before running the attack, use the following code. Note: For the conversion to take place, set task.save_diffuser to True and to use the converted model task.sld should be set to None. from mu_attack.configs.nudity import no_attack_esd_nudity_classifier_compvis_config from mu_attack.execs.attack import MUAttack from mu.algorithms.scissorhands.configs import scissorhands_train_mu def run_attack_for_nudity(): overridable_params = { \"task.compvis_ckpt_path\" : \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\" : scissorhands_train_mu.model_config_path, \"task.dataset_path\" : \"outputs/dataset/i2p_nude\", \"logger.json.root\" : \"results/hard_prompt_esd_nudity_P4D_scissorhands\", \"attacker.no_attack.dataset_path\" : \"outputs/dataset/i2p_nude\", \"task.save_diffuser\": True, # This flag triggers conversion \"task.sld\": None, # Set sld to None for conversion \"task.model_name\": \"SD-v1-4\" } MUAttack( config=no_attack_esd_nudity_classifier_compvis_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() For Conversion: When converting a CompVis model to the Diffusers format, ensure that task.save_diffuser is set to True and task.sld is set to None. This instructs the pipeline to perform the conversion during initialization and then load the converted checkpoint. Code Explanation & Important Notes from mu_attack.configs.nudity import hard_prompt_esd_nudity_P4D_compvis_config \u2192 This imports the predefined Hard Prompt Attack configuration for nudity unlearning in the CompVis model. It sets up the attack parameters and methodologies. from mu.algorithms.scissorhands.configs import scissorhands_train_mu \u2192 Imports the Scissorhands model configuration, required to set the task.compvis_config_path parameter correctly. Overriding Parameters in JSON Configuration The overridable_params dictionary allows dynamic modification of parameters defined in the JSON configuration. This enables users to override default values by passing them as arguments. Example usage overridable_params = { \"task.compvis_ckpt_path\": \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\": scissorhands_train_mu.model_config_path, # Overrides model config \"task.dataset_path\": \"outputs/dataset/i2p_nude\", # Overrides dataset path \"logger.json.root\": \"results/hard_prompt_esd_nudity_P4D_scissorhands\", # Overrides logging path \"attacker.k\" = 3, \"attacker.no_attack.dataset_path\" = \"path/to/dataset\" #overrides the datset path for no attack } No Attack - diffuser from mu_attack.configs.nudity import no_attack_esd_nudity_classifier_diffusers_config from mu_attack.execs.attack import MUAttack def run_no_attack_for_nudity(): overridable_params = { \"task.diffusers_model_name_or_path\" :\"outputs/forget_me_not/finetuned_models/Abstractionism\", \"task.dataset_path\" : \"outputs/dataset/i2p_nude\", \"logger.json.root\" : \"results/hard_prompt_esd_nudity_P4D_abstrc\", \"attacker.no_attack.dataset_path\" : \"outputs/dataset/i2p_nude\" } MUAttack( config=no_attack_esd_nudity_classifier_diffusers_config, **overridable_params ) if __name__ == \"__main__\": run_no_attack_for_nudity() Code Explanation & Important Notes from mu_attack.configs.nudity import no_attack_esd_nudity_P4D_diffusers_config \u2192 This imports the predefined no attack Attack configuration for nudity unlearning in the diffusers model. It sets up the attack parameters and methodologies.","title":"Run Attack"},{"location":"mu_attack/attack/no_attack/#description-of-fields-in-config-json-file","text":"overall This section defines the high-level configuration for the attack. task : The name of the task being performed. Type: str Example: classifer attacker: Specifies the attack type. Type: str Example: no_attack logger: Defines the logging mechanism. Type: str Example: JSON resume: Option to resume from previous checkpoint. task concept: The concept targeted by the attack. Type: str Example: nudity, harm diffusers_model_name_or_path: Path to the pre-trained checkpoint of the diffuser model. (For diffuser) Type: str Example: \"outputs/semipermeable_membrane/finetuned_models/\" target_ckpt: Path to the target model checkpoint used in the attack. (For diffuser) Type: str Example: \"files/pretrained/SD-1-4/ESD_ckpt/Nudity-ESDx1-UNET-SD.pt\" compvis_ckpt_path: Path to the pre-trained checkpoint of the CompVis model. (For compvis) Type: str Example: \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\" compvis_config_path: Path to the configuration file for the CompVis model. (For compvis) Type: str Example: \"configs/scissorhands/model_config.yaml\" cache_path: Directory to cache intermediate results. Type: str Example: \".cache\" dataset_path: Path to the dataset used for the attack. Type: str Example: \"outputs/dataset/i2p_nude\" criterion: The loss function or criterion used during the attack. Type: str Example: \"l2\" classifier_dir: Directory for the classifier, if applicable. null if not used. Type: str Example: \"/path/classifier_dir\" sampling_step_num: Number of sampling steps during the attack. Type: int Example: 1 sld: Strength of latent disentanglement. Type: str Example: \"weak\" sld_concept: Concept tied to latent disentanglement. Type: str Example: \"nudity\" negative_prompt: The negative prompt used to steer the generation. Type: str Example: \"sth\" model_name: Name of the model. The model_name parameter determines which base Stable Diffusion model is used by the pipeline. Type: str Example: \"SD-v1-4\" Choices: \"SD-v1-4\", \"SD-V2\", \"SD-V2-1\" save_diffuser: A Boolean flag that determines whether the CompVis model should be converted into the Diffusers format before being used. Type: str Example: True Behavior: * If set to True, the pipeline will perform a conversion of the CompVis model into the Diffusers format and then load the converted checkpoint. If set to False, the conversion is skipped and the model remains in its original CompVis format for use and uses compvis based implementation. converted_model_folder_path: Folder path to save the converted compvis model to diffuser. Type: str Example: \"outputs\" backend: Specifies the backend model i.e \"diffusers\". Type: str Options: \"diffusers\" or \"compvis\" attacker insertion_location: The point of insertion for the prompt. Type: str Example: \"prefix_k\" k: The value of k for the prompt insertion point. Type: int Example: 5 iteration: Number of iterations for the attack. Type: int Example: 1 seed_iteration: Random seed for the iterative process. Type: int Example: 1 attack_idx: Index of the attack for evaluation purposes. Type: int Example: 0 eval_seed: Seed value used for evaluation. Type: int Example: 0 universal: Whether the attack is universal (true or false). Type: bool Example: false sequential: Whether the attack is applied sequentially. Type: bool Example: true lr: Learning rate for the attack optimization process. Type: float Example: 0.01 weight_decay: Weight decay applied during optimization. Type: float Example: 0.1 logger json: Logging configuration. root: Path to the directory where logs will be saved. Type: str Example: \"results/hard_prompt_esd_nudity_P4D\" name: Name for the log file or experiment. Type: str Example: \"NoAttackEsdNudity\" Example usage: \"json\": { \"root\": \"results/no_attack_esd_nudity_esd\", \"name\": \"NoAttackEsdNudity\" }","title":"Description of fields in config json file"},{"location":"mu_attack/attack/random/","text":"UnlearnDiffAttak This repository contains the implementation of UnlearnDiffAttack for random, a framework for evaluating the robustness of safety-driven unlearned Models using adversarial prompts. Create Environment If you prefer not to use the default environment, you can create and activate a dedicated environment for mu_attack. create_env <algorithm_name> Example create_env mu_attack Generate Dataset python -m scripts.generate_dataset --prompts_path data/prompts/prompts.csv --concept i2p_nude --save_path outputs/dataset Run Attack Random Attack - compvis Use the following code if you wish to run the hard prompt attack using the CompVis model directly (without converting it into Diffusers format): from mu_attack.configs.nudity import random_esd_nudity_compvis_config from mu_attack.execs.attack import MUAttack from mu.algorithms.scissorhands.configs import scissorhands_train_mu def run_attack_for_nudity(): overridable_params = { \"task.compvis_ckpt_path\" :\"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\" : scissorhands_train_mu.model_config_path , \"task.dataset_path\" : \"outputs/dataset/i2p_nude\", \"logger.json.root\" : \"results/hard_prompt_esd_nudity_P4D_scissorhands\" } MUAttack( config=random_esd_nudity_compvis_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() Rnadom Attack \u2013 CompVis to Diffusers Conversion If you want to convert the CompVis model into the Diffusers format before running the attack, use the following code. Note: For the conversion to take place, set task.save_diffuser to True and to use the converted model task.sld should be set to None. from mu_attack.configs.nudity import random_esd_nudity_compvis_config from mu_attack.execs.attack import MUAttack from mu.algorithms.scissorhands.configs import scissorhands_train_mu def run_attack_for_nudity(): overridable_params = { \"task.compvis_ckpt_path\" :\"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\" : scissorhands_train_mu.model_config_path , \"task.dataset_path\" : \"outputs/dataset/i2p_nude\", \"logger.json.root\" : \"results/hard_prompt_esd_nudity_P4D_scissorhands\", \"task.save_diffuser\": True, # This flag triggers conversion \"task.sld\": None, # Set sld to None for conversion \"task.model_name\": \"SD-v1-4\" } MUAttack( config=random_esd_nudity_compvis_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() For Conversion: When converting a CompVis model to the Diffusers format, ensure that task.save_diffuser is set to True and task.sld is set to None. This instructs the pipeline to perform the conversion during initialization and then load the converted checkpoint. Code Explanation & Important Notes from mu_attack.configs.nudity import hard_prompt_esd_nudity_P4D_compvis_config \u2192 This imports the predefined Hard Prompt Attack configuration for nudity unlearning in the CompVis model. It sets up the attack parameters and methodologies. from mu.algorithms.scissorhands.configs import scissorhands_train_mu \u2192 Imports the Scissorhands model configuration, required to set the task.compvis_config_path parameter correctly. Overriding Parameters in JSON Configuration The overridable_params dictionary allows dynamic modification of parameters defined in the JSON configuration. This enables users to override default values by passing them as arguments. Example usage overridable_params = { \"task.compvis_ckpt_path\": \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\": scissorhands_train_mu.model_config_path, # Overrides model config \"task.dataset_path\": \"outputs/dataset/i2p_nude\", # Overrides dataset path \"logger.json.root\": \"results/hard_prompt_esd_nudity_P4D_scissorhands\", # Overrides logging path \"attacker.k\" = 3, \"attacker.no_attack.dataset_path\" = \"path/to/dataset\" #overrides the datset path for no attack } Random Attack - diffuser from mu_attack.configs.nudity import random_esd_nudity_diffuser_config from mu_attack.execs.attack import MUAttack def run_attack_for_nudity(): overridable_params = { \"task.diffusers_model_name_or_path\" : \"outputs/forget_me_not/finetuned_models/Abstractionism\", \"task.dataset_path\" : \"outputs/dataset/i2p_nude\", \"logger.json.root\" : \"results/random_esd_nudity_diffuser_uce\" } MUAttack( config=random_esd_nudity_diffuser_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() Code Explanation & Important Notes from mu_attack.configs.nudity import random_esd_nudity_P4D_diffusers_config \u2192 This imports the predefined random Attack configuration for nudity unlearning in the diffusers model. It sets up the attack parameters and methodologies. Description of fields in config json file overall This section defines the high-level configuration for the attack. task : The name of the task being performed. Type: str Example: classifer attacker: Specifies the attack type. Type: str Example: random logger: Defines the logging mechanism. Type: str Example: JSON resume: Option to resume from previous checkpoint. task concept: The concept targeted by the attack. Type: str Example: nudity diffusers_model_name_or_path: Path to the pre-trained checkpoint of the diffuser model. (For diffuser) Type: str Example: \"outputs/semipermeable_membrane/finetuned_models/\" target_ckpt: Path to the target model checkpoint used in the attack. (For diffuser) Type: str Example: \"files/pretrained/SD-1-4/ESD_ckpt/Nudity-ESDx1-UNET-SD.pt\" compvis_ckpt_path: Path to the pre-trained checkpoint of the CompVis model. (For compvis) Type: str Example: \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\" compvis_config_path: Path to the configuration file for the CompVis model. (For compvis) Type: str Example: \"configs/scissorhands/model_config.yaml\" cache_path: Directory to cache intermediate results. Type: str Example: \".cache\" dataset_path: Path to the dataset used for the attack. Type: str Example: \"outputs/dataset/i2p_nude\" criterion: The loss function or criterion used during the attack. Type: str Example: \"l2\" classifier_dir: Directory for the classifier, if applicable. null if not used. Type: str Example: \"/path/classifier_dir\" sampling_step_num: Number of sampling steps during the attack. Type: int Example: 1 sld: Strength of latent disentanglement. Type: str Example: \"weak\" sld_concept: Concept tied to latent disentanglement. Type: str Example: \"nudity\" negative_prompt: The negative prompt used to steer the generation. Type: str Example: \"sth\" model_name: Name of the model. The model_name parameter determines which base Stable Diffusion model is used by the pipeline. Type: str Example: \"SD-v1-4\" Choices: \"SD-v1-4\", \"SD-V2\", \"SD-V2-1\" save_diffuser: A Boolean flag that determines whether the CompVis model should be converted into the Diffusers format before being used. Type: str Example: True Behavior: * If set to True, the pipeline will perform a conversion of the CompVis model into the Diffusers format and then load the converted checkpoint. If set to False, the conversion is skipped and the model remains in its original CompVis format for use and uses compvis based implementation. converted_model_folder_path: Folder path to save the converted compvis model to diffuser. Type: str Example: \"outputs\" backend: Specifies the backend model i.e \"diffusers\". Type: str Options: \"diffusers\" or \"compvis\" attacker insertion_location: The point of insertion for the prompt. Type: str Example: \"prefix_k\" k: The value of k for the prompt insertion point. Type: int Example: 5 iteration: Number of iterations for the attack. Type: int Example: 1 seed_iteration: Random seed for the iterative process. Type: int Example: 1 attack_idx: Index of the attack for evaluation purposes. Type: int Example: 0 eval_seed: Seed value used for evaluation. Type: int Example: 0 universal: Whether the attack is universal (true or false). Type: bool Example: false sequential: Whether the attack is applied sequentially. Type: bool Example: true logger json: Logging configuration. root: Path to the directory where logs will be saved. Type: str Example: \"results/hard_prompt_esd_nudity_P4D\" name: Name for the log file or experiment. Type: str Example: \"Hard Prompt\" Example usage: \"json\": { \"root\": \"results/no_attack_esd_nudity_esd\", \"name\": \"Hard Prompt\" }","title":"Usage"},{"location":"mu_attack/attack/random/#unlearndiffattak","text":"This repository contains the implementation of UnlearnDiffAttack for random, a framework for evaluating the robustness of safety-driven unlearned Models using adversarial prompts.","title":"UnlearnDiffAttak"},{"location":"mu_attack/attack/random/#create-environment","text":"If you prefer not to use the default environment, you can create and activate a dedicated environment for mu_attack. create_env <algorithm_name> Example create_env mu_attack","title":"Create Environment"},{"location":"mu_attack/attack/random/#generate-dataset","text":"python -m scripts.generate_dataset --prompts_path data/prompts/prompts.csv --concept i2p_nude --save_path outputs/dataset","title":"Generate Dataset"},{"location":"mu_attack/attack/random/#run-attack","text":"Random Attack - compvis Use the following code if you wish to run the hard prompt attack using the CompVis model directly (without converting it into Diffusers format): from mu_attack.configs.nudity import random_esd_nudity_compvis_config from mu_attack.execs.attack import MUAttack from mu.algorithms.scissorhands.configs import scissorhands_train_mu def run_attack_for_nudity(): overridable_params = { \"task.compvis_ckpt_path\" :\"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\" : scissorhands_train_mu.model_config_path , \"task.dataset_path\" : \"outputs/dataset/i2p_nude\", \"logger.json.root\" : \"results/hard_prompt_esd_nudity_P4D_scissorhands\" } MUAttack( config=random_esd_nudity_compvis_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() Rnadom Attack \u2013 CompVis to Diffusers Conversion If you want to convert the CompVis model into the Diffusers format before running the attack, use the following code. Note: For the conversion to take place, set task.save_diffuser to True and to use the converted model task.sld should be set to None. from mu_attack.configs.nudity import random_esd_nudity_compvis_config from mu_attack.execs.attack import MUAttack from mu.algorithms.scissorhands.configs import scissorhands_train_mu def run_attack_for_nudity(): overridable_params = { \"task.compvis_ckpt_path\" :\"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\" : scissorhands_train_mu.model_config_path , \"task.dataset_path\" : \"outputs/dataset/i2p_nude\", \"logger.json.root\" : \"results/hard_prompt_esd_nudity_P4D_scissorhands\", \"task.save_diffuser\": True, # This flag triggers conversion \"task.sld\": None, # Set sld to None for conversion \"task.model_name\": \"SD-v1-4\" } MUAttack( config=random_esd_nudity_compvis_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() For Conversion: When converting a CompVis model to the Diffusers format, ensure that task.save_diffuser is set to True and task.sld is set to None. This instructs the pipeline to perform the conversion during initialization and then load the converted checkpoint. Code Explanation & Important Notes from mu_attack.configs.nudity import hard_prompt_esd_nudity_P4D_compvis_config \u2192 This imports the predefined Hard Prompt Attack configuration for nudity unlearning in the CompVis model. It sets up the attack parameters and methodologies. from mu.algorithms.scissorhands.configs import scissorhands_train_mu \u2192 Imports the Scissorhands model configuration, required to set the task.compvis_config_path parameter correctly. Overriding Parameters in JSON Configuration The overridable_params dictionary allows dynamic modification of parameters defined in the JSON configuration. This enables users to override default values by passing them as arguments. Example usage overridable_params = { \"task.compvis_ckpt_path\": \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\": scissorhands_train_mu.model_config_path, # Overrides model config \"task.dataset_path\": \"outputs/dataset/i2p_nude\", # Overrides dataset path \"logger.json.root\": \"results/hard_prompt_esd_nudity_P4D_scissorhands\", # Overrides logging path \"attacker.k\" = 3, \"attacker.no_attack.dataset_path\" = \"path/to/dataset\" #overrides the datset path for no attack } Random Attack - diffuser from mu_attack.configs.nudity import random_esd_nudity_diffuser_config from mu_attack.execs.attack import MUAttack def run_attack_for_nudity(): overridable_params = { \"task.diffusers_model_name_or_path\" : \"outputs/forget_me_not/finetuned_models/Abstractionism\", \"task.dataset_path\" : \"outputs/dataset/i2p_nude\", \"logger.json.root\" : \"results/random_esd_nudity_diffuser_uce\" } MUAttack( config=random_esd_nudity_diffuser_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() Code Explanation & Important Notes from mu_attack.configs.nudity import random_esd_nudity_P4D_diffusers_config \u2192 This imports the predefined random Attack configuration for nudity unlearning in the diffusers model. It sets up the attack parameters and methodologies.","title":"Run Attack"},{"location":"mu_attack/attack/random/#description-of-fields-in-config-json-file","text":"overall This section defines the high-level configuration for the attack. task : The name of the task being performed. Type: str Example: classifer attacker: Specifies the attack type. Type: str Example: random logger: Defines the logging mechanism. Type: str Example: JSON resume: Option to resume from previous checkpoint. task concept: The concept targeted by the attack. Type: str Example: nudity diffusers_model_name_or_path: Path to the pre-trained checkpoint of the diffuser model. (For diffuser) Type: str Example: \"outputs/semipermeable_membrane/finetuned_models/\" target_ckpt: Path to the target model checkpoint used in the attack. (For diffuser) Type: str Example: \"files/pretrained/SD-1-4/ESD_ckpt/Nudity-ESDx1-UNET-SD.pt\" compvis_ckpt_path: Path to the pre-trained checkpoint of the CompVis model. (For compvis) Type: str Example: \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\" compvis_config_path: Path to the configuration file for the CompVis model. (For compvis) Type: str Example: \"configs/scissorhands/model_config.yaml\" cache_path: Directory to cache intermediate results. Type: str Example: \".cache\" dataset_path: Path to the dataset used for the attack. Type: str Example: \"outputs/dataset/i2p_nude\" criterion: The loss function or criterion used during the attack. Type: str Example: \"l2\" classifier_dir: Directory for the classifier, if applicable. null if not used. Type: str Example: \"/path/classifier_dir\" sampling_step_num: Number of sampling steps during the attack. Type: int Example: 1 sld: Strength of latent disentanglement. Type: str Example: \"weak\" sld_concept: Concept tied to latent disentanglement. Type: str Example: \"nudity\" negative_prompt: The negative prompt used to steer the generation. Type: str Example: \"sth\" model_name: Name of the model. The model_name parameter determines which base Stable Diffusion model is used by the pipeline. Type: str Example: \"SD-v1-4\" Choices: \"SD-v1-4\", \"SD-V2\", \"SD-V2-1\" save_diffuser: A Boolean flag that determines whether the CompVis model should be converted into the Diffusers format before being used. Type: str Example: True Behavior: * If set to True, the pipeline will perform a conversion of the CompVis model into the Diffusers format and then load the converted checkpoint. If set to False, the conversion is skipped and the model remains in its original CompVis format for use and uses compvis based implementation. converted_model_folder_path: Folder path to save the converted compvis model to diffuser. Type: str Example: \"outputs\" backend: Specifies the backend model i.e \"diffusers\". Type: str Options: \"diffusers\" or \"compvis\" attacker insertion_location: The point of insertion for the prompt. Type: str Example: \"prefix_k\" k: The value of k for the prompt insertion point. Type: int Example: 5 iteration: Number of iterations for the attack. Type: int Example: 1 seed_iteration: Random seed for the iterative process. Type: int Example: 1 attack_idx: Index of the attack for evaluation purposes. Type: int Example: 0 eval_seed: Seed value used for evaluation. Type: int Example: 0 universal: Whether the attack is universal (true or false). Type: bool Example: false sequential: Whether the attack is applied sequentially. Type: bool Example: true logger json: Logging configuration. root: Path to the directory where logs will be saved. Type: str Example: \"results/hard_prompt_esd_nudity_P4D\" name: Name for the log file or experiment. Type: str Example: \"Hard Prompt\" Example usage: \"json\": { \"root\": \"results/no_attack_esd_nudity_esd\", \"name\": \"Hard Prompt\" }","title":"Description of fields in config json file"},{"location":"mu_attack/attack/seed_search/","text":"UnlearnDiffAttak This repository contains the implementation of UnlearnDiffAttack for seed search, a framework for evaluating the robustness of safety-driven unlearned Models using adversarial prompts. Create Environment create_env <algorithm_name> eg: create_env mu_attack conda activate <environment_name> eg: conda activate mu_attack Generate Dataset python -m scripts.generate_dataset --prompts_path data/prompts/prompts.csv --concept i2p_nude --save_path outputs/dataset Run Attack Seed search Attack - compvis Use the following code if you wish to run the hard prompt attack using the CompVis model directly (without converting it into Diffusers format): from mu_attack.configs.nudity import seed_search_esd_nudity_classifier_compvis_config from mu_attack.execs.attack import MUAttack from mu.algorithms.scissorhands.configs import scissorhands_train_mu def run_attack_for_nudity(): overridable_params = { \"task.compvis_ckpt_path\" :\"/home/ubuntu/Projects/dipesh/unlearn_diff/outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\" : scissorhands_train_mu.model_config_path, \"task.dataset_path\" : \"/home/ubuntu/Projects/Palistha/unlearn_diff_attack/outputs/dataset/i2p_nude\", \"attacker.text_grad.lr\": 0.02, \"logger.json.root\" : \"results/hard_prompt_esd_nudity_P4D_scissorhands\" } MUAttack( config=seed_search_esd_nudity_classifier_compvis_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() Seed search Attack \u2013 CompVis to Diffusers Conversion If you want to convert the CompVis model into the Diffusers format before running the attack, use the following code. Note: For the conversion to take place, set task.save_diffuser to True and to use the converted model task.sld should be set to None. from mu_attack.configs.nudity import seed_search_esd_nudity_classifier_compvis_config from mu_attack.execs.attack import MUAttack from mu.algorithms.scissorhands.configs import scissorhands_train_mu def run_attack_for_nudity(): overridable_params = { \"task.compvis_ckpt_path\" :\"/home/ubuntu/Projects/dipesh/unlearn_diff/outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\" : scissorhands_train_mu.model_config_path, \"task.dataset_path\" : \"/home/ubuntu/Projects/Palistha/unlearn_diff_attack/outputs/dataset/i2p_nude\", \"attacker.text_grad.lr\": 0.02, \"logger.json.root\" : \"results/hard_prompt_esd_nudity_P4D_scissorhands\", \"task.save_diffuser\": True, # This flag triggers conversion \"task.sld\": None, # Set sld to None for conversion \"task.model_name\": \"SD-v1-4\" } MUAttack( config=seed_search_esd_nudity_classifier_compvis_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() For Conversion: When converting a CompVis model to the Diffusers format, ensure that task.save_diffuser is set to True and task.sld is set to None. This instructs the pipeline to perform the conversion during initialization and then load the converted checkpoint. Code Explanation & Important Notes from mu_attack.configs.nudity import hard_prompt_esd_nudity_P4D_compvis_config \u2192 This imports the predefined Hard Prompt Attack configuration for nudity unlearning in the CompVis model. It sets up the attack parameters and methodologies. from mu.algorithms.scissorhands.configs import scissorhands_train_mu \u2192 Imports the Scissorhands model configuration, required to set the task.compvis_config_path parameter correctly. Overriding Parameters in JSON Configuration The overridable_params dictionary allows dynamic modification of parameters defined in the JSON configuration. This enables users to override default values by passing them as arguments. Example usage overridable_params = { \"task.compvis_ckpt_path\": \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\": scissorhands_train_mu.model_config_path, # Overrides model config \"task.dataset_path\": \"outputs/dataset/i2p_nude\", # Overrides dataset path \"logger.json.root\": \"results/hard_prompt_esd_nudity_P4D_scissorhands\", # Overrides logging path \"attacker.k\" = 3, \"attacker.no_attack.dataset_path\" = \"path/to/dataset\" #overrides the datset path for no attack } Random Attack - diffuser from mu_attack.configs.nudity import seed_search_esd_nudity_classifier_diffuser_config from mu_attack.execs.attack import MUAttack def run_attack_for_nudity(): overridable_params = { \"task.diffusers_model_name_or_path\" : \"outputs/forget_me_not/finetuned_models/Abstractionism\", \"task.dataset_path\" : \"outputs/dataset/i2p_nude\", \"logger.json.root\" : \"results/random_esd_nudity_diffuser_uce\" } MUAttack( config=seed_search_esd_nudity_classifier_diffuser_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() Code Explanation & Important Notes from mu_attack.configs.nudity import seed_search_esd_nudity_P4D_diffusers_config \u2192 This imports the predefined seed search Attack configuration for nudity unlearning in the diffusers model. It sets up the attack parameters and methodologies. Description of fields in config json file overall This section defines the high-level configuration for the attack. task : The name of the task being performed. Type: str Example: classifer attacker: Specifies the attack type. Type: str Example: seed_search logger: Defines the logging mechanism. Type: str Example: JSON resume: Option to resume from previous checkpoint. task concept: The concept targeted by the attack. Type: str Example: nudity diffusers_model_name_or_path: Path to the pre-trained checkpoint of the diffuser model. (For diffuser) Type: str Example: \"outputs/semipermeable_membrane/finetuned_models/\" target_ckpt: Path to the target model checkpoint used in the attack. (For diffuser) Type: str Example: \"files/pretrained/SD-1-4/ESD_ckpt/Nudity-ESDx1-UNET-SD.pt\" compvis_ckpt_path: Path to the pre-trained checkpoint of the CompVis model. (For compvis) Type: str Example: \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\" compvis_config_path: Path to the configuration file for the CompVis model. (For compvis) Type: str Example: \"configs/scissorhands/model_config.yaml\" cache_path: Directory to cache intermediate results. Type: str Example: \".cache\" dataset_path: Path to the dataset used for the attack. Type: str Example: \"outputs/dataset/i2p_nude\" criterion: The loss function or criterion used during the attack. Type: str Example: \"l2\" classifier_dir: Directory for the classifier, if applicable. null if not used. Type: str Example: \"/path/classifier_dir\" sampling_step_num: Number of sampling steps during the attack. Type: int Example: 1 sld: Strength of latent disentanglement. Type: str Example: \"weak\" sld_concept: Concept tied to latent disentanglement. Type: str Example: \"nudity\" negative_prompt: The negative prompt used to steer the generation. Type: str Example: \"sth\" model_name: Name of the model. The model_name parameter determines which base Stable Diffusion model is used by the pipeline. Type: str Example: \"SD-v1-4\" Choices: \"SD-v1-4\", \"SD-V2\", \"SD-V2-1\" save_diffuser: A Boolean flag that determines whether the CompVis model should be converted into the Diffusers format before being used. Type: str Example: True Behavior: * If set to True, the pipeline will perform a conversion of the CompVis model into the Diffusers format and then load the converted checkpoint. If set to False, the conversion is skipped and the model remains in its original CompVis format for use and uses compvis based implementation. converted_model_folder_path: Folder path to save the converted compvis model to diffuser. Type: str Example: \"outputs\" backend: Specifies the backend model i.e \"diffusers\". Type: str Options: \"diffusers\" or \"compvis\" attacker insertion_location: The point of insertion for the prompt. Type: str Example: \"prefix_k\" k: The value of k for the prompt insertion point. Type: int Example: 5 iteration: Number of iterations for the attack. Type: int Example: 1 seed_iteration: Random seed for the iterative process. Type: int Example: 1 attack_idx: Index of the attack for evaluation purposes. Type: int Example: 0 eval_seed: Seed value used for evaluation. Type: int Example: 0 universal: Whether the attack is universal (true or false). Type: bool Example: false sequential: Whether the attack is applied sequentially. Type: bool Example: true logger json: Logging configuration. root: Path to the directory where logs will be saved. Type: str Example: \"results/hard_prompt_esd_nudity_P4D\" name: Name for the log file or experiment. Type: str Example: \"Seed Search Nudity\" Example usage: \"json\": { \"root\": \"results/seed_search_esd_nudity_esd\", \"name\": \"Seed Search Nudity\" }","title":"Usage"},{"location":"mu_attack/attack/seed_search/#unlearndiffattak","text":"This repository contains the implementation of UnlearnDiffAttack for seed search, a framework for evaluating the robustness of safety-driven unlearned Models using adversarial prompts.","title":"UnlearnDiffAttak"},{"location":"mu_attack/attack/seed_search/#create-environment","text":"create_env <algorithm_name> eg: create_env mu_attack conda activate <environment_name> eg: conda activate mu_attack","title":"Create Environment"},{"location":"mu_attack/attack/seed_search/#generate-dataset","text":"python -m scripts.generate_dataset --prompts_path data/prompts/prompts.csv --concept i2p_nude --save_path outputs/dataset","title":"Generate Dataset"},{"location":"mu_attack/attack/seed_search/#run-attack","text":"Seed search Attack - compvis Use the following code if you wish to run the hard prompt attack using the CompVis model directly (without converting it into Diffusers format): from mu_attack.configs.nudity import seed_search_esd_nudity_classifier_compvis_config from mu_attack.execs.attack import MUAttack from mu.algorithms.scissorhands.configs import scissorhands_train_mu def run_attack_for_nudity(): overridable_params = { \"task.compvis_ckpt_path\" :\"/home/ubuntu/Projects/dipesh/unlearn_diff/outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\" : scissorhands_train_mu.model_config_path, \"task.dataset_path\" : \"/home/ubuntu/Projects/Palistha/unlearn_diff_attack/outputs/dataset/i2p_nude\", \"attacker.text_grad.lr\": 0.02, \"logger.json.root\" : \"results/hard_prompt_esd_nudity_P4D_scissorhands\" } MUAttack( config=seed_search_esd_nudity_classifier_compvis_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() Seed search Attack \u2013 CompVis to Diffusers Conversion If you want to convert the CompVis model into the Diffusers format before running the attack, use the following code. Note: For the conversion to take place, set task.save_diffuser to True and to use the converted model task.sld should be set to None. from mu_attack.configs.nudity import seed_search_esd_nudity_classifier_compvis_config from mu_attack.execs.attack import MUAttack from mu.algorithms.scissorhands.configs import scissorhands_train_mu def run_attack_for_nudity(): overridable_params = { \"task.compvis_ckpt_path\" :\"/home/ubuntu/Projects/dipesh/unlearn_diff/outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\" : scissorhands_train_mu.model_config_path, \"task.dataset_path\" : \"/home/ubuntu/Projects/Palistha/unlearn_diff_attack/outputs/dataset/i2p_nude\", \"attacker.text_grad.lr\": 0.02, \"logger.json.root\" : \"results/hard_prompt_esd_nudity_P4D_scissorhands\", \"task.save_diffuser\": True, # This flag triggers conversion \"task.sld\": None, # Set sld to None for conversion \"task.model_name\": \"SD-v1-4\" } MUAttack( config=seed_search_esd_nudity_classifier_compvis_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() For Conversion: When converting a CompVis model to the Diffusers format, ensure that task.save_diffuser is set to True and task.sld is set to None. This instructs the pipeline to perform the conversion during initialization and then load the converted checkpoint. Code Explanation & Important Notes from mu_attack.configs.nudity import hard_prompt_esd_nudity_P4D_compvis_config \u2192 This imports the predefined Hard Prompt Attack configuration for nudity unlearning in the CompVis model. It sets up the attack parameters and methodologies. from mu.algorithms.scissorhands.configs import scissorhands_train_mu \u2192 Imports the Scissorhands model configuration, required to set the task.compvis_config_path parameter correctly. Overriding Parameters in JSON Configuration The overridable_params dictionary allows dynamic modification of parameters defined in the JSON configuration. This enables users to override default values by passing them as arguments. Example usage overridable_params = { \"task.compvis_ckpt_path\": \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\": scissorhands_train_mu.model_config_path, # Overrides model config \"task.dataset_path\": \"outputs/dataset/i2p_nude\", # Overrides dataset path \"logger.json.root\": \"results/hard_prompt_esd_nudity_P4D_scissorhands\", # Overrides logging path \"attacker.k\" = 3, \"attacker.no_attack.dataset_path\" = \"path/to/dataset\" #overrides the datset path for no attack } Random Attack - diffuser from mu_attack.configs.nudity import seed_search_esd_nudity_classifier_diffuser_config from mu_attack.execs.attack import MUAttack def run_attack_for_nudity(): overridable_params = { \"task.diffusers_model_name_or_path\" : \"outputs/forget_me_not/finetuned_models/Abstractionism\", \"task.dataset_path\" : \"outputs/dataset/i2p_nude\", \"logger.json.root\" : \"results/random_esd_nudity_diffuser_uce\" } MUAttack( config=seed_search_esd_nudity_classifier_diffuser_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() Code Explanation & Important Notes from mu_attack.configs.nudity import seed_search_esd_nudity_P4D_diffusers_config \u2192 This imports the predefined seed search Attack configuration for nudity unlearning in the diffusers model. It sets up the attack parameters and methodologies.","title":"Run Attack"},{"location":"mu_attack/attack/seed_search/#description-of-fields-in-config-json-file","text":"overall This section defines the high-level configuration for the attack. task : The name of the task being performed. Type: str Example: classifer attacker: Specifies the attack type. Type: str Example: seed_search logger: Defines the logging mechanism. Type: str Example: JSON resume: Option to resume from previous checkpoint. task concept: The concept targeted by the attack. Type: str Example: nudity diffusers_model_name_or_path: Path to the pre-trained checkpoint of the diffuser model. (For diffuser) Type: str Example: \"outputs/semipermeable_membrane/finetuned_models/\" target_ckpt: Path to the target model checkpoint used in the attack. (For diffuser) Type: str Example: \"files/pretrained/SD-1-4/ESD_ckpt/Nudity-ESDx1-UNET-SD.pt\" compvis_ckpt_path: Path to the pre-trained checkpoint of the CompVis model. (For compvis) Type: str Example: \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\" compvis_config_path: Path to the configuration file for the CompVis model. (For compvis) Type: str Example: \"configs/scissorhands/model_config.yaml\" cache_path: Directory to cache intermediate results. Type: str Example: \".cache\" dataset_path: Path to the dataset used for the attack. Type: str Example: \"outputs/dataset/i2p_nude\" criterion: The loss function or criterion used during the attack. Type: str Example: \"l2\" classifier_dir: Directory for the classifier, if applicable. null if not used. Type: str Example: \"/path/classifier_dir\" sampling_step_num: Number of sampling steps during the attack. Type: int Example: 1 sld: Strength of latent disentanglement. Type: str Example: \"weak\" sld_concept: Concept tied to latent disentanglement. Type: str Example: \"nudity\" negative_prompt: The negative prompt used to steer the generation. Type: str Example: \"sth\" model_name: Name of the model. The model_name parameter determines which base Stable Diffusion model is used by the pipeline. Type: str Example: \"SD-v1-4\" Choices: \"SD-v1-4\", \"SD-V2\", \"SD-V2-1\" save_diffuser: A Boolean flag that determines whether the CompVis model should be converted into the Diffusers format before being used. Type: str Example: True Behavior: * If set to True, the pipeline will perform a conversion of the CompVis model into the Diffusers format and then load the converted checkpoint. If set to False, the conversion is skipped and the model remains in its original CompVis format for use and uses compvis based implementation. converted_model_folder_path: Folder path to save the converted compvis model to diffuser. Type: str Example: \"outputs\" backend: Specifies the backend model i.e \"diffusers\". Type: str Options: \"diffusers\" or \"compvis\" attacker insertion_location: The point of insertion for the prompt. Type: str Example: \"prefix_k\" k: The value of k for the prompt insertion point. Type: int Example: 5 iteration: Number of iterations for the attack. Type: int Example: 1 seed_iteration: Random seed for the iterative process. Type: int Example: 1 attack_idx: Index of the attack for evaluation purposes. Type: int Example: 0 eval_seed: Seed value used for evaluation. Type: int Example: 0 universal: Whether the attack is universal (true or false). Type: bool Example: false sequential: Whether the attack is applied sequentially. Type: bool Example: true logger json: Logging configuration. root: Path to the directory where logs will be saved. Type: str Example: \"results/hard_prompt_esd_nudity_P4D\" name: Name for the log file or experiment. Type: str Example: \"Seed Search Nudity\" Example usage: \"json\": { \"root\": \"results/seed_search_esd_nudity_esd\", \"name\": \"Seed Search Nudity\" }","title":"Description of fields in config json file"},{"location":"mu_attack/attack/soft_prompt/","text":"UnlearnDiffAttak This project implements a novel adversarial unlearning framework designed to perform soft prompt attacks on diffusion models. The primary objective is to subtly perturb the latent conditioning (or prompt) in order to manipulate the generated outputs, such as images, in a controlled and adversarial manner. Create Environment create_env <algorithm_name> eg: create_env mu_attack conda activate <environment_name> eg: conda activate mu_attack Run Soft Prompt Attack Soft Prompt Attack - compvis from mu_attack.execs.adv_attack import AdvAttack from mu_attack.configs.adv_unlearn import adv_attack_config from mu.algorithms.esd.configs import esd_train_mu def mu_defense(): adv_unlearn = AdvAttack( config=adv_attack_config, compvis_ckpt_path = \"/home/ubuntu/Projects/dipesh/unlearn_diff/models/sd-v1-4-full-ema.ckpt\", attack_step = 2, backend = \"compvis\", config_path = esd_train_mu.model_config_path ) adv_unlearn.attack() if __name__ == \"__main__\": mu_defense() Soft Prompt Attack - diffuser from mu_attack.execs.adv_attack import AdvAttack from mu_attack.configs.adv_unlearn import adv_attack_config def mu_defense(): adv_unlearn = AdvAttack( config=adv_attack_config, diffusers_model_name_or_path = \"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/diffuser/style50\", attack_step = 2, backend = \"diffusers\" ) adv_unlearn.attack() if __name__ == \"__main__\": mu_defense() Run the python file in offline mode WANDB_MODE=offline python_file.py Code Explanation & Important Notes from mu_attack.configs.adv_unlearn import adv_unlearn_config \u2192 This imports the predefined Soft Prompt Attack configuration. It sets up the attack parameters and methodologies. How It Works * Default Values: The script first loads default values from the train config file as in configs section. Parameter Overrides: Any parameters passed directly to the algorithm, overrides these configs. Final Configuration: The script merges the configs and convert them into dictionary to proceed with the training. Description of fields in soft prompt attack config Model setup config_path : Path to the inference configuration file for Stable Diffusion v1.4. Type: str Default: \"model_config.yaml\" compvis_ckpt_path : Path to the Stable Diffusion v1.4 checkpoint file. Type: str Default: \"models/sd-v1-4-full-ema.ckpt\" encoder_model_name_or_path : Path to the pre-trained encoder model used for text-to-image training. Type: str Default: \"CompVis/stable-diffusion-v1-4\" diffusers_model_name_or_path : Path to the Diffusers-based implementation of the model. Type: str Default: \"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/diffuser/style50\" target_ckpt : Checkpoint path for sampling. If None, it uses the default model. Type: str Default: None Devices & I/O devices : Specifies the CUDA devices used for training. Type: str Default: \"0,0\" seperator : Defines the separator used when processing multiple words for unlearning. Type: str Default: None cache_path : Path where intermediate results and cache files are stored. Type: str Default: \".cache\" Image & Diffusion Sampling start_guidance : Guidance scale used for generating the initial image. Type: float Default: 3.0 ddim_steps : Number of DDIM sampling steps used for inference. Type: int Default: 50 image_size : The resolution of images generated during training. Type: int Default: 512 ddim_eta : Noise scaling factor for DDIM inference. Type: float Default: 0 prompt: The text prompt associated with the concept to erase. Type: str Default: \"nudity\" attack_method: The adversarial attack method used during training. Type: str Choices: [\"pgd\", \"multi_pgd\", \"fast_at\", \"free_at\"] Default: \"pgd\" ddim_eta: The DDIM sampling noise parameter. Type: float Default: 0 Adversarial Attack Hyperparameters adv_prompt_num: Number of prompt tokens used for adversarial learning. Type: int Default: 1 attack_embd_type: Type of embedding targeted for attack. Type: str Choices: [\"word_embd\", \"condition_embd\"] Default: \"word_embd\" attack_type: The type of attack applied. Type: str Choices: [\"replace_k\", \"add\", \"prefix_k\", \"suffix_k\", \"mid_k\", \"insert_k\", \"per_k_words\"] Default: \"prefix_k\" attack_init: Method for initializing adversarial attacks. Type: str Choices: [\"random\", \"latest\"] Default: \"latest\" attack_step: Number of attack optimization steps. Type: int Default: 30 attack_lr: Learning rate for adversarial attack updates. Type: float Default: 1e-3 Backend & Logging backend: Specifies the backend for diffusion-based training. Type: str Default: \"diffusers\" project_name: Name of the WandB project for logging. Type: str Default: \"quick-canvas-machine-unlearning\"","title":"Soft prompt"},{"location":"mu_attack/attack/soft_prompt/#unlearndiffattak","text":"This project implements a novel adversarial unlearning framework designed to perform soft prompt attacks on diffusion models. The primary objective is to subtly perturb the latent conditioning (or prompt) in order to manipulate the generated outputs, such as images, in a controlled and adversarial manner.","title":"UnlearnDiffAttak"},{"location":"mu_attack/attack/soft_prompt/#create-environment","text":"create_env <algorithm_name> eg: create_env mu_attack conda activate <environment_name> eg: conda activate mu_attack","title":"Create Environment"},{"location":"mu_attack/attack/soft_prompt/#run-soft-prompt-attack","text":"Soft Prompt Attack - compvis from mu_attack.execs.adv_attack import AdvAttack from mu_attack.configs.adv_unlearn import adv_attack_config from mu.algorithms.esd.configs import esd_train_mu def mu_defense(): adv_unlearn = AdvAttack( config=adv_attack_config, compvis_ckpt_path = \"/home/ubuntu/Projects/dipesh/unlearn_diff/models/sd-v1-4-full-ema.ckpt\", attack_step = 2, backend = \"compvis\", config_path = esd_train_mu.model_config_path ) adv_unlearn.attack() if __name__ == \"__main__\": mu_defense() Soft Prompt Attack - diffuser from mu_attack.execs.adv_attack import AdvAttack from mu_attack.configs.adv_unlearn import adv_attack_config def mu_defense(): adv_unlearn = AdvAttack( config=adv_attack_config, diffusers_model_name_or_path = \"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/diffuser/style50\", attack_step = 2, backend = \"diffusers\" ) adv_unlearn.attack() if __name__ == \"__main__\": mu_defense() Run the python file in offline mode WANDB_MODE=offline python_file.py Code Explanation & Important Notes from mu_attack.configs.adv_unlearn import adv_unlearn_config \u2192 This imports the predefined Soft Prompt Attack configuration. It sets up the attack parameters and methodologies. How It Works * Default Values: The script first loads default values from the train config file as in configs section. Parameter Overrides: Any parameters passed directly to the algorithm, overrides these configs. Final Configuration: The script merges the configs and convert them into dictionary to proceed with the training.","title":"Run Soft Prompt Attack"},{"location":"mu_attack/attack/soft_prompt/#description-of-fields-in-soft-prompt-attack-config","text":"Model setup config_path : Path to the inference configuration file for Stable Diffusion v1.4. Type: str Default: \"model_config.yaml\" compvis_ckpt_path : Path to the Stable Diffusion v1.4 checkpoint file. Type: str Default: \"models/sd-v1-4-full-ema.ckpt\" encoder_model_name_or_path : Path to the pre-trained encoder model used for text-to-image training. Type: str Default: \"CompVis/stable-diffusion-v1-4\" diffusers_model_name_or_path : Path to the Diffusers-based implementation of the model. Type: str Default: \"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/diffuser/style50\" target_ckpt : Checkpoint path for sampling. If None, it uses the default model. Type: str Default: None Devices & I/O devices : Specifies the CUDA devices used for training. Type: str Default: \"0,0\" seperator : Defines the separator used when processing multiple words for unlearning. Type: str Default: None cache_path : Path where intermediate results and cache files are stored. Type: str Default: \".cache\" Image & Diffusion Sampling start_guidance : Guidance scale used for generating the initial image. Type: float Default: 3.0 ddim_steps : Number of DDIM sampling steps used for inference. Type: int Default: 50 image_size : The resolution of images generated during training. Type: int Default: 512 ddim_eta : Noise scaling factor for DDIM inference. Type: float Default: 0 prompt: The text prompt associated with the concept to erase. Type: str Default: \"nudity\" attack_method: The adversarial attack method used during training. Type: str Choices: [\"pgd\", \"multi_pgd\", \"fast_at\", \"free_at\"] Default: \"pgd\" ddim_eta: The DDIM sampling noise parameter. Type: float Default: 0 Adversarial Attack Hyperparameters adv_prompt_num: Number of prompt tokens used for adversarial learning. Type: int Default: 1 attack_embd_type: Type of embedding targeted for attack. Type: str Choices: [\"word_embd\", \"condition_embd\"] Default: \"word_embd\" attack_type: The type of attack applied. Type: str Choices: [\"replace_k\", \"add\", \"prefix_k\", \"suffix_k\", \"mid_k\", \"insert_k\", \"per_k_words\"] Default: \"prefix_k\" attack_init: Method for initializing adversarial attacks. Type: str Choices: [\"random\", \"latest\"] Default: \"latest\" attack_step: Number of attack optimization steps. Type: int Default: 30 attack_lr: Learning rate for adversarial attack updates. Type: float Default: 1e-3 Backend & Logging backend: Specifies the backend for diffusion-based training. Type: str Default: \"diffusers\" project_name: Name of the WandB project for logging. Type: str Default: \"quick-canvas-machine-unlearning\"","title":"Description of fields in soft prompt attack config"},{"location":"mu_attack/attack/text_grad/","text":"UnlearnDiffAttak This repository contains the implementation of UnlearnDiffAttack for text grad, a framework for evaluating the robustness of safety-driven unlearned Models using adversarial prompts. Create Environment If you prefer not to use the default environment, you can create and activate a dedicated environment for mu_attack. create_env <algorithm_name> Example create_env mu_attack Generate Dataset python -m scripts.generate_dataset --prompts_path data/prompts/prompts.csv --concept i2p_nude --save_path outputs/dataset Run Attack Text Grad Attack - compvis Use the following code if you wish to run the hard prompt attack using the CompVis model directly (without converting it into Diffusers format): from mu_attack.configs.nudity import text_grad_esd_nudity_classifier_compvis_config from mu_attack.execs.attack import MUAttack from mu.algorithms.scissorhands.configs import scissorhands_train_mu def run_attack_for_nudity(): overridable_params = { \"task.compvis_ckpt_path\" :\"/home/ubuntu/Projects/dipesh/unlearn_diff/outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\" : scissorhands_train_mu.model_config_path, \"task.dataset_path\" : \"/home/ubuntu/Projects/Palistha/unlearn_diff_attack/outputs/dataset/i2p_nude\", \"attacker.text_grad.lr\": 0.02, \"logger.json.root\" : \"results/hard_prompt_esd_nudity_P4D_scissorhands\" } MUAttack( config=text_grad_esd_nudity_classifier_compvis_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() Text Grad Attack \u2013 CompVis to Diffusers Conversion If you want to convert the CompVis model into the Diffusers format before running the attack, use the following code. Note: For the conversion to take place, set task.save_diffuser to True and to use the converted model task.sld should be set to None. from mu_attack.configs.nudity import text_grad_esd_nudity_classifier_compvis_config from mu_attack.execs.attack import MUAttack from mu.algorithms.scissorhands.configs import scissorhands_train_mu def run_attack_for_nudity(): overridable_params = { \"task.compvis_ckpt_path\" :\"/home/ubuntu/Projects/dipesh/unlearn_diff/outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\" : scissorhands_train_mu.model_config_path, \"task.dataset_path\" : \"/home/ubuntu/Projects/Palistha/unlearn_diff_attack/outputs/dataset/i2p_nude\", \"attacker.text_grad.lr\": 0.02, \"logger.json.root\" : \"results/hard_prompt_esd_nudity_P4D_scissorhands\", \"task.save_diffuser\": True, # This flag triggers conversion \"task.sld\": None, # Set sld to None for conversion \"task.model_name\": \"SD-v1-4\" } MUAttack( config=text_grad_esd_nudity_classifier_compvis_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() For Conversion: When converting a CompVis model to the Diffusers format, ensure that task.save_diffuser is set to True and task.sld is set to None. This instructs the pipeline to perform the conversion during initialization and then load the converted checkpoint. Code Explanation & Important Notes from mu_attack.configs.nudity import text_grad_esd_nudity_P4D_compvis_config \u2192 This imports the predefined text grad Attack configuration for nudity unlearning in the CompVis model. It sets up the attack parameters and methodologies. from mu.algorithms.scissorhands.configs import scissorhands_train_mu \u2192 Imports the Scissorhands model configuration, required to set the task.compvis_config_path parameter correctly. Overriding Parameters in JSON Configuration The overridable_params dictionary allows dynamic modification of parameters defined in the JSON configuration. This enables users to override default values by passing them as arguments. Example usage overridable_params = { \"task.compvis_ckpt_path\": \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\": scissorhands_train_mu.model_config_path, # Overrides model config \"task.dataset_path\": \"outputs/dataset/i2p_nude\", # Overrides dataset path \"logger.json.root\": \"results/hard_prompt_esd_nudity_P4D_scissorhands\", # Overrides logging path \"attacker.k\" = 3, \"attacker.no_attack.dataset_path\" = \"path/to/dataset\" #overrides the datset path for no attack } Text Grad Attack - diffuser from mu_attack.configs.nudity import text_grad_esd_nudity_classifier_diffuser_config from mu_attack.execs.attack import MUAttack def run_attack_for_nudity(): overridable_params = { \"task.diffusers_model_name_or_path\" : \"outputs/forget_me_not/finetuned_models/Abstractionism\", \"task.dataset_path\" : \"outputs/dataset/i2p_nude\", \"logger.json.root\" : \"results/random_esd_nudity_diffuser_uce\" } MUAttack( config=text_grad_esd_nudity_classifier_diffuser_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() Code Explanation & Important Notes from mu_attack.configs.nudity import text_grad_esd_nudity_P4D_diffusers_config \u2192 This imports the predefined Text Grad Attack configuration for nudity unlearning in the diffusers model. It sets up the attack parameters and methodologies. Description of fields in config json file overall This section defines the high-level configuration for the attack. task : The name of the task being performed. Type: str Example: classifer attacker: Specifies the attack type. Type: str Example: text_grad logger: Defines the logging mechanism. Type: str Example: JSON resume: Option to resume from previous checkpoint. task concept: The concept targeted by the attack. Type: str Example: nudity diffusers_model_name_or_path: Path to the pre-trained checkpoint of the diffuser model. (For diffuser) Type: str Example: \"outputs/semipermeable_membrane/finetuned_models/\" target_ckpt: Path to the target model checkpoint used in the attack. (For diffuser) Type: str Example: \"files/pretrained/SD-1-4/ESD_ckpt/Nudity-ESDx1-UNET-SD.pt\" compvis_ckpt_path: Path to the pre-trained checkpoint of the CompVis model. (For compvis) Type: str Example: \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\" compvis_config_path: Path to the configuration file for the CompVis model. (For compvis) Type: str Example: \"configs/scissorhands/model_config.yaml\" cache_path: Directory to cache intermediate results. Type: str Example: \".cache\" dataset_path: Path to the dataset used for the attack. Type: str Example: \"outputs/dataset/i2p_nude\" criterion: The loss function or criterion used during the attack. Type: str Example: \"l2\" classifier_dir: Directory for the classifier, if applicable. null if not used. Type: str Example: \"/path/classifier_dir\" sampling_step_num: Number of sampling steps during the attack. Type: int Example: 1 sld: Strength of latent disentanglement. Type: str Example: \"weak\" sld_concept: Concept tied to latent disentanglement. Type: str Example: \"nudity\" negative_prompt: The negative prompt used to steer the generation. Type: str Example: \"sth\" model_name: Name of the model. The model_name parameter determines which base Stable Diffusion model is used by the pipeline. Type: str Example: \"SD-v1-4\" Choices: \"SD-v1-4\", \"SD-V2\", \"SD-V2-1\" save_diffuser: A Boolean flag that determines whether the CompVis model should be converted into the Diffusers format before being used. Type: str Example: True Behavior: * If set to True, the pipeline will perform a conversion of the CompVis model into the Diffusers format and then load the converted checkpoint. If set to False, the conversion is skipped and the model remains in its original CompVis format for use and uses compvis based implementation. converted_model_folder_path: Folder path to save the converted compvis model to diffuser. Type: str Example: \"outputs\" backend: Specifies the backend model i.e \"diffusers\". Type: str Options: \"diffusers\" or \"compvis\" text_grad: Json that contains lr and weight_decay. Type: Json Example: json \"text_grad\": { \"lr\": 0.01, \"weight_decay\": 0.1 } attacker insertion_location: The point of insertion for the prompt. Type: str Example: \"prefix_k\" k: The value of k for the prompt insertion point. Type: int Example: 5 iteration: Number of iterations for the attack. Type: int Example: 1 seed_iteration: Random seed for the iterative process. Type: int Example: 1 attack_idx: Index of the attack for evaluation purposes. Type: int Example: 0 eval_seed: Seed value used for evaluation. Type: int Example: 0 universal: Whether the attack is universal (true or false). Type: bool Example: false sequential: Whether the attack is applied sequentially. Type: bool Example: true logger json: Logging configuration. root: Path to the directory where logs will be saved. Type: str Example: \"results/hard_prompt_esd_nudity_P4D\" name: Name for the log file or experiment. Type: str Example: \"Seed Search Nudity\" Example usage: \"json\": { \"root\": \"results/text_grad_esd_nudity_esd\", \"name\": \"TextGradNudity\" }","title":"Usage"},{"location":"mu_attack/attack/text_grad/#unlearndiffattak","text":"This repository contains the implementation of UnlearnDiffAttack for text grad, a framework for evaluating the robustness of safety-driven unlearned Models using adversarial prompts.","title":"UnlearnDiffAttak"},{"location":"mu_attack/attack/text_grad/#create-environment","text":"If you prefer not to use the default environment, you can create and activate a dedicated environment for mu_attack. create_env <algorithm_name> Example create_env mu_attack","title":"Create Environment"},{"location":"mu_attack/attack/text_grad/#generate-dataset","text":"python -m scripts.generate_dataset --prompts_path data/prompts/prompts.csv --concept i2p_nude --save_path outputs/dataset","title":"Generate Dataset"},{"location":"mu_attack/attack/text_grad/#run-attack","text":"Text Grad Attack - compvis Use the following code if you wish to run the hard prompt attack using the CompVis model directly (without converting it into Diffusers format): from mu_attack.configs.nudity import text_grad_esd_nudity_classifier_compvis_config from mu_attack.execs.attack import MUAttack from mu.algorithms.scissorhands.configs import scissorhands_train_mu def run_attack_for_nudity(): overridable_params = { \"task.compvis_ckpt_path\" :\"/home/ubuntu/Projects/dipesh/unlearn_diff/outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\" : scissorhands_train_mu.model_config_path, \"task.dataset_path\" : \"/home/ubuntu/Projects/Palistha/unlearn_diff_attack/outputs/dataset/i2p_nude\", \"attacker.text_grad.lr\": 0.02, \"logger.json.root\" : \"results/hard_prompt_esd_nudity_P4D_scissorhands\" } MUAttack( config=text_grad_esd_nudity_classifier_compvis_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() Text Grad Attack \u2013 CompVis to Diffusers Conversion If you want to convert the CompVis model into the Diffusers format before running the attack, use the following code. Note: For the conversion to take place, set task.save_diffuser to True and to use the converted model task.sld should be set to None. from mu_attack.configs.nudity import text_grad_esd_nudity_classifier_compvis_config from mu_attack.execs.attack import MUAttack from mu.algorithms.scissorhands.configs import scissorhands_train_mu def run_attack_for_nudity(): overridable_params = { \"task.compvis_ckpt_path\" :\"/home/ubuntu/Projects/dipesh/unlearn_diff/outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\" : scissorhands_train_mu.model_config_path, \"task.dataset_path\" : \"/home/ubuntu/Projects/Palistha/unlearn_diff_attack/outputs/dataset/i2p_nude\", \"attacker.text_grad.lr\": 0.02, \"logger.json.root\" : \"results/hard_prompt_esd_nudity_P4D_scissorhands\", \"task.save_diffuser\": True, # This flag triggers conversion \"task.sld\": None, # Set sld to None for conversion \"task.model_name\": \"SD-v1-4\" } MUAttack( config=text_grad_esd_nudity_classifier_compvis_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() For Conversion: When converting a CompVis model to the Diffusers format, ensure that task.save_diffuser is set to True and task.sld is set to None. This instructs the pipeline to perform the conversion during initialization and then load the converted checkpoint. Code Explanation & Important Notes from mu_attack.configs.nudity import text_grad_esd_nudity_P4D_compvis_config \u2192 This imports the predefined text grad Attack configuration for nudity unlearning in the CompVis model. It sets up the attack parameters and methodologies. from mu.algorithms.scissorhands.configs import scissorhands_train_mu \u2192 Imports the Scissorhands model configuration, required to set the task.compvis_config_path parameter correctly. Overriding Parameters in JSON Configuration The overridable_params dictionary allows dynamic modification of parameters defined in the JSON configuration. This enables users to override default values by passing them as arguments. Example usage overridable_params = { \"task.compvis_ckpt_path\": \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\": scissorhands_train_mu.model_config_path, # Overrides model config \"task.dataset_path\": \"outputs/dataset/i2p_nude\", # Overrides dataset path \"logger.json.root\": \"results/hard_prompt_esd_nudity_P4D_scissorhands\", # Overrides logging path \"attacker.k\" = 3, \"attacker.no_attack.dataset_path\" = \"path/to/dataset\" #overrides the datset path for no attack } Text Grad Attack - diffuser from mu_attack.configs.nudity import text_grad_esd_nudity_classifier_diffuser_config from mu_attack.execs.attack import MUAttack def run_attack_for_nudity(): overridable_params = { \"task.diffusers_model_name_or_path\" : \"outputs/forget_me_not/finetuned_models/Abstractionism\", \"task.dataset_path\" : \"outputs/dataset/i2p_nude\", \"logger.json.root\" : \"results/random_esd_nudity_diffuser_uce\" } MUAttack( config=text_grad_esd_nudity_classifier_diffuser_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() Code Explanation & Important Notes from mu_attack.configs.nudity import text_grad_esd_nudity_P4D_diffusers_config \u2192 This imports the predefined Text Grad Attack configuration for nudity unlearning in the diffusers model. It sets up the attack parameters and methodologies.","title":"Run Attack"},{"location":"mu_attack/attack/text_grad/#description-of-fields-in-config-json-file","text":"overall This section defines the high-level configuration for the attack. task : The name of the task being performed. Type: str Example: classifer attacker: Specifies the attack type. Type: str Example: text_grad logger: Defines the logging mechanism. Type: str Example: JSON resume: Option to resume from previous checkpoint. task concept: The concept targeted by the attack. Type: str Example: nudity diffusers_model_name_or_path: Path to the pre-trained checkpoint of the diffuser model. (For diffuser) Type: str Example: \"outputs/semipermeable_membrane/finetuned_models/\" target_ckpt: Path to the target model checkpoint used in the attack. (For diffuser) Type: str Example: \"files/pretrained/SD-1-4/ESD_ckpt/Nudity-ESDx1-UNET-SD.pt\" compvis_ckpt_path: Path to the pre-trained checkpoint of the CompVis model. (For compvis) Type: str Example: \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\" compvis_config_path: Path to the configuration file for the CompVis model. (For compvis) Type: str Example: \"configs/scissorhands/model_config.yaml\" cache_path: Directory to cache intermediate results. Type: str Example: \".cache\" dataset_path: Path to the dataset used for the attack. Type: str Example: \"outputs/dataset/i2p_nude\" criterion: The loss function or criterion used during the attack. Type: str Example: \"l2\" classifier_dir: Directory for the classifier, if applicable. null if not used. Type: str Example: \"/path/classifier_dir\" sampling_step_num: Number of sampling steps during the attack. Type: int Example: 1 sld: Strength of latent disentanglement. Type: str Example: \"weak\" sld_concept: Concept tied to latent disentanglement. Type: str Example: \"nudity\" negative_prompt: The negative prompt used to steer the generation. Type: str Example: \"sth\" model_name: Name of the model. The model_name parameter determines which base Stable Diffusion model is used by the pipeline. Type: str Example: \"SD-v1-4\" Choices: \"SD-v1-4\", \"SD-V2\", \"SD-V2-1\" save_diffuser: A Boolean flag that determines whether the CompVis model should be converted into the Diffusers format before being used. Type: str Example: True Behavior: * If set to True, the pipeline will perform a conversion of the CompVis model into the Diffusers format and then load the converted checkpoint. If set to False, the conversion is skipped and the model remains in its original CompVis format for use and uses compvis based implementation. converted_model_folder_path: Folder path to save the converted compvis model to diffuser. Type: str Example: \"outputs\" backend: Specifies the backend model i.e \"diffusers\". Type: str Options: \"diffusers\" or \"compvis\" text_grad: Json that contains lr and weight_decay. Type: Json Example: json \"text_grad\": { \"lr\": 0.01, \"weight_decay\": 0.1 } attacker insertion_location: The point of insertion for the prompt. Type: str Example: \"prefix_k\" k: The value of k for the prompt insertion point. Type: int Example: 5 iteration: Number of iterations for the attack. Type: int Example: 1 seed_iteration: Random seed for the iterative process. Type: int Example: 1 attack_idx: Index of the attack for evaluation purposes. Type: int Example: 0 eval_seed: Seed value used for evaluation. Type: int Example: 0 universal: Whether the attack is universal (true or false). Type: bool Example: false sequential: Whether the attack is applied sequentially. Type: bool Example: true logger json: Logging configuration. root: Path to the directory where logs will be saved. Type: str Example: \"results/hard_prompt_esd_nudity_P4D\" name: Name for the log file or experiment. Type: str Example: \"Seed Search Nudity\" Example usage: \"json\": { \"root\": \"results/text_grad_esd_nudity_esd\", \"name\": \"TextGradNudity\" }","title":"Description of fields in config json file"},{"location":"mu_attack/configs/hard_prompt/","text":"Sample hard_prompt_config for compvis # mu_attack/configs/nudity/hard_prompt_esd_nudity_P4D_compvis.py import os from mu_attack.core import BaseConfig, OverallConfig, TaskConfig, AttackerConfig, LoggerConfig class HardPromptESDNudityP4DConfigCompvis(BaseConfig): overall: OverallConfig = OverallConfig( task=\"P4D\", attacker=\"hard_prompt\", logger=\"json\", resume=None ) task: TaskConfig = TaskConfig( classifier_dir=None, sampling_step_num=1, sld=\"weak\", sld_concept=\"nudity\", negative_prompt=\"\", backend=\"compvis\", diffusers_config_file = None, save_diffuser = False, converted_model_folder_path = \"outputs\" ) attacker: AttackerConfig = AttackerConfig( sequential = True, lr=0.01, weight_decay=0.1 ) logger: LoggerConfig = LoggerConfig( json={\"root\": \"results/hard_prompt_esd_nudity_P4D_scissorhands\", \"name\": \"P4d\"} ) hard_prompt_esd_nudity_P4D_compvis_config = HardPromptESDNudityP4DConfigCompvis() Sample hard_prompt config json for compvis { \"overall\": { \"task\": \"P4D\", \"attacker\": \"hard_prompt\", \"logger\": \"json\", \"resume\": null }, \"task\": { \"concept\": \"nudity\", \"compvis_ckpt_path\": \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"compvis_config_path\":\"mu/algorithms/scissorhands/configs/model_config.yaml\", \"cache_path\": \".cache\", \"dataset_path\": \"outputs/dataset/i2p_nude\", \"criterion\": \"l2\", \"classifier_dir\": null, \"sampling_step_num\": 1, \"sld\": \"weak\", \"sld_concept\": \"nudity\", \"negative_prompt\": \"\", \"backend\":\"compvis\" }, \"attacker\": { \"insertion_location\": \"prefix_k\", \"k\": 5, \"iteration\": 1, \"seed_iteration\": 1, \"attack_idx\": 0, \"eval_seed\": 0, \"universal\": false, \"sequential\": true, \"lr\": 0.01, \"weight_decay\": 0.1 }, \"logger\": { \"json\": { \"root\": \"results/hard_prompt_esd_nudity_P4D_scissorhands\", \"name\": \"P4d\" } } } Sample hard_prompt_config for diffusers class HardPromptESDNudityP4DConfigDiffusers(BaseConfig): overall: OverallConfig = OverallConfig( task=\"P4D\", attacker=\"hard_prompt\", logger=\"json\", resume=None ) task: TaskConfig = TaskConfig( classifier_dir=None, sampling_step_num=1, sld=\"weak\", sld_concept=\"nudity\", negative_prompt=\"\", backend=\"diffusers\" ) attacker: AttackerConfig = AttackerConfig( sequential = True, lr=0.01, weight_decay=0.1 ) logger: LoggerConfig = LoggerConfig( json={ \"root\": \"results/hard_prompt_esd_nudity_P4D_scissorhands\", \"name\": \"P4d\" } ) Sample hard_prompt config json for diffusers { \"overall\": { \"task\": \"P4D\", \"attacker\": \"hard_prompt\", \"logger\": \"json\", \"resume\": null }, \"task\": { \"concept\": \"nudity\", \"diffusers_model_name_or_path\": \"/home/ubuntu/Projects/dipesh/unlearn_diff/outputs/forget_me_not/finetuned_models/Abstractionism\", \"target_ckpt\": \"files/pretrained/SD-1-4/ESD_ckpt/Nudity-ESDx1-UNET-SD.pt\", \"cache_path\": \".cache\", \"dataset_path\": \"/home/ubuntu/Projects/Palistha/unlearn_diff_attack/outputs/dataset/i2p_nude\", \"criterion\": \"l2\", \"classifier_dir\": null, \"sampling_step_num\": 1, \"sld\": \"weak\", \"sld_concept\": \"nudity\", \"negative_prompt\": \"\", \"backend\":\"diffusers\" }, \"attacker\": { \"insertion_location\": \"prefix_k\", \"k\": 5, \"iteration\": 1, \"seed_iteration\": 1, \"attack_idx\": 0, \"eval_seed\": 0, \"universal\": false, \"sequential\": true, \"lr\": 0.01, \"weight_decay\": 0.1 }, \"logger\": { \"json\": { \"root\": \"results/hard_prompt_esd_nudity_P4D_semipermeable_membrane\", \"name\": \"P4d\" } } }","title":"Config"},{"location":"mu_attack/configs/no_attack/","text":"Sample no attack config for compvis # mu_attack/configs/nudity/no_attack_esd_nudity_classifier_compvis.py from mu_attack.core import BaseConfig, OverallConfig, TaskConfig, AttackerConfig, LoggerConfig class NoAttackESDNudityClassifierConfigCompvis(BaseConfig): overall: OverallConfig = OverallConfig( task=\"classifier\", attacker=\"no_attack\", logger=\"json\", resume=None ) task: TaskConfig = TaskConfig( classifier_dir=None, sampling_step_num=1, criterion=\"l1\", sld=\"weak\", sld_concept=\"nudity\", negative_prompt=\"sth\", backend=\"compvis\", diffusers_config_file = None, save_diffuser = False ) attacker: AttackerConfig = AttackerConfig( iteration=1, attack_idx=1, no_attack = { \"dataset_path\": \"outputs/dataset/i2p_nude\" } ) logger: LoggerConfig = LoggerConfig( json={\"root\": \"results/no_attack_esd_nudity_esd\", \"name\": \"NoAttackEsdNudity\"} ) no_attack_esd_nudity_classifier_compvis_config = NoAttackESDNudityClassifierConfigCompvis() Sample compvis config json for no attack { \"overall\": { \"task\": \"classifier\", \"attacker\": \"no_attack\", \"logger\": \"json\", \"resume\": null }, \"task\": { \"concept\": \"nudity\", \"compvis_ckpt_path\": \"outputs/esd/esd_Abstractionism_model.pth\", \"compvis_config_path\":\"mu/algorithms/esd/configs/model_config.yaml\", \"cache_path\": \".cache\", \"dataset_path\": \"outputs/dataset/i2p_nude\", \"criterion\": \"l1\", \"sampling_step_num\": 1, \"sld\": \"weak\", \"sld_concept\": \"nudity\", \"negative_prompt\": \"sth\", \"backend\":\"compvis\" }, \"attacker\": { \"insertion_location\": \"prefix_k\", \"k\": 5, \"seed_iteration\": 1, \"sequential\": true, \"iteration\": 40, \"attack_idx\": 1, \"eval_seed\": 0, \"universal\": false, \"no_attack\": { \"dataset_path\": \"outputs/dataset/i2p_nude\" } }, \"logger\": { \"json\": { \"root\": \"results/no_attack_esd_nudity_esd\", \"name\": \"NoAttackEsdNudity\" } } } Sample config for no attack for diffuser class NoAttackESDNudityClassifierConfigDiffusers(BaseConfig): overall: OverallConfig = OverallConfig( task=\"classifier\", attacker=\"no_attack\", logger=\"json\", resume=None ) task: TaskConfig = TaskConfig( classifier_dir=None, sampling_step_num=1, target_ckpt= \"files/pretrained/SD-1-4/ESD_ckpt/Nudity-ESDx1-UNET-SD.pt\", criterion=\"l1\", sld=\"weak\", sld_concept=\"nudity\", negative_prompt=\"sth\", backend=\"diffusers\" ) attacker: AttackerConfig = AttackerConfig( iteration=1, no_attack = { \"dataset_path\": \"outputs/dataset/i2p_nude\" } ) logger: LoggerConfig = LoggerConfig( json={\"root\": \"results/no_attack_esd_nudity_esd\", \"name\": \"NoAttackEsdNudity\"} ) Sample diffusers config json for no attack { \"overall\": { \"task\": \"classifier\", \"attacker\": \"no_attack\", \"logger\": \"json\", \"resume\": null }, \"task\": { \"concept\": \"nudity\", \"diffusers_model_name_or_path\": \"outputs/uce/finetuned_models/uce_Abstractionism_model\", \"target_ckpt\": \"files/pretrained/SD-1-4/ESD_ckpt/Nudity-ESDx1-UNET-SD.pt\", \"cache_path\": \".cache\", \"dataset_path\": \"outputs/dataset/i2p_nude\", \"criterion\": \"l1\", \"sampling_step_num\": 1, \"sld\": \"weak\", \"sld_concept\": \"nudity\", \"negative_prompt\": \"sth\", \"backend\":\"diffusers\" }, \"attacker\": { \"insertion_location\": \"prefix_k\", \"k\": 5, \"seed_iteration\": 1, \"sequential\": true, \"iteration\": 40, \"attack_idx\": 1, \"eval_seed\": 0, \"universal\": false, \"no_attack\": { \"dataset_path\": \"outputs/dataset/i2p_nude\" } }, \"logger\": { \"json\": { \"root\": \"results/no_attack_esd_nudity_uce\", \"name\": \"NoAttackEsdNudity\" } } }","title":"Config"},{"location":"mu_attack/configs/random/","text":"Sample config for random attack for compvis # mu_attack/configs/nudity/no_attack_esd_nudity_classifier_diffuser.py from mu_attack.core.base_config import BaseConfig, OverallConfig, TaskConfig, AttackerConfig, LoggerConfig class NoAttackESDNudityClassifierConfigDiffusers(BaseConfig): overall: OverallConfig = OverallConfig( task=\"classifier\", attacker=\"no_attack\", logger=\"json\", resume=None ) task: TaskConfig = TaskConfig( classifier_dir=None, sampling_step_num=1, target_ckpt= \"files/pretrained/SD-1-4/ESD_ckpt/Nudity-ESDx1-UNET-SD.pt\", criterion=\"l1\", sld=\"weak\", sld_concept=\"nudity\", negative_prompt=\"sth\", backend=\"diffusers\" ) attacker: AttackerConfig = AttackerConfig( iteration=1, no_attack = { \"dataset_path\": \"outputs/dataset/i2p_nude\" } ) logger: LoggerConfig = LoggerConfig( json={\"root\": \"results/no_attack_esd_nudity_esd\", \"name\": \"NoAttackEsdNudity\"} ) no_attack_esd_nudity_classifier_diffusers_config = NoAttackESDNudityClassifierConfigDiffusers() Sample compvis config json for random attack { \"overall\": { \"task\": \"classifier\", \"attacker\": \"random\", \"logger\": \"json\", \"resume\": null }, \"task\": { \"concept\": \"nudity\", \"compvis_ckpt_path\": \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"compvis_config_path\":\"mu/algorithms/scissorhands/configs/model_config.yaml\", \"cache_path\": \".cache\", \"dataset_path\": \"outputs/dataset/i2p_nude\", \"criterion\": \"l2\", \"sampling_step_num\": 1, \"sld\": \"weak\", \"sld_concept\": \"nudity\", \"negative_prompt\": \"sth\", \"backend\":\"compvis\" }, \"attacker\": { \"insertion_location\": \"prefix_k\", \"k\": 5, \"iteration\": 40, \"attack_idx\": 1, \"eval_seed\": 0, \"universal\": false, \"sequential\": true, \"seed_iteration\": 1 }, \"logger\": { \"json\": { \"root\": \"results/random_esd_nudity_scissorhands\", \"name\": \"Hard Prompt\" } } } Sample config for random attack for diffuser class RandomESDNudityDiffuser(BaseConfig): overall: OverallConfig = OverallConfig( task=\"classifier\", attacker=\"random\", logger=\"json\", resume=None ) task: TaskConfig = TaskConfig( sampling_step_num=1, sld=\"weak\", sld_concept=\"nudity\", negative_prompt=\"sth\", backend=\"diffusers\", target_ckpt=\"files/pretrained/SD-1-4/ESD_ckpt/Nudity-ESDx1-UNET-SD.pt\" ) attacker: AttackerConfig = AttackerConfig( sequential=True, attack_idx=1 ) logger: LoggerConfig = LoggerConfig( json={\"root\": \"results/random_esd_nudity_scissorhands\", \"name\": \"Hard Prompt\"} ) Sample diffusers config json for random attack { \"overall\": { \"task\": \"classifier\", \"attacker\": \"random\", \"logger\": \"json\", \"resume\": null }, \"task\": { \"concept\": \"nudity\", \"diffusers_model_name_or_path\": \"outputs/uce/finetuned_models/uce_Abstractionism_model\", \"target_ckpt\":\"files/pretrained/SD-1-4/ESD_ckpt/Nudity-ESDx1-UNET-SD.pt\", \"cache_path\": \".cache\", \"dataset_path\": \"outputs/dataset/i2p_nude\", \"criterion\": \"l2\", \"sampling_step_num\": 1, \"sld\": \"weak\", \"sld_concept\": \"nudity\", \"negative_prompt\": \"sth\", \"backend\":\"diffusers\" }, \"attacker\": { \"insertion_location\": \"prefix_k\", \"k\": 5, \"iteration\": 40, \"attack_idx\": 1, \"eval_seed\": 0, \"universal\": false, \"sequential\": true, \"seed_iteration\": 1 }, \"logger\": { \"json\": { \"root\": \"results/random_esd_nudity_uce\", \"name\": \"Hard Prompt\" } } }","title":"Config"},{"location":"mu_attack/configs/seed_search/","text":"Sample config for seed search attack for compvis # mu_attack/configs/nudity/seed_search_esd_nudity_classifier_compvis.py from mu_attack.core import BaseConfig, OverallConfig, TaskConfig, AttackerConfig, LoggerConfig class SeedSearchESDNudityClassifierCompvis(BaseConfig): overall: OverallConfig = OverallConfig( task=\"classifier\", attacker=\"seed_search\", logger=\"json\", resume=None ) task: TaskConfig = TaskConfig( sampling_step_num=1, sld=\"weak\", sld_concept=\"nudity\", negative_prompt=\"sth\", backend=\"compvis\", diffusers_config_file = None, save_diffuser = False ) attacker: AttackerConfig = AttackerConfig( sequential=True, attack_idx=1, ) logger: LoggerConfig = LoggerConfig( json={\"root\": \"results/seed_search_esd_nudity_scissorhands\", \"name\": \"Seed Search Nudity\"} ) seed_search_esd_nudity_classifier_compvis_config = SeedSearchESDNudityClassifierCompvis() Sample compvis config for seed search attack { \"overall\": { \"task\": \"classifier\", \"attacker\": \"seed_search\", \"logger\": \"json\", \"resume\": null }, \"task\": { \"concept\": \"nudity\", \"compvis_ckpt_path\": \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"compvis_config_path\":\"mu/algorithms/scissorhands/configs/model_config.yaml\", \"cache_path\": \".cache\", \"dataset_path\": \"outputs/dataset/i2p_nude\", \"criterion\": \"l1\", \"sampling_step_num\": 1, \"sld\": \"weak\", \"sld_concept\": \"nudity\", \"negative_prompt\": \"sth\", \"backend\":\"compvis\" }, \"attacker\": { \"insertion_location\": \"prefix_k\", \"k\": 5, \"iteration\": 40, \"attack_idx\": 1, \"eval_seed\": 0, \"universal\": false, \"sequential\": true, \"seed_iteration\": 1 }, \"logger\": { \"json\": { \"root\": \"results/seed_search_esd_nudity_scissorhands\", \"name\": \"Seed Search Nudity\" } } } Sample config for seed search diffuser class SeedSearchESDNudityClassifierDiffusers(BaseConfig): overall: OverallConfig = OverallConfig( task=\"classifier\", attacker=\"seed_search\", logger=\"json\", resume=None ) task: TaskConfig = TaskConfig( sampling_step_num=1, sld=\"weak\", sld_concept=\"nudity\", negative_prompt=\"sth\", backend=\"diffusers\", target_ckpt= \"files/pretrained/SD-1-4/ESD_ckpt/Nudity-ESDx1-UNET-SD.pt\" ) attacker: AttackerConfig = AttackerConfig( sequential=True, attack_idx=1, ) logger: LoggerConfig = LoggerConfig( json={\"root\": \"results/seed_search_esd_nudity_scissorhands\", \"name\": \"Seed Search Nudity\"} ) Sample diffusers config for seed search attack { \"overall\": { \"task\": \"classifier\", \"attacker\": \"seed_search\", \"logger\": \"json\", \"resume\": null }, \"task\": { \"concept\": \"nudity\", \"diffusers_model_name_or_path\": \"outputs/uce/finetuned_models/uce_Abstractionism_model\", \"target_ckpt\": \"files/pretrained/SD-1-4/ESD_ckpt/Nudity-ESDx1-UNET-SD.pt\", \"cache_path\": \".cache\", \"dataset_path\": \"outputs/dataset/i2p_nude\", \"criterion\": \"l1\", \"sampling_step_num\": 1, \"sld\": \"weak\", \"sld_concept\": \"nudity\", \"negative_prompt\": \"sth\", \"backend\":\"diffusers\" }, \"attacker\": { \"insertion_location\": \"prefix_k\", \"k\": 5, \"iteration\": 40, \"attack_idx\": 1, \"eval_seed\": 0, \"universal\": false, \"sequential\": true, \"seed_iteration\": 1 }, \"logger\": { \"json\": { \"root\": \"results/seed_search_esd_nudity_uce\", \"name\": \"Seed Search Nudity\" } } }","title":"Config"},{"location":"mu_attack/configs/text_grad/","text":"Sample config for text grad for compvis # mu_attack/configs/nudity/text_grad_esd_nudity_classifier_compvis.py from mu_attack.core import BaseConfig, OverallConfig, TaskConfig, AttackerConfig, LoggerConfig class TextGradESDNudityClassifierCompvis(BaseConfig): overall: OverallConfig = OverallConfig( task=\"classifier\", attacker=\"text_grad\", logger=\"json\", resume=None ) task: TaskConfig = TaskConfig( sampling_step_num=1, sld=\"weak\", sld_concept=\"nudity\", negative_prompt=\"sth\", backend=\"compvis\", diffusers_config_file = None, save_diffuser = False ) attacker: AttackerConfig = AttackerConfig( sequential=True, iteration = 1, text_grad = { \"lr\": 0.01, \"weight_decay\": 0.1 } ) logger: LoggerConfig = LoggerConfig( json={\"root\": \"results/text_grad_esd_nudity_classifier_scissorhands\", \"name\": \"TextGradNudity\"} ) text_grad_esd_nudity_classifier_compvis_config = TextGradESDNudityClassifierCompvis() Sample compvis config for text grad attack { \"overall\": { \"task\": \"classifier\", \"attacker\": \"text_grad\", \"logger\": \"json\", \"resume\": null }, \"task\": { \"concept\": \"nudity\", \"compvis_ckpt_path\": \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"compvis_config_path\":\"mu/algorithms/scissorhands/configs/model_config.yaml\", \"cache_path\": \".cache\", \"dataset_path\": \"outputs/dataset/i2p_nude\", \"criterion\": \"l2\", \"classifier_dir\": null, \"sampling_step_num\": 1, \"sld\": \"weak\", \"sld_concept\": \"nudity\", \"negative_prompt\": \"sth\", \"backend\":\"compvis\" }, \"attacker\": { \"insertion_location\": \"prefix_k\", \"k\": 5, \"iteration\": 1, \"seed_iteration\": 1, \"attack_idx\": 0, \"eval_seed\": 0, \"universal\": false, \"sequential\": true, \"text_grad\": { \"lr\": 0.01, \"weight_decay\": 0.1 } }, \"logger\": { \"json\": { \"root\": \"results/text_grad_esd_nudity_classifier_scissorhands\", \"name\": \"TextGradNudity\" } } } Sample config for text grad for diffusers class TextGradESDNudityClassifierDiffuser(BaseConfig): overall: OverallConfig = OverallConfig( task=\"classifier\", attacker=\"text_grad\", logger=\"json\", resume=None ) task: TaskConfig = TaskConfig( sampling_step_num=1, sld=\"weak\", sld_concept=\"nudity\", negative_prompt=\"sth\", backend=\"diffusers\" ) attacker: AttackerConfig = AttackerConfig( sequential=True, iteration = 1, text_grad = { \"lr\": 0.01, \"weight_decay\": 0.1 } ) logger: LoggerConfig = LoggerConfig( json={\"root\": \"results/text_grad_esd_nudity_classifier_scissorhands\", \"name\": \"TextGradNudity\"} ) Sample diffusers config for text grad attack { \"overall\": { \"task\": \"classifier\", \"attacker\": \"text_grad\", \"logger\": \"json\", \"resume\": null }, \"task\": { \"concept\": \"nudity\", \"diffusers_model_name_or_path\": \"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/diffuser/style50\", \"target_ckpt\": \"files/pretrained/SD-1-4/ESD_ckpt/Nudity-ESDx1-UNET-SD.pt\", \"cache_path\": \".cache\", \"dataset_path\": \"outputs/dataset/i2p_nude\", \"criterion\": \"l2\", \"classifier_dir\": null, \"sampling_step_num\": 1, \"sld\": \"weak\", \"sld_concept\": \"nudity\", \"negative_prompt\": \"sth\", \"backend\":\"diffusers\" }, \"attacker\": { \"insertion_location\": \"prefix_k\", \"k\": 5, \"iteration\": 1, \"seed_iteration\": 1, \"attack_idx\": 0, \"eval_seed\": 0, \"universal\": false, \"sequential\": true, \"text_grad\": { \"lr\": 0.01, \"weight_decay\": 0.1 } }, \"logger\": { \"json\": { \"root\": \"results/text_grad_esd_nudity_classifier_uce\", \"name\": \"TextGradNudity\" } } }","title":"Config"},{"location":"mu_defense/adv_unlearn/","text":"MU_DEFENSE This repository is for mu_defense that implements adversarial unlearning by integrating a soft prompt attack into the training loop. In this process, a random prompt is selected and its embedding is adversarially perturbed\u2014either at the word or conditional embedding level\u2014to steer the model into unlearning unwanted associations while preserving overall performance. Installation pip install unlearn_diff Prerequisities Ensure conda is installed on your system. You can install Miniconda or Anaconda: Miniconda (recommended): https://docs.conda.io/en/latest/miniconda.html Anaconda : https://www.anaconda.com/products/distribution After installing conda , ensure it is available in your PATH by running. You may require to restart the terminal session: Create Environment create_env <algorithm_name> Example create_env mu_defense Example usage to Run Defense for compvis To test the below code snippet, you can create a file, copy the below code in eg, mu_defense.py and execute it with python mu_defense.py or use WANDB_MODE=offline python mu_defense.py for offline mode. Run with default config from mu_defense.algorithms.adv_unlearn.algorithm import AdvUnlearnAlgorithm from mu_defense.algorithms.adv_unlearn.configs import adv_unlearn_config from mu.algorithms.erase_diff.configs import erase_diff_train_mu def mu_defense(): mu_defense = AdvUnlearnAlgorithm( config=adv_unlearn_config ) mu_defense.run() if __name__ == \"__main__\": mu_defense() Modify some train parameters in pre defined config class. View the config descriptions to see a list of available parameters. from mu_defense.algorithms.adv_unlearn.algorithm import AdvUnlearnAlgorithm from mu_defense.algorithms.adv_unlearn.configs import adv_unlearn_config from mu.algorithms.erase_diff.configs import erase_diff_train_mu def mu_defense(): mu_defense = AdvUnlearnAlgorithm( config=adv_unlearn_config, compvis_ckpt_path = \"/home/ubuntu/Projects/dipesh/unlearn_diff/outputs/erase_diff/erase_diff_Abstractionism_model.pth\", attack_step = 2, backend = \"compvis\", attack_method = \"fast_at\", warmup_iter = 1, iterations = 2, model_config_path = erase_diff_train_mu.model_config_path ) mu_defense.run() if __name__ == \"__main__\": mu_defense() Note: import the model_config_path from the relevant algorithm's configuration module in the mu package Example usage to Run defense for diffusers Run with default config from mu_defense.algorithms.adv_unlearn.algorithm import AdvUnlearnAlgorithm from mu_defense.algorithms.adv_unlearn.configs import adv_unlearn_config def mu_defense(): mu_defense = AdvUnlearnAlgorithm( config=adv_unlearn_config ) mu_defense.run() if __name__ == \"__main__\": mu_defense() Modify some train parameters in pre defined config class. View the config descriptions to see a list of available parameters. from mu_defense.algorithms.adv_unlearn.algorithm import AdvUnlearnAlgorithm from mu_defense.algorithms.adv_unlearn.configs import adv_unlearn_config def mu_defense(): mu_defense = AdvUnlearnAlgorithm( config=adv_unlearn_config, diffusers_model_name_or_path = \"/home/ubuntu/Projects/dipesh/unlearn_diff/outputs/forget_me_not/finetuned_models/Abstractionism\", attack_step = 2, backend = \"diffusers\", attack_method = \"fast_at\", warmup_iter = 1, iterations = 2 ) mu_defense.run() if __name__ == \"__main__\": mu_defense() Description of fields in config file Below is a detailed description of the configuration fields available in the adv_unlearn_config.py file. The descriptions match those provided in the help section of the command-line arguments. Inference & Model Paths model_config_path Description : Config path for stable diffusion model. Use for compvis model only. Type : str Example : configs/stable-diffusion/v1-inference.yaml compvis_ckpt_path Description : Checkpoint path for stable diffusion v1-4. Type : str Example : models/sd-v1-4-full-ema.ckpt encoder_model_name_or_path Description : Model name or path for the encoder. Type : str Example : CompVis/stable-diffusion-v1-4 cache_path Description : Directory used for caching model files. Type : str Example : .cache diffusers_model_name_or_path Description : Model name or path for the diffusers (if used). Type : str Example : outputs/forget_me_not/finetuned_models/Abstractionism target_ckpt Description : Optionally load a target checkpoint into the model for diffuser sampling. Type : Typically str or None Example : path to target checkpoint path Devices & IO devices Description : CUDA devices to train on. Type : str Example : 0,0 seperator Description : Separator used if you want to train a bunch of words separately. Type : str or None Example : None output_dir Description : Directory where output files (e.g., checkpoints, logs) are saved. Type : str Example : outputs/adv_unlearn Image & Diffusion Sampling image_size Description : Image size used to train. Type : int Example : 512 ddim_steps Description : Number of DDIM steps for inference during training. Type : int Example : 50 start_guidance Description : Guidance of start image used to train. Type : float Example : 3.0 negative_guidance Description : Guidance of negative training used to train. Type : float Example : 1.0 ddim_eta Description : DDIM eta parameter for sampling. Type : int or float Example : 0 Training Setup prompt Description : Prompt corresponding to the concept to erase. Type : str Example : nudity dataset_retain Description : Prompts corresponding to non-target concepts to retain. Type : str Choices : coco_object , coco_object_no_filter , imagenet243 , imagenet243_no_filter Example : coco_object retain_batch Description : Batch size of retaining prompts during training. Type : int Example : 5 retain_train Description : Retaining training mode; choose between iterative ( iter ) or regularization ( reg ). Type : str Choices : iter , reg Example : iter retain_step Description : Number of steps for retaining prompts. Type : int Example : 1 retain_loss_w Description : Retaining loss weight. Type : float Example : 1.0 train_method Description : Method of training. Type : str Choices : text_encoder_full , text_encoder_layer0 , text_encoder_layer01 , text_encoder_layer012 , text_encoder_layer0123 , text_encoder_layer01234 , text_encoder_layer012345 , text_encoder_layer0123456 , text_encoder_layer01234567 , text_encoder_layer012345678 , text_encoder_layer0123456789 , text_encoder_layer012345678910 , text_encoder_layer01234567891011 , text_encoder_layer0_11 , text_encoder_layer01_1011 , text_encoder_layer012_91011 , noxattn , selfattn , xattn , full , notime , xlayer , selflayer Example : text_encoder_full norm_layer Description : Flag indicating whether to update the norm layer during training. Type : bool Example : False attack_method Description : Method for adversarial attack training. Type : str Choices : pgd , multi_pgd , fast_at , free_at Example : pgd component Description : Component to apply the attack on. Type : str Choices : all , ffn , attn Example : all iterations Description : Total number of training iterations. Type : int Example : 10 (Note: The help argument may default to a higher value, e.g., 1000, but the config file sets it to 10.) save_interval Description : Interval (in iterations) at which checkpoints are saved. Type : int Example : 200 lr Description : Learning rate used during training. Type : float Example : 1e-5 Adversarial Attack Hyperparameters adv_prompt_num Description : Number of prompt tokens for adversarial soft prompt learning. Type : int Example : 1 attack_embd_type Description : The adversarial embedding type; options are word embedding or condition embedding. Type : str Choices : word_embd , condition_embd Example : word_embd attack_type Description : The type of adversarial attack applied to the prompt. Type : str Choices : replace_k , add , prefix_k , suffix_k , mid_k , insert_k , per_k_words Example : prefix_k attack_init Description : Strategy for initializing the adversarial attack; either randomly or using the latest parameters. Type : str Choices : random , latest Example : latest attack_step Description : Number of steps for the adversarial attack. Type : int Example : 30 attack_init_embd Description : Initial embedding for the attack (optional). Type : Depends on implementation; default is None Example : None adv_prompt_update_step Description : Frequency (in iterations) at which the adversarial prompt is updated. Type : int Example : 1 attack_lr Description : Learning rate for adversarial attack training. Type : float Example : 1e-3 warmup_iter Description : Number of warmup iterations before starting the adversarial attack. Type : int Example : 200 Backend backend Description : Backend framework to be used (e.g., CompVis). Type : str Example : compvis Choices : compvis or diffusers Directory Structure algorithm.py : Implementation of the AdvUnlearnAlgorithm class. configs/ : Contains configuration files for AdvUnlearn for compvis and diffusers. model.py : Implementation of the AdvUnlearnModel class for compvis and diffusers. trainer.py : Trainer for adversarial unlearning for compvis and diffusers. utils.py : Utility functions used in the project. dataset_handler.py : handles prompt cleaning and retaining dataset creation for adversarial unlearning. compvis_trainer.py : Trainer for adversarial unlearning for compvis. diffusers_trainer.py : Trainer for adversarial unlearning for diffusers.","title":"Usage"},{"location":"mu_defense/adv_unlearn/#mu_defense","text":"This repository is for mu_defense that implements adversarial unlearning by integrating a soft prompt attack into the training loop. In this process, a random prompt is selected and its embedding is adversarially perturbed\u2014either at the word or conditional embedding level\u2014to steer the model into unlearning unwanted associations while preserving overall performance.","title":"MU_DEFENSE"},{"location":"mu_defense/adv_unlearn/#installation","text":"pip install unlearn_diff","title":"Installation"},{"location":"mu_defense/adv_unlearn/#prerequisities","text":"Ensure conda is installed on your system. You can install Miniconda or Anaconda: Miniconda (recommended): https://docs.conda.io/en/latest/miniconda.html Anaconda : https://www.anaconda.com/products/distribution After installing conda , ensure it is available in your PATH by running. You may require to restart the terminal session:","title":"Prerequisities"},{"location":"mu_defense/adv_unlearn/#create-environment","text":"create_env <algorithm_name> Example create_env mu_defense","title":"Create Environment"},{"location":"mu_defense/adv_unlearn/#example-usage-to-run-defense-for-compvis","text":"To test the below code snippet, you can create a file, copy the below code in eg, mu_defense.py and execute it with python mu_defense.py or use WANDB_MODE=offline python mu_defense.py for offline mode.","title":"Example usage to Run Defense for compvis"},{"location":"mu_defense/adv_unlearn/#run-with-default-config","text":"from mu_defense.algorithms.adv_unlearn.algorithm import AdvUnlearnAlgorithm from mu_defense.algorithms.adv_unlearn.configs import adv_unlearn_config from mu.algorithms.erase_diff.configs import erase_diff_train_mu def mu_defense(): mu_defense = AdvUnlearnAlgorithm( config=adv_unlearn_config ) mu_defense.run() if __name__ == \"__main__\": mu_defense()","title":"Run with default config"},{"location":"mu_defense/adv_unlearn/#modify-some-train-parameters-in-pre-defined-config-class","text":"View the config descriptions to see a list of available parameters. from mu_defense.algorithms.adv_unlearn.algorithm import AdvUnlearnAlgorithm from mu_defense.algorithms.adv_unlearn.configs import adv_unlearn_config from mu.algorithms.erase_diff.configs import erase_diff_train_mu def mu_defense(): mu_defense = AdvUnlearnAlgorithm( config=adv_unlearn_config, compvis_ckpt_path = \"/home/ubuntu/Projects/dipesh/unlearn_diff/outputs/erase_diff/erase_diff_Abstractionism_model.pth\", attack_step = 2, backend = \"compvis\", attack_method = \"fast_at\", warmup_iter = 1, iterations = 2, model_config_path = erase_diff_train_mu.model_config_path ) mu_defense.run() if __name__ == \"__main__\": mu_defense() Note: import the model_config_path from the relevant algorithm's configuration module in the mu package","title":"Modify some train parameters in pre defined config class."},{"location":"mu_defense/adv_unlearn/#example-usage-to-run-defense-for-diffusers","text":"","title":"Example usage to Run defense for diffusers"},{"location":"mu_defense/adv_unlearn/#run-with-default-config_1","text":"from mu_defense.algorithms.adv_unlearn.algorithm import AdvUnlearnAlgorithm from mu_defense.algorithms.adv_unlearn.configs import adv_unlearn_config def mu_defense(): mu_defense = AdvUnlearnAlgorithm( config=adv_unlearn_config ) mu_defense.run() if __name__ == \"__main__\": mu_defense()","title":"Run with default config"},{"location":"mu_defense/adv_unlearn/#modify-some-train-parameters-in-pre-defined-config-class_1","text":"View the config descriptions to see a list of available parameters. from mu_defense.algorithms.adv_unlearn.algorithm import AdvUnlearnAlgorithm from mu_defense.algorithms.adv_unlearn.configs import adv_unlearn_config def mu_defense(): mu_defense = AdvUnlearnAlgorithm( config=adv_unlearn_config, diffusers_model_name_or_path = \"/home/ubuntu/Projects/dipesh/unlearn_diff/outputs/forget_me_not/finetuned_models/Abstractionism\", attack_step = 2, backend = \"diffusers\", attack_method = \"fast_at\", warmup_iter = 1, iterations = 2 ) mu_defense.run() if __name__ == \"__main__\": mu_defense()","title":"Modify some train parameters in pre defined config class."},{"location":"mu_defense/adv_unlearn/#description-of-fields-in-config-file","text":"Below is a detailed description of the configuration fields available in the adv_unlearn_config.py file. The descriptions match those provided in the help section of the command-line arguments. Inference & Model Paths model_config_path Description : Config path for stable diffusion model. Use for compvis model only. Type : str Example : configs/stable-diffusion/v1-inference.yaml compvis_ckpt_path Description : Checkpoint path for stable diffusion v1-4. Type : str Example : models/sd-v1-4-full-ema.ckpt encoder_model_name_or_path Description : Model name or path for the encoder. Type : str Example : CompVis/stable-diffusion-v1-4 cache_path Description : Directory used for caching model files. Type : str Example : .cache diffusers_model_name_or_path Description : Model name or path for the diffusers (if used). Type : str Example : outputs/forget_me_not/finetuned_models/Abstractionism target_ckpt Description : Optionally load a target checkpoint into the model for diffuser sampling. Type : Typically str or None Example : path to target checkpoint path Devices & IO devices Description : CUDA devices to train on. Type : str Example : 0,0 seperator Description : Separator used if you want to train a bunch of words separately. Type : str or None Example : None output_dir Description : Directory where output files (e.g., checkpoints, logs) are saved. Type : str Example : outputs/adv_unlearn Image & Diffusion Sampling image_size Description : Image size used to train. Type : int Example : 512 ddim_steps Description : Number of DDIM steps for inference during training. Type : int Example : 50 start_guidance Description : Guidance of start image used to train. Type : float Example : 3.0 negative_guidance Description : Guidance of negative training used to train. Type : float Example : 1.0 ddim_eta Description : DDIM eta parameter for sampling. Type : int or float Example : 0 Training Setup prompt Description : Prompt corresponding to the concept to erase. Type : str Example : nudity dataset_retain Description : Prompts corresponding to non-target concepts to retain. Type : str Choices : coco_object , coco_object_no_filter , imagenet243 , imagenet243_no_filter Example : coco_object retain_batch Description : Batch size of retaining prompts during training. Type : int Example : 5 retain_train Description : Retaining training mode; choose between iterative ( iter ) or regularization ( reg ). Type : str Choices : iter , reg Example : iter retain_step Description : Number of steps for retaining prompts. Type : int Example : 1 retain_loss_w Description : Retaining loss weight. Type : float Example : 1.0 train_method Description : Method of training. Type : str Choices : text_encoder_full , text_encoder_layer0 , text_encoder_layer01 , text_encoder_layer012 , text_encoder_layer0123 , text_encoder_layer01234 , text_encoder_layer012345 , text_encoder_layer0123456 , text_encoder_layer01234567 , text_encoder_layer012345678 , text_encoder_layer0123456789 , text_encoder_layer012345678910 , text_encoder_layer01234567891011 , text_encoder_layer0_11 , text_encoder_layer01_1011 , text_encoder_layer012_91011 , noxattn , selfattn , xattn , full , notime , xlayer , selflayer Example : text_encoder_full norm_layer Description : Flag indicating whether to update the norm layer during training. Type : bool Example : False attack_method Description : Method for adversarial attack training. Type : str Choices : pgd , multi_pgd , fast_at , free_at Example : pgd component Description : Component to apply the attack on. Type : str Choices : all , ffn , attn Example : all iterations Description : Total number of training iterations. Type : int Example : 10 (Note: The help argument may default to a higher value, e.g., 1000, but the config file sets it to 10.) save_interval Description : Interval (in iterations) at which checkpoints are saved. Type : int Example : 200 lr Description : Learning rate used during training. Type : float Example : 1e-5 Adversarial Attack Hyperparameters adv_prompt_num Description : Number of prompt tokens for adversarial soft prompt learning. Type : int Example : 1 attack_embd_type Description : The adversarial embedding type; options are word embedding or condition embedding. Type : str Choices : word_embd , condition_embd Example : word_embd attack_type Description : The type of adversarial attack applied to the prompt. Type : str Choices : replace_k , add , prefix_k , suffix_k , mid_k , insert_k , per_k_words Example : prefix_k attack_init Description : Strategy for initializing the adversarial attack; either randomly or using the latest parameters. Type : str Choices : random , latest Example : latest attack_step Description : Number of steps for the adversarial attack. Type : int Example : 30 attack_init_embd Description : Initial embedding for the attack (optional). Type : Depends on implementation; default is None Example : None adv_prompt_update_step Description : Frequency (in iterations) at which the adversarial prompt is updated. Type : int Example : 1 attack_lr Description : Learning rate for adversarial attack training. Type : float Example : 1e-3 warmup_iter Description : Number of warmup iterations before starting the adversarial attack. Type : int Example : 200 Backend backend Description : Backend framework to be used (e.g., CompVis). Type : str Example : compvis Choices : compvis or diffusers","title":"Description of fields in config file"},{"location":"mu_defense/adv_unlearn/#directory-structure","text":"algorithm.py : Implementation of the AdvUnlearnAlgorithm class. configs/ : Contains configuration files for AdvUnlearn for compvis and diffusers. model.py : Implementation of the AdvUnlearnModel class for compvis and diffusers. trainer.py : Trainer for adversarial unlearning for compvis and diffusers. utils.py : Utility functions used in the project. dataset_handler.py : handles prompt cleaning and retaining dataset creation for adversarial unlearning. compvis_trainer.py : Trainer for adversarial unlearning for compvis. diffusers_trainer.py : Trainer for adversarial unlearning for diffusers.","title":"Directory Structure"},{"location":"mu_defense/config/","text":"Sample config for Advattack (mu_defense) #mu_defense/algorithms/adv_unlearn/configs/adv_unlearn_config.py import os from pathlib import Path from mu_defense.core.base_config import BaseConfig class AdvUnlearnConfig(BaseConfig): def __init__(self, **kwargs): # Inference & Model Paths self.model_config_path = \"configs/stable-diffusion/v1-inference.yaml\" #for compvis self.compvis_ckpt_path = \"models/sd-v1-4-full-ema.ckpt\" self.encoder_model_name_or_path = \"CompVis/stable-diffusion-v1-4\" self.cache_path = \".cache\" self.diffusers_model_name_or_path = \"\" self.target_ckpt = None #Optionally load a target checkpoint into model for diffuser sampling # Devices & IO self.devices = \"0,0\" # You can later parse this string into a list if needed. self.seperator = None self.output_dir = \"outputs/adv_unlearn\" # Image & Diffusion Sampling self.image_size = 512 self.ddim_steps = 50 self.start_guidance = 3.0 self.negative_guidance = 1.0 # Training Setup self.prompt = \"nudity\" self.dataset_retain = \"coco_object\" # Choices: 'coco_object', 'coco_object_no_filter', 'imagenet243', 'imagenet243_no_filter' self.retain_batch = 5 self.retain_train = \"iter\" # Options: 'iter' or 'reg' self.retain_step = 1 self.retain_loss_w = 1.0 self.ddim_eta = 0 self.train_method = \"text_encoder_full\" #choices: text_encoder_full', 'text_encoder_layer0', 'text_encoder_layer01', 'text_encoder_layer012', 'text_encoder_layer0123', 'text_encoder_layer01234', 'text_encoder_layer012345', 'text_encoder_layer0123456', 'text_encoder_layer01234567', 'text_encoder_layer012345678', 'text_encoder_layer0123456789', 'text_encoder_layer012345678910', 'text_encoder_layer01234567891011', 'text_encoder_layer0_11','text_encoder_layer01_1011', 'text_encoder_layer012_91011', 'noxattn', 'selfattn', 'xattn', 'full', 'notime', 'xlayer', 'selflayer self.norm_layer = False # This is a flag; use True if you wish to update the norm layer. self.attack_method = \"pgd\" # Choices: 'pgd', 'multi_pgd', 'fast_at', 'free_at' self.component = \"all\" # Choices: 'all', 'ffn', 'attn' self.iterations = 10 self.save_interval = 200 self.lr = 1e-5 # Adversarial Attack Hyperparameters self.adv_prompt_num = 1 self.attack_embd_type = \"word_embd\" # Choices: 'word_embd', 'condition_embd' self.attack_type = \"prefix_k\" # Choices: 'replace_k', 'add', 'prefix_k', 'suffix_k', 'mid_k', 'insert_k', 'per_k_words' self.attack_init = \"latest\" # Choices: 'random', 'latest' self.attack_step = 30 self.attack_init_embd = None self.adv_prompt_update_step = 1 self.attack_lr = 1e-3 self.warmup_iter = 200 #backend self.backend = \"compvis\" # Override default values with any provided keyword arguments. for key, value in kwargs.items(): setattr(self, key, value) def validate_config(self): \"\"\" Perform basic validation on the config parameters. \"\"\" if self.retain_batch <= 0: raise ValueError(\"retain_batch should be a positive integer.\") if self.lr <= 0: raise ValueError(\"Learning rate (lr) should be positive.\") if self.image_size <= 0: raise ValueError(\"Image size should be a positive integer.\") if self.iterations <= 0: raise ValueError(\"Iterations must be a positive integer.\") if not os.path.exists(self.output_dir): os.makedirs(self.output_dir) adv_unlearn_config = AdvUnlearnConfig() Sample config for image generator for mu_defense # mu_defense/algorithms/adv_unlearn/configs/example_img_generator_config.py import os from mu.core.base_config import BaseConfig class ImageGeneratorConfig(BaseConfig): def __init__(self): self.model_name = \"SD-v1-4\" self.target_ckpt = \"\" self.save_path = \"\" self.prompts_path = \"data/prompts/visualization_example.csv\" self.device = \"0\" self.guidance_scale = 7.5 self.image_size = 512 self.ddim_steps = 100 self.num_samples = 1 self.from_case = 0 self.folder_suffix = \"\" self.origin_or_target = \"target\" #target or origin self.encoder_model_name_or_path = \"CompVis/stable-diffusion-v1-4\" def validate_config(self): \"\"\" Perform basic validation on the config parameters. \"\"\" if not os.path.exists(self.prompt_path): raise FileNotFoundError(f\"Prompt dataset file {self.prompt_path} does not exist.\") example_image_generator_config = ImageGeneratorConfig() Sample config for evaluation framework for mu_defense # mu_defense/algorithms/adv_unlearn/configs/evaluation_config.py import os from mu.core.base_config import BaseConfig class MUDefenseEvaluationConfig(BaseConfig): def __init__(self): self.job = \"fid, clip\" self.gen_imgs_path = \"outputs/adv_unlearn/models_visualizations_imagenette/SD-v1-4/\" self.coco_imgs_path = \"coco_dataset/extracted_files/coco_sample\" self.prompt_path = \"data/prompts/coco_10k.csv\" self.classify_prompt_path = \"data/prompts/imagenette_5k.csv\" self.devices = \"0,0\" self.classification_model_path = \"openai/clip-vit-base-patch32\" self.output_path = \"outputs/adv_unlearn/evaluation\" def validate_config(self): \"\"\" Perform basic validation on the config parameters. \"\"\" if not os.path.exists(self.prompt_path): raise FileNotFoundError(f\"Prompt dataset file {self.prompt_path} does not exist.\") mu_defense_evaluation_config = MUDefenseEvaluationConfig()","title":"Config"},{"location":"mu_defense/config/#sample-config-for-advattack-mu_defense","text":"#mu_defense/algorithms/adv_unlearn/configs/adv_unlearn_config.py import os from pathlib import Path from mu_defense.core.base_config import BaseConfig class AdvUnlearnConfig(BaseConfig): def __init__(self, **kwargs): # Inference & Model Paths self.model_config_path = \"configs/stable-diffusion/v1-inference.yaml\" #for compvis self.compvis_ckpt_path = \"models/sd-v1-4-full-ema.ckpt\" self.encoder_model_name_or_path = \"CompVis/stable-diffusion-v1-4\" self.cache_path = \".cache\" self.diffusers_model_name_or_path = \"\" self.target_ckpt = None #Optionally load a target checkpoint into model for diffuser sampling # Devices & IO self.devices = \"0,0\" # You can later parse this string into a list if needed. self.seperator = None self.output_dir = \"outputs/adv_unlearn\" # Image & Diffusion Sampling self.image_size = 512 self.ddim_steps = 50 self.start_guidance = 3.0 self.negative_guidance = 1.0 # Training Setup self.prompt = \"nudity\" self.dataset_retain = \"coco_object\" # Choices: 'coco_object', 'coco_object_no_filter', 'imagenet243', 'imagenet243_no_filter' self.retain_batch = 5 self.retain_train = \"iter\" # Options: 'iter' or 'reg' self.retain_step = 1 self.retain_loss_w = 1.0 self.ddim_eta = 0 self.train_method = \"text_encoder_full\" #choices: text_encoder_full', 'text_encoder_layer0', 'text_encoder_layer01', 'text_encoder_layer012', 'text_encoder_layer0123', 'text_encoder_layer01234', 'text_encoder_layer012345', 'text_encoder_layer0123456', 'text_encoder_layer01234567', 'text_encoder_layer012345678', 'text_encoder_layer0123456789', 'text_encoder_layer012345678910', 'text_encoder_layer01234567891011', 'text_encoder_layer0_11','text_encoder_layer01_1011', 'text_encoder_layer012_91011', 'noxattn', 'selfattn', 'xattn', 'full', 'notime', 'xlayer', 'selflayer self.norm_layer = False # This is a flag; use True if you wish to update the norm layer. self.attack_method = \"pgd\" # Choices: 'pgd', 'multi_pgd', 'fast_at', 'free_at' self.component = \"all\" # Choices: 'all', 'ffn', 'attn' self.iterations = 10 self.save_interval = 200 self.lr = 1e-5 # Adversarial Attack Hyperparameters self.adv_prompt_num = 1 self.attack_embd_type = \"word_embd\" # Choices: 'word_embd', 'condition_embd' self.attack_type = \"prefix_k\" # Choices: 'replace_k', 'add', 'prefix_k', 'suffix_k', 'mid_k', 'insert_k', 'per_k_words' self.attack_init = \"latest\" # Choices: 'random', 'latest' self.attack_step = 30 self.attack_init_embd = None self.adv_prompt_update_step = 1 self.attack_lr = 1e-3 self.warmup_iter = 200 #backend self.backend = \"compvis\" # Override default values with any provided keyword arguments. for key, value in kwargs.items(): setattr(self, key, value) def validate_config(self): \"\"\" Perform basic validation on the config parameters. \"\"\" if self.retain_batch <= 0: raise ValueError(\"retain_batch should be a positive integer.\") if self.lr <= 0: raise ValueError(\"Learning rate (lr) should be positive.\") if self.image_size <= 0: raise ValueError(\"Image size should be a positive integer.\") if self.iterations <= 0: raise ValueError(\"Iterations must be a positive integer.\") if not os.path.exists(self.output_dir): os.makedirs(self.output_dir) adv_unlearn_config = AdvUnlearnConfig()","title":"Sample config for Advattack (mu_defense)"},{"location":"mu_defense/config/#sample-config-for-image-generator-for-mu_defense","text":"# mu_defense/algorithms/adv_unlearn/configs/example_img_generator_config.py import os from mu.core.base_config import BaseConfig class ImageGeneratorConfig(BaseConfig): def __init__(self): self.model_name = \"SD-v1-4\" self.target_ckpt = \"\" self.save_path = \"\" self.prompts_path = \"data/prompts/visualization_example.csv\" self.device = \"0\" self.guidance_scale = 7.5 self.image_size = 512 self.ddim_steps = 100 self.num_samples = 1 self.from_case = 0 self.folder_suffix = \"\" self.origin_or_target = \"target\" #target or origin self.encoder_model_name_or_path = \"CompVis/stable-diffusion-v1-4\" def validate_config(self): \"\"\" Perform basic validation on the config parameters. \"\"\" if not os.path.exists(self.prompt_path): raise FileNotFoundError(f\"Prompt dataset file {self.prompt_path} does not exist.\") example_image_generator_config = ImageGeneratorConfig()","title":"Sample config for image generator for mu_defense"},{"location":"mu_defense/config/#sample-config-for-evaluation-framework-for-mu_defense","text":"# mu_defense/algorithms/adv_unlearn/configs/evaluation_config.py import os from mu.core.base_config import BaseConfig class MUDefenseEvaluationConfig(BaseConfig): def __init__(self): self.job = \"fid, clip\" self.gen_imgs_path = \"outputs/adv_unlearn/models_visualizations_imagenette/SD-v1-4/\" self.coco_imgs_path = \"coco_dataset/extracted_files/coco_sample\" self.prompt_path = \"data/prompts/coco_10k.csv\" self.classify_prompt_path = \"data/prompts/imagenette_5k.csv\" self.devices = \"0,0\" self.classification_model_path = \"openai/clip-vit-base-patch32\" self.output_path = \"outputs/adv_unlearn/evaluation\" def validate_config(self): \"\"\" Perform basic validation on the config parameters. \"\"\" if not os.path.exists(self.prompt_path): raise FileNotFoundError(f\"Prompt dataset file {self.prompt_path} does not exist.\") mu_defense_evaluation_config = MUDefenseEvaluationConfig()","title":"Sample config for evaluation framework for mu_defense"},{"location":"mu_defense/evaluation/","text":"Evaluation for mu_defense This section provides instructions for running the evaluation framework for the unlearned Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying adversial unlearning. Image Generator Before proceeding with evaluation, you must generate images using the output from mu_defense. To generate images add the following code snippet in image_generator.py and modify your configs to run the file. Example code Run with default config from mu_defense.algorithms.adv_unlearn.configs import example_image_generator_config from mu_defense.algorithms.adv_unlearn import ImageGenerator from mu.algorithms.erase_diff.configs import erase_diff_train_mu def generate_image(): generate_image = ImageGenerator( config = example_image_generator_config ) generate_image.generate_images() if __name__ == \"__main__\": generate_image() Run with your configs Check the config descriptions to use your own confgs. from mu_defense.algorithms.adv_unlearn.configs import example_image_generator_config from mu_defense.algorithms.adv_unlearn import ImageGenerator from mu.algorithms.erase_diff.configs import erase_diff_train_mu def generate_image(): generate_image = ImageGenerator( config = example_image_generator_config, target_ckpt = \"outputs/adv_unlearn/models/TextEncoder-text_encoder_full-epoch_0.pt\", model_config_path = erase_diff_train_mu.model_config_path, save_path = \"outputs/adv_unlearn/models\", prompts_path = \"data/prompts/sample_prompt.csv\", num_samples = 1, folder_suffix = \"imagenette\", devices = \"0\" ) generate_image.generate_images() if __name__ == \"__main__\": generate_image() Running the image generation Script in Offline Mode WANDB_MODE=offline python image_generator.py How It Works Default Values: The script first loads default values from the evluation config file as in configs section. Parameter Overrides: Any parameters passed directly to the algorithm, overrides these configs. Final Configuration: The script merges the configs and convert them into dictionary to proceed with the evaluation. Description of parameters in image_generator_config model_name: Type: str Description: Name of the model to use. Options include \"SD-v1-4\" , \"SD-V2\" , \"SD-V2-1\" , etc. required: False encoder_model_name_or_path Description : Model name or path for the encoder. Type : str Example : CompVis/stable-diffusion-v1-4 target_ckpt: Type: str Description: Path to the target checkpoint. If empty, the script will load the default model weights. If provided, it supports both Diffusers-format checkpoints (directory) and CompVis checkpoints (file ending with .pt ). For CompVis, use the checkpoint of the model saved as Diffuser format. save_path: Type: str Description: Directory where the generated images will be saved. prompts_path: Type: str Description: Path to the CSV file containing prompts, evaluation seeds, and case numbers. Default: \"data/prompts/visualization_example.csv\" device: Type: str Description: Device(s) used for image generation. For example, \"0\" will use cuda:0 . guidance_scale: Type: float Description: Parameter that controls the classifier-free guidance during generation. Default: 7.5 image_size: Type: int Description: Dimensions of the generated images (height and width). Default: 512 ddim_steps: Type: int Description: Number of denoising steps (used in the diffusion process). Default: 100 num_samples: Type: int Description: Number of samples generated for each prompt. Default: 1 from_case: Type: int Description: Minimum case number from which to start generating images. Default: 0 folder_suffix: Type: str Description: Suffix added to the output folder name for visualizations. origin_or_target: Type: str Description: Indicates whether to generate images for the \"target\" model or the \"origin\" . Default: \"target\" Running the Evaluation Framework Create a file, eg, evaluate.py and use examples and modify your configs to run the file. Example code Run with default config from mu_defense.algorithms.adv_unlearn import MUDefenseEvaluator from mu_defense.algorithms.adv_unlearn.configs import mu_defense_evaluation_config def mu_defense_evaluator(): evaluator = MUDefenseEvaluator( config = mu_defense_evaluation_config ) evaluator.run() if __name__ == \"__main__\": mu_defense_evaluator() Run with your own config from mu_defense.algorithms.adv_unlearn import MUDefenseEvaluator from mu_defense.algorithms.adv_unlearn.configs import mu_defense_evaluation_config def mu_defense_evaluator(): evaluator = MUDefenseEvaluator( config = mu_defense_evaluation_config, gen_imgs_path = \"outputs/adv_unlearn/models_visualizations_imagenette/SD-v1-4/\", coco_imgs_path = \"coco_dataset/extracted_files/coco_sample\", output_path = \"outputs/adv_unlearn/evaluation/\", job = \"clip\", #donot use this if you want to calculate both clip and fid score ) evaluator.run() if __name__ == \"__main__\": mu_defense_evaluator() Running the evaluation Script in Offline Mode WANDB_MODE=offline python evaluate.py Description of Evaluation Configuration Parameters job: Type: str Description: Evaluation tasks to perform. If nothing is passed it cacluates both. Example: \"fid\" or \"clip\" gen_imgs_path: Type: str Description: Path to the directory containing the generated images (from adversarial unlearning). Example: \"outputs/adv_unlearn/models_visualizations_imagenette/SD-v1-4/\" coco_imgs_path: Type: str Description: Path to the directory containing COCO dataset images for evaluation. Example: \"coco_dataset/extracted_files/coco_sample\" prompt_path: Type: str Description: Path to the CSV file containing prompts for evaluation. Example: \"data/prompts/coco_10k.csv\" classify_prompt_path: Type: str Description: Path to the CSV file containing classification prompts. Example: \"data/prompts/imagenette_5k.csv\" devices: Type: str Description: Comma-separated list of device IDs to be used during evaluation. Example: \"0,0\" classification_model_path: Type: str Description: Path or identifier of the classification model to use. Example: \"openai/clip-vit-base-patch32\" output_path: Type: str Description: Directory where the evaluation results will be saved. Example: \"outputs/adv_unlearn/evaluation\"","title":"Usage"},{"location":"mu_defense/evaluation/#evaluation-for-mu_defense","text":"This section provides instructions for running the evaluation framework for the unlearned Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying adversial unlearning.","title":"Evaluation for mu_defense"},{"location":"mu_defense/evaluation/#image-generator","text":"Before proceeding with evaluation, you must generate images using the output from mu_defense. To generate images add the following code snippet in image_generator.py and modify your configs to run the file. Example code Run with default config from mu_defense.algorithms.adv_unlearn.configs import example_image_generator_config from mu_defense.algorithms.adv_unlearn import ImageGenerator from mu.algorithms.erase_diff.configs import erase_diff_train_mu def generate_image(): generate_image = ImageGenerator( config = example_image_generator_config ) generate_image.generate_images() if __name__ == \"__main__\": generate_image() Run with your configs Check the config descriptions to use your own confgs. from mu_defense.algorithms.adv_unlearn.configs import example_image_generator_config from mu_defense.algorithms.adv_unlearn import ImageGenerator from mu.algorithms.erase_diff.configs import erase_diff_train_mu def generate_image(): generate_image = ImageGenerator( config = example_image_generator_config, target_ckpt = \"outputs/adv_unlearn/models/TextEncoder-text_encoder_full-epoch_0.pt\", model_config_path = erase_diff_train_mu.model_config_path, save_path = \"outputs/adv_unlearn/models\", prompts_path = \"data/prompts/sample_prompt.csv\", num_samples = 1, folder_suffix = \"imagenette\", devices = \"0\" ) generate_image.generate_images() if __name__ == \"__main__\": generate_image() Running the image generation Script in Offline Mode WANDB_MODE=offline python image_generator.py How It Works Default Values: The script first loads default values from the evluation config file as in configs section. Parameter Overrides: Any parameters passed directly to the algorithm, overrides these configs. Final Configuration: The script merges the configs and convert them into dictionary to proceed with the evaluation.","title":"Image Generator"},{"location":"mu_defense/evaluation/#description-of-parameters-in-image_generator_config","text":"model_name: Type: str Description: Name of the model to use. Options include \"SD-v1-4\" , \"SD-V2\" , \"SD-V2-1\" , etc. required: False encoder_model_name_or_path Description : Model name or path for the encoder. Type : str Example : CompVis/stable-diffusion-v1-4 target_ckpt: Type: str Description: Path to the target checkpoint. If empty, the script will load the default model weights. If provided, it supports both Diffusers-format checkpoints (directory) and CompVis checkpoints (file ending with .pt ). For CompVis, use the checkpoint of the model saved as Diffuser format. save_path: Type: str Description: Directory where the generated images will be saved. prompts_path: Type: str Description: Path to the CSV file containing prompts, evaluation seeds, and case numbers. Default: \"data/prompts/visualization_example.csv\" device: Type: str Description: Device(s) used for image generation. For example, \"0\" will use cuda:0 . guidance_scale: Type: float Description: Parameter that controls the classifier-free guidance during generation. Default: 7.5 image_size: Type: int Description: Dimensions of the generated images (height and width). Default: 512 ddim_steps: Type: int Description: Number of denoising steps (used in the diffusion process). Default: 100 num_samples: Type: int Description: Number of samples generated for each prompt. Default: 1 from_case: Type: int Description: Minimum case number from which to start generating images. Default: 0 folder_suffix: Type: str Description: Suffix added to the output folder name for visualizations. origin_or_target: Type: str Description: Indicates whether to generate images for the \"target\" model or the \"origin\" . Default: \"target\"","title":"Description of parameters in image_generator_config"},{"location":"mu_defense/evaluation/#running-the-evaluation-framework","text":"Create a file, eg, evaluate.py and use examples and modify your configs to run the file. Example code Run with default config from mu_defense.algorithms.adv_unlearn import MUDefenseEvaluator from mu_defense.algorithms.adv_unlearn.configs import mu_defense_evaluation_config def mu_defense_evaluator(): evaluator = MUDefenseEvaluator( config = mu_defense_evaluation_config ) evaluator.run() if __name__ == \"__main__\": mu_defense_evaluator() Run with your own config from mu_defense.algorithms.adv_unlearn import MUDefenseEvaluator from mu_defense.algorithms.adv_unlearn.configs import mu_defense_evaluation_config def mu_defense_evaluator(): evaluator = MUDefenseEvaluator( config = mu_defense_evaluation_config, gen_imgs_path = \"outputs/adv_unlearn/models_visualizations_imagenette/SD-v1-4/\", coco_imgs_path = \"coco_dataset/extracted_files/coco_sample\", output_path = \"outputs/adv_unlearn/evaluation/\", job = \"clip\", #donot use this if you want to calculate both clip and fid score ) evaluator.run() if __name__ == \"__main__\": mu_defense_evaluator() Running the evaluation Script in Offline Mode WANDB_MODE=offline python evaluate.py","title":"Running the Evaluation Framework"},{"location":"mu_defense/evaluation/#description-of-evaluation-configuration-parameters","text":"job: Type: str Description: Evaluation tasks to perform. If nothing is passed it cacluates both. Example: \"fid\" or \"clip\" gen_imgs_path: Type: str Description: Path to the directory containing the generated images (from adversarial unlearning). Example: \"outputs/adv_unlearn/models_visualizations_imagenette/SD-v1-4/\" coco_imgs_path: Type: str Description: Path to the directory containing COCO dataset images for evaluation. Example: \"coco_dataset/extracted_files/coco_sample\" prompt_path: Type: str Description: Path to the CSV file containing prompts for evaluation. Example: \"data/prompts/coco_10k.csv\" classify_prompt_path: Type: str Description: Path to the CSV file containing classification prompts. Example: \"data/prompts/imagenette_5k.csv\" devices: Type: str Description: Comma-separated list of device IDs to be used during evaluation. Example: \"0,0\" classification_model_path: Type: str Description: Path or identifier of the classification model to use. Example: \"openai/clip-vit-base-patch32\" output_path: Type: str Description: Directory where the evaluation results will be saved. Example: \"outputs/adv_unlearn/evaluation\"","title":"Description of Evaluation Configuration Parameters"},{"location":"unlearn/algorithms/concept_ablation/","text":"Concept Ablation Algorithm for Machine Unlearning This repository provides an implementation of the Concept Ablation algorithm for machine unlearning in Stable Diffusion models. The Concept Ablation algorithm enables the removal of specific concepts or styles from a pre-trained model without the need for retraining from scratch. Installation Create the Conda Environment First, create and activate the Conda environment using the provided environment.yaml file: conda env create -f mu/algorithms/concept_ablation/environment.yaml -n mu_concept_ablation conda --version Create environment: create_env <algorithm_name> eg: create_env concept_ablation Activate environment: conda activate <environment_name> eg: conda activate concept_ablation The has to be one of the folders in the mu/algorithms folder. Downloading data and models. After you install the package, you can use the following commands to download. Dataset : i2p : Sample : download_data sample i2p Full : download_data full i2p quick_canvas : Sample : download_data sample quick_canvas Full : download_data full quick_canvas Model : compvis : download_model compvis diffuser : download_model diffuser Verify the Downloaded Files After downloading, verify that the datasets have been correctly extracted: ls -lh ./data/i2p-dataset/sample/ ls -lh ./data/quick-canvas-dataset/sample/ Example usage Add the following code snippet to a python script trainer.py . Run the script using python trainer.py . from mu.algorithms.concept_ablation.algorithm import ( ConceptAblationAlgorithm, ) from mu.algorithms.concept_ablation.configs import ( concept_ablation_train_mu, ConceptAblationConfig, ) if __name__ == \"__main__\": concept_ablation_train_mu.lightning.trainer.max_steps = 5 algorithm = ConceptAblationAlgorithm( concept_ablation_train_mu, config_path=\"/home/ubuntu/Projects/balaram/msu_unlearningalgorithm/mu/algorithms/concept_ablation/configs/train_config.yaml\", ckpt_path=\"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\", prompts=\"/home/ubuntu/Projects/balaram/msu_unlearningalgorithm/mu/algorithms/concept_ablation/data/anchor_prompts/finetune_prompts/sd_prompt_Architectures_sample.txt\", output_dir=\"/opt/dlami/nvme/outputs\", # devices=\"1\", ) algorithm.run() Notes Ensure all dependencies are installed as per the environment file. The training process generates logs in the logs/ directory for easy monitoring. Use appropriate CUDA devices for optimal performance during training. Regularly verify dataset and model configurations to avoid errors during execution. Directory Structure algorithm.py : Core implementation of the Concept Ablation Algorithm. configs/ : Configuration files for training and generation. data_handler.py : Data handling and preprocessing. scripts/train.py : Script to train the Concept Ablation Algorithm. callbacks/ : Custom callbacks for logging and monitoring training. utils.py : Utility functions. Configuration File ( train_config.yaml ) Training Parameters seed: Random seed for reproducibility. Type: int Example: 23 scale_lr: Whether to scale the base learning rate. Type: bool Example: True caption_target: Target style to remove. Type: str Example: \"Abstractionism Style\" regularization: Adds regularization loss during training. Type: bool Example: True n_samples: Number of batch sizes for image generation. Type: int Example: 10 train_size: Number of generated images for training. Type: int Example: 1000 base_lr: Learning rate for the optimizer. Type: float Example: 2.0e-06 Model Configuration model_config_path: Path to the Stable Diffusion model configuration YAML file. Type: str Example: \"/path/to/model_config.yaml\" ckpt_path: Path to the Stable Diffusion model checkpoint. Type: str Example: \"/path/to/compvis.ckpt\" Dataset Directories raw_dataset_dir: Directory containing the raw dataset categorized by themes or classes. Type: str Example: \"/path/to/raw_dataset\" processed_dataset_dir: Directory to save the processed dataset. Type: str Example: \"/path/to/processed_dataset\" dataset_type: Specifies the dataset type for training. Choices: [\"unlearncanvas\", \"i2p\"] Example: \"unlearncanvas\" template: Type of template to use during training. Choices: [\"object\", \"style\", \"i2p\"] Example: \"style\" template_name: Name of the concept or style to erase. Choices: [\"self-harm\", \"Abstractionism\"] Example: \"Abstractionism\" Output Configurations output_dir: Directory to save fine-tuned models and results. Type: str Example: \"outputs/concept_ablation/finetuned_models\" Device Configuration devices: CUDA devices for training (comma-separated). Type: str Example: \"0\" Concept ablation Evaluation Framework This section provides instructions for running the evaluation framework for the Concept ablation algorithm on Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying machine unlearning. Running the Evaluation Framework You can run the evaluation framework using the evaluate.py script located in the mu/algorithms/concept_ablation/scripts/ directory. Basic Command to Run Evaluation: conda activate <env_name> python -m mu.algorithms.concept_ablation.scripts.evaluate \\ --config_path mu/algorithms/concept_ablation/configs/evaluation_config.yaml Running in Offline Mode: WANDB_MODE=offline python -m mu.algorithms.concept_ablation.scripts.evaluate \\ --config_path mu/algorithms/concept_ablation/configs/evaluation_config.yaml Example with CLI Overrides: python -m mu.algorithms.concept_ablation.scripts.evaluate \\ --config_path mu/algorithms/concept_ablation/configs/evaluation_config.yaml \\ --devices \"0\" \\ --seed 123 \\ --cfg_text 8.5 \\ --batch_size 16 Description of parameters in evaluation_config.yaml The evaluation_config.yaml file contains the necessary parameters for running the Concept ablation evaluation framework. Below is a detailed description of each parameter along with examples. Model Configuration: model_config : Path to the YAML file specifying the model architecture and settings. Type: str Example: \"mu/algorithms/concept_ablation/configs/model_config.yaml\" ckpt_path : Path to the finetuned Stable Diffusion checkpoint file to be evaluated. Type: str Example: \"outputs/concept_ablation/finetuned_models/concept_ablation_Abstractionism_model.pth\" classification_model : Specifies the classification model used for evaluating the generated outputs. Type: str Example: \"vit_large_patch16_224\" model_ckpt_path: Path to pretrained Stable Diffusion model. Type : str Example : models/compvis/style50/compvis.ckpt Training and Sampling Parameters: theme : Specifies the theme or concept being evaluated for removal from the model's outputs. Type: str Example: \"Bricks\" devices : CUDA device IDs to be used for the evaluation process. Type: str Example: \"0\" cfg_text : Classifier-free guidance scale value for image generation. Higher values increase the strength of the conditioning prompt. Type: float Example: 9.0 seed : Random seed for reproducibility of results. Type: int Example: 188 ddim_steps : Number of steps for the DDIM (Denoising Diffusion Implicit Models) sampling process. Type: int Example: 100 ddim_eta : DDIM eta value for controlling the amount of randomness during sampling. Set to 0 for deterministic sampling. Type: float Example: 0.0 image_height : Height of the generated images in pixels. Type: int Example: 512 image_width : Width of the generated images in pixels. Type: int Example: 512 Output and Logging Parameters: sampler_output_dir : Directory where generated images will be saved during evaluation. Type: str Example: \"outputs/eval_results/mu_results/concept_ablation/\" eval_output_dir : Directory where evaluation metrics and results will be stored. Type: str Example: \"outputs/eval_results/mu_results/concept_ablation/\" reference_dir : Directory containing original images for comparison during evaluation. Type: str Example: \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\" Performance and Efficiency Parameters: multiprocessing : Enables multiprocessing for faster evaluation for FID score. Recommended for large datasets. Type: bool Example: False batch_size : Batch size used during FID computation and evaluation. Type: int Example: 16 Optimization Parameters: forget_theme : Concept or style intended for removal in the evaluation process. Type: str Example: \"Bricks\" seed_list : List of random seeds for performing multiple evaluations with different randomness levels. Type: list Example: [\"188\"]","title":"Usage"},{"location":"unlearn/algorithms/concept_ablation/#concept-ablation-algorithm-for-machine-unlearning","text":"This repository provides an implementation of the Concept Ablation algorithm for machine unlearning in Stable Diffusion models. The Concept Ablation algorithm enables the removal of specific concepts or styles from a pre-trained model without the need for retraining from scratch.","title":"Concept Ablation Algorithm for Machine Unlearning"},{"location":"unlearn/algorithms/concept_ablation/#installation","text":"","title":"Installation"},{"location":"unlearn/algorithms/concept_ablation/#create-the-conda-environment","text":"First, create and activate the Conda environment using the provided environment.yaml file: conda env create -f mu/algorithms/concept_ablation/environment.yaml -n mu_concept_ablation conda --version","title":"Create the Conda Environment"},{"location":"unlearn/algorithms/concept_ablation/#create-environment","text":"create_env <algorithm_name> eg: create_env concept_ablation","title":"Create environment:"},{"location":"unlearn/algorithms/concept_ablation/#activate-environment","text":"conda activate <environment_name> eg: conda activate concept_ablation The has to be one of the folders in the mu/algorithms folder.","title":"Activate environment:"},{"location":"unlearn/algorithms/concept_ablation/#downloading-data-and-models","text":"After you install the package, you can use the following commands to download. Dataset : i2p : Sample : download_data sample i2p Full : download_data full i2p quick_canvas : Sample : download_data sample quick_canvas Full : download_data full quick_canvas Model : compvis : download_model compvis diffuser : download_model diffuser Verify the Downloaded Files After downloading, verify that the datasets have been correctly extracted: ls -lh ./data/i2p-dataset/sample/ ls -lh ./data/quick-canvas-dataset/sample/","title":"Downloading data and models."},{"location":"unlearn/algorithms/concept_ablation/#example-usage","text":"Add the following code snippet to a python script trainer.py . Run the script using python trainer.py . from mu.algorithms.concept_ablation.algorithm import ( ConceptAblationAlgorithm, ) from mu.algorithms.concept_ablation.configs import ( concept_ablation_train_mu, ConceptAblationConfig, ) if __name__ == \"__main__\": concept_ablation_train_mu.lightning.trainer.max_steps = 5 algorithm = ConceptAblationAlgorithm( concept_ablation_train_mu, config_path=\"/home/ubuntu/Projects/balaram/msu_unlearningalgorithm/mu/algorithms/concept_ablation/configs/train_config.yaml\", ckpt_path=\"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\", prompts=\"/home/ubuntu/Projects/balaram/msu_unlearningalgorithm/mu/algorithms/concept_ablation/data/anchor_prompts/finetune_prompts/sd_prompt_Architectures_sample.txt\", output_dir=\"/opt/dlami/nvme/outputs\", # devices=\"1\", ) algorithm.run()","title":"Example usage"},{"location":"unlearn/algorithms/concept_ablation/#notes","text":"Ensure all dependencies are installed as per the environment file. The training process generates logs in the logs/ directory for easy monitoring. Use appropriate CUDA devices for optimal performance during training. Regularly verify dataset and model configurations to avoid errors during execution.","title":"Notes"},{"location":"unlearn/algorithms/concept_ablation/#directory-structure","text":"algorithm.py : Core implementation of the Concept Ablation Algorithm. configs/ : Configuration files for training and generation. data_handler.py : Data handling and preprocessing. scripts/train.py : Script to train the Concept Ablation Algorithm. callbacks/ : Custom callbacks for logging and monitoring training. utils.py : Utility functions.","title":"Directory Structure"},{"location":"unlearn/algorithms/concept_ablation/#configuration-file-train_configyaml","text":"","title":"Configuration File (train_config.yaml)"},{"location":"unlearn/algorithms/concept_ablation/#training-parameters","text":"seed: Random seed for reproducibility. Type: int Example: 23 scale_lr: Whether to scale the base learning rate. Type: bool Example: True caption_target: Target style to remove. Type: str Example: \"Abstractionism Style\" regularization: Adds regularization loss during training. Type: bool Example: True n_samples: Number of batch sizes for image generation. Type: int Example: 10 train_size: Number of generated images for training. Type: int Example: 1000 base_lr: Learning rate for the optimizer. Type: float Example: 2.0e-06","title":"Training Parameters"},{"location":"unlearn/algorithms/concept_ablation/#model-configuration","text":"model_config_path: Path to the Stable Diffusion model configuration YAML file. Type: str Example: \"/path/to/model_config.yaml\" ckpt_path: Path to the Stable Diffusion model checkpoint. Type: str Example: \"/path/to/compvis.ckpt\"","title":"Model Configuration"},{"location":"unlearn/algorithms/concept_ablation/#dataset-directories","text":"raw_dataset_dir: Directory containing the raw dataset categorized by themes or classes. Type: str Example: \"/path/to/raw_dataset\" processed_dataset_dir: Directory to save the processed dataset. Type: str Example: \"/path/to/processed_dataset\" dataset_type: Specifies the dataset type for training. Choices: [\"unlearncanvas\", \"i2p\"] Example: \"unlearncanvas\" template: Type of template to use during training. Choices: [\"object\", \"style\", \"i2p\"] Example: \"style\" template_name: Name of the concept or style to erase. Choices: [\"self-harm\", \"Abstractionism\"] Example: \"Abstractionism\"","title":"Dataset Directories"},{"location":"unlearn/algorithms/concept_ablation/#output-configurations","text":"output_dir: Directory to save fine-tuned models and results. Type: str Example: \"outputs/concept_ablation/finetuned_models\"","title":"Output Configurations"},{"location":"unlearn/algorithms/concept_ablation/#device-configuration","text":"devices: CUDA devices for training (comma-separated). Type: str Example: \"0\"","title":"Device Configuration"},{"location":"unlearn/algorithms/concept_ablation/#concept-ablation-evaluation-framework","text":"This section provides instructions for running the evaluation framework for the Concept ablation algorithm on Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying machine unlearning.","title":"Concept ablation Evaluation Framework"},{"location":"unlearn/algorithms/concept_ablation/#running-the-evaluation-framework","text":"You can run the evaluation framework using the evaluate.py script located in the mu/algorithms/concept_ablation/scripts/ directory.","title":"Running the Evaluation Framework"},{"location":"unlearn/algorithms/concept_ablation/#basic-command-to-run-evaluation","text":"conda activate <env_name> python -m mu.algorithms.concept_ablation.scripts.evaluate \\ --config_path mu/algorithms/concept_ablation/configs/evaluation_config.yaml Running in Offline Mode: WANDB_MODE=offline python -m mu.algorithms.concept_ablation.scripts.evaluate \\ --config_path mu/algorithms/concept_ablation/configs/evaluation_config.yaml Example with CLI Overrides: python -m mu.algorithms.concept_ablation.scripts.evaluate \\ --config_path mu/algorithms/concept_ablation/configs/evaluation_config.yaml \\ --devices \"0\" \\ --seed 123 \\ --cfg_text 8.5 \\ --batch_size 16","title":"Basic Command to Run Evaluation:"},{"location":"unlearn/algorithms/concept_ablation/#description-of-parameters-in-evaluation_configyaml","text":"The evaluation_config.yaml file contains the necessary parameters for running the Concept ablation evaluation framework. Below is a detailed description of each parameter along with examples.","title":"Description of parameters in evaluation_config.yaml"},{"location":"unlearn/algorithms/concept_ablation/#model-configuration_1","text":"model_config : Path to the YAML file specifying the model architecture and settings. Type: str Example: \"mu/algorithms/concept_ablation/configs/model_config.yaml\" ckpt_path : Path to the finetuned Stable Diffusion checkpoint file to be evaluated. Type: str Example: \"outputs/concept_ablation/finetuned_models/concept_ablation_Abstractionism_model.pth\" classification_model : Specifies the classification model used for evaluating the generated outputs. Type: str Example: \"vit_large_patch16_224\" model_ckpt_path: Path to pretrained Stable Diffusion model. Type : str Example : models/compvis/style50/compvis.ckpt","title":"Model Configuration:"},{"location":"unlearn/algorithms/concept_ablation/#training-and-sampling-parameters","text":"theme : Specifies the theme or concept being evaluated for removal from the model's outputs. Type: str Example: \"Bricks\" devices : CUDA device IDs to be used for the evaluation process. Type: str Example: \"0\" cfg_text : Classifier-free guidance scale value for image generation. Higher values increase the strength of the conditioning prompt. Type: float Example: 9.0 seed : Random seed for reproducibility of results. Type: int Example: 188 ddim_steps : Number of steps for the DDIM (Denoising Diffusion Implicit Models) sampling process. Type: int Example: 100 ddim_eta : DDIM eta value for controlling the amount of randomness during sampling. Set to 0 for deterministic sampling. Type: float Example: 0.0 image_height : Height of the generated images in pixels. Type: int Example: 512 image_width : Width of the generated images in pixels. Type: int Example: 512","title":"Training and Sampling Parameters:"},{"location":"unlearn/algorithms/concept_ablation/#output-and-logging-parameters","text":"sampler_output_dir : Directory where generated images will be saved during evaluation. Type: str Example: \"outputs/eval_results/mu_results/concept_ablation/\" eval_output_dir : Directory where evaluation metrics and results will be stored. Type: str Example: \"outputs/eval_results/mu_results/concept_ablation/\" reference_dir : Directory containing original images for comparison during evaluation. Type: str Example: \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\"","title":"Output and Logging Parameters:"},{"location":"unlearn/algorithms/concept_ablation/#performance-and-efficiency-parameters","text":"multiprocessing : Enables multiprocessing for faster evaluation for FID score. Recommended for large datasets. Type: bool Example: False batch_size : Batch size used during FID computation and evaluation. Type: int Example: 16","title":"Performance and Efficiency Parameters:"},{"location":"unlearn/algorithms/concept_ablation/#optimization-parameters","text":"forget_theme : Concept or style intended for removal in the evaluation process. Type: str Example: \"Bricks\" seed_list : List of random seeds for performing multiple evaluations with different randomness levels. Type: list Example: [\"188\"]","title":"Optimization Parameters:"},{"location":"unlearn/algorithms/contributing/","text":"Contributing to Unlearn Diff Thank you for your interest in contributing to Unlearn ! This document outlines the steps and best practices for adding new algorithms, making pull requests, filing issues, and more. Table of Contents Introduction Code of Conduct Project Structure How to Contribute Reporting Issues Suggesting Enhancements Submitting Pull Requests Adding a New Algorithm Folder Structure Creating an Environment Documentation Code Style Contact Introduction Unlearn is an open-source Python package designed for unlearning algorithms in diffusion models. We welcome contributions from the community, whether it\u2019s a new feature, bug fix, or an entirely new unlearning algorithm. Code of Conduct Please note that we expect all contributors to abide by our code of conduct. Be respectful and considerate of others, and help keep our community healthy and inclusive. Project Structure A quick overview of relevant folders: mu/algorithms/ This is where all algorithms reside. Each algorithm has its own subfolder containing: algorithm.py (core logic) data_handler.py (if data-processing is needed) datasets/ (special dataset classes) environment.yaml (conda environment details) evaluator.py (evaluation routines) model.py (model definitions) sampler.py (sampling methods) scripts/ (training, evaluation scripts, etc.) trainer.py (training loop, optimization) utils.py (auxiliary functions) Readme.md (algorithm-specific documentation) mu/core/ Contains abstract/base classes ( base_algorithm.py , base_trainer.py , etc.) that algorithms can extend or override. mu/datasets/ Common dataset classes and utilities used by multiple algorithms. helpers/ Shared utility functions (e.g., logging, path setup, config loading). mu_attack/ & mu_defense/ : These directories contain the implementation of attack strategies and defensive unlearning mechanisms, respectively. Each contains: Attack Modules ( mu_attack/ ) : attackers/ : Different attack implementations (e.g., hard/soft prompts, seed search). configs/ : YAML configuration files for attack routines. tasks/ : Task definitions to evaluate attack efficacy. Defense Modules ( mu_defense/ ) : algorithms/ : Defensive algorithms including adversarial unlearning methods. configs/ : Configurations for training and evaluating defenses. Other Directories : data/ : Contains datasets. docs/ : Project documentation and API references. outputs/ : Generated outputs from algorithms. logs/ : Log files for debugging. models/ : Trained models and checkpoints. tests/ : Test suites to ensure code reliability. stable_diffusion/ & lora_diffusion/ : Diffusion components. How to Contribute Reporting Issues Check the to see if your bug or feature request has already been reported. If not, open a new issue. Provide a clear title and description, including steps to reproduce if it\u2019s a bug. Label your issue appropriately (e.g., \u201cbug\u201d, \u201cenhancement\u201d, \u201cdocumentation\u201d). Suggesting Enhancements If you have an idea for a new feature or improvement, open a GitHub issue labeled \u201cenhancement\u201d or \u201cfeature request.\u201d Include any relevant use-cases, examples, or background information to help the community understand your proposal. Submitting Pull Requests Fork the repository and create your feature branch from main (or the appropriate branch if instructed otherwise): bash git checkout -b feature/my-awesome-feature Make your changes, including tests if applicable. Commit and push your changes: bash git push origin feature/my-awesome-feature Open a Pull Request from your fork into the main repo. Provide a clear description of what your PR does, referencing any related issues. Please ensure your PR follows the Code Style guidelines and includes updated or new tests if needed. Adding a New Algorithm One of the core goals of Unlearn Diff is to make it easy to add and benchmark new unlearning algorithms. Follow these steps when introducing a new method. Folder Structure Create a new subfolder in mu/algorithms/ with a clear, descriptive name (e.g., my_new_algo ). Inside this subfolder, include the following (at minimum): algorithm.py trainer.py scripts/train.py (and optionally scripts/evaluate.py if your training and evaluation are separate) configs/ containing YAML config files for training, evaluation, etc. environment.yaml specifying dependencies for conda Readme.md explaining how to install, run, and configure your new algorithm (Optional) data_handler.py , datasets/ , model.py , sampler.py , utils.py , etc., depending on your algorithm\u2019s design. Extend or import from base classes in mu/core/ if your algorithm logic aligns with any existing abstract classes (e.g., BaseAlgorithm , BaseTrainer , etc.). This ensures code consistency and easier maintenance. Adding a New Attack To add a new attack method, follow these guidelines: Folder Structure : Create a new file or subfolder under mu_attack/attackers/ with a clear, descriptive name (e.g., my_new_attack.py or mu_attack/attackers/my_new_attack/ ). If creating a subfolder, include essential files specific to attack implementation: Attacks : The main implementation file for the attack logic (e.g., attack.py ). Execs : Scripts or modules that execute the attack routines. Tasks : Task definitions for integrating and testing the attack. Any helper modules or configuration files specific to your attack. Update or add corresponding YAML configuration files and config class under mu_attack/configs/ if your attack requires custom settings. Implementation : Extend or import from the base class BaseAttacker (located in mu_attack/core/base_attacker.py ) if applicable. Ensure that your attack method adheres to the input-output standards defined by the project. Documentation & Testing : Add detailed documentation within your new attack module and update the main documentation if needed. Include tests covering your new attack method under the appropriate test directories. Environment : If your attack method has unique dependencies, update the environment.yaml file within the relevant directory or provide instructions in your documentation on how to create a dedicated environment. Also update the common environment file that is located in the project's root directory. Adding a New Defense To integrate a new defense mechanism, please follow these steps: Folder Structure : Create a new subfolder under mu_defense/algorithms/ with a descriptive name (e.g., my_new_defense ). Within this subfolder, include essential files such as: algorithm.py : Contains the core logic of your defense method. trainer.py : Contains training routines and optimization strategies. configs/ : Include configuration class for training and evaluation. environment.yaml : Specify dependencies unique to your defense method. Readme.md : Document usage instructions, configuration details, and any other relevant information. Implementation : Extend or use base classes provided in mu_defense/algorithms/core/ (e.g., base_algorithm.py , base_trainer.py ) to ensure consistency with existing methods. Implement any unique evaluation metrics or procedures if your defense requires them. Documentation & Testing : Document your defense method thoroughly within its Readme.md and update the global documentation if necessary. Provide tests for your defense implementation to ensure its reliability and compatibility with the rest of the system. Environment : If your defense algorithm has specific dependencies, use the provided environment.yaml file as a template and adjust it accordingly. Include clear instructions for users to create and activate the environment. Also update the common environment file that is located in the project's root directory. Creating an Environment The default environment file is located in the project root directory ( environment.yaml ). Contributors should update this file as needed to include any new packages required by their contributions. If your module or algorithm requires unique dependencies, you may add a dedicated environment file in its respective directory, but be sure to update and maintain the default environment in the root. Optionally, to keep dependencies organized, each algorithm has its own environment.yaml . Contributors are encouraged to: Specify all required packages (e.g., torch , torchvision , diffusers , etc.). Name the environment after the algorithm, e.g., mu_<algorithm_name> . Update instructions in your algorithm\u2019s Readme.md on how to install and activate this environment. Documentation Each algorithm folder must contain a Readme.md that documents: High-level description of the algorithm (goals, references to papers, etc.). How to install any special dependencies (if not covered by environment.yaml ). How to run training and evaluation (with example commands). Explanation of the config files in configs/ . Any unique parameters or hyperparameters. If you introduce new metrics or logs, please outline them in your algorithm\u2019s README and update any relevant docs in the main docs/ folder as needed. Code Style We recommend following PEP 8 for Python code. Use descriptive variable names, docstrings, and type hints where possible. Keep functions and methods short and focused\u2014avoid large, monolithic methods. Use Python\u2019s built-in logging or the logging utilities in helpers/logger.py for consistent log outputs. Use wandb global wandb logger if required. Additionally, we prefer: - 4 spaces for indentation (no tabs). - Meaningful commit messages: \u201cFix dataset loader bug for large images\u201d vs. \u201cFix stuff.\u201d Contact If you have any questions, feel free to open an issue or reach out to us via email. We appreciate your contributions and look forward to collaborating with you on Unlearn Diff ! Thank you again for your interest in contributing. We\u2019re excited to see your ideas and improvements!","title":"docs"},{"location":"unlearn/algorithms/contributing/#contributing-to-unlearn-diff","text":"Thank you for your interest in contributing to Unlearn ! This document outlines the steps and best practices for adding new algorithms, making pull requests, filing issues, and more.","title":"Contributing to Unlearn Diff"},{"location":"unlearn/algorithms/contributing/#table-of-contents","text":"Introduction Code of Conduct Project Structure How to Contribute Reporting Issues Suggesting Enhancements Submitting Pull Requests Adding a New Algorithm Folder Structure Creating an Environment Documentation Code Style Contact","title":"Table of Contents"},{"location":"unlearn/algorithms/contributing/#introduction","text":"Unlearn is an open-source Python package designed for unlearning algorithms in diffusion models. We welcome contributions from the community, whether it\u2019s a new feature, bug fix, or an entirely new unlearning algorithm.","title":"Introduction"},{"location":"unlearn/algorithms/contributing/#code-of-conduct","text":"Please note that we expect all contributors to abide by our code of conduct. Be respectful and considerate of others, and help keep our community healthy and inclusive.","title":"Code of Conduct"},{"location":"unlearn/algorithms/contributing/#project-structure","text":"A quick overview of relevant folders: mu/algorithms/ This is where all algorithms reside. Each algorithm has its own subfolder containing: algorithm.py (core logic) data_handler.py (if data-processing is needed) datasets/ (special dataset classes) environment.yaml (conda environment details) evaluator.py (evaluation routines) model.py (model definitions) sampler.py (sampling methods) scripts/ (training, evaluation scripts, etc.) trainer.py (training loop, optimization) utils.py (auxiliary functions) Readme.md (algorithm-specific documentation) mu/core/ Contains abstract/base classes ( base_algorithm.py , base_trainer.py , etc.) that algorithms can extend or override. mu/datasets/ Common dataset classes and utilities used by multiple algorithms. helpers/ Shared utility functions (e.g., logging, path setup, config loading). mu_attack/ & mu_defense/ : These directories contain the implementation of attack strategies and defensive unlearning mechanisms, respectively. Each contains: Attack Modules ( mu_attack/ ) : attackers/ : Different attack implementations (e.g., hard/soft prompts, seed search). configs/ : YAML configuration files for attack routines. tasks/ : Task definitions to evaluate attack efficacy. Defense Modules ( mu_defense/ ) : algorithms/ : Defensive algorithms including adversarial unlearning methods. configs/ : Configurations for training and evaluating defenses. Other Directories : data/ : Contains datasets. docs/ : Project documentation and API references. outputs/ : Generated outputs from algorithms. logs/ : Log files for debugging. models/ : Trained models and checkpoints. tests/ : Test suites to ensure code reliability. stable_diffusion/ & lora_diffusion/ : Diffusion components.","title":"Project Structure"},{"location":"unlearn/algorithms/contributing/#how-to-contribute","text":"","title":"How to Contribute"},{"location":"unlearn/algorithms/contributing/#reporting-issues","text":"Check the to see if your bug or feature request has already been reported. If not, open a new issue. Provide a clear title and description, including steps to reproduce if it\u2019s a bug. Label your issue appropriately (e.g., \u201cbug\u201d, \u201cenhancement\u201d, \u201cdocumentation\u201d).","title":"Reporting Issues"},{"location":"unlearn/algorithms/contributing/#suggesting-enhancements","text":"If you have an idea for a new feature or improvement, open a GitHub issue labeled \u201cenhancement\u201d or \u201cfeature request.\u201d Include any relevant use-cases, examples, or background information to help the community understand your proposal.","title":"Suggesting Enhancements"},{"location":"unlearn/algorithms/contributing/#submitting-pull-requests","text":"Fork the repository and create your feature branch from main (or the appropriate branch if instructed otherwise): bash git checkout -b feature/my-awesome-feature Make your changes, including tests if applicable. Commit and push your changes: bash git push origin feature/my-awesome-feature Open a Pull Request from your fork into the main repo. Provide a clear description of what your PR does, referencing any related issues. Please ensure your PR follows the Code Style guidelines and includes updated or new tests if needed.","title":"Submitting Pull Requests"},{"location":"unlearn/algorithms/contributing/#adding-a-new-algorithm","text":"One of the core goals of Unlearn Diff is to make it easy to add and benchmark new unlearning algorithms. Follow these steps when introducing a new method.","title":"Adding a New Algorithm"},{"location":"unlearn/algorithms/contributing/#folder-structure","text":"Create a new subfolder in mu/algorithms/ with a clear, descriptive name (e.g., my_new_algo ). Inside this subfolder, include the following (at minimum): algorithm.py trainer.py scripts/train.py (and optionally scripts/evaluate.py if your training and evaluation are separate) configs/ containing YAML config files for training, evaluation, etc. environment.yaml specifying dependencies for conda Readme.md explaining how to install, run, and configure your new algorithm (Optional) data_handler.py , datasets/ , model.py , sampler.py , utils.py , etc., depending on your algorithm\u2019s design. Extend or import from base classes in mu/core/ if your algorithm logic aligns with any existing abstract classes (e.g., BaseAlgorithm , BaseTrainer , etc.). This ensures code consistency and easier maintenance.","title":"Folder Structure"},{"location":"unlearn/algorithms/contributing/#adding-a-new-attack","text":"To add a new attack method, follow these guidelines: Folder Structure : Create a new file or subfolder under mu_attack/attackers/ with a clear, descriptive name (e.g., my_new_attack.py or mu_attack/attackers/my_new_attack/ ). If creating a subfolder, include essential files specific to attack implementation: Attacks : The main implementation file for the attack logic (e.g., attack.py ). Execs : Scripts or modules that execute the attack routines. Tasks : Task definitions for integrating and testing the attack. Any helper modules or configuration files specific to your attack. Update or add corresponding YAML configuration files and config class under mu_attack/configs/ if your attack requires custom settings. Implementation : Extend or import from the base class BaseAttacker (located in mu_attack/core/base_attacker.py ) if applicable. Ensure that your attack method adheres to the input-output standards defined by the project. Documentation & Testing : Add detailed documentation within your new attack module and update the main documentation if needed. Include tests covering your new attack method under the appropriate test directories. Environment : If your attack method has unique dependencies, update the environment.yaml file within the relevant directory or provide instructions in your documentation on how to create a dedicated environment. Also update the common environment file that is located in the project's root directory.","title":"Adding a New Attack"},{"location":"unlearn/algorithms/contributing/#adding-a-new-defense","text":"To integrate a new defense mechanism, please follow these steps: Folder Structure : Create a new subfolder under mu_defense/algorithms/ with a descriptive name (e.g., my_new_defense ). Within this subfolder, include essential files such as: algorithm.py : Contains the core logic of your defense method. trainer.py : Contains training routines and optimization strategies. configs/ : Include configuration class for training and evaluation. environment.yaml : Specify dependencies unique to your defense method. Readme.md : Document usage instructions, configuration details, and any other relevant information. Implementation : Extend or use base classes provided in mu_defense/algorithms/core/ (e.g., base_algorithm.py , base_trainer.py ) to ensure consistency with existing methods. Implement any unique evaluation metrics or procedures if your defense requires them. Documentation & Testing : Document your defense method thoroughly within its Readme.md and update the global documentation if necessary. Provide tests for your defense implementation to ensure its reliability and compatibility with the rest of the system. Environment : If your defense algorithm has specific dependencies, use the provided environment.yaml file as a template and adjust it accordingly. Include clear instructions for users to create and activate the environment. Also update the common environment file that is located in the project's root directory.","title":"Adding a New Defense"},{"location":"unlearn/algorithms/contributing/#creating-an-environment","text":"The default environment file is located in the project root directory ( environment.yaml ). Contributors should update this file as needed to include any new packages required by their contributions. If your module or algorithm requires unique dependencies, you may add a dedicated environment file in its respective directory, but be sure to update and maintain the default environment in the root. Optionally, to keep dependencies organized, each algorithm has its own environment.yaml . Contributors are encouraged to: Specify all required packages (e.g., torch , torchvision , diffusers , etc.). Name the environment after the algorithm, e.g., mu_<algorithm_name> . Update instructions in your algorithm\u2019s Readme.md on how to install and activate this environment.","title":"Creating an Environment"},{"location":"unlearn/algorithms/contributing/#documentation","text":"Each algorithm folder must contain a Readme.md that documents: High-level description of the algorithm (goals, references to papers, etc.). How to install any special dependencies (if not covered by environment.yaml ). How to run training and evaluation (with example commands). Explanation of the config files in configs/ . Any unique parameters or hyperparameters. If you introduce new metrics or logs, please outline them in your algorithm\u2019s README and update any relevant docs in the main docs/ folder as needed.","title":"Documentation"},{"location":"unlearn/algorithms/contributing/#code-style","text":"We recommend following PEP 8 for Python code. Use descriptive variable names, docstrings, and type hints where possible. Keep functions and methods short and focused\u2014avoid large, monolithic methods. Use Python\u2019s built-in logging or the logging utilities in helpers/logger.py for consistent log outputs. Use wandb global wandb logger if required. Additionally, we prefer: - 4 spaces for indentation (no tabs). - Meaningful commit messages: \u201cFix dataset loader bug for large images\u201d vs. \u201cFix stuff.\u201d","title":"Code Style"},{"location":"unlearn/algorithms/contributing/#contact","text":"If you have any questions, feel free to open an issue or reach out to us via email. We appreciate your contributions and look forward to collaborating with you on Unlearn Diff ! Thank you again for your interest in contributing. We\u2019re excited to see your ideas and improvements!","title":"Contact"},{"location":"unlearn/algorithms/erase_diff/","text":"EraseDiff Algorithm for Machine Unlearning This repository provides an implementation of the erase diff algorithm for machine unlearning in Stable Diffusion models. The erasediff algorithm allows you to remove specific concepts or styles from a pre-trained model without retraining it from scratch. Installation pip install unlearn_diff Prerequisities Ensure conda is installed on your system. You can install Miniconda or Anaconda: Miniconda (recommended): https://docs.conda.io/en/latest/miniconda.html Anaconda : https://www.anaconda.com/products/distribution After installing conda , ensure it is available in your PATH by running. You may require to restart the terminal session: conda --version Create environment: create_env <algorithm_name> eg: create_env erase_diff Activate environment: conda activate <environment_name> eg: conda activate erase_diff The has to be one of the folders in the mu/algorithms folder. Downloading data and models. After you install the package, you can use the following commands to download. Dataset : i2p : Sample : download_data sample i2p Full : download_data full i2p quick_canvas : Sample : download_data sample quick_canvas Full : download_data full quick_canvas Model : compvis : download_model compvis diffuser : download_model diffuser Verify the Downloaded Files After downloading, verify that the datasets have been correctly extracted: ls -lh ./data/i2p-dataset/sample/ ls -lh ./data/quick-canvas-dataset/sample/ Run Train Create a file, eg, my_trainer.py and use examples and modify your configs to run the file. Example Code from mu.algorithms.erase_diff.algorithm import EraseDiffAlgorithm from mu.algorithms.erase_diff.configs import ( erase_diff_train_mu, ) algorithm = EraseDiffAlgorithm( erase_diff_train_mu, ckpt_path=\"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\", raw_dataset_dir=( \"/home/ubuntu/Projects/balaram/packaging/data/quick-canvas-dataset/sample\" ), ) algorithm.run() Running the Training Script in Offline Mode WANDB_MODE=offline python my_trainer.py How It Works * Default Values: The script first loads default values from the train config file as in configs section. Parameter Overrides: Any parameters passed directly to the algorithm, overrides these configs. Final Configuration: The script merges the configs and convert them into dictionary to proceed with the training. Directory Structure algorithm.py : Implementation of the EraseDiffAlgorithm class. configs/ : Contains configuration files for training and generation. model.py : Implementation of the EraseDiffModel class. scripts/train.py : Script to train the EraseDiff algorithm. trainer.py : Implementation of the EraseDiffTrainer class. utils.py : Utility functions used in the project. data_handler.py : Implementation of DataHandler class Description of Arguments in train_config.yaml Training Parameters train_method: Specifies the method of training for concept erasure. Choices: [\"noxattn\", \"selfattn\", \"xattn\", \"full\", \"notime\", \"xlayer\", \"selflayer\"] Example: \"xattn\" alpha: Guidance strength for the starting image during training. Type: float Example: 0.1 epochs: Number of epochs to train the model. Type: int Example: 1 K_steps: Number of K optimization steps during training. Type: int Example: 2 lr: Learning rate used for the optimizer during training. Type: float Example: 5e-5 Model Configuration model_config_path: File path to the Stable Diffusion model configuration YAML file. type: str Example: \"/path/to/model_config.yaml\" ckpt_path: File path to the checkpoint of the Stable Diffusion model. Type: str Example: \"/path/to/model_checkpoint.ckpt\" Dataset Directories raw_dataset_dir: Directory containing the raw dataset categorized by themes or classes. Type: str Example: \"/path/to/raw_dataset\" processed_dataset_dir: Directory to save the processed dataset. Type: str Example: \"/path/to/processed_dataset\" dataset_type: Specifies the dataset type for the training process. Choices: [\"unlearncanvas\", \"i2p\"] Example: \"unlearncanvas\" template: Type of template to use during training. Choices: [\"object\", \"style\", \"i2p\"] Example: \"style\" template_name: Name of the specific concept or style to be erased. Choices: [\"self-harm\", \"Abstractionism\"] Example: \"Abstractionism\" Output Configurations output_dir: Directory where the fine-tuned models and results will be saved. Type: str Example: \"outputs/erase_diff/finetuned_models\" separator: String separator used to train multiple words separately, if applicable. Type: str or null Example: null Sampling and Image Configurations image_size: Size of the training images (height and width in pixels). Type: int Example: 512 interpolation: Interpolation method used for image resizing. Choices: [\"bilinear\", \"bicubic\", \"lanczos\"] Example: \"bicubic\" ddim_steps: Number of DDIM inference steps during training. Type: int Example: 50 ddim_eta: DDIM eta parameter for stochasticity during sampling. Type: float Example: 0.0 Device Configuration devices: Specifies the CUDA devices to be used for training (comma-separated). Type: str Example: \"0\" Additional Flags use_sample: Flag to indicate whether a sample dataset should be used for training. Type: bool Example: True num_workers: Number of worker threads for data loading. Type: int Example: 4 pin_memory: Flag to enable pinning memory during data loading for faster GPU transfers. Type: bool Example: true","title":"Usage"},{"location":"unlearn/algorithms/erase_diff/#erasediff-algorithm-for-machine-unlearning","text":"This repository provides an implementation of the erase diff algorithm for machine unlearning in Stable Diffusion models. The erasediff algorithm allows you to remove specific concepts or styles from a pre-trained model without retraining it from scratch.","title":"EraseDiff Algorithm for Machine Unlearning"},{"location":"unlearn/algorithms/erase_diff/#installation","text":"pip install unlearn_diff","title":"Installation"},{"location":"unlearn/algorithms/erase_diff/#prerequisities","text":"Ensure conda is installed on your system. You can install Miniconda or Anaconda: Miniconda (recommended): https://docs.conda.io/en/latest/miniconda.html Anaconda : https://www.anaconda.com/products/distribution After installing conda , ensure it is available in your PATH by running. You may require to restart the terminal session: conda --version","title":"Prerequisities"},{"location":"unlearn/algorithms/erase_diff/#create-environment","text":"create_env <algorithm_name> eg: create_env erase_diff","title":"Create environment:"},{"location":"unlearn/algorithms/erase_diff/#activate-environment","text":"conda activate <environment_name> eg: conda activate erase_diff The has to be one of the folders in the mu/algorithms folder.","title":"Activate environment:"},{"location":"unlearn/algorithms/erase_diff/#downloading-data-and-models","text":"After you install the package, you can use the following commands to download. Dataset : i2p : Sample : download_data sample i2p Full : download_data full i2p quick_canvas : Sample : download_data sample quick_canvas Full : download_data full quick_canvas Model : compvis : download_model compvis diffuser : download_model diffuser Verify the Downloaded Files After downloading, verify that the datasets have been correctly extracted: ls -lh ./data/i2p-dataset/sample/ ls -lh ./data/quick-canvas-dataset/sample/","title":"Downloading data and models."},{"location":"unlearn/algorithms/erase_diff/#run-train","text":"Create a file, eg, my_trainer.py and use examples and modify your configs to run the file. Example Code from mu.algorithms.erase_diff.algorithm import EraseDiffAlgorithm from mu.algorithms.erase_diff.configs import ( erase_diff_train_mu, ) algorithm = EraseDiffAlgorithm( erase_diff_train_mu, ckpt_path=\"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\", raw_dataset_dir=( \"/home/ubuntu/Projects/balaram/packaging/data/quick-canvas-dataset/sample\" ), ) algorithm.run() Running the Training Script in Offline Mode WANDB_MODE=offline python my_trainer.py How It Works * Default Values: The script first loads default values from the train config file as in configs section. Parameter Overrides: Any parameters passed directly to the algorithm, overrides these configs. Final Configuration: The script merges the configs and convert them into dictionary to proceed with the training.","title":"Run Train"},{"location":"unlearn/algorithms/erase_diff/#directory-structure","text":"algorithm.py : Implementation of the EraseDiffAlgorithm class. configs/ : Contains configuration files for training and generation. model.py : Implementation of the EraseDiffModel class. scripts/train.py : Script to train the EraseDiff algorithm. trainer.py : Implementation of the EraseDiffTrainer class. utils.py : Utility functions used in the project. data_handler.py : Implementation of DataHandler class","title":"Directory Structure"},{"location":"unlearn/algorithms/erase_diff/#description-of-arguments-in-train_configyaml","text":"Training Parameters train_method: Specifies the method of training for concept erasure. Choices: [\"noxattn\", \"selfattn\", \"xattn\", \"full\", \"notime\", \"xlayer\", \"selflayer\"] Example: \"xattn\" alpha: Guidance strength for the starting image during training. Type: float Example: 0.1 epochs: Number of epochs to train the model. Type: int Example: 1 K_steps: Number of K optimization steps during training. Type: int Example: 2 lr: Learning rate used for the optimizer during training. Type: float Example: 5e-5 Model Configuration model_config_path: File path to the Stable Diffusion model configuration YAML file. type: str Example: \"/path/to/model_config.yaml\" ckpt_path: File path to the checkpoint of the Stable Diffusion model. Type: str Example: \"/path/to/model_checkpoint.ckpt\" Dataset Directories raw_dataset_dir: Directory containing the raw dataset categorized by themes or classes. Type: str Example: \"/path/to/raw_dataset\" processed_dataset_dir: Directory to save the processed dataset. Type: str Example: \"/path/to/processed_dataset\" dataset_type: Specifies the dataset type for the training process. Choices: [\"unlearncanvas\", \"i2p\"] Example: \"unlearncanvas\" template: Type of template to use during training. Choices: [\"object\", \"style\", \"i2p\"] Example: \"style\" template_name: Name of the specific concept or style to be erased. Choices: [\"self-harm\", \"Abstractionism\"] Example: \"Abstractionism\" Output Configurations output_dir: Directory where the fine-tuned models and results will be saved. Type: str Example: \"outputs/erase_diff/finetuned_models\" separator: String separator used to train multiple words separately, if applicable. Type: str or null Example: null Sampling and Image Configurations image_size: Size of the training images (height and width in pixels). Type: int Example: 512 interpolation: Interpolation method used for image resizing. Choices: [\"bilinear\", \"bicubic\", \"lanczos\"] Example: \"bicubic\" ddim_steps: Number of DDIM inference steps during training. Type: int Example: 50 ddim_eta: DDIM eta parameter for stochasticity during sampling. Type: float Example: 0.0 Device Configuration devices: Specifies the CUDA devices to be used for training (comma-separated). Type: str Example: \"0\" Additional Flags use_sample: Flag to indicate whether a sample dataset should be used for training. Type: bool Example: True num_workers: Number of worker threads for data loading. Type: int Example: 4 pin_memory: Flag to enable pinning memory during data loading for faster GPU transfers. Type: bool Example: true","title":"Description of Arguments in train_config.yaml"},{"location":"unlearn/algorithms/esd/","text":"ESD Algorithm for Machine Unlearning This repository provides an implementation of the ESD algorithm for machine unlearning in Stable Diffusion models. The ESD algorithm allows you to remove specific concepts or styles from a pre-trained model without retraining it from scratch. Installation pip install unlearn_diff Prerequisities Ensure conda is installed on your system. You can install Miniconda or Anaconda: Miniconda (recommended): https://docs.conda.io/en/latest/miniconda.html Anaconda : https://www.anaconda.com/products/distribution After installing conda , ensure it is available in your PATH by running. You may require to restart the terminal session: conda --version Create environment: create_env <algorithm_name> eg: create_env esd Activate environment: conda activate <environment_name> eg: conda activate esd The has to be one of the folders in the mu/algorithms folder. Downloading data and models. After you install the package, you can use the following commands to download. Dataset : i2p : Sample : download_data sample i2p Full : download_data full i2p quick_canvas : Sample : download_data sample quick_canvas Full : download_data full quick_canvas Model : compvis : download_model compvis diffuser : download_model diffuser Verify the Downloaded Files After downloading, verify that the datasets have been correctly extracted: ls -lh ./data/i2p-dataset/sample/ ls -lh ./data/quick-canvas-dataset/sample/ Run Train Create a file, eg, my_trainer.py and use examples and modify your configs to run the file. Example Code from mu.algorithms.esd.algorithm import ESDAlgorithm from mu.algorithms.esd.configs import ( esd_train_mu, ) algorithm = ESDAlgorithm( esd_train_mu, ckpt_path=\"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\", raw_dataset_dir=( \"/home/ubuntu/Projects/balaram/packaging/data/quick-canvas-dataset/sample\" ), ) algorithm.run() Running the Training Script in Offline Mode WANDB_MODE=offline python my_trainer.py How It Works * Default Values: The script first loads default values from the train config file as in configs section. Parameter Overrides: Any parameters passed directly to the algorithm, overrides these configs. Final Configuration: The script merges the configs and convert them into dictionary to proceed with the training. Directory Structure algorithm.py : Implementation of the ESDAlgorithm class. configs/ : Contains configuration files for training and generation. constants/const.py : Constants used throughout the project. model.py : Implementation of the ESDModel class. scripts/train.py : Script to train the ESD algorithm. trainer.py : Implementation of the ESDTrainer class. utils.py : Utility functions used in the project. Description of arguments being used in train_config class These are the configuration used for training a Stable Diffusion model using the ESD (Erase Stable Diffusion) method. It defines various parameters related to training, model setup, dataset handling, and output configuration. Below is a detailed description of each section and parameter: Training Parameters These parameters control the fine-tuning process, including the method of training, guidance scales, learning rate, and iteration settings. train_method: Specifies the method of training to decide which parts of the model to update. Type: str Choices: noxattn, selfattn, xattn, full, notime, xlayer, selflayer Example: xattn start_guidance: Guidance scale for generating initial images during training. Affects the diversity of the training set. Type: float Example: 0.1 negative_guidance: Guidance scale for erasing the target concept during training. Type: float Example: 0.0 iterations: Number of training iterations (similar to epochs). Type: int Example: 1 lr: Learning rate used by the optimizer for fine-tuning. Type: float Example: 5e-5 image_size: Size of images used during training and sampling (in pixels). Type: int Example: 512 ddim_steps: Number of diffusion steps used in the DDIM sampling process. Type: int Example: 50 Model Configuration These parameters specify the Stable Diffusion model checkpoint and configuration file. model_config_path: Path to the YAML file defining the model architecture and parameters. Type: str Example: mu/algorithms/esd/configs/model_config.yaml ckpt_path: Path to the finetuned Stable Diffusion model checkpoint. Type: str Example: '../models/compvis/style50/compvis.ckpt' Dataset Configuration These parameters define the dataset type and template for training, specifying whether to focus on objects, styles, or inappropriate content. dataset_type: Type of dataset used for training. Type: str Choices: unlearncanvas, i2p Example: unlearncanvas template: Type of concept or style to erase during training. Type: str Choices: object, style, i2p Example: style template_name: Specific name of the object or style to erase (e.g., \"Abstractionism\"). Type: str Example Choices: Abstractionism, self-harm Example: Abstractionism Output Configuration These parameters control where the outputs of the training process, such as fine-tuned models, are stored. output_dir: Directory where the fine-tuned model and training results will be saved. Type: str Example: outputs/esd/finetuned_models separator: Separator character used to handle multiple prompts during training. If set to null, no special handling occurs. Type: str or null Example: null Device Configuration These parameters define the compute resources for training. devices: Specifies the CUDA devices used for training. Provide a comma-separated list of device IDs. Type: str Example: 0,1 use_sample: Boolean flag indicating whether to use a sample dataset for testing or debugging. Type: bool Example: True","title":"Usage"},{"location":"unlearn/algorithms/esd/#esd-algorithm-for-machine-unlearning","text":"This repository provides an implementation of the ESD algorithm for machine unlearning in Stable Diffusion models. The ESD algorithm allows you to remove specific concepts or styles from a pre-trained model without retraining it from scratch.","title":"ESD Algorithm for Machine Unlearning"},{"location":"unlearn/algorithms/esd/#installation","text":"pip install unlearn_diff","title":"Installation"},{"location":"unlearn/algorithms/esd/#prerequisities","text":"Ensure conda is installed on your system. You can install Miniconda or Anaconda: Miniconda (recommended): https://docs.conda.io/en/latest/miniconda.html Anaconda : https://www.anaconda.com/products/distribution After installing conda , ensure it is available in your PATH by running. You may require to restart the terminal session: conda --version","title":"Prerequisities"},{"location":"unlearn/algorithms/esd/#create-environment","text":"create_env <algorithm_name> eg: create_env esd","title":"Create environment:"},{"location":"unlearn/algorithms/esd/#activate-environment","text":"conda activate <environment_name> eg: conda activate esd The has to be one of the folders in the mu/algorithms folder.","title":"Activate environment:"},{"location":"unlearn/algorithms/esd/#downloading-data-and-models","text":"After you install the package, you can use the following commands to download. Dataset : i2p : Sample : download_data sample i2p Full : download_data full i2p quick_canvas : Sample : download_data sample quick_canvas Full : download_data full quick_canvas Model : compvis : download_model compvis diffuser : download_model diffuser Verify the Downloaded Files After downloading, verify that the datasets have been correctly extracted: ls -lh ./data/i2p-dataset/sample/ ls -lh ./data/quick-canvas-dataset/sample/","title":"Downloading data and models."},{"location":"unlearn/algorithms/esd/#run-train","text":"Create a file, eg, my_trainer.py and use examples and modify your configs to run the file. Example Code from mu.algorithms.esd.algorithm import ESDAlgorithm from mu.algorithms.esd.configs import ( esd_train_mu, ) algorithm = ESDAlgorithm( esd_train_mu, ckpt_path=\"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\", raw_dataset_dir=( \"/home/ubuntu/Projects/balaram/packaging/data/quick-canvas-dataset/sample\" ), ) algorithm.run() Running the Training Script in Offline Mode WANDB_MODE=offline python my_trainer.py How It Works * Default Values: The script first loads default values from the train config file as in configs section. Parameter Overrides: Any parameters passed directly to the algorithm, overrides these configs. Final Configuration: The script merges the configs and convert them into dictionary to proceed with the training.","title":"Run Train"},{"location":"unlearn/algorithms/esd/#directory-structure","text":"algorithm.py : Implementation of the ESDAlgorithm class. configs/ : Contains configuration files for training and generation. constants/const.py : Constants used throughout the project. model.py : Implementation of the ESDModel class. scripts/train.py : Script to train the ESD algorithm. trainer.py : Implementation of the ESDTrainer class. utils.py : Utility functions used in the project.","title":"Directory Structure"},{"location":"unlearn/algorithms/esd/#description-of-arguments-being-used-in-train_config-class","text":"These are the configuration used for training a Stable Diffusion model using the ESD (Erase Stable Diffusion) method. It defines various parameters related to training, model setup, dataset handling, and output configuration. Below is a detailed description of each section and parameter: Training Parameters These parameters control the fine-tuning process, including the method of training, guidance scales, learning rate, and iteration settings. train_method: Specifies the method of training to decide which parts of the model to update. Type: str Choices: noxattn, selfattn, xattn, full, notime, xlayer, selflayer Example: xattn start_guidance: Guidance scale for generating initial images during training. Affects the diversity of the training set. Type: float Example: 0.1 negative_guidance: Guidance scale for erasing the target concept during training. Type: float Example: 0.0 iterations: Number of training iterations (similar to epochs). Type: int Example: 1 lr: Learning rate used by the optimizer for fine-tuning. Type: float Example: 5e-5 image_size: Size of images used during training and sampling (in pixels). Type: int Example: 512 ddim_steps: Number of diffusion steps used in the DDIM sampling process. Type: int Example: 50 Model Configuration These parameters specify the Stable Diffusion model checkpoint and configuration file. model_config_path: Path to the YAML file defining the model architecture and parameters. Type: str Example: mu/algorithms/esd/configs/model_config.yaml ckpt_path: Path to the finetuned Stable Diffusion model checkpoint. Type: str Example: '../models/compvis/style50/compvis.ckpt' Dataset Configuration These parameters define the dataset type and template for training, specifying whether to focus on objects, styles, or inappropriate content. dataset_type: Type of dataset used for training. Type: str Choices: unlearncanvas, i2p Example: unlearncanvas template: Type of concept or style to erase during training. Type: str Choices: object, style, i2p Example: style template_name: Specific name of the object or style to erase (e.g., \"Abstractionism\"). Type: str Example Choices: Abstractionism, self-harm Example: Abstractionism Output Configuration These parameters control where the outputs of the training process, such as fine-tuned models, are stored. output_dir: Directory where the fine-tuned model and training results will be saved. Type: str Example: outputs/esd/finetuned_models separator: Separator character used to handle multiple prompts during training. If set to null, no special handling occurs. Type: str or null Example: null Device Configuration These parameters define the compute resources for training. devices: Specifies the CUDA devices used for training. Provide a comma-separated list of device IDs. Type: str Example: 0,1 use_sample: Boolean flag indicating whether to use a sample dataset for testing or debugging. Type: bool Example: True","title":"Description of arguments being used in train_config class"},{"location":"unlearn/algorithms/forget_me_not/","text":"Forget Me Not Algorithm for Machine Unlearning This repository provides an implementation of the erase diff algorithm for machine unlearning in Stable Diffusion models. The Forget Me Not algorithm allows you to remove specific concepts or styles from a pre-trained model without retraining it from scratch. Installation pip install unlearn_diff Prerequisities Ensure conda is installed on your system. You can install Miniconda or Anaconda: Miniconda (recommended): https://docs.conda.io/en/latest/miniconda.html Anaconda : https://www.anaconda.com/products/distribution After installing conda , ensure it is available in your PATH by running. You may require to restart the terminal session: conda --version Create environment: create_env <algorithm_name> eg: create_env forget_me_not Activate environment: conda activate <environment_name> eg: conda activate forget_me_not The has to be one of the folders in the mu/algorithms folder. Downloading data and models. After you install the package, you can use the following commands to download. Dataset : i2p : Sample : download_data sample i2p Full : download_data full i2p quick_canvas : Sample : download_data sample quick_canvas Full : download_data full quick_canvas Model : compvis : download_model compvis diffuser : download_model diffuser Verify the Downloaded Files After downloading, verify that the datasets have been correctly extracted: ls -lh ./data/i2p-dataset/sample/ ls -lh ./data/quick-canvas-dataset/sample/ Run Train Create a file, eg, my_trainer.py and use examples and modify your configs to run the file. Train a Text Inversion from mu.algorithms.forget_me_not.algorithm import ForgetMeNotAlgorithm from mu.algorithms.forget_me_not.configs import ( forget_me_not_train_ti_mu, ) algorithm = ForgetMeNotAlgorithm( forget_me_not_train_ti_mu, ckpt_path=\"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/diffuser/style50\", raw_dataset_dir=( \"/home/ubuntu/Projects/balaram/packaging/data/quick-canvas-dataset/sample\" ), steps=10 ) algorithm.run(train_type=\"train_ti\") Running the Script in Offlikne Mode WANDB_MODE=offline python my_trainer_ti.py Perform Unlearning Before running the train_attn script, update the ti_weights_path parameter in the configuration file to point to the output generated from the Text Inversion (train_ti.py) stage from mu.algorithms.forget_me_not.algorithm import ForgetMeNotAlgorithm from mu.algorithms.forget_me_not.configs import ( forget_me_not_train_attn_mu, ) algorithm = ForgetMeNotAlgorithm( forget_me_not_train_attn_mu, ckpt_path=\"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/diffuser/style50\", raw_dataset_dir=( \"/home/ubuntu/Projects/balaram/packaging/data/quick-canvas-dataset/sample\" ), steps=10, ti_weights_path=\"outputs/forget_me_not/finetuned_models/Abstractionism/step_inv_10.safetensors\" ) algorithm.run(train_type=\"train_attn\") Running the Script in Offlikne Mode WANDB_MODE=offline python my_trainer_attn.py How It Works * Default Values: The script first loads default values from the train config file as in configs section. Parameter Overrides: Any parameters passed directly to the algorithm, overrides these configs. Final Configuration: The script merges the configs and convert them into dictionary to proceed with the training. Directory Structure algorithm.py : Implementation of the Forget Me NotAlgorithm class. configs/ : Contains configuration files for training and generation. model.py : Implementation of the Forget Me NotModel class. scripts/train.py : Script to train the Forget Me Not algorithm. trainer.py : Implementation of the Forget Me NotTrainer class. utils.py : Utility functions used in the project. data_handler.py : Implementation of DataHandler class This method involves two stages: Train a Text Inversion : The first stage involves training a Text Inversion. Refer to the script train_ti.py for details and implementation. It uses train_ti_config.yaml as config file. Perform Unlearning : The second stage uses the outputs from the first stage to perform unlearning. Refer to the script train_attn.py for details and implementation. It uses train_attn_config.yaml as config file. Description of Arguments in train_ti_config.yaml Pretrained Model ckpt_path : File path to the pretrained model's checkpoint file. Dataset raw_dataset_dir : Directory containing the original dataset organized by themes and classes. processed_dataset_dir : Directory where the processed datasets will be saved. dataset_type : Type of dataset to use (e.g., unlearncanvas ). template : Type of template to use (e.g., style ). template_name : Name of the template, defining the style or theme (e.g., Abstractionism ). use_sample : Boolean indicating whether to use the sample dataset for training. Training Configuration initializer_tokens : Tokens used to initialize the training process, referencing the template name. steps : Number of training steps. lr : Learning rate for the training optimizer. weight_decay_ti : Weight decay for Text Inversion training. seed : Random seed for reproducibility. placeholder_tokens : Tokens used as placeholders during training. placeholder_token_at_data : Placeholders used in the dataset for Text Inversion training. gradient_checkpointing : Boolean to enable or disable gradient checkpointing. scale_lr : Boolean indicating whether to scale the learning rate based on batch size. gradient_accumulation_steps : Number of steps to accumulate gradients before updating weights. train_batch_size : Batch size for training. lr_warmup_steps : Number of steps for linear warmup of the learning rate. Output Configuration output_dir : Directory path to save training results, including models and logs. Device Configuration devices : CUDA devices to train on (comma-separated). Description of Arguments in train_attn_config.yaml Key Parameters Pretrained Model ckpt_path : File path to the pretrained model's checkpoint file. Dataset raw_dataset_dir : Directory containing the original dataset organized by themes and classes. processed_dataset_dir : Directory where the processed datasets will be saved. dataset_type : Type of dataset to use (e.g., unlearncanvas ). template : Type of template to use (e.g., style ). template_name : Name of the template, defining the style or theme (e.g., Abstractionism ). use_sample : Boolean indicating whether to use the sample dataset for training. Text Inversion use_ti : Boolean indicating whether to use Text Inversion weights. ti_weights_path : File path to the Text Inversion model weights. Tokens initializer_tokens : Tokens used to initialize the training process, referencing the template name. placeholder_tokens : Tokens used as placeholders during training. Training Configuration mixed_precision : Precision type to use during training (e.g., fp16 or fp32 ). gradient_accumulation_steps : Number of steps to accumulate gradients before updating weights. train_text_encoder : Boolean to enable or disable training of the text encoder. enable_xformers_memory_efficient_attention : Boolean to enable memory-efficient attention mechanisms. gradient_checkpointing : Boolean to enable or disable gradient checkpointing. allow_tf32 : Boolean to allow TensorFloat-32 computation for faster training. scale_lr : Boolean indicating whether to scale the learning rate based on batch size. train_batch_size : Batch size for training. use_8bit_adam : Boolean to enable or disable 8-bit Adam optimizer. adam_beta1 : Beta1 parameter for the Adam optimizer. adam_beta2 : Beta2 parameter for the Adam optimizer. adam_weight_decay : Weight decay for the Adam optimizer. adam_epsilon : Epsilon value for the Adam optimizer. size : Image resolution size for training. with_prior_preservation : Boolean indicating whether to use prior preservation during training. num_train_epochs : Number of training epochs. lr_warmup_steps : Number of steps for linear warmup of the learning rate. lr_num_cycles : Number of cycles for learning rate scheduling. lr_power : Exponent to control the shape of the learning rate curve. max-steps : Maximum number of training steps. no_real_image : Boolean to skip using real images in training. max_grad_norm : Maximum norm for gradient clipping. checkpointing_steps : Number of steps between model checkpoints. set_grads_to_none : Boolean to set gradients to None instead of zeroing them out. lr : Learning rate for the training optimizer. Output Configuration output_dir : Directory path to save training results, including models and logs. Device Configuration devices : CUDA devices to train on (comma-separated). Miscellaneous only-xa : Boolean to enable additional configurations specific to the XA pipeline.","title":"Usage"},{"location":"unlearn/algorithms/forget_me_not/#forget-me-not-algorithm-for-machine-unlearning","text":"This repository provides an implementation of the erase diff algorithm for machine unlearning in Stable Diffusion models. The Forget Me Not algorithm allows you to remove specific concepts or styles from a pre-trained model without retraining it from scratch.","title":"Forget Me Not Algorithm for Machine Unlearning"},{"location":"unlearn/algorithms/forget_me_not/#installation","text":"pip install unlearn_diff","title":"Installation"},{"location":"unlearn/algorithms/forget_me_not/#prerequisities","text":"Ensure conda is installed on your system. You can install Miniconda or Anaconda: Miniconda (recommended): https://docs.conda.io/en/latest/miniconda.html Anaconda : https://www.anaconda.com/products/distribution After installing conda , ensure it is available in your PATH by running. You may require to restart the terminal session: conda --version","title":"Prerequisities"},{"location":"unlearn/algorithms/forget_me_not/#create-environment","text":"create_env <algorithm_name> eg: create_env forget_me_not","title":"Create environment:"},{"location":"unlearn/algorithms/forget_me_not/#activate-environment","text":"conda activate <environment_name> eg: conda activate forget_me_not The has to be one of the folders in the mu/algorithms folder.","title":"Activate environment:"},{"location":"unlearn/algorithms/forget_me_not/#downloading-data-and-models","text":"After you install the package, you can use the following commands to download. Dataset : i2p : Sample : download_data sample i2p Full : download_data full i2p quick_canvas : Sample : download_data sample quick_canvas Full : download_data full quick_canvas Model : compvis : download_model compvis diffuser : download_model diffuser Verify the Downloaded Files After downloading, verify that the datasets have been correctly extracted: ls -lh ./data/i2p-dataset/sample/ ls -lh ./data/quick-canvas-dataset/sample/","title":"Downloading data and models."},{"location":"unlearn/algorithms/forget_me_not/#run-train","text":"Create a file, eg, my_trainer.py and use examples and modify your configs to run the file. Train a Text Inversion from mu.algorithms.forget_me_not.algorithm import ForgetMeNotAlgorithm from mu.algorithms.forget_me_not.configs import ( forget_me_not_train_ti_mu, ) algorithm = ForgetMeNotAlgorithm( forget_me_not_train_ti_mu, ckpt_path=\"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/diffuser/style50\", raw_dataset_dir=( \"/home/ubuntu/Projects/balaram/packaging/data/quick-canvas-dataset/sample\" ), steps=10 ) algorithm.run(train_type=\"train_ti\") Running the Script in Offlikne Mode WANDB_MODE=offline python my_trainer_ti.py Perform Unlearning Before running the train_attn script, update the ti_weights_path parameter in the configuration file to point to the output generated from the Text Inversion (train_ti.py) stage from mu.algorithms.forget_me_not.algorithm import ForgetMeNotAlgorithm from mu.algorithms.forget_me_not.configs import ( forget_me_not_train_attn_mu, ) algorithm = ForgetMeNotAlgorithm( forget_me_not_train_attn_mu, ckpt_path=\"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/diffuser/style50\", raw_dataset_dir=( \"/home/ubuntu/Projects/balaram/packaging/data/quick-canvas-dataset/sample\" ), steps=10, ti_weights_path=\"outputs/forget_me_not/finetuned_models/Abstractionism/step_inv_10.safetensors\" ) algorithm.run(train_type=\"train_attn\") Running the Script in Offlikne Mode WANDB_MODE=offline python my_trainer_attn.py How It Works * Default Values: The script first loads default values from the train config file as in configs section. Parameter Overrides: Any parameters passed directly to the algorithm, overrides these configs. Final Configuration: The script merges the configs and convert them into dictionary to proceed with the training.","title":"Run Train"},{"location":"unlearn/algorithms/forget_me_not/#directory-structure","text":"algorithm.py : Implementation of the Forget Me NotAlgorithm class. configs/ : Contains configuration files for training and generation. model.py : Implementation of the Forget Me NotModel class. scripts/train.py : Script to train the Forget Me Not algorithm. trainer.py : Implementation of the Forget Me NotTrainer class. utils.py : Utility functions used in the project. data_handler.py : Implementation of DataHandler class This method involves two stages: Train a Text Inversion : The first stage involves training a Text Inversion. Refer to the script train_ti.py for details and implementation. It uses train_ti_config.yaml as config file. Perform Unlearning : The second stage uses the outputs from the first stage to perform unlearning. Refer to the script train_attn.py for details and implementation. It uses train_attn_config.yaml as config file.","title":"Directory Structure"},{"location":"unlearn/algorithms/forget_me_not/#description-of-arguments-in-train_ti_configyaml","text":"Pretrained Model ckpt_path : File path to the pretrained model's checkpoint file. Dataset raw_dataset_dir : Directory containing the original dataset organized by themes and classes. processed_dataset_dir : Directory where the processed datasets will be saved. dataset_type : Type of dataset to use (e.g., unlearncanvas ). template : Type of template to use (e.g., style ). template_name : Name of the template, defining the style or theme (e.g., Abstractionism ). use_sample : Boolean indicating whether to use the sample dataset for training. Training Configuration initializer_tokens : Tokens used to initialize the training process, referencing the template name. steps : Number of training steps. lr : Learning rate for the training optimizer. weight_decay_ti : Weight decay for Text Inversion training. seed : Random seed for reproducibility. placeholder_tokens : Tokens used as placeholders during training. placeholder_token_at_data : Placeholders used in the dataset for Text Inversion training. gradient_checkpointing : Boolean to enable or disable gradient checkpointing. scale_lr : Boolean indicating whether to scale the learning rate based on batch size. gradient_accumulation_steps : Number of steps to accumulate gradients before updating weights. train_batch_size : Batch size for training. lr_warmup_steps : Number of steps for linear warmup of the learning rate. Output Configuration output_dir : Directory path to save training results, including models and logs. Device Configuration devices : CUDA devices to train on (comma-separated).","title":"Description of Arguments in train_ti_config.yaml"},{"location":"unlearn/algorithms/forget_me_not/#description-of-arguments-in-train_attn_configyaml","text":"","title":"Description of Arguments in train_attn_config.yaml"},{"location":"unlearn/algorithms/forget_me_not/#key-parameters","text":"Pretrained Model ckpt_path : File path to the pretrained model's checkpoint file. Dataset raw_dataset_dir : Directory containing the original dataset organized by themes and classes. processed_dataset_dir : Directory where the processed datasets will be saved. dataset_type : Type of dataset to use (e.g., unlearncanvas ). template : Type of template to use (e.g., style ). template_name : Name of the template, defining the style or theme (e.g., Abstractionism ). use_sample : Boolean indicating whether to use the sample dataset for training. Text Inversion use_ti : Boolean indicating whether to use Text Inversion weights. ti_weights_path : File path to the Text Inversion model weights. Tokens initializer_tokens : Tokens used to initialize the training process, referencing the template name. placeholder_tokens : Tokens used as placeholders during training. Training Configuration mixed_precision : Precision type to use during training (e.g., fp16 or fp32 ). gradient_accumulation_steps : Number of steps to accumulate gradients before updating weights. train_text_encoder : Boolean to enable or disable training of the text encoder. enable_xformers_memory_efficient_attention : Boolean to enable memory-efficient attention mechanisms. gradient_checkpointing : Boolean to enable or disable gradient checkpointing. allow_tf32 : Boolean to allow TensorFloat-32 computation for faster training. scale_lr : Boolean indicating whether to scale the learning rate based on batch size. train_batch_size : Batch size for training. use_8bit_adam : Boolean to enable or disable 8-bit Adam optimizer. adam_beta1 : Beta1 parameter for the Adam optimizer. adam_beta2 : Beta2 parameter for the Adam optimizer. adam_weight_decay : Weight decay for the Adam optimizer. adam_epsilon : Epsilon value for the Adam optimizer. size : Image resolution size for training. with_prior_preservation : Boolean indicating whether to use prior preservation during training. num_train_epochs : Number of training epochs. lr_warmup_steps : Number of steps for linear warmup of the learning rate. lr_num_cycles : Number of cycles for learning rate scheduling. lr_power : Exponent to control the shape of the learning rate curve. max-steps : Maximum number of training steps. no_real_image : Boolean to skip using real images in training. max_grad_norm : Maximum norm for gradient clipping. checkpointing_steps : Number of steps between model checkpoints. set_grads_to_none : Boolean to set gradients to None instead of zeroing them out. lr : Learning rate for the training optimizer. Output Configuration output_dir : Directory path to save training results, including models and logs. Device Configuration devices : CUDA devices to train on (comma-separated). Miscellaneous only-xa : Boolean to enable additional configurations specific to the XA pipeline.","title":"Key Parameters"},{"location":"unlearn/algorithms/saliency/","text":"Saliency Unlearning Algorithm for Machine Unlearning This repository provides an implementation of the Saliency Unlearning algorithm for machine unlearning in Stable Diffusion models. The Saliency Unlearning algorithm allows you to remove specific concepts or styles from a pre-trained model without retraining it from scratch. Installation pip install unlearn_diff Prerequisities Ensure conda is installed on your system. You can install Miniconda or Anaconda: Miniconda (recommended): https://docs.conda.io/en/latest/miniconda.html Anaconda : https://www.anaconda.com/products/distribution After installing conda , ensure it is available in your PATH by running. You may require to restart the terminal session: conda --version Create environment: create_env <algorithm_name> eg: create_env saliency_unlearning Activate environment: conda activate <environment_name> eg: conda activate saliency_unlearning The has to be one of the folders in the mu/algorithms folder. Downloading data and models. After you install the package, you can use the following commands to download. Dataset : i2p : Sample : download_data sample i2p Full : download_data full i2p quick_canvas : Sample : download_data sample quick_canvas Full : download_data full quick_canvas Model : compvis : download_model compvis diffuser : download_model diffuser Verify the Downloaded Files After downloading, verify that the datasets have been correctly extracted: ls -lh ./data/i2p-dataset/sample/ ls -lh ./data/quick-canvas-dataset/sample/ Usage To train the saliency unlearning algorithm to unlearn a specific concept or style from the Stable Diffusion model, use the train.py script located in the scripts directory. Step 1: Generate mask python -m mu.algorithms.saliency_unlearning.scripts.generate_mask \\ --config_path mu/algorithms/saliency_unlearning/configs/mask_config.yaml Run Train Create a file, eg, my_trainer.py and use examples and modify your configs to run the file. Example Code from mu.algorithms.saliency_unlearning.algorithm import ( SaliencyUnlearningAlgorithm, ) from mu.algorithms.saliency_unlearning.configs import ( saliency_unlearning_train_mu, ) algorithm = SaliencyUnlearningAlgorithm( saliency_unlearning_train_mu, output_dir=\"/opt/dlami/nvme/outputs\", ) algorithm.run() Running the Training Script in Offline Mode WANDB_MODE=offline python my_trainer.py How It Works * Default Values: The script first loads default values from the train config file as in configs section. Parameter Overrides: Any parameters passed directly to the algorithm, overrides these configs. Final Configuration: The script merges the configs and convert them into dictionary to proceed with the training. Similarly, you can pass arguments during runtime to generate mask. How It Works * Default Values: The script first loads default values from the YAML file specified by --config_path. Command-Line Overrides: Any arguments passed on the command line will override the corresponding keys in the YAML configuration file. Final Configuration: The script merges the YAML file and command-line arguments into a single configuration dictionary and uses it for training. Directory Structure algorithm.py : Implementation of the SaliencyUnlearnAlgorithm class. configs/ : Contains configuration files for training and generation. model.py : Implementation of the SaliencyUnlearnModel class. scripts/train.py : Script to train the SaliencyUnlearn algorithm. trainer.py : Implementation of the SaliencyUnlearnTrainer class. utils.py : Utility functions used in the project. data_handler.py : Implementation of DataHandler class The unlearning has two stages: Generate the mask Unlearn the weights. Description of Arguments in mask_config.yaml The config/mask_config.yaml file is a configuration file for generating saliency masks using the scripts/generate_mask.py script. It defines various parameters related to the model, dataset, output, and training. Below is a detailed description of each section and parameter: Model Configuration These parameters specify settings for the Stable Diffusion model and guidance configurations. c_guidance: Guidance scale used during loss computation in the model. Higher values may emphasize certain features in mask generation. Type: float Example: 7.5 batch_size: Number of images processed in a single batch. Type: int Example: 4 ckpt_path: Path to the model checkpoint file for Stable Diffusion. Type: str Example: /path/to/compvis.ckpt model_config_path: Path to the model configuration YAML file for Stable Diffusion. Type: str Example: /path/to/model_config.yaml num_timesteps: Number of timesteps used in the diffusion process. Type: int Example: 1000 image_size: Size of the input images used for training and mask generation (in pixels). Type: int Example: 512 Dataset Configuration These parameters define the dataset paths and settings for mask generation. raw_dataset_dir: Path to the directory containing the original dataset, organized by themes and classes. Type: str Example: /path/to/raw/dataset processed_dataset_dir: Path to the directory where processed datasets will be saved after mask generation. Type: str Example: /path/to/processed/dataset dataset_type: Type of dataset being used. Choices: unlearncanvas, i2p Type: str Example: i2p template: Type of template for mask generation. Choices: object, style, i2p Type: str Example: style template_name: Specific template name for the mask generation process. Example Choices: self-harm, Abstractionism Type: str Example: Abstractionism threshold: Threshold value for mask generation to filter salient regions. Type: float Example: 0.5 Output Configuration These parameters specify the directory where the results are saved. output_dir: Directory where the generated masks will be saved. Type: str Example: outputs/saliency_unlearning/masks Training Configuration These parameters control the training process for mask generation. lr: Learning rate used for training the masking algorithm. Type: float Example: 0.00001 devices: CUDA devices used for training, specified as a comma-separated list. Type: str Example: 0 use_sample: Flag indicating whether to use a sample dataset for training and mask generation. Type: bool Example: True Description of Arguments train_config.yaml The scripts/train.py script is used to fine-tune the Stable Diffusion model to perform saliency-based unlearning. This script relies on a configuration file ( config/train_config.yaml ) and supports additional runtime arguments for further customization. Below is a detailed description of each argument: General Arguments alpha: Guidance scale used to balance the loss components during training. Type: float Example: 0.1 epochs: Number of epochs to train the model. Type: int Example: 5 train_method: Specifies the training method or strategy to be used. Choices: noxattn, selfattn, xattn, full, notime, xlayer, selflayer Type: str Example: noxattn model_config_path: Path to the model configuration YAML file for Stable Diffusion. Type: str Example: 'mu/algorithms/saliency_unlearning/configs/model_config.yaml' Dataset Arguments raw_dataset_dir: Path to the directory containing the raw dataset, organized by themes and classes. Type: str Example: 'path/raw_dataset/' processed_dataset_dir: Path to the directory where the processed dataset will be saved. Type: str Example: 'path/processed_dataset_dir' dataset_type: Specifies the type of dataset to use for training. Choices: unlearncanvas, i2p Type: str Example: i2p template: Specifies the template type for training. Choices: object, style, i2p Type: str Example: style template_name: Name of the specific template used for training. Example Choices: self-harm, Abstractionism Type: str Example: Abstractionism Output Arguments output_dir: Directory where the fine-tuned model and training outputs will be saved. Type: str Example: 'output/folder_name' mask_path: Path to the saliency mask file used during training. Type: str Example:","title":"Usage"},{"location":"unlearn/algorithms/saliency/#saliency-unlearning-algorithm-for-machine-unlearning","text":"This repository provides an implementation of the Saliency Unlearning algorithm for machine unlearning in Stable Diffusion models. The Saliency Unlearning algorithm allows you to remove specific concepts or styles from a pre-trained model without retraining it from scratch.","title":"Saliency Unlearning Algorithm for Machine Unlearning"},{"location":"unlearn/algorithms/saliency/#installation","text":"pip install unlearn_diff","title":"Installation"},{"location":"unlearn/algorithms/saliency/#prerequisities","text":"Ensure conda is installed on your system. You can install Miniconda or Anaconda: Miniconda (recommended): https://docs.conda.io/en/latest/miniconda.html Anaconda : https://www.anaconda.com/products/distribution After installing conda , ensure it is available in your PATH by running. You may require to restart the terminal session: conda --version","title":"Prerequisities"},{"location":"unlearn/algorithms/saliency/#create-environment","text":"create_env <algorithm_name> eg: create_env saliency_unlearning","title":"Create environment:"},{"location":"unlearn/algorithms/saliency/#activate-environment","text":"conda activate <environment_name> eg: conda activate saliency_unlearning The has to be one of the folders in the mu/algorithms folder.","title":"Activate environment:"},{"location":"unlearn/algorithms/saliency/#downloading-data-and-models","text":"After you install the package, you can use the following commands to download. Dataset : i2p : Sample : download_data sample i2p Full : download_data full i2p quick_canvas : Sample : download_data sample quick_canvas Full : download_data full quick_canvas Model : compvis : download_model compvis diffuser : download_model diffuser Verify the Downloaded Files After downloading, verify that the datasets have been correctly extracted: ls -lh ./data/i2p-dataset/sample/ ls -lh ./data/quick-canvas-dataset/sample/","title":"Downloading data and models."},{"location":"unlearn/algorithms/saliency/#usage","text":"To train the saliency unlearning algorithm to unlearn a specific concept or style from the Stable Diffusion model, use the train.py script located in the scripts directory. Step 1: Generate mask python -m mu.algorithms.saliency_unlearning.scripts.generate_mask \\ --config_path mu/algorithms/saliency_unlearning/configs/mask_config.yaml","title":"Usage"},{"location":"unlearn/algorithms/saliency/#run-train","text":"Create a file, eg, my_trainer.py and use examples and modify your configs to run the file. Example Code from mu.algorithms.saliency_unlearning.algorithm import ( SaliencyUnlearningAlgorithm, ) from mu.algorithms.saliency_unlearning.configs import ( saliency_unlearning_train_mu, ) algorithm = SaliencyUnlearningAlgorithm( saliency_unlearning_train_mu, output_dir=\"/opt/dlami/nvme/outputs\", ) algorithm.run() Running the Training Script in Offline Mode WANDB_MODE=offline python my_trainer.py How It Works * Default Values: The script first loads default values from the train config file as in configs section. Parameter Overrides: Any parameters passed directly to the algorithm, overrides these configs. Final Configuration: The script merges the configs and convert them into dictionary to proceed with the training. Similarly, you can pass arguments during runtime to generate mask. How It Works * Default Values: The script first loads default values from the YAML file specified by --config_path. Command-Line Overrides: Any arguments passed on the command line will override the corresponding keys in the YAML configuration file. Final Configuration: The script merges the YAML file and command-line arguments into a single configuration dictionary and uses it for training.","title":"Run Train"},{"location":"unlearn/algorithms/saliency/#directory-structure","text":"algorithm.py : Implementation of the SaliencyUnlearnAlgorithm class. configs/ : Contains configuration files for training and generation. model.py : Implementation of the SaliencyUnlearnModel class. scripts/train.py : Script to train the SaliencyUnlearn algorithm. trainer.py : Implementation of the SaliencyUnlearnTrainer class. utils.py : Utility functions used in the project. data_handler.py : Implementation of DataHandler class The unlearning has two stages: Generate the mask Unlearn the weights.","title":"Directory Structure"},{"location":"unlearn/algorithms/saliency/#description-of-arguments-in-mask_configyaml","text":"The config/mask_config.yaml file is a configuration file for generating saliency masks using the scripts/generate_mask.py script. It defines various parameters related to the model, dataset, output, and training. Below is a detailed description of each section and parameter: Model Configuration These parameters specify settings for the Stable Diffusion model and guidance configurations. c_guidance: Guidance scale used during loss computation in the model. Higher values may emphasize certain features in mask generation. Type: float Example: 7.5 batch_size: Number of images processed in a single batch. Type: int Example: 4 ckpt_path: Path to the model checkpoint file for Stable Diffusion. Type: str Example: /path/to/compvis.ckpt model_config_path: Path to the model configuration YAML file for Stable Diffusion. Type: str Example: /path/to/model_config.yaml num_timesteps: Number of timesteps used in the diffusion process. Type: int Example: 1000 image_size: Size of the input images used for training and mask generation (in pixels). Type: int Example: 512 Dataset Configuration These parameters define the dataset paths and settings for mask generation. raw_dataset_dir: Path to the directory containing the original dataset, organized by themes and classes. Type: str Example: /path/to/raw/dataset processed_dataset_dir: Path to the directory where processed datasets will be saved after mask generation. Type: str Example: /path/to/processed/dataset dataset_type: Type of dataset being used. Choices: unlearncanvas, i2p Type: str Example: i2p template: Type of template for mask generation. Choices: object, style, i2p Type: str Example: style template_name: Specific template name for the mask generation process. Example Choices: self-harm, Abstractionism Type: str Example: Abstractionism threshold: Threshold value for mask generation to filter salient regions. Type: float Example: 0.5 Output Configuration These parameters specify the directory where the results are saved. output_dir: Directory where the generated masks will be saved. Type: str Example: outputs/saliency_unlearning/masks Training Configuration These parameters control the training process for mask generation. lr: Learning rate used for training the masking algorithm. Type: float Example: 0.00001 devices: CUDA devices used for training, specified as a comma-separated list. Type: str Example: 0 use_sample: Flag indicating whether to use a sample dataset for training and mask generation. Type: bool Example: True","title":"Description of Arguments in mask_config.yaml"},{"location":"unlearn/algorithms/saliency/#description-of-arguments-train_configyaml","text":"The scripts/train.py script is used to fine-tune the Stable Diffusion model to perform saliency-based unlearning. This script relies on a configuration file ( config/train_config.yaml ) and supports additional runtime arguments for further customization. Below is a detailed description of each argument: General Arguments alpha: Guidance scale used to balance the loss components during training. Type: float Example: 0.1 epochs: Number of epochs to train the model. Type: int Example: 5 train_method: Specifies the training method or strategy to be used. Choices: noxattn, selfattn, xattn, full, notime, xlayer, selflayer Type: str Example: noxattn model_config_path: Path to the model configuration YAML file for Stable Diffusion. Type: str Example: 'mu/algorithms/saliency_unlearning/configs/model_config.yaml' Dataset Arguments raw_dataset_dir: Path to the directory containing the raw dataset, organized by themes and classes. Type: str Example: 'path/raw_dataset/' processed_dataset_dir: Path to the directory where the processed dataset will be saved. Type: str Example: 'path/processed_dataset_dir' dataset_type: Specifies the type of dataset to use for training. Choices: unlearncanvas, i2p Type: str Example: i2p template: Specifies the template type for training. Choices: object, style, i2p Type: str Example: style template_name: Name of the specific template used for training. Example Choices: self-harm, Abstractionism Type: str Example: Abstractionism Output Arguments output_dir: Directory where the fine-tuned model and training outputs will be saved. Type: str Example: 'output/folder_name' mask_path: Path to the saliency mask file used during training. Type: str Example:","title":"Description of Arguments train_config.yaml"},{"location":"unlearn/algorithms/scissorhands/","text":"ScissorHands Algorithm for Machine Unlearning This repository provides an implementation of the scissor hands algorithm for machine unlearning in Stable Diffusion models. The scissor hands algorithm allows you to remove specific concepts or styles from a pre-trained model without retraining it from scratch. Installation pip install unlearn_diff Prerequisities Ensure conda is installed on your system. You can install Miniconda or Anaconda: Miniconda (recommended): https://docs.conda.io/en/latest/miniconda.html Anaconda : https://www.anaconda.com/products/distribution After installing conda , ensure it is available in your PATH by running. You may require to restart the terminal session: conda --version Create environment: create_env <algorithm_name> eg: create_env scissorhands Activate environment: conda activate <environment_name> eg: conda activate scissorhands The has to be one of the folders in the mu/algorithms folder. Downloading data and models. After you install the package, you can use the following commands to download. Dataset : i2p : Sample : download_data sample i2p Full : download_data full i2p quick_canvas : Sample : download_data sample quick_canvas Full : download_data full quick_canvas Model : compvis : download_model compvis diffuser : download_model diffuser Verify the Downloaded Files After downloading, verify that the datasets have been correctly extracted: ls -lh ./data/i2p-dataset/sample/ ls -lh ./data/quick-canvas-dataset/sample/ Usage To train the ScissorHands algorithm to unlearn a specific concept or style from the Stable Diffusion model, use the train.py script located in the scripts directory. Run Train Create a file, eg, my_trainer.py and use examples and modify your configs to run the file. Example Code from mu.algorithms.scissorhands.algorithm import ScissorHandsAlgorithm from mu.algorithms.scissorhands.configs import ( scissorhands_train_mu, ) algorithm = ScissorHandsAlgorithm( scissorhands_train_mu, ckpt_path=\"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\", raw_dataset_dir=( \"/home/ubuntu/Projects/balaram/packaging/data/quick-canvas-dataset/sample\" ), output_dir=\"/opt/dlami/nvme/outputs\", ) algorithm.run() Running the Training Script in Offline Mode WANDB_MODE=offline python my_trainer.py How It Works * Default Values: The script first loads default values from the train config file as in configs section. Parameter Overrides: Any parameters passed directly to the algorithm, overrides these configs. Final Configuration: The script merges the configs and convert them into dictionary to proceed with the training. Directory Structure algorithm.py : Implementation of the ScissorHandsAlgorithm class. configs/ : Contains configuration files for training and generation. model.py : Implementation of the ScissorHandsModel class. scripts/train.py : Script to train the ScissorHands algorithm. trainer.py : Implementation of the ScissorHandsTrainer class. utils.py : Utility functions used in the project. data_handler.py : Implementation of DataHandler class Description of Arguments in train_config.yaml Training Parameters train_method: Specifies the method of training for concept erasure. Choices: [\"noxattn\", \"selfattn\", \"xattn\", \"full\", \"notime\", \"xlayer\", \"selflayer\"] Example: \"xattn\" alpha: Guidance strength for the starting image during training. Type: float Example: 0.1 epochs: Number of epochs to train the model. Type: int Example: 1 Model Configuration model_config_path: File path to the Stable Diffusion model configuration YAML file. type: str Example: \"/path/to/model_config.yaml\" ckpt_path: File path to the checkpoint of the Stable Diffusion model. Type: str Example: \"/path/to/model_checkpoint.ckpt\" Dataset Directories raw_dataset_dir: Directory containing the raw dataset categorized by themes or classes. Type: str Example: \"/path/to/raw_dataset\" processed_dataset_dir: Directory to save the processed dataset. Type: str Example: \"/path/to/processed_dataset\" dataset_type: Specifies the dataset type for the training process. Choices: [\"unlearncanvas\", \"i2p\"] Example: \"unlearncanvas\" template: Type of template to use during training. Choices: [\"object\", \"style\", \"i2p\"] Example: \"style\" template_name: Name of the specific concept or style to be erased. Choices: [\"self-harm\", \"Abstractionism\"] Example: \"Abstractionism\" Output Configurations output_dir: Directory where the fine-tuned models and results will be saved. Type: str Example: \"outputs/erase_diff/finetuned_models\" Sampling and Image Configurations sparsity: Threshold for mask. Type: float Example: 0.99 project: Type: bool Example: false memory_num: Type: Int Example: 1 prune_num: Type: Int Example: 1 Device Configuration devices: Specifies the CUDA devices to be used for training (comma-separated). Type: str (Comma separated) Example: \"0, 1\" Additional Flags use_sample: Flag to indicate whether a sample dataset should be used for training. Type: bool Example: True","title":"Usage"},{"location":"unlearn/algorithms/scissorhands/#scissorhands-algorithm-for-machine-unlearning","text":"This repository provides an implementation of the scissor hands algorithm for machine unlearning in Stable Diffusion models. The scissor hands algorithm allows you to remove specific concepts or styles from a pre-trained model without retraining it from scratch.","title":"ScissorHands Algorithm for Machine Unlearning"},{"location":"unlearn/algorithms/scissorhands/#installation","text":"pip install unlearn_diff","title":"Installation"},{"location":"unlearn/algorithms/scissorhands/#prerequisities","text":"Ensure conda is installed on your system. You can install Miniconda or Anaconda: Miniconda (recommended): https://docs.conda.io/en/latest/miniconda.html Anaconda : https://www.anaconda.com/products/distribution After installing conda , ensure it is available in your PATH by running. You may require to restart the terminal session: conda --version","title":"Prerequisities"},{"location":"unlearn/algorithms/scissorhands/#create-environment","text":"create_env <algorithm_name> eg: create_env scissorhands","title":"Create environment:"},{"location":"unlearn/algorithms/scissorhands/#activate-environment","text":"conda activate <environment_name> eg: conda activate scissorhands The has to be one of the folders in the mu/algorithms folder.","title":"Activate environment:"},{"location":"unlearn/algorithms/scissorhands/#downloading-data-and-models","text":"After you install the package, you can use the following commands to download. Dataset : i2p : Sample : download_data sample i2p Full : download_data full i2p quick_canvas : Sample : download_data sample quick_canvas Full : download_data full quick_canvas Model : compvis : download_model compvis diffuser : download_model diffuser Verify the Downloaded Files After downloading, verify that the datasets have been correctly extracted: ls -lh ./data/i2p-dataset/sample/ ls -lh ./data/quick-canvas-dataset/sample/","title":"Downloading data and models."},{"location":"unlearn/algorithms/scissorhands/#usage","text":"To train the ScissorHands algorithm to unlearn a specific concept or style from the Stable Diffusion model, use the train.py script located in the scripts directory.","title":"Usage"},{"location":"unlearn/algorithms/scissorhands/#run-train","text":"Create a file, eg, my_trainer.py and use examples and modify your configs to run the file. Example Code from mu.algorithms.scissorhands.algorithm import ScissorHandsAlgorithm from mu.algorithms.scissorhands.configs import ( scissorhands_train_mu, ) algorithm = ScissorHandsAlgorithm( scissorhands_train_mu, ckpt_path=\"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\", raw_dataset_dir=( \"/home/ubuntu/Projects/balaram/packaging/data/quick-canvas-dataset/sample\" ), output_dir=\"/opt/dlami/nvme/outputs\", ) algorithm.run() Running the Training Script in Offline Mode WANDB_MODE=offline python my_trainer.py How It Works * Default Values: The script first loads default values from the train config file as in configs section. Parameter Overrides: Any parameters passed directly to the algorithm, overrides these configs. Final Configuration: The script merges the configs and convert them into dictionary to proceed with the training.","title":"Run Train"},{"location":"unlearn/algorithms/scissorhands/#directory-structure","text":"algorithm.py : Implementation of the ScissorHandsAlgorithm class. configs/ : Contains configuration files for training and generation. model.py : Implementation of the ScissorHandsModel class. scripts/train.py : Script to train the ScissorHands algorithm. trainer.py : Implementation of the ScissorHandsTrainer class. utils.py : Utility functions used in the project. data_handler.py : Implementation of DataHandler class","title":"Directory Structure"},{"location":"unlearn/algorithms/scissorhands/#description-of-arguments-in-train_configyaml","text":"Training Parameters train_method: Specifies the method of training for concept erasure. Choices: [\"noxattn\", \"selfattn\", \"xattn\", \"full\", \"notime\", \"xlayer\", \"selflayer\"] Example: \"xattn\" alpha: Guidance strength for the starting image during training. Type: float Example: 0.1 epochs: Number of epochs to train the model. Type: int Example: 1 Model Configuration model_config_path: File path to the Stable Diffusion model configuration YAML file. type: str Example: \"/path/to/model_config.yaml\" ckpt_path: File path to the checkpoint of the Stable Diffusion model. Type: str Example: \"/path/to/model_checkpoint.ckpt\" Dataset Directories raw_dataset_dir: Directory containing the raw dataset categorized by themes or classes. Type: str Example: \"/path/to/raw_dataset\" processed_dataset_dir: Directory to save the processed dataset. Type: str Example: \"/path/to/processed_dataset\" dataset_type: Specifies the dataset type for the training process. Choices: [\"unlearncanvas\", \"i2p\"] Example: \"unlearncanvas\" template: Type of template to use during training. Choices: [\"object\", \"style\", \"i2p\"] Example: \"style\" template_name: Name of the specific concept or style to be erased. Choices: [\"self-harm\", \"Abstractionism\"] Example: \"Abstractionism\" Output Configurations output_dir: Directory where the fine-tuned models and results will be saved. Type: str Example: \"outputs/erase_diff/finetuned_models\" Sampling and Image Configurations sparsity: Threshold for mask. Type: float Example: 0.99 project: Type: bool Example: false memory_num: Type: Int Example: 1 prune_num: Type: Int Example: 1 Device Configuration devices: Specifies the CUDA devices to be used for training (comma-separated). Type: str (Comma separated) Example: \"0, 1\" Additional Flags use_sample: Flag to indicate whether a sample dataset should be used for training. Type: bool Example: True","title":"Description of Arguments in train_config.yaml"},{"location":"unlearn/algorithms/selective_amnesia/","text":"Selective Amnesia Algorithm for Machine Unlearning This repository provides an implementation of the Selective Amnesia algorithm for machine unlearning in Stable Diffusion models. The Selective Amnesia algorithm focuses on removing specific concepts or styles from a pre-trained model while retaining the rest of the knowledge. Installation pip install unlearn_diff Prerequisities Ensure conda is installed on your system. You can install Miniconda or Anaconda: Miniconda (recommended): https://docs.conda.io/en/latest/miniconda.html Anaconda : https://www.anaconda.com/products/distribution After installing conda , ensure it is available in your PATH by running. You may require to restart the terminal session: conda --version Create environment: create_env <algorithm_name> eg: create_env selective_amnesia Activate environment: conda activate <environment_name> eg: conda activate selective_amnesia The has to be one of the folders in the mu/algorithms folder. Downloading data and models. After you install the package, you can use the following commands to download. Dataset : i2p : Sample : download_data sample i2p Full : download_data full i2p quick_canvas : Sample : download_data sample quick_canvas Full : download_data full quick_canvas Model : compvis : download_model compvis diffuser : download_model diffuser Verify the Downloaded Files After downloading, verify that the datasets have been correctly extracted: ls -lh ./data/i2p-dataset/sample/ ls -lh ./data/quick-canvas-dataset/sample/ Usage To train the Selective Amnesia algorithm to remove specific concepts or styles from the Stable Diffusion model, use the train.py script located in the scripts directory. Example Command First download the full_fisher_dict.pkl file. wget https://huggingface.co/ajrheng/selective-amnesia/resolve/main/full_fisher_dict.pkl Run the script python -m mu.algorithms.selective_amnesia.scripts.train --config_path mu/algorithms/selective_amnesia/configs/train_config.yaml --full_fisher_dict_pkl_path /path/full_fisher_dict.pkl Run train Create a file, eg, my_trainer.py and use examples and modify your configs to run the file. from mu.algorithms.selective_amnesia.algorithm import SelectiveAmnesiaAlgorithm from mu.algorithms.selective_amnesia.configs import ( selective_amnesia_config_quick_canvas, ) algorithm = SelectiveAmnesiaAlgorithm( selective_amnesia_config_quick_canvas, ckpt_path=\"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\", raw_dataset_dir=( \"/home/ubuntu/Projects/balaram/packaging/data/quick-canvas-dataset/sample\" ), ) algorithm.run() WANDB_MODE=offline python my_trainer.py How It Works * Default Values: The script first loads default values from the train config file as in configs section. Parameter Overrides: Any parameters passed directly to the algorithm, overrides these configs. Final Configuration: The script merges the configs and convert them into dictionary to proceed with the training. Notes Ensure all dependencies are installed as per the environment file. The training process generates logs in the logs/ directory for easy monitoring. Use appropriate CUDA devices for optimal performance during training. Regularly verify dataset and model configurations to avoid errors during execution. Configuration File ( train_config.yaml ) Training Parameters seed: Random seed for reproducibility. Type: int Example: 23 scale_lr: Whether to scale the base learning rate. Type: bool Example: True Model Configuration model_config_path: Path to the Stable Diffusion model configuration YAML file. Type: str Example: \"/path/to/model_config.yaml\" ckpt_path: Path to the Stable Diffusion model checkpoint. Type: str Example: \"/path/to/compvis.ckpt\" full_fisher_dict_pkl_path: Path to the full fisher dict pkl file Type: str Example: \"full_fisher_dict.pkl\" Dataset Directories raw_dataset_dir: Directory containing the raw dataset categorized by themes or classes. Type: str Example: \"/path/to/raw_dataset\" processed_dataset_dir: Directory to save the processed dataset. Type: str Example: \"/path/to/processed_dataset\" dataset_type: Specifies the dataset type for training. Choices: [\"unlearncanvas\", \"i2p\"] Example: \"unlearncanvas\" template: Type of template to use during training. Choices: [\"object\", \"style\", \"i2p\"] Example: \"style\" template_name: Name of the concept or style to erase. Choices: [\"self-harm\", \"Abstractionism\"] Example: \"Abstractionism\" Output Configurations output_dir: Directory to save fine-tuned models and results. Type: str Example: \"outputs/selective_amnesia/finetuned_models\" Device Configuration devices: CUDA devices for training (comma-separated). Type: str Example: \"0\" Data Parameters train_batch_size: Batch size for training. Type: int Example: 4 val_batch_size: Batch size for validation. Type: int Example: 6 num_workers: Number of worker threads for data loading. Type: int Example: 4 forget_prompt: Prompt to specify the style or concept to forget. Type: str Example: \"An image in Artist_Sketch style\" Lightning Configuration max_epochs: Maximum number of epochs for training. Type: int Example: 50 callbacks: batch_frequency: Frequency for logging image batches. Type: int Example: 1 max_images: Maximum number of images to log. Type: int Example: 999","title":"Usage"},{"location":"unlearn/algorithms/selective_amnesia/#selective-amnesia-algorithm-for-machine-unlearning","text":"This repository provides an implementation of the Selective Amnesia algorithm for machine unlearning in Stable Diffusion models. The Selective Amnesia algorithm focuses on removing specific concepts or styles from a pre-trained model while retaining the rest of the knowledge.","title":"Selective Amnesia Algorithm for Machine Unlearning"},{"location":"unlearn/algorithms/selective_amnesia/#installation","text":"pip install unlearn_diff","title":"Installation"},{"location":"unlearn/algorithms/selective_amnesia/#prerequisities","text":"Ensure conda is installed on your system. You can install Miniconda or Anaconda: Miniconda (recommended): https://docs.conda.io/en/latest/miniconda.html Anaconda : https://www.anaconda.com/products/distribution After installing conda , ensure it is available in your PATH by running. You may require to restart the terminal session: conda --version","title":"Prerequisities"},{"location":"unlearn/algorithms/selective_amnesia/#create-environment","text":"create_env <algorithm_name> eg: create_env selective_amnesia","title":"Create environment:"},{"location":"unlearn/algorithms/selective_amnesia/#activate-environment","text":"conda activate <environment_name> eg: conda activate selective_amnesia The has to be one of the folders in the mu/algorithms folder.","title":"Activate environment:"},{"location":"unlearn/algorithms/selective_amnesia/#downloading-data-and-models","text":"After you install the package, you can use the following commands to download. Dataset : i2p : Sample : download_data sample i2p Full : download_data full i2p quick_canvas : Sample : download_data sample quick_canvas Full : download_data full quick_canvas Model : compvis : download_model compvis diffuser : download_model diffuser Verify the Downloaded Files After downloading, verify that the datasets have been correctly extracted: ls -lh ./data/i2p-dataset/sample/ ls -lh ./data/quick-canvas-dataset/sample/","title":"Downloading data and models."},{"location":"unlearn/algorithms/selective_amnesia/#usage","text":"To train the Selective Amnesia algorithm to remove specific concepts or styles from the Stable Diffusion model, use the train.py script located in the scripts directory.","title":"Usage"},{"location":"unlearn/algorithms/selective_amnesia/#example-command","text":"First download the full_fisher_dict.pkl file. wget https://huggingface.co/ajrheng/selective-amnesia/resolve/main/full_fisher_dict.pkl Run the script python -m mu.algorithms.selective_amnesia.scripts.train --config_path mu/algorithms/selective_amnesia/configs/train_config.yaml --full_fisher_dict_pkl_path /path/full_fisher_dict.pkl","title":"Example Command"},{"location":"unlearn/algorithms/selective_amnesia/#run-train","text":"Create a file, eg, my_trainer.py and use examples and modify your configs to run the file. from mu.algorithms.selective_amnesia.algorithm import SelectiveAmnesiaAlgorithm from mu.algorithms.selective_amnesia.configs import ( selective_amnesia_config_quick_canvas, ) algorithm = SelectiveAmnesiaAlgorithm( selective_amnesia_config_quick_canvas, ckpt_path=\"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\", raw_dataset_dir=( \"/home/ubuntu/Projects/balaram/packaging/data/quick-canvas-dataset/sample\" ), ) algorithm.run() WANDB_MODE=offline python my_trainer.py How It Works * Default Values: The script first loads default values from the train config file as in configs section. Parameter Overrides: Any parameters passed directly to the algorithm, overrides these configs. Final Configuration: The script merges the configs and convert them into dictionary to proceed with the training.","title":"Run train"},{"location":"unlearn/algorithms/selective_amnesia/#notes","text":"Ensure all dependencies are installed as per the environment file. The training process generates logs in the logs/ directory for easy monitoring. Use appropriate CUDA devices for optimal performance during training. Regularly verify dataset and model configurations to avoid errors during execution.","title":"Notes"},{"location":"unlearn/algorithms/selective_amnesia/#configuration-file-train_configyaml","text":"","title":"Configuration File (train_config.yaml)"},{"location":"unlearn/algorithms/selective_amnesia/#training-parameters","text":"seed: Random seed for reproducibility. Type: int Example: 23 scale_lr: Whether to scale the base learning rate. Type: bool Example: True","title":"Training Parameters"},{"location":"unlearn/algorithms/selective_amnesia/#model-configuration","text":"model_config_path: Path to the Stable Diffusion model configuration YAML file. Type: str Example: \"/path/to/model_config.yaml\" ckpt_path: Path to the Stable Diffusion model checkpoint. Type: str Example: \"/path/to/compvis.ckpt\" full_fisher_dict_pkl_path: Path to the full fisher dict pkl file Type: str Example: \"full_fisher_dict.pkl\"","title":"Model Configuration"},{"location":"unlearn/algorithms/selective_amnesia/#dataset-directories","text":"raw_dataset_dir: Directory containing the raw dataset categorized by themes or classes. Type: str Example: \"/path/to/raw_dataset\" processed_dataset_dir: Directory to save the processed dataset. Type: str Example: \"/path/to/processed_dataset\" dataset_type: Specifies the dataset type for training. Choices: [\"unlearncanvas\", \"i2p\"] Example: \"unlearncanvas\" template: Type of template to use during training. Choices: [\"object\", \"style\", \"i2p\"] Example: \"style\" template_name: Name of the concept or style to erase. Choices: [\"self-harm\", \"Abstractionism\"] Example: \"Abstractionism\"","title":"Dataset Directories"},{"location":"unlearn/algorithms/selective_amnesia/#output-configurations","text":"output_dir: Directory to save fine-tuned models and results. Type: str Example: \"outputs/selective_amnesia/finetuned_models\"","title":"Output Configurations"},{"location":"unlearn/algorithms/selective_amnesia/#device-configuration","text":"devices: CUDA devices for training (comma-separated). Type: str Example: \"0\"","title":"Device Configuration"},{"location":"unlearn/algorithms/selective_amnesia/#data-parameters","text":"train_batch_size: Batch size for training. Type: int Example: 4 val_batch_size: Batch size for validation. Type: int Example: 6 num_workers: Number of worker threads for data loading. Type: int Example: 4 forget_prompt: Prompt to specify the style or concept to forget. Type: str Example: \"An image in Artist_Sketch style\"","title":"Data Parameters"},{"location":"unlearn/algorithms/selective_amnesia/#lightning-configuration","text":"max_epochs: Maximum number of epochs for training. Type: int Example: 50 callbacks: batch_frequency: Frequency for logging image batches. Type: int Example: 1 max_images: Maximum number of images to log. Type: int Example: 999","title":"Lightning Configuration"},{"location":"unlearn/algorithms/semipermeable_membrane/","text":"Semi Permeable Membrane Algorithm for Machine Unlearning This repository provides an implementation of the semipermeable membrane algorithm for machine unlearning in Stable Diffusion models. The semipermeable membrane algorithm allows you to remove specific concepts or styles from a pre-trained model without retraining it from scratch. Installation pip install unlearn_diff Prerequisities Ensure conda is installed on your system. You can install Miniconda or Anaconda: Miniconda (recommended): https://docs.conda.io/en/latest/miniconda.html Anaconda : https://www.anaconda.com/products/distribution After installing conda , ensure it is available in your PATH by running. You may require to restart the terminal session: conda --version Create environment: create_env <algorithm_name> eg: create_env semipermeable_membrane Activate environment: conda activate <environment_name> eg: conda activate semipermeable_membrane The has to be one of the folders in the mu/algorithms folder. Downloading data and models. After you install the package, you can use the following commands to download. Dataset : i2p : Sample : download_data sample i2p Full : download_data full i2p quick_canvas : Sample : download_data sample quick_canvas Full : download_data full quick_canvas Model : compvis : download_model compvis diffuser : download_model diffuser Verify the Downloaded Files After downloading, verify that the datasets have been correctly extracted: ls -lh ./data/i2p-dataset/sample/ ls -lh ./data/quick-canvas-dataset/sample/ Usage To train the Semi Permeable Membrane algorithm to unlearn a specific concept or style from the Stable Diffusion model, use the train.py script located in the scripts directory. Run Train Create a file, eg, my_trainer.py and use examples and modify your configs to run the file. Example Code from mu.algorithms.semipermeable_membrane.algorithm import ( SemipermeableMembraneAlgorithm, ) from mu.algorithms.semipermeable_membrane.configs import ( semipermiable_membrane_train_mu, SemipermeableMembraneConfig, ) algorithm = SemipermeableMembraneAlgorithm( semipermiable_membrane_train_mu, output_dir=\"/opt/dlami/nvme/outputs\", train={\"iterations\": 2}, ) algorithm.run() Running the Training Script in Offline Mode WANDB_MODE=offline python my_trainer.py How It Works * Default Values: The script first loads default values from the train config file as in configs section. Parameter Overrides: Any parameters passed directly to the algorithm, overrides these configs. Final Configuration: The script merges the configs and convert them into dictionary to proceed with the training. Directory Structure algorithm.py : Implementation of the Semi Permeable MembraneAlgorithm class. configs/ : Contains configuration files for training and generation. model.py : Implementation of the Semi Permeable MembraneModel class. scripts/train.py : Script to train the Semi Permeable Membrane algorithm. trainer.py : Implementation of the Semi Permeable MembraneTrainer class. utils.py : Utility functions used in the project. data_handler.py : Implementation of DataHandler class Description of Arguments in train_config.yaml pretrained_model ckpt_path: File path to the pretrained model's checkpoint file. v2: Boolean indicating whether the pretrained model is version 2 or not. v_pred: Boolean to enable/disable \"v-prediction\" mode for diffusion models. clip_skip: Number of CLIP layers to skip during inference. network rank: Rank of the low-rank adaptation network. alpha: Scaling factor for the network during training. train precision: Numerical precision to use during training (e.g., float32 or float16). noise_scheduler: Type of noise scheduler to use in the training loop (e.g., ddim). iterations: Number of training iterations. batch_size: Batch size for training. lr: Learning rate for the training optimizer. unet_lr: Learning rate for the U-Net model. text_encoder_lr: Learning rate for the text encoder. optimizer_type: Optimizer to use for training (e.g., AdamW8bit). lr_scheduler: Learning rate scheduler to apply during training. lr_warmup_steps: Number of steps for linear warmup of the learning rate. lr_scheduler_num_cycles: Number of cycles for a cosine-with-restarts scheduler. max_denoising_steps: Maximum denoising steps to use during training. save per_steps: Frequency of saving the model (in steps). precision: Numerical precision for saved model weights other use_xformers: Boolean to enable xformers memory-efficient attention. wandb_project and wandb_run Configuration for tracking the training progress using Weights & Biases. wandb_project: Project name in W&B. wandb_run: Specific run name in the W&B dashboard. use_sample Boolean to indicate whether to use the sample dataset for training. dataset_type Type of dataset to use, options are unlearncanvas or i2p. template * Specifies the template type, choices are: * object: Focus on specific objects. * style: Focus on artistic styles. * i2p: Intermediate style processing. template_name Name of the template, choices are: self-harm Abstractionism prompt target: Target template or concept to guide training (references template_name). positive: Positive prompt based on the template. unconditional: Unconditional prompt text. neutral: Neutral prompt text. action: Specifies the action applied to the prompt (e.g., erase_with_la). guidance_scale: Guidance scale for classifier-free guidance. resolution: Image resolution for training. batch_size: Batch size for generating prompts. dynamic_resolution: Boolean to allow dynamic resolution. la_strength: Strength of local adaptation. sampling_batch_size: Batch size for sampling images. devices CUDA devices to use for training (specified as a comma-separated list, e.g., \"0,1\"). output_dir Directory to save the fine-tuned model and other outputs. verbose Boolean flag for verbose logging during training.","title":"Usage"},{"location":"unlearn/algorithms/semipermeable_membrane/#semi-permeable-membrane-algorithm-for-machine-unlearning","text":"This repository provides an implementation of the semipermeable membrane algorithm for machine unlearning in Stable Diffusion models. The semipermeable membrane algorithm allows you to remove specific concepts or styles from a pre-trained model without retraining it from scratch.","title":"Semi Permeable Membrane Algorithm for Machine Unlearning"},{"location":"unlearn/algorithms/semipermeable_membrane/#installation","text":"pip install unlearn_diff","title":"Installation"},{"location":"unlearn/algorithms/semipermeable_membrane/#prerequisities","text":"Ensure conda is installed on your system. You can install Miniconda or Anaconda: Miniconda (recommended): https://docs.conda.io/en/latest/miniconda.html Anaconda : https://www.anaconda.com/products/distribution After installing conda , ensure it is available in your PATH by running. You may require to restart the terminal session: conda --version","title":"Prerequisities"},{"location":"unlearn/algorithms/semipermeable_membrane/#create-environment","text":"create_env <algorithm_name> eg: create_env semipermeable_membrane","title":"Create environment:"},{"location":"unlearn/algorithms/semipermeable_membrane/#activate-environment","text":"conda activate <environment_name> eg: conda activate semipermeable_membrane The has to be one of the folders in the mu/algorithms folder.","title":"Activate environment:"},{"location":"unlearn/algorithms/semipermeable_membrane/#downloading-data-and-models","text":"After you install the package, you can use the following commands to download. Dataset : i2p : Sample : download_data sample i2p Full : download_data full i2p quick_canvas : Sample : download_data sample quick_canvas Full : download_data full quick_canvas Model : compvis : download_model compvis diffuser : download_model diffuser Verify the Downloaded Files After downloading, verify that the datasets have been correctly extracted: ls -lh ./data/i2p-dataset/sample/ ls -lh ./data/quick-canvas-dataset/sample/","title":"Downloading data and models."},{"location":"unlearn/algorithms/semipermeable_membrane/#usage","text":"To train the Semi Permeable Membrane algorithm to unlearn a specific concept or style from the Stable Diffusion model, use the train.py script located in the scripts directory.","title":"Usage"},{"location":"unlearn/algorithms/semipermeable_membrane/#run-train","text":"Create a file, eg, my_trainer.py and use examples and modify your configs to run the file. Example Code from mu.algorithms.semipermeable_membrane.algorithm import ( SemipermeableMembraneAlgorithm, ) from mu.algorithms.semipermeable_membrane.configs import ( semipermiable_membrane_train_mu, SemipermeableMembraneConfig, ) algorithm = SemipermeableMembraneAlgorithm( semipermiable_membrane_train_mu, output_dir=\"/opt/dlami/nvme/outputs\", train={\"iterations\": 2}, ) algorithm.run() Running the Training Script in Offline Mode WANDB_MODE=offline python my_trainer.py How It Works * Default Values: The script first loads default values from the train config file as in configs section. Parameter Overrides: Any parameters passed directly to the algorithm, overrides these configs. Final Configuration: The script merges the configs and convert them into dictionary to proceed with the training.","title":"Run Train"},{"location":"unlearn/algorithms/semipermeable_membrane/#directory-structure","text":"algorithm.py : Implementation of the Semi Permeable MembraneAlgorithm class. configs/ : Contains configuration files for training and generation. model.py : Implementation of the Semi Permeable MembraneModel class. scripts/train.py : Script to train the Semi Permeable Membrane algorithm. trainer.py : Implementation of the Semi Permeable MembraneTrainer class. utils.py : Utility functions used in the project. data_handler.py : Implementation of DataHandler class","title":"Directory Structure"},{"location":"unlearn/algorithms/semipermeable_membrane/#description-of-arguments-in-train_configyaml","text":"pretrained_model ckpt_path: File path to the pretrained model's checkpoint file. v2: Boolean indicating whether the pretrained model is version 2 or not. v_pred: Boolean to enable/disable \"v-prediction\" mode for diffusion models. clip_skip: Number of CLIP layers to skip during inference. network rank: Rank of the low-rank adaptation network. alpha: Scaling factor for the network during training. train precision: Numerical precision to use during training (e.g., float32 or float16). noise_scheduler: Type of noise scheduler to use in the training loop (e.g., ddim). iterations: Number of training iterations. batch_size: Batch size for training. lr: Learning rate for the training optimizer. unet_lr: Learning rate for the U-Net model. text_encoder_lr: Learning rate for the text encoder. optimizer_type: Optimizer to use for training (e.g., AdamW8bit). lr_scheduler: Learning rate scheduler to apply during training. lr_warmup_steps: Number of steps for linear warmup of the learning rate. lr_scheduler_num_cycles: Number of cycles for a cosine-with-restarts scheduler. max_denoising_steps: Maximum denoising steps to use during training. save per_steps: Frequency of saving the model (in steps). precision: Numerical precision for saved model weights other use_xformers: Boolean to enable xformers memory-efficient attention. wandb_project and wandb_run Configuration for tracking the training progress using Weights & Biases. wandb_project: Project name in W&B. wandb_run: Specific run name in the W&B dashboard. use_sample Boolean to indicate whether to use the sample dataset for training. dataset_type Type of dataset to use, options are unlearncanvas or i2p. template * Specifies the template type, choices are: * object: Focus on specific objects. * style: Focus on artistic styles. * i2p: Intermediate style processing. template_name Name of the template, choices are: self-harm Abstractionism prompt target: Target template or concept to guide training (references template_name). positive: Positive prompt based on the template. unconditional: Unconditional prompt text. neutral: Neutral prompt text. action: Specifies the action applied to the prompt (e.g., erase_with_la). guidance_scale: Guidance scale for classifier-free guidance. resolution: Image resolution for training. batch_size: Batch size for generating prompts. dynamic_resolution: Boolean to allow dynamic resolution. la_strength: Strength of local adaptation. sampling_batch_size: Batch size for sampling images. devices CUDA devices to use for training (specified as a comma-separated list, e.g., \"0,1\"). output_dir Directory to save the fine-tuned model and other outputs. verbose Boolean flag for verbose logging during training.","title":"Description of Arguments in train_config.yaml"},{"location":"unlearn/algorithms/uce/","text":"Unified Concept Editing Algorithm for Machine Unlearning This repository provides an implementation of the unified concept editing algorithm for machine unlearning in Stable Diffusion models. The unified concept editing algorithm allows you to remove specific concepts or styles from a pre-trained model without retraining it from scratch. Installation pip install unlearn_diff Prerequisities Ensure conda is installed on your system. You can install Miniconda or Anaconda: Miniconda (recommended): https://docs.conda.io/en/latest/miniconda.html Anaconda : https://www.anaconda.com/products/distribution After installing conda , ensure it is available in your PATH by running. You may require to restart the terminal session: conda --version Create environment: create_env <algorithm_name> eg: create_env unified_concept_editing Activate environment: conda activate <environment_name> eg: conda activate unified_concept_editing The has to be one of the folders in the mu/algorithms folder. Downloading data and models. After you install the package, you can use the following commands to download. Dataset : i2p : Sample : download_data sample i2p Full : download_data full i2p quick_canvas : Sample : download_data sample quick_canvas Full : download_data full quick_canvas Model : compvis : download_model compvis diffuser : download_model diffuser Verify the Downloaded Files After downloading, verify that the datasets have been correctly extracted: ls -lh ./data/i2p-dataset/sample/ ls -lh ./data/quick-canvas-dataset/sample/ Usage To train the Unified Concept Editing algorithm to unlearn a specific concept or style from the Stable Diffusion model, use the train.py script located in the scripts directory. Run Train Create a file, eg, my_trainer.py and use examples and modify your configs to run the file. Example Code from mu.algorithms.unified_concept_editing.algorithm import ( UnifiedConceptEditingAlgorithm, ) from mu.algorithms.unified_concept_editing.configs import ( unified_concept_editing_train_mu, ) algorithm = UnifiedConceptEditingAlgorithm( unified_concept_editing_train_mu, ckpt_path=\"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/diffuser/style50/\", raw_dataset_dir=( \"/home/ubuntu/Projects/balaram/packaging/data/quick-canvas-dataset/sample\" ), output_dir=\"/opt/dlami/nvme/outputs\", ) algorithm.run() Running the Training Script in Offline Mode WANDB_MODE=offline python my_trainer.py How It Works * Default Values: The script first loads default values from the train config file as in configs section. Parameter Overrides: Any parameters passed directly to the algorithm, overrides these configs. Final Configuration: The script merges the configs and convert them into dictionary to proceed with the training. Directory Structure algorithm.py : Implementation of the ScissorHandsAlgorithm class. configs/ : Contains configuration files for training and generation. model.py : Implementation of the ScissorHandsModel class. scripts/train.py : Script to train the ScissorHands algorithm. trainer.py : Implementation of the ScissorHandsTrainer class. utils.py : Utility functions used in the project. data_handler.py : Implementation of DataHandler class Description of Arguments in train_config.yaml Training Parameters train_method : Specifies the method of training for concept erasure. Choices: [\"full\", \"partial\"] Example: \"full\" alpha : Guidance strength for the starting image during training. Type: float Example: 0.1 epochs : Number of epochs to train the model. Type: int Example: 10 lr : Learning rate used for the optimizer during training. Type: float Example: 5e-5 Model Configuration * ckpt_path : File path to the checkpoint of the Stable Diffusion model. * Type: str * Example: \"/path/to/model_checkpoint.ckpt\" config_path : File path to the Stable Diffusion model configuration YAML file. Type: str Example: \"/path/to/config.yaml\" Dataset Directories dataset_type : Specifies the dataset type for the training process. Choices: [\"unlearncanvas\", \"i2p\"] Example: \"unlearncanvas\" template : Type of template to use during training. Choices: [\"object\", \"style\", \"i2p\"] Example: \"style\" template_name : Name of the specific concept or style to be erased. Choices: [\"self-harm\", \"Abstractionism\"] Example: \"Abstractionism\" Output Configurations output_dir : Directory where the fine-tuned models and results will be saved. Type: str Example: \"outputs/erase_diff/finetuned_models\" Sampling and Image Configurations use_sample : Flag to indicate whether a sample dataset should be used for training. Type: bool Example: True guided_concepts : Concepts to guide the editing process. Type: str Example: \"Nature, Abstract\" technique : Specifies the editing technique. Choices: [\"replace\", \"tensor\"] Example: \"replace\" preserve_scale : Scale for preservation during the editing process. Type: float Example: 0.5 preserve_number : Number of items to preserve during editing. Type: int Example: 10 erase_scale : Scale for erasure during the editing process. Type: float Example: 0.8 lamb : Lambda parameter for controlling balance during editing. Type: float Example: 0.01 add_prompts : Flag to indicate whether additional prompts should be used. Type: bool Example: True Device Configuration devices : Specifies the CUDA devices to be used for training (comma-separated). Type: str (Comma-separated) Example: \"0,1\"","title":"Usage"},{"location":"unlearn/algorithms/uce/#unified-concept-editing-algorithm-for-machine-unlearning","text":"This repository provides an implementation of the unified concept editing algorithm for machine unlearning in Stable Diffusion models. The unified concept editing algorithm allows you to remove specific concepts or styles from a pre-trained model without retraining it from scratch.","title":"Unified Concept Editing Algorithm for Machine Unlearning"},{"location":"unlearn/algorithms/uce/#installation","text":"pip install unlearn_diff","title":"Installation"},{"location":"unlearn/algorithms/uce/#prerequisities","text":"Ensure conda is installed on your system. You can install Miniconda or Anaconda: Miniconda (recommended): https://docs.conda.io/en/latest/miniconda.html Anaconda : https://www.anaconda.com/products/distribution After installing conda , ensure it is available in your PATH by running. You may require to restart the terminal session: conda --version","title":"Prerequisities"},{"location":"unlearn/algorithms/uce/#create-environment","text":"create_env <algorithm_name> eg: create_env unified_concept_editing","title":"Create environment:"},{"location":"unlearn/algorithms/uce/#activate-environment","text":"conda activate <environment_name> eg: conda activate unified_concept_editing The has to be one of the folders in the mu/algorithms folder.","title":"Activate environment:"},{"location":"unlearn/algorithms/uce/#downloading-data-and-models","text":"After you install the package, you can use the following commands to download. Dataset : i2p : Sample : download_data sample i2p Full : download_data full i2p quick_canvas : Sample : download_data sample quick_canvas Full : download_data full quick_canvas Model : compvis : download_model compvis diffuser : download_model diffuser Verify the Downloaded Files After downloading, verify that the datasets have been correctly extracted: ls -lh ./data/i2p-dataset/sample/ ls -lh ./data/quick-canvas-dataset/sample/","title":"Downloading data and models."},{"location":"unlearn/algorithms/uce/#usage","text":"To train the Unified Concept Editing algorithm to unlearn a specific concept or style from the Stable Diffusion model, use the train.py script located in the scripts directory.","title":"Usage"},{"location":"unlearn/algorithms/uce/#run-train","text":"Create a file, eg, my_trainer.py and use examples and modify your configs to run the file. Example Code from mu.algorithms.unified_concept_editing.algorithm import ( UnifiedConceptEditingAlgorithm, ) from mu.algorithms.unified_concept_editing.configs import ( unified_concept_editing_train_mu, ) algorithm = UnifiedConceptEditingAlgorithm( unified_concept_editing_train_mu, ckpt_path=\"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/diffuser/style50/\", raw_dataset_dir=( \"/home/ubuntu/Projects/balaram/packaging/data/quick-canvas-dataset/sample\" ), output_dir=\"/opt/dlami/nvme/outputs\", ) algorithm.run() Running the Training Script in Offline Mode WANDB_MODE=offline python my_trainer.py How It Works * Default Values: The script first loads default values from the train config file as in configs section. Parameter Overrides: Any parameters passed directly to the algorithm, overrides these configs. Final Configuration: The script merges the configs and convert them into dictionary to proceed with the training.","title":"Run Train"},{"location":"unlearn/algorithms/uce/#directory-structure","text":"algorithm.py : Implementation of the ScissorHandsAlgorithm class. configs/ : Contains configuration files for training and generation. model.py : Implementation of the ScissorHandsModel class. scripts/train.py : Script to train the ScissorHands algorithm. trainer.py : Implementation of the ScissorHandsTrainer class. utils.py : Utility functions used in the project. data_handler.py : Implementation of DataHandler class","title":"Directory Structure"},{"location":"unlearn/algorithms/uce/#description-of-arguments-in-train_configyaml","text":"Training Parameters train_method : Specifies the method of training for concept erasure. Choices: [\"full\", \"partial\"] Example: \"full\" alpha : Guidance strength for the starting image during training. Type: float Example: 0.1 epochs : Number of epochs to train the model. Type: int Example: 10 lr : Learning rate used for the optimizer during training. Type: float Example: 5e-5 Model Configuration * ckpt_path : File path to the checkpoint of the Stable Diffusion model. * Type: str * Example: \"/path/to/model_checkpoint.ckpt\" config_path : File path to the Stable Diffusion model configuration YAML file. Type: str Example: \"/path/to/config.yaml\" Dataset Directories dataset_type : Specifies the dataset type for the training process. Choices: [\"unlearncanvas\", \"i2p\"] Example: \"unlearncanvas\" template : Type of template to use during training. Choices: [\"object\", \"style\", \"i2p\"] Example: \"style\" template_name : Name of the specific concept or style to be erased. Choices: [\"self-harm\", \"Abstractionism\"] Example: \"Abstractionism\" Output Configurations output_dir : Directory where the fine-tuned models and results will be saved. Type: str Example: \"outputs/erase_diff/finetuned_models\" Sampling and Image Configurations use_sample : Flag to indicate whether a sample dataset should be used for training. Type: bool Example: True guided_concepts : Concepts to guide the editing process. Type: str Example: \"Nature, Abstract\" technique : Specifies the editing technique. Choices: [\"replace\", \"tensor\"] Example: \"replace\" preserve_scale : Scale for preservation during the editing process. Type: float Example: 0.5 preserve_number : Number of items to preserve during editing. Type: int Example: 10 erase_scale : Scale for erasure during the editing process. Type: float Example: 0.8 lamb : Lambda parameter for controlling balance during editing. Type: float Example: 0.01 add_prompts : Flag to indicate whether additional prompts should be used. Type: bool Example: True Device Configuration devices : Specifies the CUDA devices to be used for training (comma-separated). Type: str (Comma-separated) Example: \"0,1\"","title":"Description of Arguments in train_config.yaml"},{"location":"unlearn/configs/concept_ablation/","text":"Train Config class ConceptAblationConfig(BaseConfig): def __init__(self, **kwargs): # Training parameters self.seed = 23 # Seed for random number generators self.scale_lr = True # Flag to scale the learning rate self.caption_target = \"Abstractionism Style\" # Caption target for the training self.regularization = True # Whether to apply regularization self.n_samples = 10 # Number of samples to generate self.train_size = 200 # Number of training samples self.base_lr = 2.0e-06 # Base learning rate # Model configuration self.config_path = current_dir / \"train_config.yaml\" self.model_config_path = ( current_dir / \"model_config.yaml\" ) # Path to model config self.ckpt_path = ( \"models/compvis/style50/compvis.ckpt\" # Path to model checkpoint ) # Dataset directories self.raw_dataset_dir = ( \"data/quick-canvas-dataset/sample\" # Raw dataset directory ) self.processed_dataset_dir = ( \"mu/algorithms/concept_ablation/data\" # Processed dataset directory ) self.dataset_type = \"unlearncanvas\" # Dataset type self.template = \"style\" # Template used for training self.template_name = \"Abstractionism\" # Template name # Learning rate for training self.lr = 5e-5 # Learning rate # Output directory for saving models self.output_dir = ( \"outputs/concept_ablation/finetuned_models\" # Output directory for results ) # Device configuration self.devices = \"0\" # CUDA devices (comma-separated) # Additional flags self.use_sample = True # Whether to use the sample dataset for training # Data configuration self.data = { \"target\": \"mu.algorithms.concept_ablation.data_handler.ConceptAblationDataHandler\", \"params\": { \"batch_size\": 1, # Batch size for training \"num_workers\": 1, # Number of workers for loading data \"wrap\": False, # Whether to wrap the dataset \"train\": { \"target\": \"mu.algorithms.concept_ablation.src.finetune_data.MaskBase\", \"params\": {\"size\": 512}, # Image size for the training set }, \"train2\": { \"target\": \"mu.algorithms.concept_ablation.src.finetune_data.MaskBase\", \"params\": {\"size\": 512}, # Image size for the second training set }, }, } # Lightning configuration self.lightning = { \"callbacks\": { \"image_logger\": { \"target\": \"mu.algorithms.concept_ablation.callbacks.ImageLogger\", \"params\": { \"batch_frequency\": 20000, # Frequency to log images \"save_freq\": 10000, # Frequency to save images \"max_images\": 8, # Maximum number of images to log \"increase_log_steps\": False, # Whether to increase the logging steps }, } }, \"modelcheckpoint\": { \"params\": { \"every_n_train_steps\": 10000 # Save the model every N training steps } }, \"trainer\": {\"max_steps\": 2000}, # Maximum number of training steps } self.prompts = \"mu/algorithms/concept_ablation/data/anchor_prompts/finetune_prompts/sd_prompt_Architectures_sample.txt\" Model Config # Training parameters seed : 23 scale_lr : True caption_target : \"Abstractionism Style\" regularization : True n_samples : 10 train_size : 200 base_lr : 2.0e-06 # Model configuration model_config_path: \"mu/algorithms/concept_ablation/configs/model_config.yaml\" # Config path for Stable Diffusion ckpt_path: \"models/compvis/style50/compvis.ckpt\" # Checkpoint path for Stable Diffusion # Dataset directories raw_dataset_dir: \"data/quick-canvas-dataset/sample\" processed_dataset_dir: \"mu/algorithms/concept_ablation/data\" dataset_type : \"unlearncanvas\" template : \"style\" template_name : \"Abstractionism\" lr: 5e-5 # Output configurations output_dir: \"outputs/concept_ablation/finetuned_models\" # Output directory to save results # Sampling and image configurations # Device configuration devices: \"0,\" # CUDA devices to train on (comma-separated) # Additional flags use_sample: True # Use the sample dataset for training data: target: mu.algorithms.concept_ablation.data_handler.ConceptAblationDataHandler params: batch_size: 4 num_workers: 4 wrap: false train: target: mu.algorithms.concept_ablation.src.finetune_data.MaskBase params: size: 512 train2: target: mu.algorithms.concept_ablation.src.finetune_data.MaskBase params: size: 512 lightning: callbacks: image_logger: target: mu.algorithms.concept_ablation.callbacks.ImageLogger params: batch_frequency: 20000 save_freq: 10000 max_images: 8 increase_log_steps: False modelcheckpoint: params: every_n_train_steps: 10000 trainer: max_steps: 2000 Evaluation config #mu/algorithms/concept_ablation/configs/evaluation_config.py import os from pathlib import Path from mu.core.base_config import BaseConfig current_dir = Path(__file__).parent class ConceptAblationEvaluationConfig(BaseConfig): def __init__(self, **kwargs): self.model_config_path = current_dir/\"model_config.yaml\" # path to model config self.ckpt_path = \"outputs/concept_ablation/finetuned_models/checkpoints/last-v2.ckpt\" # path to finetuned model checkpoint self.classifier_ckpt_path = \"models/classifier_ckpt_path/style50_cls.pth\" # path to classifier checkpoint self.forget_theme = \"Bricks\" # theme to forget self.cfg_text = 9.0 # classifier-free guidance scale self.devices = \"0\" # GPU device ID self.seed = 188 # random seed self.task = \"class\" # task type self.ddim_steps = 100 # number of DDIM steps self.image_height = 512 # height of the image self.image_width = 512 # width of the image self.ddim_eta = 0.0 # DDIM eta parameter self.sampler_output_dir = \"outputs/eval_results/mu_results/ca/\" # directory to save sampler outputs self.seed_list = [\"188\"] # list of seeds for evaluation self.classification_model = \"vit_large_patch16_224\" # classification model for evaluation self.eval_output_dir = \"outputs/eval_results/mu_results/ca/\" # directory to save evaluation results self.reference_dir = \"data/quick-canvas-dataset/sample/\" # path to the original dataset self.multiprocessing = False # whether to use multiprocessing # Override defaults with any provided kwargs for key, value in kwargs.items(): setattr(self, key, value) def validate_config(self): \"\"\" Perform basic validation on the config parameters. \"\"\" if not os.path.exists(self.model_config_path): raise FileNotFoundError(f\"Model config file {self.model_config_path} does not exist.\") if not os.path.exists(self.ckpt_path): raise FileNotFoundError(f\"Checkpoint file {self.ckpt_path} does not exist.\") if not os.path.exists(self.classifier_ckpt_path): raise FileNotFoundError(f\"Classifier checkpoint file {self.classifier_ckpt_path} does not exist.\") if not os.path.exists(self.reference_dir): raise FileNotFoundError(f\"Reference directory {self.reference_dir} does not exist.\") if not os.path.exists(self.eval_output_dir): os.makedirs(self.eval_output_dir) if not os.path.exists(self.sampler_output_dir): os.makedirs(self.sampler_output_dir) if self.cfg_text <= 0: raise ValueError(\"Classifier-free guidance scale (cfg_text) should be positive.\") if self.ddim_steps <= 0: raise ValueError(\"DDIM steps should be a positive integer.\") if self.image_height <= 0 or self.image_width <= 0: raise ValueError(\"Image height and width should be positive.\") if self.task not in [\"class\", \"other_task\"]: # Add other valid tasks if needed raise ValueError(\"Invalid task type.\") # Example usage concept_ablation_evaluation_config = ConceptAblationEvaluationConfig()","title":"Configs"},{"location":"unlearn/configs/concept_ablation/#train-config","text":"class ConceptAblationConfig(BaseConfig): def __init__(self, **kwargs): # Training parameters self.seed = 23 # Seed for random number generators self.scale_lr = True # Flag to scale the learning rate self.caption_target = \"Abstractionism Style\" # Caption target for the training self.regularization = True # Whether to apply regularization self.n_samples = 10 # Number of samples to generate self.train_size = 200 # Number of training samples self.base_lr = 2.0e-06 # Base learning rate # Model configuration self.config_path = current_dir / \"train_config.yaml\" self.model_config_path = ( current_dir / \"model_config.yaml\" ) # Path to model config self.ckpt_path = ( \"models/compvis/style50/compvis.ckpt\" # Path to model checkpoint ) # Dataset directories self.raw_dataset_dir = ( \"data/quick-canvas-dataset/sample\" # Raw dataset directory ) self.processed_dataset_dir = ( \"mu/algorithms/concept_ablation/data\" # Processed dataset directory ) self.dataset_type = \"unlearncanvas\" # Dataset type self.template = \"style\" # Template used for training self.template_name = \"Abstractionism\" # Template name # Learning rate for training self.lr = 5e-5 # Learning rate # Output directory for saving models self.output_dir = ( \"outputs/concept_ablation/finetuned_models\" # Output directory for results ) # Device configuration self.devices = \"0\" # CUDA devices (comma-separated) # Additional flags self.use_sample = True # Whether to use the sample dataset for training # Data configuration self.data = { \"target\": \"mu.algorithms.concept_ablation.data_handler.ConceptAblationDataHandler\", \"params\": { \"batch_size\": 1, # Batch size for training \"num_workers\": 1, # Number of workers for loading data \"wrap\": False, # Whether to wrap the dataset \"train\": { \"target\": \"mu.algorithms.concept_ablation.src.finetune_data.MaskBase\", \"params\": {\"size\": 512}, # Image size for the training set }, \"train2\": { \"target\": \"mu.algorithms.concept_ablation.src.finetune_data.MaskBase\", \"params\": {\"size\": 512}, # Image size for the second training set }, }, } # Lightning configuration self.lightning = { \"callbacks\": { \"image_logger\": { \"target\": \"mu.algorithms.concept_ablation.callbacks.ImageLogger\", \"params\": { \"batch_frequency\": 20000, # Frequency to log images \"save_freq\": 10000, # Frequency to save images \"max_images\": 8, # Maximum number of images to log \"increase_log_steps\": False, # Whether to increase the logging steps }, } }, \"modelcheckpoint\": { \"params\": { \"every_n_train_steps\": 10000 # Save the model every N training steps } }, \"trainer\": {\"max_steps\": 2000}, # Maximum number of training steps } self.prompts = \"mu/algorithms/concept_ablation/data/anchor_prompts/finetune_prompts/sd_prompt_Architectures_sample.txt\"","title":"Train Config"},{"location":"unlearn/configs/concept_ablation/#model-config","text":"# Training parameters seed : 23 scale_lr : True caption_target : \"Abstractionism Style\" regularization : True n_samples : 10 train_size : 200 base_lr : 2.0e-06 # Model configuration model_config_path: \"mu/algorithms/concept_ablation/configs/model_config.yaml\" # Config path for Stable Diffusion ckpt_path: \"models/compvis/style50/compvis.ckpt\" # Checkpoint path for Stable Diffusion # Dataset directories raw_dataset_dir: \"data/quick-canvas-dataset/sample\" processed_dataset_dir: \"mu/algorithms/concept_ablation/data\" dataset_type : \"unlearncanvas\" template : \"style\" template_name : \"Abstractionism\" lr: 5e-5 # Output configurations output_dir: \"outputs/concept_ablation/finetuned_models\" # Output directory to save results # Sampling and image configurations # Device configuration devices: \"0,\" # CUDA devices to train on (comma-separated) # Additional flags use_sample: True # Use the sample dataset for training data: target: mu.algorithms.concept_ablation.data_handler.ConceptAblationDataHandler params: batch_size: 4 num_workers: 4 wrap: false train: target: mu.algorithms.concept_ablation.src.finetune_data.MaskBase params: size: 512 train2: target: mu.algorithms.concept_ablation.src.finetune_data.MaskBase params: size: 512 lightning: callbacks: image_logger: target: mu.algorithms.concept_ablation.callbacks.ImageLogger params: batch_frequency: 20000 save_freq: 10000 max_images: 8 increase_log_steps: False modelcheckpoint: params: every_n_train_steps: 10000 trainer: max_steps: 2000","title":"Model Config"},{"location":"unlearn/configs/concept_ablation/#evaluation-config","text":"#mu/algorithms/concept_ablation/configs/evaluation_config.py import os from pathlib import Path from mu.core.base_config import BaseConfig current_dir = Path(__file__).parent class ConceptAblationEvaluationConfig(BaseConfig): def __init__(self, **kwargs): self.model_config_path = current_dir/\"model_config.yaml\" # path to model config self.ckpt_path = \"outputs/concept_ablation/finetuned_models/checkpoints/last-v2.ckpt\" # path to finetuned model checkpoint self.classifier_ckpt_path = \"models/classifier_ckpt_path/style50_cls.pth\" # path to classifier checkpoint self.forget_theme = \"Bricks\" # theme to forget self.cfg_text = 9.0 # classifier-free guidance scale self.devices = \"0\" # GPU device ID self.seed = 188 # random seed self.task = \"class\" # task type self.ddim_steps = 100 # number of DDIM steps self.image_height = 512 # height of the image self.image_width = 512 # width of the image self.ddim_eta = 0.0 # DDIM eta parameter self.sampler_output_dir = \"outputs/eval_results/mu_results/ca/\" # directory to save sampler outputs self.seed_list = [\"188\"] # list of seeds for evaluation self.classification_model = \"vit_large_patch16_224\" # classification model for evaluation self.eval_output_dir = \"outputs/eval_results/mu_results/ca/\" # directory to save evaluation results self.reference_dir = \"data/quick-canvas-dataset/sample/\" # path to the original dataset self.multiprocessing = False # whether to use multiprocessing # Override defaults with any provided kwargs for key, value in kwargs.items(): setattr(self, key, value) def validate_config(self): \"\"\" Perform basic validation on the config parameters. \"\"\" if not os.path.exists(self.model_config_path): raise FileNotFoundError(f\"Model config file {self.model_config_path} does not exist.\") if not os.path.exists(self.ckpt_path): raise FileNotFoundError(f\"Checkpoint file {self.ckpt_path} does not exist.\") if not os.path.exists(self.classifier_ckpt_path): raise FileNotFoundError(f\"Classifier checkpoint file {self.classifier_ckpt_path} does not exist.\") if not os.path.exists(self.reference_dir): raise FileNotFoundError(f\"Reference directory {self.reference_dir} does not exist.\") if not os.path.exists(self.eval_output_dir): os.makedirs(self.eval_output_dir) if not os.path.exists(self.sampler_output_dir): os.makedirs(self.sampler_output_dir) if self.cfg_text <= 0: raise ValueError(\"Classifier-free guidance scale (cfg_text) should be positive.\") if self.ddim_steps <= 0: raise ValueError(\"DDIM steps should be a positive integer.\") if self.image_height <= 0 or self.image_width <= 0: raise ValueError(\"Image height and width should be positive.\") if self.task not in [\"class\", \"other_task\"]: # Add other valid tasks if needed raise ValueError(\"Invalid task type.\") # Example usage concept_ablation_evaluation_config = ConceptAblationEvaluationConfig()","title":"Evaluation config"},{"location":"unlearn/configs/erase_diff/","text":"Sample Train Config class EraseDiffConfig(BaseConfig): def __init__(self, **kwargs): self.train_method = \"xattn\" self.alpha = 0.1 self.epochs = 1 self.K_steps = 2 self.lr = 5e-5 self.model_config_path = current_dir / \"model_config.yaml\" self.ckpt_path = \"models/compvis/style50/compvis.ckpt\" self.raw_dataset_dir = \"data/quick-canvas-dataset/sample\" self.processed_dataset_dir = \"mu/algorithms/erase_diff/data\" self.dataset_type = \"unlearncanvas\" self.template = \"style\" self.template_name = \"Abstractionism\" self.output_dir = \"outputs/erase_diff/finetuned_models\" self.separator = None self.image_size = 512 self.interpolation = \"bicubic\" self.ddim_steps = 50 self.ddim_eta = 0.0 self.devices = \"0\" self.use_sample = True self.num_workers = 4 self.pin_memory = True Sample Model Config model: base_learning_rate: 1.0e-04 target: stable_diffusion.ldm.models.diffusion.ddpm.LatentDiffusion params: linear_start: 0.00085 linear_end: 0.0120 num_timesteps_cond: 1 log_every_t: 200 timesteps: 1000 first_stage_key: \"edited\" cond_stage_key: \"edit\" image_size: 64 channels: 4 cond_stage_trainable: false # Note: different from the one we trained before conditioning_key: crossattn monitor: val/loss_simple_ema scale_factor: 0.18215 use_ema: False scheduler_config: # 10000 warmup steps target: stable_diffusion.ldm.lr_scheduler.LambdaLinearScheduler params: warm_up_steps: [ 10000 ] cycle_lengths: [ 10000000000000 ] # incredibly large number to prevent corner cases f_start: [ 1.e-6 ] f_max: [ 1. ] f_min: [ 1. ] unet_config: target: stable_diffusion.ldm.modules.diffusionmodules.openaimodel.UNetModel params: image_size: 32 # unused in_channels: 4 out_channels: 4 model_channels: 320 attention_resolutions: [ 4, 2, 1 ] num_res_blocks: 2 channel_mult: [ 1, 2, 4, 4 ] num_heads: 8 use_spatial_transformer: True transformer_depth: 1 context_dim: 768 use_checkpoint: True legacy: False first_stage_config: target: stable_diffusion.ldm.models.autoencoder.AutoencoderKL params: embed_dim: 4 monitor: val/rec_loss ddconfig: double_z: true z_channels: 4 resolution: 256 in_channels: 3 out_ch: 3 ch: 128 ch_mult: - 1 - 2 - 4 - 4 num_res_blocks: 2 attn_resolutions: [] dropout: 0.0 lossconfig: target: torch.nn.Identity cond_stage_config: target: stable_diffusion.ldm.modules.encoders.modules.FrozenCLIPEmbedder Evaluation config # mu/algorithms/erase_diff/configs/evaluation_config.py import os from pathlib import Path from mu.core.base_config import BaseConfig current_dir = Path(__file__).parent class ErasediffEvaluationConfig(BaseConfig): def __init__(self, **kwargs): self.model_config_path = current_dir / \"model_config.yaml\" # path to model config self.ckpt_path = \"outputs/erase_diff/finetuned_models/erase_diff_Abstractionism_model.pth\" # path to finetuned model checkpoint self.classifier_ckpt_path = \"models/classifier_ckpt_path/style50_cls.pth\" # path to classifier checkpoint self.forget_theme = \"Bricks\" # theme to forget self.cfg_text = 9.0 # classifier-free guidance scale self.devices = \"0\" # GPU device ID self.seed = 188 # random seed self.task = \"class\" # task type self.ddim_steps = 100 # number of DDIM steps self.image_height = 512 # height of the image self.image_width = 512 # width of the image self.ddim_eta = 0.0 # DDIM eta parameter self.sampler_output_dir = \"outputs/eval_results/mu_results/erase_diff/\" # directory to save sampler outputs self.seed_list = [\"188\"] # list of seeds for evaluation self.classification_model = \"vit_large_patch16_224\" # classification model for evaluation self.eval_output_dir = \"outputs/eval_results/mu_results/erase_diff/\" # directory to save evaluation results self.reference_dir = \"data/quick-canvas-dataset/sample/\" # path to the original dataset self.multiprocessing = False # whether to use multiprocessing # Override defaults with any provided kwargs for key, value in kwargs.items(): setattr(self, key, value) def validate_config(self): \"\"\" Perform basic validation on the config parameters. \"\"\" if not os.path.exists(self.model_config_path): raise FileNotFoundError(f\"Model config file {self.model_config_path} does not exist.\") if not os.path.exists(self.ckpt_path): raise FileNotFoundError(f\"Checkpoint file {self.ckpt_path} does not exist.\") if not os.path.exists(self.classifier_ckpt_path): raise FileNotFoundError(f\"Classifier checkpoint file {self.classifier_ckpt_path} does not exist.\") if not os.path.exists(self.reference_dir): raise FileNotFoundError(f\"Reference directory {self.reference_dir} does not exist.\") if not os.path.exists(self.eval_output_dir): os.makedirs(self.eval_output_dir) if not os.path.exists(self.sampler_output_dir): os.makedirs(self.sampler_output_dir) if self.cfg_text <= 0: raise ValueError(\"Classifier-free guidance scale (cfg_text) should be positive.\") if self.ddim_steps <= 0: raise ValueError(\"DDIM steps should be a positive integer.\") if self.image_height <= 0 or self.image_width <= 0: raise ValueError(\"Image height and width should be positive.\") if self.task not in [\"class\", \"other_task\"]: # Add other valid tasks if needed raise ValueError(\"Invalid task type.\") # Example usage erase_diff_evaluation_config = ErasediffEvaluationConfig()","title":"Configs"},{"location":"unlearn/configs/erase_diff/#sample-train-config","text":"class EraseDiffConfig(BaseConfig): def __init__(self, **kwargs): self.train_method = \"xattn\" self.alpha = 0.1 self.epochs = 1 self.K_steps = 2 self.lr = 5e-5 self.model_config_path = current_dir / \"model_config.yaml\" self.ckpt_path = \"models/compvis/style50/compvis.ckpt\" self.raw_dataset_dir = \"data/quick-canvas-dataset/sample\" self.processed_dataset_dir = \"mu/algorithms/erase_diff/data\" self.dataset_type = \"unlearncanvas\" self.template = \"style\" self.template_name = \"Abstractionism\" self.output_dir = \"outputs/erase_diff/finetuned_models\" self.separator = None self.image_size = 512 self.interpolation = \"bicubic\" self.ddim_steps = 50 self.ddim_eta = 0.0 self.devices = \"0\" self.use_sample = True self.num_workers = 4 self.pin_memory = True","title":"Sample Train Config"},{"location":"unlearn/configs/erase_diff/#sample-model-config","text":"model: base_learning_rate: 1.0e-04 target: stable_diffusion.ldm.models.diffusion.ddpm.LatentDiffusion params: linear_start: 0.00085 linear_end: 0.0120 num_timesteps_cond: 1 log_every_t: 200 timesteps: 1000 first_stage_key: \"edited\" cond_stage_key: \"edit\" image_size: 64 channels: 4 cond_stage_trainable: false # Note: different from the one we trained before conditioning_key: crossattn monitor: val/loss_simple_ema scale_factor: 0.18215 use_ema: False scheduler_config: # 10000 warmup steps target: stable_diffusion.ldm.lr_scheduler.LambdaLinearScheduler params: warm_up_steps: [ 10000 ] cycle_lengths: [ 10000000000000 ] # incredibly large number to prevent corner cases f_start: [ 1.e-6 ] f_max: [ 1. ] f_min: [ 1. ] unet_config: target: stable_diffusion.ldm.modules.diffusionmodules.openaimodel.UNetModel params: image_size: 32 # unused in_channels: 4 out_channels: 4 model_channels: 320 attention_resolutions: [ 4, 2, 1 ] num_res_blocks: 2 channel_mult: [ 1, 2, 4, 4 ] num_heads: 8 use_spatial_transformer: True transformer_depth: 1 context_dim: 768 use_checkpoint: True legacy: False first_stage_config: target: stable_diffusion.ldm.models.autoencoder.AutoencoderKL params: embed_dim: 4 monitor: val/rec_loss ddconfig: double_z: true z_channels: 4 resolution: 256 in_channels: 3 out_ch: 3 ch: 128 ch_mult: - 1 - 2 - 4 - 4 num_res_blocks: 2 attn_resolutions: [] dropout: 0.0 lossconfig: target: torch.nn.Identity cond_stage_config: target: stable_diffusion.ldm.modules.encoders.modules.FrozenCLIPEmbedder","title":"Sample Model Config"},{"location":"unlearn/configs/erase_diff/#evaluation-config","text":"# mu/algorithms/erase_diff/configs/evaluation_config.py import os from pathlib import Path from mu.core.base_config import BaseConfig current_dir = Path(__file__).parent class ErasediffEvaluationConfig(BaseConfig): def __init__(self, **kwargs): self.model_config_path = current_dir / \"model_config.yaml\" # path to model config self.ckpt_path = \"outputs/erase_diff/finetuned_models/erase_diff_Abstractionism_model.pth\" # path to finetuned model checkpoint self.classifier_ckpt_path = \"models/classifier_ckpt_path/style50_cls.pth\" # path to classifier checkpoint self.forget_theme = \"Bricks\" # theme to forget self.cfg_text = 9.0 # classifier-free guidance scale self.devices = \"0\" # GPU device ID self.seed = 188 # random seed self.task = \"class\" # task type self.ddim_steps = 100 # number of DDIM steps self.image_height = 512 # height of the image self.image_width = 512 # width of the image self.ddim_eta = 0.0 # DDIM eta parameter self.sampler_output_dir = \"outputs/eval_results/mu_results/erase_diff/\" # directory to save sampler outputs self.seed_list = [\"188\"] # list of seeds for evaluation self.classification_model = \"vit_large_patch16_224\" # classification model for evaluation self.eval_output_dir = \"outputs/eval_results/mu_results/erase_diff/\" # directory to save evaluation results self.reference_dir = \"data/quick-canvas-dataset/sample/\" # path to the original dataset self.multiprocessing = False # whether to use multiprocessing # Override defaults with any provided kwargs for key, value in kwargs.items(): setattr(self, key, value) def validate_config(self): \"\"\" Perform basic validation on the config parameters. \"\"\" if not os.path.exists(self.model_config_path): raise FileNotFoundError(f\"Model config file {self.model_config_path} does not exist.\") if not os.path.exists(self.ckpt_path): raise FileNotFoundError(f\"Checkpoint file {self.ckpt_path} does not exist.\") if not os.path.exists(self.classifier_ckpt_path): raise FileNotFoundError(f\"Classifier checkpoint file {self.classifier_ckpt_path} does not exist.\") if not os.path.exists(self.reference_dir): raise FileNotFoundError(f\"Reference directory {self.reference_dir} does not exist.\") if not os.path.exists(self.eval_output_dir): os.makedirs(self.eval_output_dir) if not os.path.exists(self.sampler_output_dir): os.makedirs(self.sampler_output_dir) if self.cfg_text <= 0: raise ValueError(\"Classifier-free guidance scale (cfg_text) should be positive.\") if self.ddim_steps <= 0: raise ValueError(\"DDIM steps should be a positive integer.\") if self.image_height <= 0 or self.image_width <= 0: raise ValueError(\"Image height and width should be positive.\") if self.task not in [\"class\", \"other_task\"]: # Add other valid tasks if needed raise ValueError(\"Invalid task type.\") # Example usage erase_diff_evaluation_config = ErasediffEvaluationConfig()","title":"Evaluation config"},{"location":"unlearn/configs/esd/","text":"Sample Train Config class ESDConfig(BaseConfig): def __init__(self, **kwargs): # Training parameters self.train_method = \"xattn\" # Choices: [\"noxattn\", \"selfattn\", \"xattn\", \"full\", \"notime\", \"xlayer\", \"selflayer\"] self.start_guidance = ( 0.1 # Optional: guidance of start image (previously alpha) ) self.negative_guidance = 0.0 # Optional: guidance of negative training self.iterations = 1 # Optional: iterations used to train (previously epochs) self.lr = 1e-5 # Optional: learning rate self.image_size = 512 # Optional: image size used to train self.ddim_steps = 50 # Optional: DDIM steps of inference # Model configuration self.model_config_path = current_dir / \"model_config.yaml\" self.ckpt_path = \"models/compvis/style50/compvis.ckpt\" # Checkpoint path for Stable Diffusion # Dataset directories self.raw_dataset_dir = \"data/quick-canvas-dataset/sample\" self.processed_dataset_dir = \"mu/algorithms/esd/data\" self.dataset_type = \"unlearncanvas\" # Choices: ['unlearncanvas', 'i2p'] self.template = \"style\" # Choices: ['object', 'style', 'i2p'] self.template_name = ( \"Abstractionism\" # Choices: ['self-harm', 'Abstractionism'] ) # Output configurations self.output_dir = \"outputs/esd/finetuned_models\" self.separator = None # Device configuration self.devices = \"0,0\" self.use_sample = True # For backward compatibility self.interpolation = \"bicubic\" # Interpolation method self.ddim_eta = 0.0 # Eta for DDIM self.num_workers = 4 # Number of workers for data loading self.pin_memory = True # Pin memory for faster transfer to GPU Sample Model Config model: base_learning_rate: 1.0e-04 target: stable_diffusion.ldm.models.diffusion.ddpm.LatentDiffusion params: linear_start: 0.00085 linear_end: 0.0120 num_timesteps_cond: 1 log_every_t: 200 timesteps: 1000 first_stage_key: \"jpg\" cond_stage_key: \"txt\" image_size: 32 channels: 4 cond_stage_trainable: false # Note: different from the one we trained before conditioning_key: crossattn monitor: val/loss_simple_ema scale_factor: 0.18215 use_ema: False scheduler_config: # 10000 warmup steps target: stable_diffusion.ldm.lr_scheduler.LambdaLinearScheduler params: warm_up_steps: [ 10000 ] cycle_lengths: [ 10000000000000 ] # incredibly large number to prevent corner cases f_start: [ 1.e-6 ] f_max: [ 1. ] f_min: [ 1. ] unet_config: target: stable_diffusion.ldm.modules.diffusionmodules.openaimodel.UNetModel params: image_size: 32 # unused in_channels: 4 out_channels: 4 model_channels: 320 attention_resolutions: [ 4, 2, 1 ] num_res_blocks: 2 channel_mult: [ 1, 2, 4, 4 ] num_heads: 8 use_spatial_transformer: True transformer_depth: 1 context_dim: 768 use_checkpoint: True legacy: False first_stage_config: target: stable_diffusion.ldm.models.autoencoder.AutoencoderKL params: embed_dim: 4 monitor: val/rec_loss ddconfig: double_z: true z_channels: 4 resolution: 256 in_channels: 3 out_ch: 3 ch: 128 ch_mult: - 1 - 2 - 4 - 4 num_res_blocks: 2 attn_resolutions: [] dropout: 0.0 lossconfig: target: torch.nn.Identity cond_stage_config: target: stable_diffusion.ldm.modules.encoders.modules.FrozenCLIPEmbedder Evaluation config # mu/algorithms/esd/configs/evaluation_config.py import os from pathlib import Path from mu.core.base_config import BaseConfig current_dir = Path(__file__).parent class ESDEvaluationConfig(BaseConfig): def __init__(self, **kwargs): self.model_config_path = current_dir/\"model_config.yaml\" # path to model config self.ckpt_path = \"outputs/esd/finetuned_models/esd_Abstractionism_model.pth\" # path to finetuned model checkpoint self.classifier_ckpt_path = \"models/classifier_ckpt_path/style50_cls.pth\" # path to classifier checkpoint self.forget_theme = \"Bricks\" # theme to forget self.cfg_text = 9.0 # classifier-free guidance scale self.devices = \"0\" # GPU device ID self.seed = 188 # random seed self.task = \"class\" # task type self.ddim_steps = 100 # number of DDIM steps self.image_height = 512 # height of the image self.image_width = 512 # width of the image self.ddim_eta = 0.0 # DDIM eta parameter self.sampler_output_dir = \"outputs/eval_results/mu_results/esd/\" # directory to save sampler outputs self.seed_list = [\"188\"] # list of seeds for evaluation self.classification_model = \"vit_large_patch16_224\" # classification model for evaluation self.eval_output_dir = \"outputs/eval_results/mu_results/esd/\" # directory to save evaluation results self.reference_dir = \"data/quick-canvas-dataset/sample/\" # path to the original dataset self.multiprocessing = False # whether to use multiprocessing # Override defaults with any provided kwargs for key, value in kwargs.items(): setattr(self, key, value) def validate_config(self): \"\"\" Perform basic validation on the config parameters. \"\"\" if not os.path.exists(self.model_config_path): raise FileNotFoundError(f\"Model config file {self.model_config_path} does not exist.\") if not os.path.exists(self.ckpt_path): raise FileNotFoundError(f\"Checkpoint file {self.ckpt_path} does not exist.\") if not os.path.exists(self.classifier_ckpt_path): raise FileNotFoundError(f\"Classifier checkpoint file {self.classifier_ckpt_path} does not exist.\") if not os.path.exists(self.reference_dir): raise FileNotFoundError(f\"Reference directory {self.reference_dir} does not exist.\") if not os.path.exists(self.eval_output_dir): os.makedirs(self.eval_output_dir) if not os.path.exists(self.sampler_output_dir): os.makedirs(self.sampler_output_dir) if self.cfg_text <= 0: raise ValueError(\"Classifier-free guidance scale (cfg_text) should be positive.\") if self.ddim_steps <= 0: raise ValueError(\"DDIM steps should be a positive integer.\") if self.image_height <= 0 or self.image_width <= 0: raise ValueError(\"Image height and width should be positive.\") if self.task not in [\"class\", \"other_task\"]: # Add other valid tasks if needed raise ValueError(\"Invalid task type.\") # Example usage esd_evaluation_config = ESDEvaluationConfig()","title":"Configs"},{"location":"unlearn/configs/esd/#sample-train-config","text":"class ESDConfig(BaseConfig): def __init__(self, **kwargs): # Training parameters self.train_method = \"xattn\" # Choices: [\"noxattn\", \"selfattn\", \"xattn\", \"full\", \"notime\", \"xlayer\", \"selflayer\"] self.start_guidance = ( 0.1 # Optional: guidance of start image (previously alpha) ) self.negative_guidance = 0.0 # Optional: guidance of negative training self.iterations = 1 # Optional: iterations used to train (previously epochs) self.lr = 1e-5 # Optional: learning rate self.image_size = 512 # Optional: image size used to train self.ddim_steps = 50 # Optional: DDIM steps of inference # Model configuration self.model_config_path = current_dir / \"model_config.yaml\" self.ckpt_path = \"models/compvis/style50/compvis.ckpt\" # Checkpoint path for Stable Diffusion # Dataset directories self.raw_dataset_dir = \"data/quick-canvas-dataset/sample\" self.processed_dataset_dir = \"mu/algorithms/esd/data\" self.dataset_type = \"unlearncanvas\" # Choices: ['unlearncanvas', 'i2p'] self.template = \"style\" # Choices: ['object', 'style', 'i2p'] self.template_name = ( \"Abstractionism\" # Choices: ['self-harm', 'Abstractionism'] ) # Output configurations self.output_dir = \"outputs/esd/finetuned_models\" self.separator = None # Device configuration self.devices = \"0,0\" self.use_sample = True # For backward compatibility self.interpolation = \"bicubic\" # Interpolation method self.ddim_eta = 0.0 # Eta for DDIM self.num_workers = 4 # Number of workers for data loading self.pin_memory = True # Pin memory for faster transfer to GPU","title":"Sample Train Config"},{"location":"unlearn/configs/esd/#sample-model-config","text":"model: base_learning_rate: 1.0e-04 target: stable_diffusion.ldm.models.diffusion.ddpm.LatentDiffusion params: linear_start: 0.00085 linear_end: 0.0120 num_timesteps_cond: 1 log_every_t: 200 timesteps: 1000 first_stage_key: \"jpg\" cond_stage_key: \"txt\" image_size: 32 channels: 4 cond_stage_trainable: false # Note: different from the one we trained before conditioning_key: crossattn monitor: val/loss_simple_ema scale_factor: 0.18215 use_ema: False scheduler_config: # 10000 warmup steps target: stable_diffusion.ldm.lr_scheduler.LambdaLinearScheduler params: warm_up_steps: [ 10000 ] cycle_lengths: [ 10000000000000 ] # incredibly large number to prevent corner cases f_start: [ 1.e-6 ] f_max: [ 1. ] f_min: [ 1. ] unet_config: target: stable_diffusion.ldm.modules.diffusionmodules.openaimodel.UNetModel params: image_size: 32 # unused in_channels: 4 out_channels: 4 model_channels: 320 attention_resolutions: [ 4, 2, 1 ] num_res_blocks: 2 channel_mult: [ 1, 2, 4, 4 ] num_heads: 8 use_spatial_transformer: True transformer_depth: 1 context_dim: 768 use_checkpoint: True legacy: False first_stage_config: target: stable_diffusion.ldm.models.autoencoder.AutoencoderKL params: embed_dim: 4 monitor: val/rec_loss ddconfig: double_z: true z_channels: 4 resolution: 256 in_channels: 3 out_ch: 3 ch: 128 ch_mult: - 1 - 2 - 4 - 4 num_res_blocks: 2 attn_resolutions: [] dropout: 0.0 lossconfig: target: torch.nn.Identity cond_stage_config: target: stable_diffusion.ldm.modules.encoders.modules.FrozenCLIPEmbedder","title":"Sample Model Config"},{"location":"unlearn/configs/esd/#evaluation-config","text":"# mu/algorithms/esd/configs/evaluation_config.py import os from pathlib import Path from mu.core.base_config import BaseConfig current_dir = Path(__file__).parent class ESDEvaluationConfig(BaseConfig): def __init__(self, **kwargs): self.model_config_path = current_dir/\"model_config.yaml\" # path to model config self.ckpt_path = \"outputs/esd/finetuned_models/esd_Abstractionism_model.pth\" # path to finetuned model checkpoint self.classifier_ckpt_path = \"models/classifier_ckpt_path/style50_cls.pth\" # path to classifier checkpoint self.forget_theme = \"Bricks\" # theme to forget self.cfg_text = 9.0 # classifier-free guidance scale self.devices = \"0\" # GPU device ID self.seed = 188 # random seed self.task = \"class\" # task type self.ddim_steps = 100 # number of DDIM steps self.image_height = 512 # height of the image self.image_width = 512 # width of the image self.ddim_eta = 0.0 # DDIM eta parameter self.sampler_output_dir = \"outputs/eval_results/mu_results/esd/\" # directory to save sampler outputs self.seed_list = [\"188\"] # list of seeds for evaluation self.classification_model = \"vit_large_patch16_224\" # classification model for evaluation self.eval_output_dir = \"outputs/eval_results/mu_results/esd/\" # directory to save evaluation results self.reference_dir = \"data/quick-canvas-dataset/sample/\" # path to the original dataset self.multiprocessing = False # whether to use multiprocessing # Override defaults with any provided kwargs for key, value in kwargs.items(): setattr(self, key, value) def validate_config(self): \"\"\" Perform basic validation on the config parameters. \"\"\" if not os.path.exists(self.model_config_path): raise FileNotFoundError(f\"Model config file {self.model_config_path} does not exist.\") if not os.path.exists(self.ckpt_path): raise FileNotFoundError(f\"Checkpoint file {self.ckpt_path} does not exist.\") if not os.path.exists(self.classifier_ckpt_path): raise FileNotFoundError(f\"Classifier checkpoint file {self.classifier_ckpt_path} does not exist.\") if not os.path.exists(self.reference_dir): raise FileNotFoundError(f\"Reference directory {self.reference_dir} does not exist.\") if not os.path.exists(self.eval_output_dir): os.makedirs(self.eval_output_dir) if not os.path.exists(self.sampler_output_dir): os.makedirs(self.sampler_output_dir) if self.cfg_text <= 0: raise ValueError(\"Classifier-free guidance scale (cfg_text) should be positive.\") if self.ddim_steps <= 0: raise ValueError(\"DDIM steps should be a positive integer.\") if self.image_height <= 0 or self.image_width <= 0: raise ValueError(\"Image height and width should be positive.\") if self.task not in [\"class\", \"other_task\"]: # Add other valid tasks if needed raise ValueError(\"Invalid task type.\") # Example usage esd_evaluation_config = ESDEvaluationConfig()","title":"Evaluation config"},{"location":"unlearn/configs/forget_me_not/","text":"Train Ti Config class ForgetMeNotTiConfig(BaseConfig): \"\"\" Configuration class for the Forget-Me-Not textual inversion training. Mirrors the fields from the second YAML snippet. \"\"\" def __init__(self, **kwargs): # Model checkpoint path self.ckpt_path = \"models/diffuser/style50\" # Dataset directories self.raw_dataset_dir = \"data/quick-canvas-dataset/sample\" self.processed_dataset_dir = \"mu/algorithms/forget_me_not/data\" self.dataset_type = \"unlearncanvas\" self.template = \"style\" self.template_name = \"Abstractionism\" self.use_sample = True # Use the sample dataset for training # Training configuration self.initializer_tokens = self.template_name self.steps = 10 self.lr = 1e-4 self.weight_decay_ti = 0.1 self.seed = 42 self.placeholder_tokens = \"<s1>|<s2>|<s3>|<s4>\" self.placeholder_token_at_data = \"<s>|<s1><s2><s3><s4>\" self.gradient_checkpointing = False self.scale_lr = False self.gradient_accumulation_steps = 1 self.train_batch_size = 1 self.lr_warmup_steps = 100 # Output configuration self.output_dir = \"outputs/forget_me_not/ti_models\" # Device configuration self.devices = \"0\" # CUDA devices to train on (comma-separated) # Additional configurations self.tokenizer_name = \"default_tokenizer\" self.instance_prompt = \"default_prompt\" self.concept_keyword = \"default_keyword\" self.lr_scheduler = \"linear\" self.prior_generation_precision = \"fp32\" self.local_rank = 0 self.class_prompt = \"default_class_prompt\" self.num_class_images = 100 self.dataloader_num_workers = 4 self.center_crop = True self.prior_loss_weight = 0.1 Train Attn config class ForgetMeNotAttnConfig(BaseConfig): \"\"\" This class encapsulates the training configuration for the 'Forget-Me-Not' TI approach. It mirrors the fields specified in the YAML-like config snippet. \"\"\" def __init__(self, **kwargs): # Model and checkpoint paths self.ckpt_path = \"models/diffuser/style50\" # Dataset directories and setup self.raw_dataset_dir = \"data/quick-canvas-dataset/sample\" self.processed_dataset_dir = \"mu/algorithms/forget_me_not/data\" self.dataset_type = \"unlearncanvas\" self.template = \"style\" self.template_name = \"Abstractionism\" self.use_sample = True # Use the sample dataset for training # Textual Inversion config self.use_ti = True self.ti_weights_path = \"outputs/forget_me_not/finetuned_models/Abstractionism/step_inv_10.safetensors\" self.initializer_tokens = self.template_name self.placeholder_tokens = \"<s1>|<s2>|<s3>|<s4>\" # Training configuration self.mixed_precision = None # or \"fp16\", if desired self.gradient_accumulation_steps = 1 self.train_text_encoder = False self.enable_xformers_memory_efficient_attention = False self.gradient_checkpointing = False self.allow_tf32 = False self.scale_lr = False self.train_batch_size = 1 self.use_8bit_adam = False self.adam_beta1 = 0.9 self.adam_beta2 = 0.999 self.adam_weight_decay = 0.01 self.adam_epsilon = 1.0e-08 self.size = 512 self.with_prior_preservation = False self.num_train_epochs = 1 self.lr_warmup_steps = 0 self.lr_num_cycles = 1 self.lr_power = 1.0 self.max_steps = 2 # originally \"max-steps\" in config self.no_real_image = False self.max_grad_norm = 1.0 self.checkpointing_steps = 500 self.set_grads_to_none = False self.lr = 5e-5 # Output configurations self.output_dir = \"outputs/forget_me_not/finetuned_models/Abstractionism\" # Device configuration self.devices = \"0\" # CUDA devices to train on (comma-separated) self.only_xa = True # originally \"only-xa\" in config # Additional 'Forget-Me-Not' parameters self.perform_inversion = True self.continue_inversion = True self.continue_inversion_lr = 0.0001 self.learning_rate_ti = 0.001 self.learning_rate_unet = 0.0003 self.learning_rate_text = 0.0003 self.lr_scheduler = \"constant\" self.lr_scheduler_lora = \"linear\" self.lr_warmup_steps_lora = 0 self.prior_loss_weight = 1.0 self.weight_decay_lora = 0.001 self.use_face_segmentation_condition = False self.max_train_steps_ti = 500 self.max_train_steps_tuning = 1000 self.save_steps = 100 self.class_data_dir = None self.stochastic_attribute = None self.class_prompt = None self.num_class_images = 100 self.resolution = 512 self.color_jitter = False self.sample_batch_size = 1 self.lora_rank = 4 self.clip_ti_decay = True Evaluation config # mu/algorithms/forget_me_not/configs/evaluation_config.py import os from pathlib import Path from mu.core.base_config import BaseConfig current_dir = Path(__file__).parent class ForgetMeNotEvaluationConfig(BaseConfig): def __init__(self, **kwargs): self.ckpt_path = \"outputs/forget_me_not/finetuned_models\" # path to finetuned model checkpoint directory self.classifier_ckpt_path = \"models/classifier_ckpt_path/style50_cls.pth\" # path to classifier checkpoint self.forget_theme = \"Bricks\" # theme to forget (load finetuned model for this theme) self.cfg_text_list = [9.0] # list of classifier-free guidance scales self.seed = 188 # random seed self.task = \"class\" # task type self.ddim_steps = 100 # number of DDIM steps self.image_height = 512 # height of the image self.image_width = 512 # width of the image self.ddim_eta = 0.0 # DDIM eta parameter self.devices = \"0\" # GPU device ID self.sampler_output_dir = \"outputs/eval_results/mu_results/forget_me_not/\" # directory to save sampler outputs self.seed_list = [\"188\"] # list of seeds for evaluation self.classification_model = \"vit_large_patch16_224\" # classification model for evaluation self.eval_output_dir = \"outputs/eval_results/mu_results/forget_me_not/\" # directory to save evaluation results self.reference_dir = \"data/quick-canvas-dataset/sample/\" # path to the original dataset self.multiprocessing = False # whether to use multiprocessing # Override defaults with any provided kwargs for key, value in kwargs.items(): setattr(self, key, value) def validate_config(self): \"\"\" Perform basic validation on the config parameters. \"\"\" if not os.path.exists(self.ckpt_path): raise FileNotFoundError(f\"Checkpoint directory {self.ckpt_path} does not exist.\") if not os.path.exists(self.classifier_ckpt_path): raise FileNotFoundError(f\"Classifier checkpoint file {self.classifier_ckpt_path} does not exist.\") if not os.path.exists(self.reference_dir): raise FileNotFoundError(f\"Reference directory {self.reference_dir} does not exist.\") if not os.path.exists(self.eval_output_dir): os.makedirs(self.eval_output_dir) if not os.path.exists(self.sampler_output_dir): os.makedirs(self.sampler_output_dir) if any(cfg <= 0 for cfg in self.cfg_text_list): raise ValueError(\"Classifier-free guidance scale (cfg_text) values should be positive.\") if self.ddim_steps <= 0: raise ValueError(\"DDIM steps should be a positive integer.\") if self.image_height <= 0 or self.image_width <= 0: raise ValueError(\"Image height and width should be positive.\") if self.task not in [\"class\", \"other_task\"]: # Add other valid tasks if needed raise ValueError(\"Invalid task type.\") # Example usage forget_me_not_evaluation_config = ForgetMeNotEvaluationConfig()","title":"Configs"},{"location":"unlearn/configs/forget_me_not/#train-ti-config","text":"class ForgetMeNotTiConfig(BaseConfig): \"\"\" Configuration class for the Forget-Me-Not textual inversion training. Mirrors the fields from the second YAML snippet. \"\"\" def __init__(self, **kwargs): # Model checkpoint path self.ckpt_path = \"models/diffuser/style50\" # Dataset directories self.raw_dataset_dir = \"data/quick-canvas-dataset/sample\" self.processed_dataset_dir = \"mu/algorithms/forget_me_not/data\" self.dataset_type = \"unlearncanvas\" self.template = \"style\" self.template_name = \"Abstractionism\" self.use_sample = True # Use the sample dataset for training # Training configuration self.initializer_tokens = self.template_name self.steps = 10 self.lr = 1e-4 self.weight_decay_ti = 0.1 self.seed = 42 self.placeholder_tokens = \"<s1>|<s2>|<s3>|<s4>\" self.placeholder_token_at_data = \"<s>|<s1><s2><s3><s4>\" self.gradient_checkpointing = False self.scale_lr = False self.gradient_accumulation_steps = 1 self.train_batch_size = 1 self.lr_warmup_steps = 100 # Output configuration self.output_dir = \"outputs/forget_me_not/ti_models\" # Device configuration self.devices = \"0\" # CUDA devices to train on (comma-separated) # Additional configurations self.tokenizer_name = \"default_tokenizer\" self.instance_prompt = \"default_prompt\" self.concept_keyword = \"default_keyword\" self.lr_scheduler = \"linear\" self.prior_generation_precision = \"fp32\" self.local_rank = 0 self.class_prompt = \"default_class_prompt\" self.num_class_images = 100 self.dataloader_num_workers = 4 self.center_crop = True self.prior_loss_weight = 0.1","title":"Train Ti Config"},{"location":"unlearn/configs/forget_me_not/#train-attn-config","text":"class ForgetMeNotAttnConfig(BaseConfig): \"\"\" This class encapsulates the training configuration for the 'Forget-Me-Not' TI approach. It mirrors the fields specified in the YAML-like config snippet. \"\"\" def __init__(self, **kwargs): # Model and checkpoint paths self.ckpt_path = \"models/diffuser/style50\" # Dataset directories and setup self.raw_dataset_dir = \"data/quick-canvas-dataset/sample\" self.processed_dataset_dir = \"mu/algorithms/forget_me_not/data\" self.dataset_type = \"unlearncanvas\" self.template = \"style\" self.template_name = \"Abstractionism\" self.use_sample = True # Use the sample dataset for training # Textual Inversion config self.use_ti = True self.ti_weights_path = \"outputs/forget_me_not/finetuned_models/Abstractionism/step_inv_10.safetensors\" self.initializer_tokens = self.template_name self.placeholder_tokens = \"<s1>|<s2>|<s3>|<s4>\" # Training configuration self.mixed_precision = None # or \"fp16\", if desired self.gradient_accumulation_steps = 1 self.train_text_encoder = False self.enable_xformers_memory_efficient_attention = False self.gradient_checkpointing = False self.allow_tf32 = False self.scale_lr = False self.train_batch_size = 1 self.use_8bit_adam = False self.adam_beta1 = 0.9 self.adam_beta2 = 0.999 self.adam_weight_decay = 0.01 self.adam_epsilon = 1.0e-08 self.size = 512 self.with_prior_preservation = False self.num_train_epochs = 1 self.lr_warmup_steps = 0 self.lr_num_cycles = 1 self.lr_power = 1.0 self.max_steps = 2 # originally \"max-steps\" in config self.no_real_image = False self.max_grad_norm = 1.0 self.checkpointing_steps = 500 self.set_grads_to_none = False self.lr = 5e-5 # Output configurations self.output_dir = \"outputs/forget_me_not/finetuned_models/Abstractionism\" # Device configuration self.devices = \"0\" # CUDA devices to train on (comma-separated) self.only_xa = True # originally \"only-xa\" in config # Additional 'Forget-Me-Not' parameters self.perform_inversion = True self.continue_inversion = True self.continue_inversion_lr = 0.0001 self.learning_rate_ti = 0.001 self.learning_rate_unet = 0.0003 self.learning_rate_text = 0.0003 self.lr_scheduler = \"constant\" self.lr_scheduler_lora = \"linear\" self.lr_warmup_steps_lora = 0 self.prior_loss_weight = 1.0 self.weight_decay_lora = 0.001 self.use_face_segmentation_condition = False self.max_train_steps_ti = 500 self.max_train_steps_tuning = 1000 self.save_steps = 100 self.class_data_dir = None self.stochastic_attribute = None self.class_prompt = None self.num_class_images = 100 self.resolution = 512 self.color_jitter = False self.sample_batch_size = 1 self.lora_rank = 4 self.clip_ti_decay = True","title":"Train Attn config"},{"location":"unlearn/configs/forget_me_not/#evaluation-config","text":"# mu/algorithms/forget_me_not/configs/evaluation_config.py import os from pathlib import Path from mu.core.base_config import BaseConfig current_dir = Path(__file__).parent class ForgetMeNotEvaluationConfig(BaseConfig): def __init__(self, **kwargs): self.ckpt_path = \"outputs/forget_me_not/finetuned_models\" # path to finetuned model checkpoint directory self.classifier_ckpt_path = \"models/classifier_ckpt_path/style50_cls.pth\" # path to classifier checkpoint self.forget_theme = \"Bricks\" # theme to forget (load finetuned model for this theme) self.cfg_text_list = [9.0] # list of classifier-free guidance scales self.seed = 188 # random seed self.task = \"class\" # task type self.ddim_steps = 100 # number of DDIM steps self.image_height = 512 # height of the image self.image_width = 512 # width of the image self.ddim_eta = 0.0 # DDIM eta parameter self.devices = \"0\" # GPU device ID self.sampler_output_dir = \"outputs/eval_results/mu_results/forget_me_not/\" # directory to save sampler outputs self.seed_list = [\"188\"] # list of seeds for evaluation self.classification_model = \"vit_large_patch16_224\" # classification model for evaluation self.eval_output_dir = \"outputs/eval_results/mu_results/forget_me_not/\" # directory to save evaluation results self.reference_dir = \"data/quick-canvas-dataset/sample/\" # path to the original dataset self.multiprocessing = False # whether to use multiprocessing # Override defaults with any provided kwargs for key, value in kwargs.items(): setattr(self, key, value) def validate_config(self): \"\"\" Perform basic validation on the config parameters. \"\"\" if not os.path.exists(self.ckpt_path): raise FileNotFoundError(f\"Checkpoint directory {self.ckpt_path} does not exist.\") if not os.path.exists(self.classifier_ckpt_path): raise FileNotFoundError(f\"Classifier checkpoint file {self.classifier_ckpt_path} does not exist.\") if not os.path.exists(self.reference_dir): raise FileNotFoundError(f\"Reference directory {self.reference_dir} does not exist.\") if not os.path.exists(self.eval_output_dir): os.makedirs(self.eval_output_dir) if not os.path.exists(self.sampler_output_dir): os.makedirs(self.sampler_output_dir) if any(cfg <= 0 for cfg in self.cfg_text_list): raise ValueError(\"Classifier-free guidance scale (cfg_text) values should be positive.\") if self.ddim_steps <= 0: raise ValueError(\"DDIM steps should be a positive integer.\") if self.image_height <= 0 or self.image_width <= 0: raise ValueError(\"Image height and width should be positive.\") if self.task not in [\"class\", \"other_task\"]: # Add other valid tasks if needed raise ValueError(\"Invalid task type.\") # Example usage forget_me_not_evaluation_config = ForgetMeNotEvaluationConfig()","title":"Evaluation config"},{"location":"unlearn/configs/saliency/","text":"Train Config class SaliencyUnlearningConfig(BaseConfig): def __init__(self, **kwargs): # Model configuration self.alpha = 0.1 # Alpha value for training self.epochs = 1 # Number of epochs for training self.train_method = ( \"xattn\" # Attention method: [\"noxattn\", \"selfattn\", \"xattn\", \"full\"] ) self.ckpt_path = \"models/compvis/style50/compvis.ckpt\" # Path to the checkpoint self.model_config_path = current_dir / \"model_config.yaml\" # Dataset directories self.raw_dataset_dir = ( \"data/quick-canvas-dataset/sample\" # Path to the raw dataset ) self.processed_dataset_dir = ( \"mu/algorithms/saliency_unlearning/data\" # Path to the processed dataset ) self.dataset_type = \"unlearncanvas\" # Type of the dataset self.template = \"style\" # Template type for training self.template_name = \"Abstractionism\" # Name of the template # Directory Configuration self.output_dir = \"outputs/saliency_unlearning/finetuned_models\" # Directory for output models self.mask_path = ( \"outputs/saliency_unlearning/masks/0.5.pt\" # Path to the mask file ) # Training configuration self.devices = \"0\" # CUDA devices for training (comma-separated) self.use_sample = True # Whether to use a sample dataset for training # Guidance and training parameters self.start_guidance = 0.5 # Start guidance for training self.negative_guidance = 0.5 # Negative guidance for training self.ddim_steps = 50 # Number of DDIM steps for sampling # Update properties based on provided kwargs for key, value in kwargs.items(): setattr(self, key, value) Model Config # Model Configuration alpha: 0.1 epochs: 1 train_method: \"xattn\" # Choices: [\"noxattn\", \"selfattn\", \"xattn\", \"full\" ] ckpt_path: \"models/compvis/style50/compvis.ckpt\" # Checkpoint path for Stable Diffusion model_config_path: \"mu/algorithms/saliency_unlearning/configs/model_config.yaml\" # Config path for Stable Diffusion # Dataset directories raw_dataset_dir: \"data/quick-canvas-dataset/sample\" processed_dataset_dir: \"mu/algorithms/saliency_unlearning/data\" dataset_type : \"unlearncanvas\" template : \"style\" template_name : \"Abstractionism\" # Directory Configuration output_dir: \"outputs/saliency_unlearning/finetuned_models\" # Output directory to save results mask_path: \"outputs/saliency_unlearning/masks/0.5.pt\" # Output directory to save results # Training Configuration devices: \"0\" # CUDA devices to train on (comma-separated) use_sample: true start_guidance: 0.5 negative_guidance: 0.5 ddim_steps: 50 Mask Config # Model Configuration c_guidance: 7.5 batch_size: 4 num_timesteps: 1000 image_size: 512 model_config_path: \"mu/algorithms/saliency_unlearning/configs/model_config.yaml\" # ckpt_path: \"models/compvis/style50/compvis.ckpt\" # Checkpoint path for Stable Diffusion ckpt_path: \"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\" # Dataset directories # raw_dataset_dir: \"data/quick-canvas-dataset/sample\" raw_dataset_dir: \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample\" processed_dataset_dir: \"mu/algorithms/saliency_unlearning/data\" dataset_type : \"unlearncanvas\" template : \"style\" template_name : \"Abstractionism\" threshold : 0.5 # Directory Configuration output_dir: \"outputs/saliency_unlearning/masks\" # Output directory to save results # Training Configuration lr: 0.00001 devices: \"0\" # CUDA devices to train on (comma-separated) use_sample: true Evaluation config # mu/algorithms/saliency_unlearning/configs/evaluation_config.py import os from pathlib import Path from mu.core.base_config import BaseConfig current_dir = Path(__file__).parent class SaliencyUnlearningEvaluationConfig(BaseConfig): def __init__(self, **kwargs): self.model_config_path = current_dir/\"model_config.yaml\" # path to model config self.ckpt_path = \"outputs/saliency_unlearning/finetuned_models/saliency_unlearning_Abstractionism_model.pth\" # path to finetuned model checkpoint self.classifier_ckpt_path = \"models/classifier_ckpt_path/style50_cls.pth\" # path to classifier checkpoint self.forget_theme = \"Bricks\" # theme to forget self.cfg_text = 9.0 # classifier-free guidance scale self.devices = \"0\" # GPU device ID self.seed = 188 # random seed self.task = \"class\" # task type self.ddim_steps = 100 # number of DDIM steps self.image_height = 512 # height of the image self.image_width = 512 # width of the image self.ddim_eta = 0.0 # DDIM eta parameter self.sampler_output_dir = \"outputs/eval_results/mu_results/saliency_unlearning/\" # directory to save sampler outputs self.seed_list = [\"188\"] # list of seeds for evaluation self.classification_model = \"vit_large_patch16_224\" # classification model for evaluation self.eval_output_dir = \"outputs/eval_results/mu_results/saliency_unlearning/\" # directory to save evaluation results self.reference_dir = \"data/quick-canvas-dataset/sample/\" # path to the original dataset self.multiprocessing = False # whether to use multiprocessing # Override defaults with any provided kwargs for key, value in kwargs.items(): setattr(self, key, value) def validate_config(self): \"\"\" Perform basic validation on the config parameters. \"\"\" if not os.path.exists(self.model_config_path): raise FileNotFoundError(f\"Model config file {self.model_config_path} does not exist.\") if not os.path.exists(self.ckpt_path): raise FileNotFoundError(f\"Checkpoint file {self.ckpt_path} does not exist.\") if not os.path.exists(self.classifier_ckpt_path): raise FileNotFoundError(f\"Classifier checkpoint file {self.classifier_ckpt_path} does not exist.\") if not os.path.exists(self.reference_dir): raise FileNotFoundError(f\"Reference directory {self.reference_dir} does not exist.\") if not os.path.exists(self.eval_output_dir): os.makedirs(self.eval_output_dir) if not os.path.exists(self.sampler_output_dir): os.makedirs(self.sampler_output_dir) if self.cfg_text <= 0: raise ValueError(\"Classifier-free guidance scale (cfg_text) should be positive.\") if self.ddim_steps <= 0: raise ValueError(\"DDIM steps should be a positive integer.\") if self.image_height <= 0 or self.image_width <= 0: raise ValueError(\"Image height and width should be positive.\") if self.task not in [\"class\", \"other_task\"]: # Add other valid tasks if needed raise ValueError(\"Invalid task type.\") saliency_unlearning_evaluation_config = SaliencyUnlearningEvaluationConfig()","title":"Configs"},{"location":"unlearn/configs/saliency/#train-config","text":"class SaliencyUnlearningConfig(BaseConfig): def __init__(self, **kwargs): # Model configuration self.alpha = 0.1 # Alpha value for training self.epochs = 1 # Number of epochs for training self.train_method = ( \"xattn\" # Attention method: [\"noxattn\", \"selfattn\", \"xattn\", \"full\"] ) self.ckpt_path = \"models/compvis/style50/compvis.ckpt\" # Path to the checkpoint self.model_config_path = current_dir / \"model_config.yaml\" # Dataset directories self.raw_dataset_dir = ( \"data/quick-canvas-dataset/sample\" # Path to the raw dataset ) self.processed_dataset_dir = ( \"mu/algorithms/saliency_unlearning/data\" # Path to the processed dataset ) self.dataset_type = \"unlearncanvas\" # Type of the dataset self.template = \"style\" # Template type for training self.template_name = \"Abstractionism\" # Name of the template # Directory Configuration self.output_dir = \"outputs/saliency_unlearning/finetuned_models\" # Directory for output models self.mask_path = ( \"outputs/saliency_unlearning/masks/0.5.pt\" # Path to the mask file ) # Training configuration self.devices = \"0\" # CUDA devices for training (comma-separated) self.use_sample = True # Whether to use a sample dataset for training # Guidance and training parameters self.start_guidance = 0.5 # Start guidance for training self.negative_guidance = 0.5 # Negative guidance for training self.ddim_steps = 50 # Number of DDIM steps for sampling # Update properties based on provided kwargs for key, value in kwargs.items(): setattr(self, key, value)","title":"Train Config"},{"location":"unlearn/configs/saliency/#model-config","text":"# Model Configuration alpha: 0.1 epochs: 1 train_method: \"xattn\" # Choices: [\"noxattn\", \"selfattn\", \"xattn\", \"full\" ] ckpt_path: \"models/compvis/style50/compvis.ckpt\" # Checkpoint path for Stable Diffusion model_config_path: \"mu/algorithms/saliency_unlearning/configs/model_config.yaml\" # Config path for Stable Diffusion # Dataset directories raw_dataset_dir: \"data/quick-canvas-dataset/sample\" processed_dataset_dir: \"mu/algorithms/saliency_unlearning/data\" dataset_type : \"unlearncanvas\" template : \"style\" template_name : \"Abstractionism\" # Directory Configuration output_dir: \"outputs/saliency_unlearning/finetuned_models\" # Output directory to save results mask_path: \"outputs/saliency_unlearning/masks/0.5.pt\" # Output directory to save results # Training Configuration devices: \"0\" # CUDA devices to train on (comma-separated) use_sample: true start_guidance: 0.5 negative_guidance: 0.5 ddim_steps: 50","title":"Model Config"},{"location":"unlearn/configs/saliency/#mask-config","text":"# Model Configuration c_guidance: 7.5 batch_size: 4 num_timesteps: 1000 image_size: 512 model_config_path: \"mu/algorithms/saliency_unlearning/configs/model_config.yaml\" # ckpt_path: \"models/compvis/style50/compvis.ckpt\" # Checkpoint path for Stable Diffusion ckpt_path: \"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\" # Dataset directories # raw_dataset_dir: \"data/quick-canvas-dataset/sample\" raw_dataset_dir: \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample\" processed_dataset_dir: \"mu/algorithms/saliency_unlearning/data\" dataset_type : \"unlearncanvas\" template : \"style\" template_name : \"Abstractionism\" threshold : 0.5 # Directory Configuration output_dir: \"outputs/saliency_unlearning/masks\" # Output directory to save results # Training Configuration lr: 0.00001 devices: \"0\" # CUDA devices to train on (comma-separated) use_sample: true","title":"Mask Config"},{"location":"unlearn/configs/saliency/#evaluation-config","text":"# mu/algorithms/saliency_unlearning/configs/evaluation_config.py import os from pathlib import Path from mu.core.base_config import BaseConfig current_dir = Path(__file__).parent class SaliencyUnlearningEvaluationConfig(BaseConfig): def __init__(self, **kwargs): self.model_config_path = current_dir/\"model_config.yaml\" # path to model config self.ckpt_path = \"outputs/saliency_unlearning/finetuned_models/saliency_unlearning_Abstractionism_model.pth\" # path to finetuned model checkpoint self.classifier_ckpt_path = \"models/classifier_ckpt_path/style50_cls.pth\" # path to classifier checkpoint self.forget_theme = \"Bricks\" # theme to forget self.cfg_text = 9.0 # classifier-free guidance scale self.devices = \"0\" # GPU device ID self.seed = 188 # random seed self.task = \"class\" # task type self.ddim_steps = 100 # number of DDIM steps self.image_height = 512 # height of the image self.image_width = 512 # width of the image self.ddim_eta = 0.0 # DDIM eta parameter self.sampler_output_dir = \"outputs/eval_results/mu_results/saliency_unlearning/\" # directory to save sampler outputs self.seed_list = [\"188\"] # list of seeds for evaluation self.classification_model = \"vit_large_patch16_224\" # classification model for evaluation self.eval_output_dir = \"outputs/eval_results/mu_results/saliency_unlearning/\" # directory to save evaluation results self.reference_dir = \"data/quick-canvas-dataset/sample/\" # path to the original dataset self.multiprocessing = False # whether to use multiprocessing # Override defaults with any provided kwargs for key, value in kwargs.items(): setattr(self, key, value) def validate_config(self): \"\"\" Perform basic validation on the config parameters. \"\"\" if not os.path.exists(self.model_config_path): raise FileNotFoundError(f\"Model config file {self.model_config_path} does not exist.\") if not os.path.exists(self.ckpt_path): raise FileNotFoundError(f\"Checkpoint file {self.ckpt_path} does not exist.\") if not os.path.exists(self.classifier_ckpt_path): raise FileNotFoundError(f\"Classifier checkpoint file {self.classifier_ckpt_path} does not exist.\") if not os.path.exists(self.reference_dir): raise FileNotFoundError(f\"Reference directory {self.reference_dir} does not exist.\") if not os.path.exists(self.eval_output_dir): os.makedirs(self.eval_output_dir) if not os.path.exists(self.sampler_output_dir): os.makedirs(self.sampler_output_dir) if self.cfg_text <= 0: raise ValueError(\"Classifier-free guidance scale (cfg_text) should be positive.\") if self.ddim_steps <= 0: raise ValueError(\"DDIM steps should be a positive integer.\") if self.image_height <= 0 or self.image_width <= 0: raise ValueError(\"Image height and width should be positive.\") if self.task not in [\"class\", \"other_task\"]: # Add other valid tasks if needed raise ValueError(\"Invalid task type.\") saliency_unlearning_evaluation_config = SaliencyUnlearningEvaluationConfig()","title":"Evaluation config"},{"location":"unlearn/configs/scissorhands/","text":"Train config class ScissorHandsConfig(BaseConfig): def __init__(self, **kwargs): # Training parameters self.train_method = \"xattn\" # choices: [\"noxattn\", \"selfattn\", \"xattn\", \"full\", \"notime\", \"xlayer\", \"selflayer\"] self.alpha = 0.75 # Guidance of start image used to train self.epochs = 5 # Number of training epochs # Model configuration self.model_config_path = \"mu/algorithms/scissorhands/configs/model_config.yaml\" # Config path for model self.ckpt_path = \"models/compvis/style50/compvis.ckpt\" # Checkpoint path for Stable Diffusion # Dataset directories self.raw_dataset_dir = \"data/quick-canvas-dataset/sample\" self.processed_dataset_dir = \"mu/algorithms/scissorhands/data\" self.dataset_type = \"unlearncanvas\" # Choices: [\"unlearncanvas\", \"i2p\"] self.template = \"style\" # Template to use self.template_name = \"Abstractionism\" # Template name # Output configuration self.output_dir = ( \"outputs/scissorhands/finetuned_models\" # Output directory to save results ) # Sampling and image configurations self.sparsity = 0.90 # Threshold for mask sparsity self.project = False # Whether to project self.memory_num = 1 # Number of memories to use self.prune_num = 10 # Number of pruned images # Device configuration self.devices = \"0,1\" # CUDA devices to train on (comma-separated) # Additional configurations self.use_sample = True # Use sample dataset for training # Guidance configurations self.start_guidence = 0.5 # Starting guidance factor self.negative_guidance = 0.3 # Negative guidance factor self.iterations = 1000 # Number of training iterations Model Config model: base_learning_rate: 1.0e-04 target: stable_diffusion.ldm.models.diffusion.ddpm.LatentDiffusion params: linear_start: 0.00085 linear_end: 0.0120 num_timesteps_cond: 1 log_every_t: 200 timesteps: 1000 first_stage_key: \"edited\" cond_stage_key: \"edit\" image_size: 32 channels: 4 cond_stage_trainable: false # Note: different from the one we trained before conditioning_key: crossattn monitor: val/loss_simple_ema scale_factor: 0.18215 use_ema: False scheduler_config: # 10000 warmup steps target: stable_diffusion.ldm.lr_scheduler.LambdaLinearScheduler params: warm_up_steps: [ 10000 ] cycle_lengths: [ 10000000000000 ] # incredibly large number to prevent corner cases f_start: [ 1.e-6 ] f_max: [ 1. ] f_min: [ 1. ] unet_config: target: stable_diffusion.ldm.modules.diffusionmodules.openaimodel.UNetModel params: image_size: 32 # unused in_channels: 4 out_channels: 4 model_channels: 320 attention_resolutions: [ 4, 2, 1 ] num_res_blocks: 2 channel_mult: [ 1, 2, 4, 4 ] num_heads: 8 use_spatial_transformer: True transformer_depth: 1 context_dim: 768 use_checkpoint: True legacy: False first_stage_config: target: stable_diffusion.ldm.models.autoencoder.AutoencoderKL params: embed_dim: 4 monitor: val/rec_loss ddconfig: double_z: true z_channels: 4 resolution: 256 in_channels: 3 out_ch: 3 ch: 128 ch_mult: - 1 - 2 - 4 - 4 num_res_blocks: 2 attn_resolutions: [] dropout: 0.0 lossconfig: target: torch.nn.Identity cond_stage_config: target: stable_diffusion.ldm.modules.encoders.modules.FrozenCLIPEmbedder Evaluation Config # mu/algorithms/scissorhands/configs/evaluation_config.py import os from pathlib import Path from mu.core.base_config import BaseConfig current_dir = Path(__file__).parent class ScissorhandsEvaluationConfig(BaseConfig): def __init__(self, **kwargs): self.model_config_path = current_dir/\"model_config.yaml\" # path to model config self.ckpt_path = \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\" # path to finetuned model checkpoint self.classifier_ckpt_path = \"models/classifier_ckpt_path/style50_cls.pth\" # path to classifier checkpoint self.cfg_text = 9.0 # classifier-free guidance scale self.seed = 188 # random seed self.task = \"class\" # task type self.devices = \"0\" # GPU device ID self.ddim_steps = 100 # number of DDIM steps self.image_height = 512 # height of the image self.image_width = 512 # width of the image self.ddim_eta = 0.0 # DDIM eta parameter self.sampler_output_dir = \"outputs/eval_results/mu_results/erase_diff/\" # directory to save sampler outputs self.seed_list = [\"188\"] # list of seeds for evaluation self.classification_model = \"vit_large_patch16_224\" # classification model for evaluation self.eval_output_dir = \"outputs/eval_results/mu_results/scissorhands/\" # directory to save evaluation results self.reference_dir = \"data/quick-canvas-dataset/sample/\" # path to the original dataset self.forget_theme = \"Bricks\" # theme to forget self.multiprocessing = False # whether to use multiprocessing # Override defaults with any provided kwargs for key, value in kwargs.items(): setattr(self, key, value) def validate_config(self): \"\"\" Perform basic validation on the config parameters. \"\"\" if not os.path.exists(self.model_config_path): raise FileNotFoundError(f\"Model config file {self.model_config_path} does not exist.\") if not os.path.exists(self.ckpt_path): raise FileNotFoundError(f\"Checkpoint file {self.ckpt_path} does not exist.\") if not os.path.exists(self.classifier_ckpt_path): raise FileNotFoundError(f\"Classifier checkpoint file {self.classifier_ckpt_path} does not exist.\") if not os.path.exists(self.reference_dir): raise FileNotFoundError(f\"Reference directory {self.reference_dir} does not exist.\") if not os.path.exists(self.eval_output_dir): os.makedirs(self.eval_output_dir) if not os.path.exists(self.sampler_output_dir): os.makedirs(self.sampler_output_dir) if self.cfg_text <= 0: raise ValueError(\"Classifier-free guidance scale (cfg_text) should be positive.\") if self.ddim_steps <= 0: raise ValueError(\"DDIM steps should be a positive integer.\") if self.image_height <= 0 or self.image_width <= 0: raise ValueError(\"Image height and width should be positive.\") if self.task not in [\"class\", \"other_task\"]: # Add other valid tasks if needed raise ValueError(\"Invalid task type.\") # Example usage scissorhands_evaluation_config = ScissorhandsEvaluationConfig()","title":"Configs"},{"location":"unlearn/configs/scissorhands/#train-config","text":"class ScissorHandsConfig(BaseConfig): def __init__(self, **kwargs): # Training parameters self.train_method = \"xattn\" # choices: [\"noxattn\", \"selfattn\", \"xattn\", \"full\", \"notime\", \"xlayer\", \"selflayer\"] self.alpha = 0.75 # Guidance of start image used to train self.epochs = 5 # Number of training epochs # Model configuration self.model_config_path = \"mu/algorithms/scissorhands/configs/model_config.yaml\" # Config path for model self.ckpt_path = \"models/compvis/style50/compvis.ckpt\" # Checkpoint path for Stable Diffusion # Dataset directories self.raw_dataset_dir = \"data/quick-canvas-dataset/sample\" self.processed_dataset_dir = \"mu/algorithms/scissorhands/data\" self.dataset_type = \"unlearncanvas\" # Choices: [\"unlearncanvas\", \"i2p\"] self.template = \"style\" # Template to use self.template_name = \"Abstractionism\" # Template name # Output configuration self.output_dir = ( \"outputs/scissorhands/finetuned_models\" # Output directory to save results ) # Sampling and image configurations self.sparsity = 0.90 # Threshold for mask sparsity self.project = False # Whether to project self.memory_num = 1 # Number of memories to use self.prune_num = 10 # Number of pruned images # Device configuration self.devices = \"0,1\" # CUDA devices to train on (comma-separated) # Additional configurations self.use_sample = True # Use sample dataset for training # Guidance configurations self.start_guidence = 0.5 # Starting guidance factor self.negative_guidance = 0.3 # Negative guidance factor self.iterations = 1000 # Number of training iterations","title":"Train config"},{"location":"unlearn/configs/scissorhands/#model-config","text":"model: base_learning_rate: 1.0e-04 target: stable_diffusion.ldm.models.diffusion.ddpm.LatentDiffusion params: linear_start: 0.00085 linear_end: 0.0120 num_timesteps_cond: 1 log_every_t: 200 timesteps: 1000 first_stage_key: \"edited\" cond_stage_key: \"edit\" image_size: 32 channels: 4 cond_stage_trainable: false # Note: different from the one we trained before conditioning_key: crossattn monitor: val/loss_simple_ema scale_factor: 0.18215 use_ema: False scheduler_config: # 10000 warmup steps target: stable_diffusion.ldm.lr_scheduler.LambdaLinearScheduler params: warm_up_steps: [ 10000 ] cycle_lengths: [ 10000000000000 ] # incredibly large number to prevent corner cases f_start: [ 1.e-6 ] f_max: [ 1. ] f_min: [ 1. ] unet_config: target: stable_diffusion.ldm.modules.diffusionmodules.openaimodel.UNetModel params: image_size: 32 # unused in_channels: 4 out_channels: 4 model_channels: 320 attention_resolutions: [ 4, 2, 1 ] num_res_blocks: 2 channel_mult: [ 1, 2, 4, 4 ] num_heads: 8 use_spatial_transformer: True transformer_depth: 1 context_dim: 768 use_checkpoint: True legacy: False first_stage_config: target: stable_diffusion.ldm.models.autoencoder.AutoencoderKL params: embed_dim: 4 monitor: val/rec_loss ddconfig: double_z: true z_channels: 4 resolution: 256 in_channels: 3 out_ch: 3 ch: 128 ch_mult: - 1 - 2 - 4 - 4 num_res_blocks: 2 attn_resolutions: [] dropout: 0.0 lossconfig: target: torch.nn.Identity cond_stage_config: target: stable_diffusion.ldm.modules.encoders.modules.FrozenCLIPEmbedder","title":"Model Config"},{"location":"unlearn/configs/scissorhands/#evaluation-config","text":"# mu/algorithms/scissorhands/configs/evaluation_config.py import os from pathlib import Path from mu.core.base_config import BaseConfig current_dir = Path(__file__).parent class ScissorhandsEvaluationConfig(BaseConfig): def __init__(self, **kwargs): self.model_config_path = current_dir/\"model_config.yaml\" # path to model config self.ckpt_path = \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\" # path to finetuned model checkpoint self.classifier_ckpt_path = \"models/classifier_ckpt_path/style50_cls.pth\" # path to classifier checkpoint self.cfg_text = 9.0 # classifier-free guidance scale self.seed = 188 # random seed self.task = \"class\" # task type self.devices = \"0\" # GPU device ID self.ddim_steps = 100 # number of DDIM steps self.image_height = 512 # height of the image self.image_width = 512 # width of the image self.ddim_eta = 0.0 # DDIM eta parameter self.sampler_output_dir = \"outputs/eval_results/mu_results/erase_diff/\" # directory to save sampler outputs self.seed_list = [\"188\"] # list of seeds for evaluation self.classification_model = \"vit_large_patch16_224\" # classification model for evaluation self.eval_output_dir = \"outputs/eval_results/mu_results/scissorhands/\" # directory to save evaluation results self.reference_dir = \"data/quick-canvas-dataset/sample/\" # path to the original dataset self.forget_theme = \"Bricks\" # theme to forget self.multiprocessing = False # whether to use multiprocessing # Override defaults with any provided kwargs for key, value in kwargs.items(): setattr(self, key, value) def validate_config(self): \"\"\" Perform basic validation on the config parameters. \"\"\" if not os.path.exists(self.model_config_path): raise FileNotFoundError(f\"Model config file {self.model_config_path} does not exist.\") if not os.path.exists(self.ckpt_path): raise FileNotFoundError(f\"Checkpoint file {self.ckpt_path} does not exist.\") if not os.path.exists(self.classifier_ckpt_path): raise FileNotFoundError(f\"Classifier checkpoint file {self.classifier_ckpt_path} does not exist.\") if not os.path.exists(self.reference_dir): raise FileNotFoundError(f\"Reference directory {self.reference_dir} does not exist.\") if not os.path.exists(self.eval_output_dir): os.makedirs(self.eval_output_dir) if not os.path.exists(self.sampler_output_dir): os.makedirs(self.sampler_output_dir) if self.cfg_text <= 0: raise ValueError(\"Classifier-free guidance scale (cfg_text) should be positive.\") if self.ddim_steps <= 0: raise ValueError(\"DDIM steps should be a positive integer.\") if self.image_height <= 0 or self.image_width <= 0: raise ValueError(\"Image height and width should be positive.\") if self.task not in [\"class\", \"other_task\"]: # Add other valid tasks if needed raise ValueError(\"Invalid task type.\") # Example usage scissorhands_evaluation_config = ScissorhandsEvaluationConfig()","title":"Evaluation Config"},{"location":"unlearn/configs/selective_amnesia/","text":"Train config import os from mu.core.base_config import BaseConfig from pathlib import Path current_dir = Path(__file__).parent class SelectiveAmnesiaConfig(BaseConfig): def __init__(self, **kwargs): # Training parameters self.seed = 23 # Random seed self.scale_lr = True # Flag for scaling learning rate # Model configuration self.config_path = current_dir / \"train_config.yaml\" self.model_config_path = ( current_dir / \"model_config.yaml\" ) # Config path for model self.ckpt_path = \"models/compvis/style50/compvis.ckpt\" # Checkpoint path for Stable Diffusion self.full_fisher_dict_pkl_path = \"mu/algorithms/selective_amnesia/data/full_fisher_dict.pkl\" # Path for Fisher dict # Dataset directories self.raw_dataset_dir = \"data/quick-canvas-dataset/sample\" self.processed_dataset_dir = \"mu/algorithms/selective_amnesia/data\" self.dataset_type = ( \"unlearncanvas\" # Dataset type (choices: unlearncanvas, i2p) ) self.template = \"style\" # Template to use self.template_name = \"Abstractionism\" # Template name self.replay_prompt_path = \"mu/algorithms/selective_amnesia/data/fim_prompts_sample.txt\" # Path for replay prompts # Output configurations self.output_dir = \"outputs/selective_amnesia/finetuned_models\" # Output directory to save results # Device configuration self.devices = \"0,\" # CUDA devices (comma-separated) # Additional flags self.use_sample = True # Use sample dataset for training # Data configuration self.data = { \"target\": \"mu.algorithms.selective_amnesia.data_handler.SelectiveAmnesiaDataHandler\", \"params\": { \"train_batch_size\": 4, \"val_batch_size\": 6, \"num_workers\": 1, \"num_val_workers\": 0, # Avoid val dataloader issue \"train\": { \"target\": \"stable_diffusion.ldm.data.ForgettingDataset\", \"params\": { \"forget_prompt\": \"An image in Artist_Sketch style\", \"forget_dataset_path\": \"./q_dist/photo_style\", }, }, \"validation\": { \"target\": \"stable_diffusion.ldm.data.VisualizationDataset\", \"params\": { \"output_size\": 512, \"n_gpus\": 1, # Number of GPUs for validation }, }, }, } # Lightning configuration self.lightning = { \"find_unused_parameters\": False, \"modelcheckpoint\": { \"params\": {\"every_n_epochs\": 0, \"save_top_k\": 0, \"monitor\": None} }, \"callbacks\": { \"image_logger\": { \"target\": \"mu.algorithms.selective_amnesia.callbacks.ImageLogger\", \"params\": { \"batch_frequency\": 1, \"max_images\": 999, \"increase_log_steps\": False, \"log_first_step\": False, \"log_all_val\": True, \"clamp\": True, \"log_images_kwargs\": { \"ddim_eta\": 0, \"ddim_steps\": 50, \"use_ema_scope\": True, \"inpaint\": False, \"plot_progressive_rows\": False, \"plot_diffusion_rows\": False, \"N\": 6, # Number of validation prompts \"unconditional_guidance_scale\": 7.5, \"unconditional_guidance_label\": [\"\"], }, }, } }, \"trainer\": { \"benchmark\": True, \"num_sanity_val_steps\": 0, \"max_epochs\": 50, # Modify epochs here! \"check_val_every_n_epoch\": 10, }, } # Update properties based on provided kwargs for key, value in kwargs.items(): setattr(self, key, value) def validate_config(self): \"\"\" Perform basic validation on the config parameters. \"\"\" # Check if necessary directories exist if not os.path.exists(self.raw_dataset_dir): raise FileNotFoundError(f\"Directory {self.raw_dataset_dir} does not exist.\") if not os.path.exists(self.processed_dataset_dir): raise FileNotFoundError( f\"Directory {self.processed_dataset_dir} does not exist.\" ) if not os.path.exists(self.output_dir): os.makedirs(self.output_dir) # Check if model and checkpoint files exist if not os.path.exists(self.model_config_path): raise FileNotFoundError( f\"Model config file {self.model_config_path} does not exist.\" ) if not os.path.exists(self.ckpt_path): raise FileNotFoundError(f\"Checkpoint file {self.ckpt_path} does not exist.\") if not os.path.exists(self.full_fisher_dict_pkl_path): raise FileNotFoundError( f\"Fisher dictionary file {self.full_fisher_dict_pkl_path} does not exist.\" ) # Check if replay prompts file exists if not os.path.exists(self.replay_prompt_path): raise FileNotFoundError( f\"Replay prompt file {self.replay_prompt_path} does not exist.\" ) # Validate dataset type if self.dataset_type not in [\"unlearncanvas\", \"i2p\"]: raise ValueError( f\"Invalid dataset type {self.dataset_type}. Choose from ['unlearncanvas', 'i2p']\" ) # Validate batch sizes if self.data[\"params\"][\"train_batch_size\"] <= 0: raise ValueError(f\"train_batch_size should be a positive integer.\") if self.data[\"params\"][\"val_batch_size\"] <= 0: raise ValueError(f\"val_batch_size should be a positive integer.\") # Validate lightning trainer max_epochs if self.lightning[\"trainer\"][\"max_epochs\"] <= 0: raise ValueError(f\"max_epochs should be a positive integer.\") selective_amnesia_config_quick_canvas = SelectiveAmnesiaConfig() selective_amnesia_config_quick_canvas.dataset_type = \"unlearncanvas\" selective_amnesia_config_quick_canvas.raw_dataset_dir = ( \"data/quick-canvas-dataset/sample\" ) selective_amnesia_config_i2p = SelectiveAmnesiaConfig() selective_amnesia_config_i2p.dataset_type = \"i2p\" selective_amnesia_config_i2p.raw_dataset_dir = \"data/i2p-dataset/sample\" Train config yaml file # Training parameters seed : 23 scale_lr : True # Model configuration model_config_path: \"mu/algorithms/selective_amnesia/configs/model_config.yaml\" ckpt_path: \"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\" # Checkpoint path for Stable Diffusion full_fisher_dict_pkl_path : \"mu/algorithms/selective_amnesia/data/full_fisher_dict.pkl\" # Dataset directories raw_dataset_dir: \"data/quick-canvas-dataset/sample\" processed_dataset_dir: \"mu/algorithms/selective_amnesia/data\" dataset_type : \"unlearncanvas\" template : \"style\" template_name : \"Abstractionism\" replay_prompt_path: \"mu/algorithms/selective_amnesia/data/fim_prompts_sample.txt\" # Output configurations output_dir: \"outputs/selective_amnesia/finetuned_models\" # Output directory to save results # Sampling and image configurations # Device configuration devices: \"0,\" # CUDA devices to train on (comma-separated) # Additional flags use_sample: True # Use the sample dataset for training data: target: mu.algorithms.selective_amnesia.data_handler.SelectiveAmnesiaDataHandler params: train_batch_size: 4 val_batch_size: 6 num_workers: 4 num_val_workers: 0 # Avoid a weird val dataloader issue (keep unchanged) train: target: stable_diffusion.ldm.data.ForgettingDataset params: forget_prompt: An image in Artist_Sketch style forget_dataset_path: ./q_dist/photo_style validation: target: stable_diffusion.ldm.data.VisualizationDataset params: output_size: 512 n_gpus: 1 # CHANGE THIS TO NUMBER OF GPUS! small hack to sure we see all our logging samples lightning: find_unused_parameters: False modelcheckpoint: params: every_n_epochs: 0 save_top_k: 0 monitor: null callbacks: image_logger: target: mu.algorithms.selective_amnesia.callbacks.ImageLogger params: batch_frequency: 1 max_images: 999 increase_log_steps: False log_first_step: False log_all_val: True clamp: True log_images_kwargs: ddim_eta: 0 ddim_steps: 50 use_ema_scope: True inpaint: False plot_progressive_rows: False plot_diffusion_rows: False N: 6 # keep this the same as number of validation prompts! unconditional_guidance_scale: 7.5 unconditional_guidance_label: [\"\"] trainer: benchmark: True num_sanity_val_steps: 0 max_epochs: 50 # modify epochs here! check_val_every_n_epoch: 10","title":"Configs"},{"location":"unlearn/configs/selective_amnesia/#train-config","text":"import os from mu.core.base_config import BaseConfig from pathlib import Path current_dir = Path(__file__).parent class SelectiveAmnesiaConfig(BaseConfig): def __init__(self, **kwargs): # Training parameters self.seed = 23 # Random seed self.scale_lr = True # Flag for scaling learning rate # Model configuration self.config_path = current_dir / \"train_config.yaml\" self.model_config_path = ( current_dir / \"model_config.yaml\" ) # Config path for model self.ckpt_path = \"models/compvis/style50/compvis.ckpt\" # Checkpoint path for Stable Diffusion self.full_fisher_dict_pkl_path = \"mu/algorithms/selective_amnesia/data/full_fisher_dict.pkl\" # Path for Fisher dict # Dataset directories self.raw_dataset_dir = \"data/quick-canvas-dataset/sample\" self.processed_dataset_dir = \"mu/algorithms/selective_amnesia/data\" self.dataset_type = ( \"unlearncanvas\" # Dataset type (choices: unlearncanvas, i2p) ) self.template = \"style\" # Template to use self.template_name = \"Abstractionism\" # Template name self.replay_prompt_path = \"mu/algorithms/selective_amnesia/data/fim_prompts_sample.txt\" # Path for replay prompts # Output configurations self.output_dir = \"outputs/selective_amnesia/finetuned_models\" # Output directory to save results # Device configuration self.devices = \"0,\" # CUDA devices (comma-separated) # Additional flags self.use_sample = True # Use sample dataset for training # Data configuration self.data = { \"target\": \"mu.algorithms.selective_amnesia.data_handler.SelectiveAmnesiaDataHandler\", \"params\": { \"train_batch_size\": 4, \"val_batch_size\": 6, \"num_workers\": 1, \"num_val_workers\": 0, # Avoid val dataloader issue \"train\": { \"target\": \"stable_diffusion.ldm.data.ForgettingDataset\", \"params\": { \"forget_prompt\": \"An image in Artist_Sketch style\", \"forget_dataset_path\": \"./q_dist/photo_style\", }, }, \"validation\": { \"target\": \"stable_diffusion.ldm.data.VisualizationDataset\", \"params\": { \"output_size\": 512, \"n_gpus\": 1, # Number of GPUs for validation }, }, }, } # Lightning configuration self.lightning = { \"find_unused_parameters\": False, \"modelcheckpoint\": { \"params\": {\"every_n_epochs\": 0, \"save_top_k\": 0, \"monitor\": None} }, \"callbacks\": { \"image_logger\": { \"target\": \"mu.algorithms.selective_amnesia.callbacks.ImageLogger\", \"params\": { \"batch_frequency\": 1, \"max_images\": 999, \"increase_log_steps\": False, \"log_first_step\": False, \"log_all_val\": True, \"clamp\": True, \"log_images_kwargs\": { \"ddim_eta\": 0, \"ddim_steps\": 50, \"use_ema_scope\": True, \"inpaint\": False, \"plot_progressive_rows\": False, \"plot_diffusion_rows\": False, \"N\": 6, # Number of validation prompts \"unconditional_guidance_scale\": 7.5, \"unconditional_guidance_label\": [\"\"], }, }, } }, \"trainer\": { \"benchmark\": True, \"num_sanity_val_steps\": 0, \"max_epochs\": 50, # Modify epochs here! \"check_val_every_n_epoch\": 10, }, } # Update properties based on provided kwargs for key, value in kwargs.items(): setattr(self, key, value) def validate_config(self): \"\"\" Perform basic validation on the config parameters. \"\"\" # Check if necessary directories exist if not os.path.exists(self.raw_dataset_dir): raise FileNotFoundError(f\"Directory {self.raw_dataset_dir} does not exist.\") if not os.path.exists(self.processed_dataset_dir): raise FileNotFoundError( f\"Directory {self.processed_dataset_dir} does not exist.\" ) if not os.path.exists(self.output_dir): os.makedirs(self.output_dir) # Check if model and checkpoint files exist if not os.path.exists(self.model_config_path): raise FileNotFoundError( f\"Model config file {self.model_config_path} does not exist.\" ) if not os.path.exists(self.ckpt_path): raise FileNotFoundError(f\"Checkpoint file {self.ckpt_path} does not exist.\") if not os.path.exists(self.full_fisher_dict_pkl_path): raise FileNotFoundError( f\"Fisher dictionary file {self.full_fisher_dict_pkl_path} does not exist.\" ) # Check if replay prompts file exists if not os.path.exists(self.replay_prompt_path): raise FileNotFoundError( f\"Replay prompt file {self.replay_prompt_path} does not exist.\" ) # Validate dataset type if self.dataset_type not in [\"unlearncanvas\", \"i2p\"]: raise ValueError( f\"Invalid dataset type {self.dataset_type}. Choose from ['unlearncanvas', 'i2p']\" ) # Validate batch sizes if self.data[\"params\"][\"train_batch_size\"] <= 0: raise ValueError(f\"train_batch_size should be a positive integer.\") if self.data[\"params\"][\"val_batch_size\"] <= 0: raise ValueError(f\"val_batch_size should be a positive integer.\") # Validate lightning trainer max_epochs if self.lightning[\"trainer\"][\"max_epochs\"] <= 0: raise ValueError(f\"max_epochs should be a positive integer.\") selective_amnesia_config_quick_canvas = SelectiveAmnesiaConfig() selective_amnesia_config_quick_canvas.dataset_type = \"unlearncanvas\" selective_amnesia_config_quick_canvas.raw_dataset_dir = ( \"data/quick-canvas-dataset/sample\" ) selective_amnesia_config_i2p = SelectiveAmnesiaConfig() selective_amnesia_config_i2p.dataset_type = \"i2p\" selective_amnesia_config_i2p.raw_dataset_dir = \"data/i2p-dataset/sample\"","title":"Train config"},{"location":"unlearn/configs/selective_amnesia/#train-config-yaml-file","text":"# Training parameters seed : 23 scale_lr : True # Model configuration model_config_path: \"mu/algorithms/selective_amnesia/configs/model_config.yaml\" ckpt_path: \"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\" # Checkpoint path for Stable Diffusion full_fisher_dict_pkl_path : \"mu/algorithms/selective_amnesia/data/full_fisher_dict.pkl\" # Dataset directories raw_dataset_dir: \"data/quick-canvas-dataset/sample\" processed_dataset_dir: \"mu/algorithms/selective_amnesia/data\" dataset_type : \"unlearncanvas\" template : \"style\" template_name : \"Abstractionism\" replay_prompt_path: \"mu/algorithms/selective_amnesia/data/fim_prompts_sample.txt\" # Output configurations output_dir: \"outputs/selective_amnesia/finetuned_models\" # Output directory to save results # Sampling and image configurations # Device configuration devices: \"0,\" # CUDA devices to train on (comma-separated) # Additional flags use_sample: True # Use the sample dataset for training data: target: mu.algorithms.selective_amnesia.data_handler.SelectiveAmnesiaDataHandler params: train_batch_size: 4 val_batch_size: 6 num_workers: 4 num_val_workers: 0 # Avoid a weird val dataloader issue (keep unchanged) train: target: stable_diffusion.ldm.data.ForgettingDataset params: forget_prompt: An image in Artist_Sketch style forget_dataset_path: ./q_dist/photo_style validation: target: stable_diffusion.ldm.data.VisualizationDataset params: output_size: 512 n_gpus: 1 # CHANGE THIS TO NUMBER OF GPUS! small hack to sure we see all our logging samples lightning: find_unused_parameters: False modelcheckpoint: params: every_n_epochs: 0 save_top_k: 0 monitor: null callbacks: image_logger: target: mu.algorithms.selective_amnesia.callbacks.ImageLogger params: batch_frequency: 1 max_images: 999 increase_log_steps: False log_first_step: False log_all_val: True clamp: True log_images_kwargs: ddim_eta: 0 ddim_steps: 50 use_ema_scope: True inpaint: False plot_progressive_rows: False plot_diffusion_rows: False N: 6 # keep this the same as number of validation prompts! unconditional_guidance_scale: 7.5 unconditional_guidance_label: [\"\"] trainer: benchmark: True num_sanity_val_steps: 0 max_epochs: 50 # modify epochs here! check_val_every_n_epoch: 10","title":"Train config yaml file"},{"location":"unlearn/configs/semipermeable_membrane/","text":"Train Config class PretrainedModelConfig(BaseConfig): def __init__( self, name_or_path=\"CompVis/stable-diffusion-v1-4\", # Model path or name ckpt_path=\"CompVis/stable-diffusion-v1-4\", # Checkpoint path v2=False, # Version 2 of the model v_pred=False, # Version prediction clip_skip=1, # Skip layers in CLIP model ): self.name_or_path = name_or_path self.ckpt_path = ckpt_path self.v2 = v2 self.v_pred = v_pred self.clip_skip = clip_skip class NetworkConfig(BaseConfig): def __init__( self, rank=1, # Network rank alpha=1.0, # Alpha parameter for the network ): self.rank = rank self.alpha = alpha class TrainConfig(BaseConfig): def __init__( self, precision=\"float32\", # Training precision (e.g., \"float32\" or \"float16\") noise_scheduler=\"ddim\", # Noise scheduler method iterations=3000, # Number of training iterations batch_size=1, # Batch size lr=0.0001, # Learning rate for the model unet_lr=0.0001, # Learning rate for UNet text_encoder_lr=5e-05, # Learning rate for text encoder optimizer_type=\"AdamW8bit\", # Optimizer type (e.g., \"AdamW\", \"AdamW8bit\") lr_scheduler=\"cosine_with_restarts\", # Learning rate scheduler type lr_warmup_steps=500, # Steps for learning rate warm-up lr_scheduler_num_cycles=3, # Number of cycles for the learning rate scheduler max_denoising_steps=30, # Max denoising steps (for DDIM) ): self.precision = precision self.noise_scheduler = noise_scheduler self.iterations = iterations self.batch_size = batch_size self.lr = lr self.unet_lr = unet_lr self.text_encoder_lr = text_encoder_lr self.optimizer_type = optimizer_type self.lr_scheduler = lr_scheduler self.lr_warmup_steps = lr_warmup_steps self.lr_scheduler_num_cycles = lr_scheduler_num_cycles self.max_denoising_steps = max_denoising_steps class SaveConfig(BaseConfig): def __init__( self, per_steps=500, # Save model every N steps precision=\"float32\", # Precision for saving model ): self.per_steps = per_steps self.precision = precision class OtherConfig(BaseConfig): def __init__( self, use_xformers=True, # Whether to use memory-efficient attention with xformers ): self.use_xformers = use_xformers class PromptConfig(BaseConfig): def __init__( self, target=\"Abstractionism\", # Prompt target positive=\"Abstractionism\", # Positive prompt unconditional=\"\", # Unconditional prompt neutral=\"\", # Neutral prompt action=\"erase_with_la\", # Action to perform guidance_scale=\"1.0\", # Guidance scale for generation resolution=512, # Image resolution batch_size=1, # Batch size for prompt generation dynamic_resolution=True, # Flag for dynamic resolution la_strength=1000, # Strength of the latent attention sampling_batch_size=4, # Batch size for sampling ): self.target = target self.positive = positive self.unconditional = unconditional self.neutral = neutral self.action = action self.guidance_scale = guidance_scale self.resolution = resolution self.batch_size = batch_size self.dynamic_resolution = dynamic_resolution self.la_strength = la_strength self.sampling_batch_size = sampling_batch_size class SemipermeableMembraneConfig(BaseConfig): \"\"\" SemipermeableMembraneConfig stores all the configuration parameters for the semipermeable membrane training, including model, network, training, saving, and other environment details. \"\"\" def __init__(self, **kwargs): # Pretrained model configuration self.pretrained_model = PretrainedModelConfig() # Network configuration self.network = NetworkConfig() # Training configuration self.train = TrainConfig() # Save configuration self.save = SaveConfig() # Other settings self.other = OtherConfig() # Weights and Biases (wandb) configuration self.wandb_project = \"semipermeable_membrane_project\" # wandb project name self.wandb_run = \"spm_run\" # wandb run name # Dataset configuration self.use_sample = True # Use sample dataset for training self.dataset_type = ( \"unlearncanvas\" # Dataset type (e.g., \"unlearncanvas\", \"i2p\") ) self.template = \"style\" # Template type (e.g., \"style\", \"object\") self.template_name = \"Abstractionism\" # Template name # Prompt configuration self.prompt = PromptConfig( target=self.template_name, positive=self.template_name, ) # Device configuration self.devices = \"0\" # CUDA devices to train on (comma-separated) # Output configuration self.output_dir = \"outputs/semipermeable_membrane/finetuned_models\" # Directory to save models # Verbose logging self.verbose = True # Whether to log verbose information during training Evaluation config # mu/algorithms/semipermeable_membrane/configs/evaluation_config.py import os from pathlib import Path from mu.core.base_config import BaseConfig current_dir = Path(__file__).parent class SemipermeableMembraneEvaluationConfig(BaseConfig): def __init__(self, **kwargs): self.precision = \"fp32\" # precision for computation self.spm_multiplier = [1.0] # list of semipermeable membrane multipliers self.v2 = False # whether to use version 2 of the model self.matching_metric = \"clipcos_tokenuni\" # matching metric for evaluation self.model_config_path = \"machine_unlearning/mu_semipermeable_membrane_spm/configs\" # path to model config self.base_model = \"CompVis/stable-diffusion-v1-4\" # base model for the algorithm self.spm_path = [\"outputs/semipermeable_membrane/finetuned_models/semipermeable_membrane_Abstractionism_last.safetensors\"] # path to semipermeable membrane model self.ckpt_path = \"outputs/semipermeable_membrane/finetuned_models/semipermeable_membrane_Abstractionism_last.safetensors\" # path to finetuned model checkpoint self.model_ckpt_path = \"CompVis/stable-diffusion-v1-4\" # path to the base model checkpoint self.theme = \"Bricks\" # theme for evaluation self.seed = 188 # random seed self.devices = \"0\" # GPU device ID self.task = \"class\" # task type self.sampler_output_dir = \"outputs/eval_results/mu_results/semipermeable_membrane/\" # directory to save sampler outputs self.seed_list = [188] # list of seeds for evaluation self.classification_model = \"vit_large_patch16_224\" # classification model for evaluation self.eval_output_dir = \"outputs/eval_results/mu_results/semipermeable_membrane/\" # directory to save evaluation results self.reference_dir = \"data/quick-canvas-dataset/sample/\" # path to the original dataset self.forget_theme = \"Bricks\" # theme to forget self.multiprocessing = False # whether to use multiprocessing # Override defaults with any provided kwargs for key, value in kwargs.items(): setattr(self, key, value) def validate_config(self): \"\"\" Perform basic validation on the config parameters. \"\"\" if not os.path.exists(self.model_config_path): raise FileNotFoundError(f\"Model config directory {self.model_config_path} does not exist.\") if not os.path.exists(self.ckpt_path): raise FileNotFoundError(f\"Checkpoint file {self.ckpt_path} does not exist.\") if not os.path.exists(self.reference_dir): raise FileNotFoundError(f\"Reference directory {self.reference_dir} does not exist.\") if not os.path.exists(self.eval_output_dir): os.makedirs(self.eval_output_dir) if not os.path.exists(self.sampler_output_dir): os.makedirs(self.sampler_output_dir) if any(multiplier <= 0 for multiplier in self.spm_multiplier): raise ValueError(\"SPM multiplier values should be positive.\") if self.task not in [\"class\", \"other_task\"]: # Add other valid tasks if needed raise ValueError(\"Invalid task type.\") # Example usage semipermeable_membrane_eval_config = SemipermeableMembraneEvaluationConfig()","title":"Configs"},{"location":"unlearn/configs/semipermeable_membrane/#train-config","text":"class PretrainedModelConfig(BaseConfig): def __init__( self, name_or_path=\"CompVis/stable-diffusion-v1-4\", # Model path or name ckpt_path=\"CompVis/stable-diffusion-v1-4\", # Checkpoint path v2=False, # Version 2 of the model v_pred=False, # Version prediction clip_skip=1, # Skip layers in CLIP model ): self.name_or_path = name_or_path self.ckpt_path = ckpt_path self.v2 = v2 self.v_pred = v_pred self.clip_skip = clip_skip class NetworkConfig(BaseConfig): def __init__( self, rank=1, # Network rank alpha=1.0, # Alpha parameter for the network ): self.rank = rank self.alpha = alpha class TrainConfig(BaseConfig): def __init__( self, precision=\"float32\", # Training precision (e.g., \"float32\" or \"float16\") noise_scheduler=\"ddim\", # Noise scheduler method iterations=3000, # Number of training iterations batch_size=1, # Batch size lr=0.0001, # Learning rate for the model unet_lr=0.0001, # Learning rate for UNet text_encoder_lr=5e-05, # Learning rate for text encoder optimizer_type=\"AdamW8bit\", # Optimizer type (e.g., \"AdamW\", \"AdamW8bit\") lr_scheduler=\"cosine_with_restarts\", # Learning rate scheduler type lr_warmup_steps=500, # Steps for learning rate warm-up lr_scheduler_num_cycles=3, # Number of cycles for the learning rate scheduler max_denoising_steps=30, # Max denoising steps (for DDIM) ): self.precision = precision self.noise_scheduler = noise_scheduler self.iterations = iterations self.batch_size = batch_size self.lr = lr self.unet_lr = unet_lr self.text_encoder_lr = text_encoder_lr self.optimizer_type = optimizer_type self.lr_scheduler = lr_scheduler self.lr_warmup_steps = lr_warmup_steps self.lr_scheduler_num_cycles = lr_scheduler_num_cycles self.max_denoising_steps = max_denoising_steps class SaveConfig(BaseConfig): def __init__( self, per_steps=500, # Save model every N steps precision=\"float32\", # Precision for saving model ): self.per_steps = per_steps self.precision = precision class OtherConfig(BaseConfig): def __init__( self, use_xformers=True, # Whether to use memory-efficient attention with xformers ): self.use_xformers = use_xformers class PromptConfig(BaseConfig): def __init__( self, target=\"Abstractionism\", # Prompt target positive=\"Abstractionism\", # Positive prompt unconditional=\"\", # Unconditional prompt neutral=\"\", # Neutral prompt action=\"erase_with_la\", # Action to perform guidance_scale=\"1.0\", # Guidance scale for generation resolution=512, # Image resolution batch_size=1, # Batch size for prompt generation dynamic_resolution=True, # Flag for dynamic resolution la_strength=1000, # Strength of the latent attention sampling_batch_size=4, # Batch size for sampling ): self.target = target self.positive = positive self.unconditional = unconditional self.neutral = neutral self.action = action self.guidance_scale = guidance_scale self.resolution = resolution self.batch_size = batch_size self.dynamic_resolution = dynamic_resolution self.la_strength = la_strength self.sampling_batch_size = sampling_batch_size class SemipermeableMembraneConfig(BaseConfig): \"\"\" SemipermeableMembraneConfig stores all the configuration parameters for the semipermeable membrane training, including model, network, training, saving, and other environment details. \"\"\" def __init__(self, **kwargs): # Pretrained model configuration self.pretrained_model = PretrainedModelConfig() # Network configuration self.network = NetworkConfig() # Training configuration self.train = TrainConfig() # Save configuration self.save = SaveConfig() # Other settings self.other = OtherConfig() # Weights and Biases (wandb) configuration self.wandb_project = \"semipermeable_membrane_project\" # wandb project name self.wandb_run = \"spm_run\" # wandb run name # Dataset configuration self.use_sample = True # Use sample dataset for training self.dataset_type = ( \"unlearncanvas\" # Dataset type (e.g., \"unlearncanvas\", \"i2p\") ) self.template = \"style\" # Template type (e.g., \"style\", \"object\") self.template_name = \"Abstractionism\" # Template name # Prompt configuration self.prompt = PromptConfig( target=self.template_name, positive=self.template_name, ) # Device configuration self.devices = \"0\" # CUDA devices to train on (comma-separated) # Output configuration self.output_dir = \"outputs/semipermeable_membrane/finetuned_models\" # Directory to save models # Verbose logging self.verbose = True # Whether to log verbose information during training","title":"Train Config"},{"location":"unlearn/configs/semipermeable_membrane/#evaluation-config","text":"# mu/algorithms/semipermeable_membrane/configs/evaluation_config.py import os from pathlib import Path from mu.core.base_config import BaseConfig current_dir = Path(__file__).parent class SemipermeableMembraneEvaluationConfig(BaseConfig): def __init__(self, **kwargs): self.precision = \"fp32\" # precision for computation self.spm_multiplier = [1.0] # list of semipermeable membrane multipliers self.v2 = False # whether to use version 2 of the model self.matching_metric = \"clipcos_tokenuni\" # matching metric for evaluation self.model_config_path = \"machine_unlearning/mu_semipermeable_membrane_spm/configs\" # path to model config self.base_model = \"CompVis/stable-diffusion-v1-4\" # base model for the algorithm self.spm_path = [\"outputs/semipermeable_membrane/finetuned_models/semipermeable_membrane_Abstractionism_last.safetensors\"] # path to semipermeable membrane model self.ckpt_path = \"outputs/semipermeable_membrane/finetuned_models/semipermeable_membrane_Abstractionism_last.safetensors\" # path to finetuned model checkpoint self.model_ckpt_path = \"CompVis/stable-diffusion-v1-4\" # path to the base model checkpoint self.theme = \"Bricks\" # theme for evaluation self.seed = 188 # random seed self.devices = \"0\" # GPU device ID self.task = \"class\" # task type self.sampler_output_dir = \"outputs/eval_results/mu_results/semipermeable_membrane/\" # directory to save sampler outputs self.seed_list = [188] # list of seeds for evaluation self.classification_model = \"vit_large_patch16_224\" # classification model for evaluation self.eval_output_dir = \"outputs/eval_results/mu_results/semipermeable_membrane/\" # directory to save evaluation results self.reference_dir = \"data/quick-canvas-dataset/sample/\" # path to the original dataset self.forget_theme = \"Bricks\" # theme to forget self.multiprocessing = False # whether to use multiprocessing # Override defaults with any provided kwargs for key, value in kwargs.items(): setattr(self, key, value) def validate_config(self): \"\"\" Perform basic validation on the config parameters. \"\"\" if not os.path.exists(self.model_config_path): raise FileNotFoundError(f\"Model config directory {self.model_config_path} does not exist.\") if not os.path.exists(self.ckpt_path): raise FileNotFoundError(f\"Checkpoint file {self.ckpt_path} does not exist.\") if not os.path.exists(self.reference_dir): raise FileNotFoundError(f\"Reference directory {self.reference_dir} does not exist.\") if not os.path.exists(self.eval_output_dir): os.makedirs(self.eval_output_dir) if not os.path.exists(self.sampler_output_dir): os.makedirs(self.sampler_output_dir) if any(multiplier <= 0 for multiplier in self.spm_multiplier): raise ValueError(\"SPM multiplier values should be positive.\") if self.task not in [\"class\", \"other_task\"]: # Add other valid tasks if needed raise ValueError(\"Invalid task type.\") # Example usage semipermeable_membrane_eval_config = SemipermeableMembraneEvaluationConfig()","title":"Evaluation config"},{"location":"unlearn/configs/uce/","text":"Train Config class UnifiedConceptEditingConfig(BaseConfig): def __init__(self, **kwargs): # Training configuration self.train_method = \"full\" # Options: full, partial self.alpha = 0.1 # Guidance factor for training self.epochs = 1 # Number of epochs self.lr = 5e-5 # Learning rate # Model configuration self.ckpt_path = \"models/diffuser/style50\" # Path to model checkpoint # Output configuration self.output_dir = ( \"outputs/uce/finetuned_models\" # Directory to save finetuned models ) self.dataset_type = \"unlearncanvas\" # Type of dataset to be used self.template = \"style\" # Template for training self.template_name = \"Abstractionism\" # Name of the template # Device configuration self.devices = \"0\" # CUDA devices to train on (comma-separated) # Additional flags self.use_sample = True # Whether to use the sample dataset # Editing-specific configuration self.guided_concepts = ( \"A Elephant image\" # Comma-separated string of guided concepts ) self.technique = ( \"replace\" # Technique for editing (Options: \"replace\", \"tensor\") ) # Parameters for the editing technique self.preserve_scale = 0.1 # Scale for preserving the concept (float) self.preserve_number = ( None # Number of concepts to preserve (int, None for all) ) self.erase_scale = 1 # Scale for erasing self.lamb = 0.1 # Regularization weight for loss self.add_prompts = False # Whether to add additional prompts # Preserver concepts (comma-separated if multiple) self.preserver_concepts = ( \"A Lion image\" # Comma-separated string of preserver concepts ) # Base model used for editing self.base = \"stable-diffusion-v1-4\" # Base version of Stable Diffusion Evaluation config # mu/algorithms/unified_concept_editing/configs/evaluation_config.py import os from pathlib import Path from mu.core.base_config import BaseConfig current_dir = Path(__file__).parent class UceEvaluationConfig(BaseConfig): def __init__(self, **kwargs): self.ckpt_path = \"outputs/uce/finetuned_models/uce_Abstractionism_model.pth\" # path to finetuned model checkpoint self.pipeline_path = \"UnlearnCanvas/machine_unlearning/models/diffuser/style50\" # path to pretrained pipeline self.classifier_ckpt_path = \"models/classifier_ckpt_path/style50_cls.pth\" # path to classifier checkpoint self.cfg_text = 9.0 # classifier-free guidance scale self.seed = 188 # random seed self.task = \"class\" # task type self.devices = \"0\" # GPU device ID self.ddim_steps = 100 # number of DDIM steps self.image_height = 512 # height of the image self.image_width = 512 # width of the image self.ddim_eta = 0.0 # DDIM eta parameter self.sampler_output_dir = \"outputs/eval_results/mu_results/uce\" # directory to save sampler outputs self.seed_list = [\"188\"] # list of seeds for evaluation self.batch_size = 1 # batch size for evaluation self.classification_model = \"vit_large_patch16_224\" # classification model for evaluation self.eval_output_dir = \"outputs/eval_results/mu_results/uce\" # directory to save evaluation results self.reference_dir = \"data/quick-canvas-dataset/sample/\" # path to the original dataset self.forget_theme = \"Bricks\" # theme to forget self.multiprocessing = False # whether to use multiprocessing # Override defaults with any provided kwargs for key, value in kwargs.items(): setattr(self, key, value) def validate_config(self): \"\"\" Perform basic validation on the config parameters. \"\"\" if not os.path.exists(self.ckpt_path): raise FileNotFoundError(f\"Checkpoint file {self.ckpt_path} does not exist.\") if not os.path.exists(self.pipeline_path): raise FileNotFoundError(f\"Pipeline directory {self.pipeline_path} does not exist.\") if not os.path.exists(self.classifier_ckpt_path): raise FileNotFoundError(f\"Classifier checkpoint file {self.classifier_ckpt_path} does not exist.\") if not os.path.exists(self.reference_dir): raise FileNotFoundError(f\"Reference directory {self.reference_dir} does not exist.\") if not os.path.exists(self.eval_output_dir): os.makedirs(self.eval_output_dir) if not os.path.exists(self.sampler_output_dir): os.makedirs(self.sampler_output_dir) if self.cfg_text <= 0: raise ValueError(\"Classifier-free guidance scale (cfg_text) should be positive.\") if self.ddim_steps <= 0: raise ValueError(\"DDIM steps should be a positive integer.\") if self.image_height <= 0 or self.image_width <= 0: raise ValueError(\"Image height and width should be positive.\") if self.batch_size <= 0: raise ValueError(\"Batch size should be a positive integer.\") if self.task not in [\"class\", \"other_task\"]: # Add other valid tasks if needed raise ValueError(\"Invalid task type.\") # Example usage uce_evaluation_config = UceEvaluationConfig()","title":"Configs"},{"location":"unlearn/configs/uce/#train-config","text":"class UnifiedConceptEditingConfig(BaseConfig): def __init__(self, **kwargs): # Training configuration self.train_method = \"full\" # Options: full, partial self.alpha = 0.1 # Guidance factor for training self.epochs = 1 # Number of epochs self.lr = 5e-5 # Learning rate # Model configuration self.ckpt_path = \"models/diffuser/style50\" # Path to model checkpoint # Output configuration self.output_dir = ( \"outputs/uce/finetuned_models\" # Directory to save finetuned models ) self.dataset_type = \"unlearncanvas\" # Type of dataset to be used self.template = \"style\" # Template for training self.template_name = \"Abstractionism\" # Name of the template # Device configuration self.devices = \"0\" # CUDA devices to train on (comma-separated) # Additional flags self.use_sample = True # Whether to use the sample dataset # Editing-specific configuration self.guided_concepts = ( \"A Elephant image\" # Comma-separated string of guided concepts ) self.technique = ( \"replace\" # Technique for editing (Options: \"replace\", \"tensor\") ) # Parameters for the editing technique self.preserve_scale = 0.1 # Scale for preserving the concept (float) self.preserve_number = ( None # Number of concepts to preserve (int, None for all) ) self.erase_scale = 1 # Scale for erasing self.lamb = 0.1 # Regularization weight for loss self.add_prompts = False # Whether to add additional prompts # Preserver concepts (comma-separated if multiple) self.preserver_concepts = ( \"A Lion image\" # Comma-separated string of preserver concepts ) # Base model used for editing self.base = \"stable-diffusion-v1-4\" # Base version of Stable Diffusion","title":"Train Config"},{"location":"unlearn/configs/uce/#evaluation-config","text":"# mu/algorithms/unified_concept_editing/configs/evaluation_config.py import os from pathlib import Path from mu.core.base_config import BaseConfig current_dir = Path(__file__).parent class UceEvaluationConfig(BaseConfig): def __init__(self, **kwargs): self.ckpt_path = \"outputs/uce/finetuned_models/uce_Abstractionism_model.pth\" # path to finetuned model checkpoint self.pipeline_path = \"UnlearnCanvas/machine_unlearning/models/diffuser/style50\" # path to pretrained pipeline self.classifier_ckpt_path = \"models/classifier_ckpt_path/style50_cls.pth\" # path to classifier checkpoint self.cfg_text = 9.0 # classifier-free guidance scale self.seed = 188 # random seed self.task = \"class\" # task type self.devices = \"0\" # GPU device ID self.ddim_steps = 100 # number of DDIM steps self.image_height = 512 # height of the image self.image_width = 512 # width of the image self.ddim_eta = 0.0 # DDIM eta parameter self.sampler_output_dir = \"outputs/eval_results/mu_results/uce\" # directory to save sampler outputs self.seed_list = [\"188\"] # list of seeds for evaluation self.batch_size = 1 # batch size for evaluation self.classification_model = \"vit_large_patch16_224\" # classification model for evaluation self.eval_output_dir = \"outputs/eval_results/mu_results/uce\" # directory to save evaluation results self.reference_dir = \"data/quick-canvas-dataset/sample/\" # path to the original dataset self.forget_theme = \"Bricks\" # theme to forget self.multiprocessing = False # whether to use multiprocessing # Override defaults with any provided kwargs for key, value in kwargs.items(): setattr(self, key, value) def validate_config(self): \"\"\" Perform basic validation on the config parameters. \"\"\" if not os.path.exists(self.ckpt_path): raise FileNotFoundError(f\"Checkpoint file {self.ckpt_path} does not exist.\") if not os.path.exists(self.pipeline_path): raise FileNotFoundError(f\"Pipeline directory {self.pipeline_path} does not exist.\") if not os.path.exists(self.classifier_ckpt_path): raise FileNotFoundError(f\"Classifier checkpoint file {self.classifier_ckpt_path} does not exist.\") if not os.path.exists(self.reference_dir): raise FileNotFoundError(f\"Reference directory {self.reference_dir} does not exist.\") if not os.path.exists(self.eval_output_dir): os.makedirs(self.eval_output_dir) if not os.path.exists(self.sampler_output_dir): os.makedirs(self.sampler_output_dir) if self.cfg_text <= 0: raise ValueError(\"Classifier-free guidance scale (cfg_text) should be positive.\") if self.ddim_steps <= 0: raise ValueError(\"DDIM steps should be a positive integer.\") if self.image_height <= 0 or self.image_width <= 0: raise ValueError(\"Image height and width should be positive.\") if self.batch_size <= 0: raise ValueError(\"Batch size should be a positive integer.\") if self.task not in [\"class\", \"other_task\"]: # Add other valid tasks if needed raise ValueError(\"Invalid task type.\") # Example usage uce_evaluation_config = UceEvaluationConfig()","title":"Evaluation config"},{"location":"unlearn/examples/concept_ablation/","text":"Train your model by using Concept Ablation Algorithm. Import pre defined config classes or create your own object. Refer the config docs for details about the parameters that you can use. To test the below code snippet, you can create a file, copy the below code in eg, my_trainer.py and execute it with python my_trainer.py or use WANDB_MODE=offline python my_trainer.py for offline mode. Use pre defined config from mu.algorithms.concept_ablation.algorithm import ( ConceptAblationAlgorithm, ) from mu.algorithms.concept_ablation.configs import ( concept_ablation_train_mu, ConceptAblationConfig, ) if __name__ == \"__main__\": concept_ablation_train_mu.lightning.trainer.max_steps = 5 algorithm = ConceptAblationAlgorithm( concept_ablation_train_mu ) algorithm.run() Modify some train parameters in pre defined config class. View the config docs to see a list of available parameters. from mu.algorithms.concept_ablation.algorithm import ( ConceptAblationAlgorithm, ) from mu.algorithms.concept_ablation.configs import ( concept_ablation_train_mu, ConceptAblationConfig, ) if __name__ == \"__main__\": concept_ablation_train_mu.lightning.trainer.max_steps = 5 algorithm = ConceptAblationAlgorithm( concept_ablation_train_mu, config_path=\"/home/ubuntu/Projects/balaram/msu_unlearningalgorithm/mu/algorithms/concept_ablation/configs/train_config.yaml\", ckpt_path=\"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\", prompts=\"/home/ubuntu/Projects/balaram/msu_unlearningalgorithm/mu/algorithms/concept_ablation/data/anchor_prompts/finetune_prompts/sd_prompt_Architectures_sample.txt\", output_dir=\"/opt/dlami/nvme/outputs\", # devices=\"1\", ) algorithm.run() Create your own config object from mu.algorithms.concept_ablation.algorithm import ( ConceptAblationAlgorithm, ) from mu.algorithms.concept_ablation.configs import ( ConceptAblationConfig, ) myconfig = ConceptAblationConfig() myconfig.ckpt_path = \"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\" myconfig.raw_dataset_dir = ( \"/home/ubuntu/Projects/balaram/packaging/data/quick-canvas-dataset/sample\" ) algorithm = ConceptAblationConfig(myconfig) algorithm.run() Override the Config class itself. from mu.algorithms.concept_ablation.algorithm import ( ConceptAblationAlgorithm, ) from mu.algorithms.concept_ablation.configs import ( ConceptAblationConfig, ) class MyNewConfigClass(ConceptAblationConfig): def __init__(self, *args, **kwargs): self.new_parameter = kwargs.get(\"new_parameter\") super().__init__() new_config_object = MyNewConfigClass() algorithm = ConceptAblationAlgorithm(new_config_object) algorithm.run()","title":"Examples"},{"location":"unlearn/examples/concept_ablation/#use-pre-defined-config","text":"from mu.algorithms.concept_ablation.algorithm import ( ConceptAblationAlgorithm, ) from mu.algorithms.concept_ablation.configs import ( concept_ablation_train_mu, ConceptAblationConfig, ) if __name__ == \"__main__\": concept_ablation_train_mu.lightning.trainer.max_steps = 5 algorithm = ConceptAblationAlgorithm( concept_ablation_train_mu ) algorithm.run()","title":"Use pre defined config"},{"location":"unlearn/examples/concept_ablation/#modify-some-train-parameters-in-pre-defined-config-class","text":"View the config docs to see a list of available parameters. from mu.algorithms.concept_ablation.algorithm import ( ConceptAblationAlgorithm, ) from mu.algorithms.concept_ablation.configs import ( concept_ablation_train_mu, ConceptAblationConfig, ) if __name__ == \"__main__\": concept_ablation_train_mu.lightning.trainer.max_steps = 5 algorithm = ConceptAblationAlgorithm( concept_ablation_train_mu, config_path=\"/home/ubuntu/Projects/balaram/msu_unlearningalgorithm/mu/algorithms/concept_ablation/configs/train_config.yaml\", ckpt_path=\"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\", prompts=\"/home/ubuntu/Projects/balaram/msu_unlearningalgorithm/mu/algorithms/concept_ablation/data/anchor_prompts/finetune_prompts/sd_prompt_Architectures_sample.txt\", output_dir=\"/opt/dlami/nvme/outputs\", # devices=\"1\", ) algorithm.run()","title":"Modify some train parameters in pre defined config class."},{"location":"unlearn/examples/concept_ablation/#create-your-own-config-object","text":"from mu.algorithms.concept_ablation.algorithm import ( ConceptAblationAlgorithm, ) from mu.algorithms.concept_ablation.configs import ( ConceptAblationConfig, ) myconfig = ConceptAblationConfig() myconfig.ckpt_path = \"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\" myconfig.raw_dataset_dir = ( \"/home/ubuntu/Projects/balaram/packaging/data/quick-canvas-dataset/sample\" ) algorithm = ConceptAblationConfig(myconfig) algorithm.run()","title":"Create your own config object"},{"location":"unlearn/examples/concept_ablation/#override-the-config-class-itself","text":"from mu.algorithms.concept_ablation.algorithm import ( ConceptAblationAlgorithm, ) from mu.algorithms.concept_ablation.configs import ( ConceptAblationConfig, ) class MyNewConfigClass(ConceptAblationConfig): def __init__(self, *args, **kwargs): self.new_parameter = kwargs.get(\"new_parameter\") super().__init__() new_config_object = MyNewConfigClass() algorithm = ConceptAblationAlgorithm(new_config_object) algorithm.run()","title":"Override the Config class itself."},{"location":"unlearn/examples/erase_diff/","text":"Train your model by using Erase Diff Algorithm. Import pre defined config classes or create your own object. Refer the config docs for details about the parameters that you can use. To test the below code snippet, you can create a file, copy the below code in eg, my_trainer.py and execute it with python my_trainer.py or use WANDB_MODE=offline python my_trainer.py for offline mode. Use pre defined config from mu.algorithms.erase_diff.algorithm import EraseDiffAlgorithm from mu.algorithms.erase_diff.configs import erase_diff_train_mu algorithm = EraseDiffAlgorithm(erase_diff_train_mu) algorithm.run() Modify some train parameters in pre defined config class. View the config docs to see a list of available parameters. from mu.algorithms.erase_diff.algorithm import EraseDiffAlgorithm from mu.algorithms.erase_diff.configs import ( erase_diff_train_mu, ) algorithm = EraseDiffAlgorithm( erase_diff_train_mu, ckpt_path=\"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\", raw_dataset_dir=( \"/home/ubuntu/Projects/balaram/packaging/data/quick-canvas-dataset/sample\" ), ) algorithm.run() Create your own config object from mu.algorithms.erase_diff.algorithm import EraseDiffAlgorithm from mu.algorithms.erase_diff.configs import ( EraseDiffConfig, ) myconfig = EraseDiffConfig() myconfig.ckpt_path = \"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\" myconfig.raw_dataset_dir = ( \"/home/ubuntu/Projects/balaram/packaging/data/quick-canvas-dataset/sample\" ) algorithm = EraseDiffAlgorithm(myconfig) algorithm.run() Override the Config class itself. from mu.algorithms.erase_diff.algorithm import EraseDiffAlgorithm from mu.algorithms.erase_diff.configs import ( EraseDiffConfig, ) class MyNewConfigClass(EraseDiffConfig): def __init__(self, *args, **kwargs): self.new_parameter = kwargs.get(\"new_parameter\") super().__init__() new_config_object = MyNewConfigClass() algorithm = EraseDiffAlgorithm(new_config_object) algorithm.run()","title":"Examples"},{"location":"unlearn/examples/erase_diff/#use-pre-defined-config","text":"from mu.algorithms.erase_diff.algorithm import EraseDiffAlgorithm from mu.algorithms.erase_diff.configs import erase_diff_train_mu algorithm = EraseDiffAlgorithm(erase_diff_train_mu) algorithm.run()","title":"Use pre defined config"},{"location":"unlearn/examples/erase_diff/#modify-some-train-parameters-in-pre-defined-config-class","text":"View the config docs to see a list of available parameters. from mu.algorithms.erase_diff.algorithm import EraseDiffAlgorithm from mu.algorithms.erase_diff.configs import ( erase_diff_train_mu, ) algorithm = EraseDiffAlgorithm( erase_diff_train_mu, ckpt_path=\"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\", raw_dataset_dir=( \"/home/ubuntu/Projects/balaram/packaging/data/quick-canvas-dataset/sample\" ), ) algorithm.run()","title":"Modify some train parameters in pre defined config class."},{"location":"unlearn/examples/erase_diff/#create-your-own-config-object","text":"from mu.algorithms.erase_diff.algorithm import EraseDiffAlgorithm from mu.algorithms.erase_diff.configs import ( EraseDiffConfig, ) myconfig = EraseDiffConfig() myconfig.ckpt_path = \"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\" myconfig.raw_dataset_dir = ( \"/home/ubuntu/Projects/balaram/packaging/data/quick-canvas-dataset/sample\" ) algorithm = EraseDiffAlgorithm(myconfig) algorithm.run()","title":"Create your own config object"},{"location":"unlearn/examples/erase_diff/#override-the-config-class-itself","text":"from mu.algorithms.erase_diff.algorithm import EraseDiffAlgorithm from mu.algorithms.erase_diff.configs import ( EraseDiffConfig, ) class MyNewConfigClass(EraseDiffConfig): def __init__(self, *args, **kwargs): self.new_parameter = kwargs.get(\"new_parameter\") super().__init__() new_config_object = MyNewConfigClass() algorithm = EraseDiffAlgorithm(new_config_object) algorithm.run()","title":"Override the Config class itself."},{"location":"unlearn/examples/esd/","text":"Train your model by using Esd Algorithm. Import pre defined config classes or create your own object. Refer the config docs for details about the parameters that you can use. To test the below code snippet, you can create a file, copy the below code in eg, my_trainer.py and execute it with python my_trainer.py or use WANDB_MODE=offline python my_trainer.py for offline mode. Use pre defined config from mu.algorithms.esd.algorithm import ESDAlgorithm from mu.algorithms.esd.configs import esd_train_mu algorithm = ESDAlgorithm(esd_train_mu) algorithm.run() Modify some train parameters in pre defined config class. from mu.algorithms.esd.algorithm import ESDAlgorithm from mu.algorithms.esd.configs import ( esd_train_mu, ) algorithm = ESDAlgorithm( esd_train_mu, ckpt_path=\"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\", raw_dataset_dir=( \"/home/ubuntu/Projects/balaram/packaging/data/quick-canvas-dataset/sample\" ), ) algorithm.run() Create your own config object from mu.algorithms.esd.algorithm import ESDAlgorithm from mu.algorithms.esd.configs import ( ESDConfig, ) myconfig = ESDConfig() myconfig.ckpt_path = \"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\" myconfig.raw_dataset_dir = ( \"/home/ubuntu/Projects/balaram/packaging/data/quick-canvas-dataset/sample\" ) algorithm = ESDAlgorithm(myconfig) algorithm.run() Override the Config class itself. from mu.algorithms.esd.algorithm import ESDAlgorithm from mu.algorithms.esd.configs import ( ESDConfig, ) class MyNewConfigClass(ESDConfig): def __init__(self, *args, **kwargs): self.new_parameter = kwargs.get(\"new_parameter\") super().__init__() new_config_object = MyNewConfigClass() algorithm = ESDAlgorithm(new_config_object) algorithm.run()","title":"Examples"},{"location":"unlearn/examples/esd/#use-pre-defined-config","text":"from mu.algorithms.esd.algorithm import ESDAlgorithm from mu.algorithms.esd.configs import esd_train_mu algorithm = ESDAlgorithm(esd_train_mu) algorithm.run()","title":"Use pre defined config"},{"location":"unlearn/examples/esd/#modify-some-train-parameters-in-pre-defined-config-class","text":"from mu.algorithms.esd.algorithm import ESDAlgorithm from mu.algorithms.esd.configs import ( esd_train_mu, ) algorithm = ESDAlgorithm( esd_train_mu, ckpt_path=\"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\", raw_dataset_dir=( \"/home/ubuntu/Projects/balaram/packaging/data/quick-canvas-dataset/sample\" ), ) algorithm.run()","title":"Modify some train parameters in pre defined config class."},{"location":"unlearn/examples/esd/#create-your-own-config-object","text":"from mu.algorithms.esd.algorithm import ESDAlgorithm from mu.algorithms.esd.configs import ( ESDConfig, ) myconfig = ESDConfig() myconfig.ckpt_path = \"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\" myconfig.raw_dataset_dir = ( \"/home/ubuntu/Projects/balaram/packaging/data/quick-canvas-dataset/sample\" ) algorithm = ESDAlgorithm(myconfig) algorithm.run()","title":"Create your own config object"},{"location":"unlearn/examples/esd/#override-the-config-class-itself","text":"from mu.algorithms.esd.algorithm import ESDAlgorithm from mu.algorithms.esd.configs import ( ESDConfig, ) class MyNewConfigClass(ESDConfig): def __init__(self, *args, **kwargs): self.new_parameter = kwargs.get(\"new_parameter\") super().__init__() new_config_object = MyNewConfigClass() algorithm = ESDAlgorithm(new_config_object) algorithm.run()","title":"Override the Config class itself."},{"location":"unlearn/examples/forget_me_not/","text":"You can modify the parameters, when using config class itself. View the config docs to see a list of available parameters that you can use. Train a text inversion (train_ti) from mu.algorithms.forget_me_not.algorithm import ForgetMeNotAlgorithm from mu.algorithms.forget_me_not.configs import ( forget_me_not_train_ti_mu, ) algorithm = ForgetMeNotAlgorithm( forget_me_not_train_ti_mu, ckpt_path=\"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/diffuser/style50\", raw_dataset_dir=( \"/home/ubuntu/Projects/balaram/packaging/data/quick-canvas-dataset/sample\" ), steps=10 ) algorithm.run(train_type=\"train_ti\") Perform unlearning by using train attn. Before running the train_attn script, update the ti_weights_path parameter in the configuration file to point to the output generated from the Text Inversion (train_ti.py) stage from mu.algorithms.forget_me_not.algorithm import ForgetMeNotAlgorithm from mu.algorithms.forget_me_not.configs import ( forget_me_not_train_attn_mu, ) algorithm = ForgetMeNotAlgorithm( forget_me_not_train_attn_mu, ckpt_path=\"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/diffuser/style50\", raw_dataset_dir=( \"/home/ubuntu/Projects/balaram/packaging/data/quick-canvas-dataset/sample\" ), steps=10, ti_weights_path=\"outputs/forget_me_not/finetuned_models/Abstractionism/step_inv_10.safetensors\" ) algorithm.run(train_type=\"train_attn\")","title":"Examples"},{"location":"unlearn/examples/forget_me_not/#train-a-text-inversion-train_ti","text":"from mu.algorithms.forget_me_not.algorithm import ForgetMeNotAlgorithm from mu.algorithms.forget_me_not.configs import ( forget_me_not_train_ti_mu, ) algorithm = ForgetMeNotAlgorithm( forget_me_not_train_ti_mu, ckpt_path=\"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/diffuser/style50\", raw_dataset_dir=( \"/home/ubuntu/Projects/balaram/packaging/data/quick-canvas-dataset/sample\" ), steps=10 ) algorithm.run(train_type=\"train_ti\")","title":"Train a text inversion (train_ti)"},{"location":"unlearn/examples/forget_me_not/#perform-unlearning-by-using-train-attn","text":"Before running the train_attn script, update the ti_weights_path parameter in the configuration file to point to the output generated from the Text Inversion (train_ti.py) stage from mu.algorithms.forget_me_not.algorithm import ForgetMeNotAlgorithm from mu.algorithms.forget_me_not.configs import ( forget_me_not_train_attn_mu, ) algorithm = ForgetMeNotAlgorithm( forget_me_not_train_attn_mu, ckpt_path=\"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/diffuser/style50\", raw_dataset_dir=( \"/home/ubuntu/Projects/balaram/packaging/data/quick-canvas-dataset/sample\" ), steps=10, ti_weights_path=\"outputs/forget_me_not/finetuned_models/Abstractionism/step_inv_10.safetensors\" ) algorithm.run(train_type=\"train_attn\")","title":"Perform unlearning by using train attn."},{"location":"unlearn/examples/saliency/","text":"Train your model by using Saliency Unlearning Algorithm. Import pre defined config classes or create your own object. Refer to the config docs for details about the parameters that you can use. To test the below code snippet, you can create a file, copy the below code in, e.g., my_trainer.py and execute it with python my_trainer.py or use WANDB_MODE=offline python my_trainer.py for offline mode. Use Pre-defined config class from mu.algorithms.saliency_unlearning.algorithm import ( SaliencyUnlearningAlgorithm, ) from mu.algorithms.saliency_unlearning.configs import saliency_unlearning_train_mu algorithm = SaliencyUnlearningAlgorithm(saliency_unlearning_train_mu) algorithm.run() Modify some parameters in pre-defined config class from mu.algorithms.saliency_unlearning.algorithm import ( SaliencyUnlearningAlgorithm, ) from mu.algorithms.saliency_unlearning.configs import ( saliency_unlearning_train_mu, ) algorithm = SaliencyUnlearningAlgorithm( saliency_unlearning_train_mu, output_dir=\"/opt/dlami/nvme/outputs\", ) algorithm.run() Create your own config object from mu.algorithms.saliency_unlearning.algorithm import SaliencyUnlearningAlgorithm from mu.algorithms.saliency_unlearning.configs import ( SaliencyUnlearningConfig, ) myconfig = SaliencyUnlearningConfig() myconfig.output_dir = ( \"/opt/dlami/nvme/outputs\" ) algorithm = SaliencyUnlearningAlgorithm(myconfig) algorithm.run() Override the Config class itself from mu.algorithms.saliency_unlearning.algorithm import SaliencyUnlearningAlgorithm from mu.algorithms.saliency_unlearning.configs import ( SaliencyUnlearningConfig, ) class MyNewConfigClass(SaliencyUnlearningConfig): def __init__(self, *args, **kwargs): self.new_parameter = kwargs.get(\"new_parameter\") super().__init__() new_config_object = MyNewConfigClass() algorithm = SaliencyUnlearningAlgorithm(new_config_object) algorithm.run()","title":"Examples"},{"location":"unlearn/examples/saliency/#use-pre-defined-config-class","text":"from mu.algorithms.saliency_unlearning.algorithm import ( SaliencyUnlearningAlgorithm, ) from mu.algorithms.saliency_unlearning.configs import saliency_unlearning_train_mu algorithm = SaliencyUnlearningAlgorithm(saliency_unlearning_train_mu) algorithm.run()","title":"Use Pre-defined config class"},{"location":"unlearn/examples/saliency/#modify-some-parameters-in-pre-defined-config-class","text":"from mu.algorithms.saliency_unlearning.algorithm import ( SaliencyUnlearningAlgorithm, ) from mu.algorithms.saliency_unlearning.configs import ( saliency_unlearning_train_mu, ) algorithm = SaliencyUnlearningAlgorithm( saliency_unlearning_train_mu, output_dir=\"/opt/dlami/nvme/outputs\", ) algorithm.run()","title":"Modify some parameters in pre-defined config class"},{"location":"unlearn/examples/saliency/#create-your-own-config-object","text":"from mu.algorithms.saliency_unlearning.algorithm import SaliencyUnlearningAlgorithm from mu.algorithms.saliency_unlearning.configs import ( SaliencyUnlearningConfig, ) myconfig = SaliencyUnlearningConfig() myconfig.output_dir = ( \"/opt/dlami/nvme/outputs\" ) algorithm = SaliencyUnlearningAlgorithm(myconfig) algorithm.run()","title":"Create your own config object"},{"location":"unlearn/examples/saliency/#override-the-config-class-itself","text":"from mu.algorithms.saliency_unlearning.algorithm import SaliencyUnlearningAlgorithm from mu.algorithms.saliency_unlearning.configs import ( SaliencyUnlearningConfig, ) class MyNewConfigClass(SaliencyUnlearningConfig): def __init__(self, *args, **kwargs): self.new_parameter = kwargs.get(\"new_parameter\") super().__init__() new_config_object = MyNewConfigClass() algorithm = SaliencyUnlearningAlgorithm(new_config_object) algorithm.run()","title":"Override the Config class itself"},{"location":"unlearn/examples/scissorhands/","text":"Train your model by using ScissorHands Algorithm. Import pre defined config classes or create your own object. Refer the config docs for details about the parameters that you can use. To test the below code snippet, you can create a file, copy the below code in eg, my_trainer.py and execute it with python my_trainer.py or use WANDB_MODE=offline python my_trainer.py for offline mode. Use pre defined config from mu.algorithms.scissorhands.algorithm import ScissorHandsAlgorithm from mu.algorithms.scissorhands.configs import scissorhands_train_mu algorithm = ScissorHandsAlgorithm(scissorhands_train_mu) algorithm.run() Modify some train parameters in pre defined config class. View the config docs to see a list of available parameters. from mu.algorithms.scissorhands.algorithm import ScissorHandsAlgorithm from mu.algorithms.scissorhands.configs import ( scissorhands_train_mu, ) algorithm = ScissorHandsAlgorithm( scissorhands_train_mu, ckpt_path=\"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\", raw_dataset_dir=( \"/home/ubuntu/Projects/balaram/packaging/data/quick-canvas-dataset/sample\" ), output_dir=\"/opt/dlami/nvme/outputs\", ) algorithm.run() Create your own config object from mu.algorithms.scissorhands.algorithm import ScissorHandsAlgorithm from mu.algorithms.scissorhands.configs import ( ScissorHandsConfig, ) myconfig = ScissorHandsConfig() myconfig.ckpt_path = \"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\" myconfig.raw_dataset_dir = ( \"/home/ubuntu/Projects/balaram/packaging/data/quick-canvas-dataset/sample\" ) algorithm = ScissorHandsAlgorithm(myconfig) algorithm.run() Override the Config class itself. from mu.algorithms.scissorhands.algorithm import ScissorHandsAlgorithm from mu.algorithms.scissorhands.configs import ( ScissorHandsConfig, ) class MyNewConfigClass(ScissorHandsConfig): def __init__(self, *args, **kwargs): self.new_parameter = kwargs.get(\"new_parameter\") super().__init__() new_config_object = MyNewConfigClass() algorithm = ScissorHandsAlgorithm(new_config_object) algorithm.run()","title":"Examples"},{"location":"unlearn/examples/scissorhands/#use-pre-defined-config","text":"from mu.algorithms.scissorhands.algorithm import ScissorHandsAlgorithm from mu.algorithms.scissorhands.configs import scissorhands_train_mu algorithm = ScissorHandsAlgorithm(scissorhands_train_mu) algorithm.run()","title":"Use pre defined config"},{"location":"unlearn/examples/scissorhands/#modify-some-train-parameters-in-pre-defined-config-class","text":"View the config docs to see a list of available parameters. from mu.algorithms.scissorhands.algorithm import ScissorHandsAlgorithm from mu.algorithms.scissorhands.configs import ( scissorhands_train_mu, ) algorithm = ScissorHandsAlgorithm( scissorhands_train_mu, ckpt_path=\"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\", raw_dataset_dir=( \"/home/ubuntu/Projects/balaram/packaging/data/quick-canvas-dataset/sample\" ), output_dir=\"/opt/dlami/nvme/outputs\", ) algorithm.run()","title":"Modify some train parameters in pre defined config class."},{"location":"unlearn/examples/scissorhands/#create-your-own-config-object","text":"from mu.algorithms.scissorhands.algorithm import ScissorHandsAlgorithm from mu.algorithms.scissorhands.configs import ( ScissorHandsConfig, ) myconfig = ScissorHandsConfig() myconfig.ckpt_path = \"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\" myconfig.raw_dataset_dir = ( \"/home/ubuntu/Projects/balaram/packaging/data/quick-canvas-dataset/sample\" ) algorithm = ScissorHandsAlgorithm(myconfig) algorithm.run()","title":"Create your own config object"},{"location":"unlearn/examples/scissorhands/#override-the-config-class-itself","text":"from mu.algorithms.scissorhands.algorithm import ScissorHandsAlgorithm from mu.algorithms.scissorhands.configs import ( ScissorHandsConfig, ) class MyNewConfigClass(ScissorHandsConfig): def __init__(self, *args, **kwargs): self.new_parameter = kwargs.get(\"new_parameter\") super().__init__() new_config_object = MyNewConfigClass() algorithm = ScissorHandsAlgorithm(new_config_object) algorithm.run()","title":"Override the Config class itself."},{"location":"unlearn/examples/selective_amnesia/","text":"To perform training using selective amnesia. You'll need to download the full fisher file first. Download, it at mu/algorithms/selective_amnesia/data folder. Use the following command wget https://huggingface.co/ajrheng/selective-amnesia/resolve/main/full_fisher_dict.pkl Use pre defined config from mu.algorithms.selective_amnesia.algorithm import SelectiveAmnesiaAlgorithm from mu.algorithms.selective_amnesia.configs import ( selective_amnesia_config_quick_canvas, ) algorithm = SelectiveAmnesiaAlgorithm( selective_amnesia_config_quick_canvas ) algorithm.run() Modify some train parameters in pre defined config class. View the config docs to see a list of available parameters. from mu.algorithms.selective_amnesia.algorithm import SelectiveAmnesiaAlgorithm from mu.algorithms.selective_amnesia.configs import ( selective_amnesia_config_quick_canvas, ) algorithm = SelectiveAmnesiaAlgorithm( selective_amnesia_config_quick_canvas, ckpt_path=\"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\", raw_dataset_dir=( \"/home/ubuntu/Projects/balaram/packaging/data/quick-canvas-dataset/sample\" ), ) algorithm.run() Create your own config object from mu.algorithms.selective_amnesia.algorithm import SelectiveAmnesiaAlgorithm from mu.algorithms.selective_amnesia.configs import ( SelectiveAmnesiaConfig, ) myconfig = SelectiveAmnesiaConfig() myconfig.ckpt_path = \"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\" myconfig.raw_dataset_dir = ( \"/home/ubuntu/Projects/balaram/packaging/data/quick-canvas-dataset/sample\" ) algorithm = SelectiveAmnesiaAlgorithm(myconfig) algorithm.run() Override the Config class itself. from mu.algorithms.selective_amnesia.algorithm import SelectiveAmnesiaAlgorithm from mu.algorithms.selective_amnesia.configs import ( SelectiveAmnesiaConfig, ) class MyNewConfigClass(SelectiveAmnesiaAlgorithm): def __init__(self, *args, **kwargs): self.new_parameter = kwargs.get(\"new_parameter\") super().__init__() new_config_object = MyNewConfigClass() algorithm = SelectiveAmnesiaAlgorithm(new_config_object) algorithm.run()","title":"Examples"},{"location":"unlearn/examples/selective_amnesia/#use-pre-defined-config","text":"from mu.algorithms.selective_amnesia.algorithm import SelectiveAmnesiaAlgorithm from mu.algorithms.selective_amnesia.configs import ( selective_amnesia_config_quick_canvas, ) algorithm = SelectiveAmnesiaAlgorithm( selective_amnesia_config_quick_canvas ) algorithm.run()","title":"Use pre defined config"},{"location":"unlearn/examples/selective_amnesia/#modify-some-train-parameters-in-pre-defined-config-class","text":"View the config docs to see a list of available parameters. from mu.algorithms.selective_amnesia.algorithm import SelectiveAmnesiaAlgorithm from mu.algorithms.selective_amnesia.configs import ( selective_amnesia_config_quick_canvas, ) algorithm = SelectiveAmnesiaAlgorithm( selective_amnesia_config_quick_canvas, ckpt_path=\"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\", raw_dataset_dir=( \"/home/ubuntu/Projects/balaram/packaging/data/quick-canvas-dataset/sample\" ), ) algorithm.run()","title":"Modify some train parameters in pre defined config class."},{"location":"unlearn/examples/selective_amnesia/#create-your-own-config-object","text":"from mu.algorithms.selective_amnesia.algorithm import SelectiveAmnesiaAlgorithm from mu.algorithms.selective_amnesia.configs import ( SelectiveAmnesiaConfig, ) myconfig = SelectiveAmnesiaConfig() myconfig.ckpt_path = \"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\" myconfig.raw_dataset_dir = ( \"/home/ubuntu/Projects/balaram/packaging/data/quick-canvas-dataset/sample\" ) algorithm = SelectiveAmnesiaAlgorithm(myconfig) algorithm.run()","title":"Create your own config object"},{"location":"unlearn/examples/selective_amnesia/#override-the-config-class-itself","text":"from mu.algorithms.selective_amnesia.algorithm import SelectiveAmnesiaAlgorithm from mu.algorithms.selective_amnesia.configs import ( SelectiveAmnesiaConfig, ) class MyNewConfigClass(SelectiveAmnesiaAlgorithm): def __init__(self, *args, **kwargs): self.new_parameter = kwargs.get(\"new_parameter\") super().__init__() new_config_object = MyNewConfigClass() algorithm = SelectiveAmnesiaAlgorithm(new_config_object) algorithm.run()","title":"Override the Config class itself."},{"location":"unlearn/examples/semipermeable_membrane/","text":"Train your model by using Semi Permeable Membrane Algorithm. Import pre defined config classes or create your own object. Refer the config docs for details about the parameters that you can use. To test the below code snippet, you can create a file, copy the below code in eg, my_trainer.py and execute it with python my_trainer.py or use WANDB_MODE=offline python my_trainer.py for offline mode. Use Pre defined config class from mu.algorithms.semipermeable_membrane.algorithm import ( SemipermeableMembraneAlgorithm, ) from mu.algorithms.semipermeable_membrane.configs import semipermiable_membrane_train_config_quick_canvas algorithm = SemipermeableMembraneAlgorithm(semipermiable_membrane_train_config_quick_canvas) algorithm.run() Modify some parameters in pre defined config class Use config docs to view available options. You can update values within dictionaries by passing only the value that you want to change as below in when passing train={'iterations :1}`. from mu.algorithms.semipermeable_membrane.algorithm import ( SemipermeableMembraneAlgorithm, ) from mu.algorithms.semipermeable_membrane.configs import ( semipermiable_membrane_train_mu, SemipermeableMembraneConfig, ) algorithm = SemipermeableMembraneAlgorithm( semipermiable_membrane_train_mu, output_dir=\"/opt/dlami/nvme/outputs\", train={\"iterations\": 2}, ) algorithm.run() Create your own config object from mu.algorithms.semipermeable_membrane.algorithm import SemipermeableMembraneAlgorithm from mu.algorithms.semipermeable_membrane.configs import ( SemipermeableMembraneConfig, ) myconfig = SemipermeableMembraneConfig() myconfig.output_dir = ( \"/opt/dlami/nvme/outputs\" ) algorithm = SemipermeableMembraneAlgorithm(myconfig) algorithm.run() Override the Config class itself. from mu.algorithms.semipermeable_membrane.algorithm import SemipermeableMembraneAlgorithm from mu.algorithms.semipermeable_membrane.configs import ( SemipermeableMembraneConfig, ) class MyNewConfigClass(SemipermeableMembraneConfig): def __init__(self, *args, **kwargs): self.new_parameter = kwargs.get(\"new_parameter\") super().__init__() new_config_object = MyNewConfigClass() algorithm = SemipermeableMembraneAlgorithm(new_config_object) algorithm.run()","title":"Examples"},{"location":"unlearn/examples/semipermeable_membrane/#use-pre-defined-config-class","text":"from mu.algorithms.semipermeable_membrane.algorithm import ( SemipermeableMembraneAlgorithm, ) from mu.algorithms.semipermeable_membrane.configs import semipermiable_membrane_train_config_quick_canvas algorithm = SemipermeableMembraneAlgorithm(semipermiable_membrane_train_config_quick_canvas) algorithm.run()","title":"Use Pre defined config class"},{"location":"unlearn/examples/semipermeable_membrane/#modify-some-parameters-in-pre-defined-config-class","text":"Use config docs to view available options. You can update values within dictionaries by passing only the value that you want to change as below in when passing train={'iterations :1}`. from mu.algorithms.semipermeable_membrane.algorithm import ( SemipermeableMembraneAlgorithm, ) from mu.algorithms.semipermeable_membrane.configs import ( semipermiable_membrane_train_mu, SemipermeableMembraneConfig, ) algorithm = SemipermeableMembraneAlgorithm( semipermiable_membrane_train_mu, output_dir=\"/opt/dlami/nvme/outputs\", train={\"iterations\": 2}, ) algorithm.run()","title":"Modify some parameters in pre defined config class"},{"location":"unlearn/examples/semipermeable_membrane/#create-your-own-config-object","text":"from mu.algorithms.semipermeable_membrane.algorithm import SemipermeableMembraneAlgorithm from mu.algorithms.semipermeable_membrane.configs import ( SemipermeableMembraneConfig, ) myconfig = SemipermeableMembraneConfig() myconfig.output_dir = ( \"/opt/dlami/nvme/outputs\" ) algorithm = SemipermeableMembraneAlgorithm(myconfig) algorithm.run()","title":"Create your own config object"},{"location":"unlearn/examples/semipermeable_membrane/#override-the-config-class-itself","text":"from mu.algorithms.semipermeable_membrane.algorithm import SemipermeableMembraneAlgorithm from mu.algorithms.semipermeable_membrane.configs import ( SemipermeableMembraneConfig, ) class MyNewConfigClass(SemipermeableMembraneConfig): def __init__(self, *args, **kwargs): self.new_parameter = kwargs.get(\"new_parameter\") super().__init__() new_config_object = MyNewConfigClass() algorithm = SemipermeableMembraneAlgorithm(new_config_object) algorithm.run()","title":"Override the Config class itself."},{"location":"unlearn/examples/uce/","text":"Train your model by using Unified Concept Editing Algorithm. Import pre defined config classes or create your own object. Refer the config docs for details about the parameters that you can use. To test the below code snippet, you can create a file, copy the below code in eg, my_trainer.py and execute it with python my_trainer.py or use WANDB_MODE=offline python my_trainer.py for offline mode. Use Pre defined config class from mu.algorithms.unified_concept_editing.algorithm import ( UnifiedConceptEditingAlgorithm, ) from mu.algorithms.unified_concept_editing.configs import unified_concept_editing_train_mu algorithm = UnifiedConceptEditingAlgorithm(unified_concept_editing_train_mu) algorithm.run() Modify some parameters in pre defined config class from mu.algorithms.unified_concept_editing.algorithm import ( UnifiedConceptEditingAlgorithm, ) from mu.algorithms.unified_concept_editing.configs import ( unified_concept_editing_train_mu, ) algorithm = UnifiedConceptEditingAlgorithm( unified_concept_editing_train_mu, ckpt_path=\"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/diffuser/style50/\", raw_dataset_dir=( \"/home/ubuntu/Projects/balaram/packaging/data/quick-canvas-dataset/sample\" ), output_dir=\"/opt/dlami/nvme/outputs\", ) algorithm.run() Create your own config object from mu.algorithms.unified_concept_editing.algorithm import UnifiedConceptEditingAlgorithm from mu.algorithms.unified_concept_editing.configs import ( UnifiedConceptEditingConfig, ) myconfig = UnifiedConceptEditingConfig() myconfig.ckpt_path = \"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\" myconfig.raw_dataset_dir = ( \"/home/ubuntu/Projects/balaram/packaging/data/quick-canvas-dataset/sample\" ) algorithm = UnifiedConceptEditingAlgorithm(myconfig) algorithm.run() Override the Config class itself. from mu.algorithms.unified_concept_editing.algorithm import UnifiedConceptEditingAlgorithm from mu.algorithms.unified_concept_editing.configs import ( UnifiedConceptEditingConfig, ) UnifiedConceptEditingAlgorithm class MyNewConfigClass(UnifiedConceptEditingConfig): def __init__(self, *args, **kwargs): self.new_parameter = kwargs.get(\"new_parameter\") super().__init__() new_config_object = MyNewConfigClass() algorithm = UnifiedConceptEditingAlgorithm(new_config_object) algorithm.run()","title":"Examples"},{"location":"unlearn/examples/uce/#use-pre-defined-config-class","text":"from mu.algorithms.unified_concept_editing.algorithm import ( UnifiedConceptEditingAlgorithm, ) from mu.algorithms.unified_concept_editing.configs import unified_concept_editing_train_mu algorithm = UnifiedConceptEditingAlgorithm(unified_concept_editing_train_mu) algorithm.run()","title":"Use Pre defined config class"},{"location":"unlearn/examples/uce/#modify-some-parameters-in-pre-defined-config-class","text":"from mu.algorithms.unified_concept_editing.algorithm import ( UnifiedConceptEditingAlgorithm, ) from mu.algorithms.unified_concept_editing.configs import ( unified_concept_editing_train_mu, ) algorithm = UnifiedConceptEditingAlgorithm( unified_concept_editing_train_mu, ckpt_path=\"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/diffuser/style50/\", raw_dataset_dir=( \"/home/ubuntu/Projects/balaram/packaging/data/quick-canvas-dataset/sample\" ), output_dir=\"/opt/dlami/nvme/outputs\", ) algorithm.run()","title":"Modify some parameters in pre defined config class"},{"location":"unlearn/examples/uce/#create-your-own-config-object","text":"from mu.algorithms.unified_concept_editing.algorithm import UnifiedConceptEditingAlgorithm from mu.algorithms.unified_concept_editing.configs import ( UnifiedConceptEditingConfig, ) myconfig = UnifiedConceptEditingConfig() myconfig.ckpt_path = \"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\" myconfig.raw_dataset_dir = ( \"/home/ubuntu/Projects/balaram/packaging/data/quick-canvas-dataset/sample\" ) algorithm = UnifiedConceptEditingAlgorithm(myconfig) algorithm.run()","title":"Create your own config object"},{"location":"unlearn/examples/uce/#override-the-config-class-itself","text":"from mu.algorithms.unified_concept_editing.algorithm import UnifiedConceptEditingAlgorithm from mu.algorithms.unified_concept_editing.configs import ( UnifiedConceptEditingConfig, ) UnifiedConceptEditingAlgorithm class MyNewConfigClass(UnifiedConceptEditingConfig): def __init__(self, *args, **kwargs): self.new_parameter = kwargs.get(\"new_parameter\") super().__init__() new_config_object = MyNewConfigClass() algorithm = UnifiedConceptEditingAlgorithm(new_config_object) algorithm.run()","title":"Override the Config class itself."}]}