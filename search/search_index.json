{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Unlearn Diff Unlearn Diff is an open-source Python package designed to streamline the development of unlearning algorithms and establish a standardized evaluation pipeline for diffusion models. It provides researchers and practitioners with tools to implement, evaluate, and extend unlearning algorithms effectively. Documentation You can find the full documentation for this project at the url given below. https://ramailotech.github.io/msu_unlearningalgorithm/ Features Comprehensive Algorithm Support : Includes commonly used concept erasing and machine unlearning algorithms tailored for diffusion models. Each algorithm is encapsulated and standardized in terms of input-output formats. Automated Evaluation : Supports automatic evaluation on datasets like UnlearnCanvas or IP2P. Performs standard and adversarial evaluations, outputting metrics as detailed in UnlearnCanvas and UnlearnDiffAtk. Extensibility : Designed for easy integration of new unlearning algorithms, attack methods, defense mechanisms, and datasets with minimal modifications. Supported Algorithms The initial version includes established methods benchmarked in UnlearnCanvas and defensive unlearning techniques: CA (Concept Ablation) ED (Erase Diff) ESD (Efficient Substitution Distillation) FMN (Forget Me Not) SU (Saliency Unlearning) SH (ScissorHands) SA (Selective Amnesia) SPM (Semi Permeable Membrane) UCE (Unified Concept Editing) For detailed information on each algorithm, please refer to the respective README.md files located inside mu/algorithms . Project Architecture The project is organized to facilitate scalability and maintainability. data/ : Stores data-related files. i2p-dataset/ : contains i2p-dataset sample/ : Sample dataset full/ : Full dataset quick-canvas-dataset/ : contains quick canvas dataset sample/ : Sample dataset full/ : Full dataset docs/ : Documentation, including API references and user guides. outputs/ : Outputs of the trained algorithms. examples/ : Sample code and notebooks demonstrating usage. logs/ : Log files for debugging and auditing. models/ : Repository of lora_diffusion and stable_diffusion. evaluation/ : Contains metrics for evalaution. core/ :Foundational classes. base_evaluator.py : Base class for evaluation. mu_defense_base_image_generator.py : Base class for image generation. helpers/ : Utility functions and helpers. parser.py : Parse attack logs for evaluation. utils.py : Utility function. metrics/ : Contains metrics for evalaution. accuracy.py asr.py clip.py fid.py mu/ : Core source code. algorithms/ : Implementation of various algorithms. Each algorithm has its own subdirectory containing code and a README.md with detailed documentation. esd/ : ESD algorithm components. README.md : Documentation specific to the ESD algorithm. algorithm.py : Core implementation of ESD. configs/ : Configuration files for training and generation tasks. constants/const.py : Constant values used across the ESD algorithm. environment.yaml : Environment setup for ESD. model.py : Model architectures specific to ESD. sampler.py : Sampling methods used during training or inference. scripts/train.py : Training script for ESD. evaluator.py : Script that generates necessary outputs for evaluation. trainer.py : Training routines and optimization strategies. utils.py : Utility functions and helpers. ca/ : Components for the CA algorithm. README.md : Documentation specific to the CA algorithm. ...and so on for other algorithms core/ : Foundational classes and utilities. base_algorithm.py : Abstract base class for algorithm implementations. base_data_handler.py : Base class for data handling. base_model.py : Base class for model definitions. base_sampler.py : Base class for sampling methods. base_trainer.py : Base class for training routines. datasets/ : Dataset management and utilities. __init__.py : Initializes the dataset package. dataset.py : Dataset classes and methods. helpers/ : Helper functions for data processing. unlearning_canvas_dataset.py : Specific dataset class for unlearning tasks. helpers/ : Utility functions and helpers. helper.py : General-purpose helper functions. logger.py : Logging utilities to standardize logging practices. path_setup.py : Path configurations and environment setup. tests/ : Test suites for ensuring code reliability. mu_attack/ : Implementation of attack algorithms. attackers/ : Contains different types of attackers. configs/ : Configurations file. illegal/ : config for illegal task. nudity/ : config for nudity task. object/ : config for object task. style/ : config for style task. violence/ : config for violence task. core/ : Foundational classes. datasets/ : script to generate dataset. exces/ : Script to run attack tasks/ : Implementation of tasks helpers/ : Utility functions mu_defense/ : Implementation of Advunlearn algorithms. algorithms/ : Implementation of various defense algorithms. Each algorithm has its own subdirectory containing code and a README.md with detailed documentation. adv_unlearn/ : Adversial Unlearn algorithm components. README.md : Documentation specific to the advunlearn algorithm. algorithm.py : Core implementation of advunlearn. configs/ : Configuration files for training and generation tasks. model.py : Model architectures specific to advunlearn. image_generator.py : Image generator methods for generating sample images for evaluation. evaluator.py : Script that generates necessary outputs for evaluation. dataset_handler.py : Dataset handler for advunlearn algorithm. compvis_trainer.py : training loop for CompVis models. diffuser_trainer.py : training loop for diffuser models. trainer.py : Trainer class orchestrates the adversarial unlearning training process. utils.py : Utility functions and helpers. scripts.py : Commands to generate datasets and download models. notebooks/ : Contains example implementation. tests/ : Contains pytests. Datasets We use the Quick Canvas benchmark dataset, available here . Currently, the algorithms are trained using 5 images belonging to the themes of Abstractionism and Architectures . Usage This section contains the usage guide for the package. Installation Prerequisities Ensure conda is installed on your system. You can install Miniconda or Anaconda: Miniconda (recommended): https://docs.conda.io/en/latest/miniconda.html Anaconda : https://www.anaconda.com/products/distribution After installing conda , ensure it is available in your PATH by running. You may require to restart the terminal session: Before installing the unlearn_diff package, follow these steps to set up your environment correctly. These instructions ensure compatibility with the required dependencies, including Python, PyTorch, and ONNX Runtime. Step-by-Step Setup: Create a Conda Environment Create a new Conda environment named myenv with Python 3.8.5: conda create -n myenv python=3.8.5 Activate the Environment Activate the environment to work within it: conda activate myenv Install Core Dependencies Install PyTorch, torchvision, CUDA Toolkit, and ONNX Runtime with specific versions: conda install pytorch==1.11.0 torchvision==0.12.0 cudatoolkit=11.3 onnxruntime==1.16.3 -c pytorch -c conda-forge Install our unlearn_diff Package using pip: pip install unlearn_diff Install Additional Git Dependencies: After installing unlearn_diff, install the following Git-based dependencies in the same Conda environment to ensure full functionality: ```bash pip install git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers ```bash pip install git+https://github.com/openai/CLIP.git@main#egg=clip pip install git+https://github.com/crowsonkb/k-diffusion.git pip install git+https://github.com/cocodataset/panopticapi.git pip install git+https://github.com/Phoveran/fastargs.git@main#egg=fastargs pip install git+https://github.com/boomb0om/text2image-benchmark Downloading data and models. After you install the package, you can use the following commands to download. Dataset : i2p : Sample : download_data sample i2p Full : download_data full i2p quick_canvas : Sample : download_data sample quick_canvas Full : download_data full quick_canvas Model : compvis : download_model compvis diffuser : download_model diffuser Download best.onnx model download_best_onnx Run Train Each algorithm has their own script to run the algorithm, Some also have different process all together. Follow usage section in readme for the algorithm you want to run with the help of the github repository. You will need to run the code snippet provided in usage section with necessary configuration passed. Example usage for erase_diff algorithm (CompVis model) The default configuration for training is provided by erase_diff_train_mu. You can run the training with the default settings as follows: Using the Default Configuration from mu.algorithms.erase_diff.algorithm import EraseDiffAlgorithm from mu.algorithms.erase_diff.configs import erase_diff_train_mu algorithm = EraseDiffAlgorithm( erase_diff_train_mu ) algorithm.run() Overriding the Default Configuration If you need to override the existing configuration settings, you can specify your custom parameters (such as ckpt_path and raw_dataset_dir) directly when initializing the algorithm. For example: from mu.algorithms.erase_diff.algorithm import EraseDiffAlgorithm from mu.algorithms.erase_diff.configs import erase_diff_train_mu algorithm = EraseDiffAlgorithm( erase_diff_train_mu, ckpt_path=\"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\", #replace it with your ckpt path raw_dataset_dir=\"data/quick-canvas-dataset/sample\", use_sample = True, #uses sample dataset template_name = \"Abstractionism\", dataset_type = \"unlearncanvas\", devices = \"0\" ) algorithm.run() Note: When fine-tuning the model, if you want to use a sample dataset, set use_sample=True (default).Otherwise, set use_sample=False to use the full dataset. Machine unlearning with i2p dataset from mu.algorithms.erase_diff.algorithm import EraseDiffAlgorithm from mu.algorithms.erase_diff.configs import erase_diff_train_i2p algorithm = EraseDiffAlgorithm( erase_diff_train_i2p, ckpt_path=\"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\", #replace it with your ckpt path raw_dataset_dir=\"data/i2p-dataset/sample\", num_samples = 1, dataset_type = \"i2p\", template = \"i2p\", template_name = \"self-harm\", use_sample = True, #uses sample dataset devices = \"0\" ) algorithm.run() Run on your own dataset Step-1: Generate your own dataset generate_images_for_prompts --model_path models/diffuser/style50 --csv_path data/prompts/generic_data.csv Note: generate_images_for_prompts: This command invokes the image generation script. It uses a diffusion model to generate images based on textual prompts. --model_path: Specifies the path to the diffusion model to be used for image generation. In this example, the model is located at models/diffuser/style50. --csv_path: Provides the path to a CSV file containing the prompts. Each prompt in this CSV will be used to generate an image, allowing you to build a dataset tailored to your needs. Step-2: Train on your own dataset from mu.algorithms.erase_diff.algorithm import EraseDiffAlgorithm from mu.algorithms.erase_diff.configs import erase_diff_train_i2p algorithm = EraseDiffAlgorithm( erase_diff_train_i2p, ckpt_path=\"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\", #replace it with your ckpt path raw_dataset_dir=\"data/generic_data\", num_samples = 1, dataset_type = \"generic\", #add the dataset type as generic template_name = \"self-harm\", #concept to erase use_sample = True, #uses sample dataset devices = \"0\" ) algorithm.run() Evaluation: Evaluate using unlearn canvas dataset: Note: Currently it supports evaluation for unlearn canvas dataset. I2p and generic dataset support needs to be added. from mu.algorithms.erase_diff import EraseDiffEvaluator from mu.algorithms.erase_diff.configs import ( erase_diff_evaluation_config ) from evaluation.metrics.accuracy import accuracy_score from evaluation.metrics.fid import fid_score evaluator = EraseDiffEvaluator( erase_diff_evaluation_config, ckpt_path=\"outputs/erase_diff/finetuned_models/erase_diff_self-harm_model.pth\", ) generated_images_path = evaluator.generate_images() accuracy = accuracy_score(gen_image_dir=generated_images_path, dataset_type = \"unlearncanvas\", classifier_ckpt_path = \"/home/ubuntu/Projects/models/classifier_ckpt_path/style50_cls.pth\", forget_theme=\"Bricks\", seed_list = [\"188\"] ) print(accuracy['acc']) print(accuracy['loss']) reference_image_dir = \"data/quick-canvas-dataset/sample\" fid, _ = fid_score(generated_image_dir=generated_images_path, reference_image_dir=reference_image_dir ) print(fid) Link to our example usage notebooks Erase-diff (compvis model) https://github.com/RamailoTech/msu_unlearningalgorithm/blob/main/notebooks/run_erase_diff.ipynb forget-me-not (Diffuser model) https://github.com/RamailoTech/msu_unlearningalgorithm/blob/main/notebooks/run_forget_me_not.ipynb","title":"Unlearn Diff"},{"location":"#unlearn-diff","text":"Unlearn Diff is an open-source Python package designed to streamline the development of unlearning algorithms and establish a standardized evaluation pipeline for diffusion models. It provides researchers and practitioners with tools to implement, evaluate, and extend unlearning algorithms effectively.","title":"Unlearn Diff"},{"location":"#documentation","text":"You can find the full documentation for this project at the url given below. https://ramailotech.github.io/msu_unlearningalgorithm/","title":"Documentation"},{"location":"#features","text":"Comprehensive Algorithm Support : Includes commonly used concept erasing and machine unlearning algorithms tailored for diffusion models. Each algorithm is encapsulated and standardized in terms of input-output formats. Automated Evaluation : Supports automatic evaluation on datasets like UnlearnCanvas or IP2P. Performs standard and adversarial evaluations, outputting metrics as detailed in UnlearnCanvas and UnlearnDiffAtk. Extensibility : Designed for easy integration of new unlearning algorithms, attack methods, defense mechanisms, and datasets with minimal modifications.","title":"Features"},{"location":"#supported-algorithms","text":"The initial version includes established methods benchmarked in UnlearnCanvas and defensive unlearning techniques: CA (Concept Ablation) ED (Erase Diff) ESD (Efficient Substitution Distillation) FMN (Forget Me Not) SU (Saliency Unlearning) SH (ScissorHands) SA (Selective Amnesia) SPM (Semi Permeable Membrane) UCE (Unified Concept Editing) For detailed information on each algorithm, please refer to the respective README.md files located inside mu/algorithms .","title":"Supported Algorithms"},{"location":"#project-architecture","text":"The project is organized to facilitate scalability and maintainability. data/ : Stores data-related files. i2p-dataset/ : contains i2p-dataset sample/ : Sample dataset full/ : Full dataset quick-canvas-dataset/ : contains quick canvas dataset sample/ : Sample dataset full/ : Full dataset docs/ : Documentation, including API references and user guides. outputs/ : Outputs of the trained algorithms. examples/ : Sample code and notebooks demonstrating usage. logs/ : Log files for debugging and auditing. models/ : Repository of lora_diffusion and stable_diffusion. evaluation/ : Contains metrics for evalaution. core/ :Foundational classes. base_evaluator.py : Base class for evaluation. mu_defense_base_image_generator.py : Base class for image generation. helpers/ : Utility functions and helpers. parser.py : Parse attack logs for evaluation. utils.py : Utility function. metrics/ : Contains metrics for evalaution. accuracy.py asr.py clip.py fid.py mu/ : Core source code. algorithms/ : Implementation of various algorithms. Each algorithm has its own subdirectory containing code and a README.md with detailed documentation. esd/ : ESD algorithm components. README.md : Documentation specific to the ESD algorithm. algorithm.py : Core implementation of ESD. configs/ : Configuration files for training and generation tasks. constants/const.py : Constant values used across the ESD algorithm. environment.yaml : Environment setup for ESD. model.py : Model architectures specific to ESD. sampler.py : Sampling methods used during training or inference. scripts/train.py : Training script for ESD. evaluator.py : Script that generates necessary outputs for evaluation. trainer.py : Training routines and optimization strategies. utils.py : Utility functions and helpers. ca/ : Components for the CA algorithm. README.md : Documentation specific to the CA algorithm. ...and so on for other algorithms core/ : Foundational classes and utilities. base_algorithm.py : Abstract base class for algorithm implementations. base_data_handler.py : Base class for data handling. base_model.py : Base class for model definitions. base_sampler.py : Base class for sampling methods. base_trainer.py : Base class for training routines. datasets/ : Dataset management and utilities. __init__.py : Initializes the dataset package. dataset.py : Dataset classes and methods. helpers/ : Helper functions for data processing. unlearning_canvas_dataset.py : Specific dataset class for unlearning tasks. helpers/ : Utility functions and helpers. helper.py : General-purpose helper functions. logger.py : Logging utilities to standardize logging practices. path_setup.py : Path configurations and environment setup. tests/ : Test suites for ensuring code reliability. mu_attack/ : Implementation of attack algorithms. attackers/ : Contains different types of attackers. configs/ : Configurations file. illegal/ : config for illegal task. nudity/ : config for nudity task. object/ : config for object task. style/ : config for style task. violence/ : config for violence task. core/ : Foundational classes. datasets/ : script to generate dataset. exces/ : Script to run attack tasks/ : Implementation of tasks helpers/ : Utility functions mu_defense/ : Implementation of Advunlearn algorithms. algorithms/ : Implementation of various defense algorithms. Each algorithm has its own subdirectory containing code and a README.md with detailed documentation. adv_unlearn/ : Adversial Unlearn algorithm components. README.md : Documentation specific to the advunlearn algorithm. algorithm.py : Core implementation of advunlearn. configs/ : Configuration files for training and generation tasks. model.py : Model architectures specific to advunlearn. image_generator.py : Image generator methods for generating sample images for evaluation. evaluator.py : Script that generates necessary outputs for evaluation. dataset_handler.py : Dataset handler for advunlearn algorithm. compvis_trainer.py : training loop for CompVis models. diffuser_trainer.py : training loop for diffuser models. trainer.py : Trainer class orchestrates the adversarial unlearning training process. utils.py : Utility functions and helpers. scripts.py : Commands to generate datasets and download models. notebooks/ : Contains example implementation. tests/ : Contains pytests.","title":"Project Architecture"},{"location":"#datasets","text":"We use the Quick Canvas benchmark dataset, available here . Currently, the algorithms are trained using 5 images belonging to the themes of Abstractionism and Architectures .","title":"Datasets"},{"location":"#usage","text":"This section contains the usage guide for the package.","title":"Usage"},{"location":"#installation","text":"","title":"Installation"},{"location":"#prerequisities","text":"Ensure conda is installed on your system. You can install Miniconda or Anaconda: Miniconda (recommended): https://docs.conda.io/en/latest/miniconda.html Anaconda : https://www.anaconda.com/products/distribution After installing conda , ensure it is available in your PATH by running. You may require to restart the terminal session: Before installing the unlearn_diff package, follow these steps to set up your environment correctly. These instructions ensure compatibility with the required dependencies, including Python, PyTorch, and ONNX Runtime. Step-by-Step Setup: Create a Conda Environment Create a new Conda environment named myenv with Python 3.8.5: conda create -n myenv python=3.8.5 Activate the Environment Activate the environment to work within it: conda activate myenv Install Core Dependencies Install PyTorch, torchvision, CUDA Toolkit, and ONNX Runtime with specific versions: conda install pytorch==1.11.0 torchvision==0.12.0 cudatoolkit=11.3 onnxruntime==1.16.3 -c pytorch -c conda-forge Install our unlearn_diff Package using pip: pip install unlearn_diff Install Additional Git Dependencies: After installing unlearn_diff, install the following Git-based dependencies in the same Conda environment to ensure full functionality: ```bash pip install git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers ```bash pip install git+https://github.com/openai/CLIP.git@main#egg=clip pip install git+https://github.com/crowsonkb/k-diffusion.git pip install git+https://github.com/cocodataset/panopticapi.git pip install git+https://github.com/Phoveran/fastargs.git@main#egg=fastargs pip install git+https://github.com/boomb0om/text2image-benchmark","title":"Prerequisities"},{"location":"#downloading-data-and-models","text":"After you install the package, you can use the following commands to download. Dataset : i2p : Sample : download_data sample i2p Full : download_data full i2p quick_canvas : Sample : download_data sample quick_canvas Full : download_data full quick_canvas Model : compvis : download_model compvis diffuser : download_model diffuser Download best.onnx model download_best_onnx","title":"Downloading data and models."},{"location":"#run-train","text":"Each algorithm has their own script to run the algorithm, Some also have different process all together. Follow usage section in readme for the algorithm you want to run with the help of the github repository. You will need to run the code snippet provided in usage section with necessary configuration passed. Example usage for erase_diff algorithm (CompVis model) The default configuration for training is provided by erase_diff_train_mu. You can run the training with the default settings as follows: Using the Default Configuration from mu.algorithms.erase_diff.algorithm import EraseDiffAlgorithm from mu.algorithms.erase_diff.configs import erase_diff_train_mu algorithm = EraseDiffAlgorithm( erase_diff_train_mu ) algorithm.run() Overriding the Default Configuration If you need to override the existing configuration settings, you can specify your custom parameters (such as ckpt_path and raw_dataset_dir) directly when initializing the algorithm. For example: from mu.algorithms.erase_diff.algorithm import EraseDiffAlgorithm from mu.algorithms.erase_diff.configs import erase_diff_train_mu algorithm = EraseDiffAlgorithm( erase_diff_train_mu, ckpt_path=\"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\", #replace it with your ckpt path raw_dataset_dir=\"data/quick-canvas-dataset/sample\", use_sample = True, #uses sample dataset template_name = \"Abstractionism\", dataset_type = \"unlearncanvas\", devices = \"0\" ) algorithm.run() Note: When fine-tuning the model, if you want to use a sample dataset, set use_sample=True (default).Otherwise, set use_sample=False to use the full dataset. Machine unlearning with i2p dataset from mu.algorithms.erase_diff.algorithm import EraseDiffAlgorithm from mu.algorithms.erase_diff.configs import erase_diff_train_i2p algorithm = EraseDiffAlgorithm( erase_diff_train_i2p, ckpt_path=\"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\", #replace it with your ckpt path raw_dataset_dir=\"data/i2p-dataset/sample\", num_samples = 1, dataset_type = \"i2p\", template = \"i2p\", template_name = \"self-harm\", use_sample = True, #uses sample dataset devices = \"0\" ) algorithm.run() Run on your own dataset Step-1: Generate your own dataset generate_images_for_prompts --model_path models/diffuser/style50 --csv_path data/prompts/generic_data.csv Note: generate_images_for_prompts: This command invokes the image generation script. It uses a diffusion model to generate images based on textual prompts. --model_path: Specifies the path to the diffusion model to be used for image generation. In this example, the model is located at models/diffuser/style50. --csv_path: Provides the path to a CSV file containing the prompts. Each prompt in this CSV will be used to generate an image, allowing you to build a dataset tailored to your needs. Step-2: Train on your own dataset from mu.algorithms.erase_diff.algorithm import EraseDiffAlgorithm from mu.algorithms.erase_diff.configs import erase_diff_train_i2p algorithm = EraseDiffAlgorithm( erase_diff_train_i2p, ckpt_path=\"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\", #replace it with your ckpt path raw_dataset_dir=\"data/generic_data\", num_samples = 1, dataset_type = \"generic\", #add the dataset type as generic template_name = \"self-harm\", #concept to erase use_sample = True, #uses sample dataset devices = \"0\" ) algorithm.run()","title":"Run Train"},{"location":"#evaluation","text":"Evaluate using unlearn canvas dataset: Note: Currently it supports evaluation for unlearn canvas dataset. I2p and generic dataset support needs to be added. from mu.algorithms.erase_diff import EraseDiffEvaluator from mu.algorithms.erase_diff.configs import ( erase_diff_evaluation_config ) from evaluation.metrics.accuracy import accuracy_score from evaluation.metrics.fid import fid_score evaluator = EraseDiffEvaluator( erase_diff_evaluation_config, ckpt_path=\"outputs/erase_diff/finetuned_models/erase_diff_self-harm_model.pth\", ) generated_images_path = evaluator.generate_images() accuracy = accuracy_score(gen_image_dir=generated_images_path, dataset_type = \"unlearncanvas\", classifier_ckpt_path = \"/home/ubuntu/Projects/models/classifier_ckpt_path/style50_cls.pth\", forget_theme=\"Bricks\", seed_list = [\"188\"] ) print(accuracy['acc']) print(accuracy['loss']) reference_image_dir = \"data/quick-canvas-dataset/sample\" fid, _ = fid_score(generated_image_dir=generated_images_path, reference_image_dir=reference_image_dir ) print(fid) Link to our example usage notebooks Erase-diff (compvis model) https://github.com/RamailoTech/msu_unlearningalgorithm/blob/main/notebooks/run_erase_diff.ipynb forget-me-not (Diffuser model) https://github.com/RamailoTech/msu_unlearningalgorithm/blob/main/notebooks/run_forget_me_not.ipynb","title":"Evaluation:"},{"location":"project_architecture/","text":"Project Architecture The project is organized to facilitate scalability and maintainability. data/ : Stores data-related files. i2p-dataset/ : contains i2p-dataset sample/ : Sample dataset full/ : Full dataset quick-canvas-dataset/ : contains quick canvas dataset sample/ : Sample dataset full/ : Full dataset docs/ : Documentation, including API references and user guides. outputs/ : Outputs of the trained algorithms. examples/ : Sample code and notebooks demonstrating usage. logs/ : Log files for debugging and auditing. models/ : Repository of trained models and checkpoints. mu/ : Core source code. algorithms/ : Implementation of various algorithms. Each algorithm has its own subdirectory containing code and a README.md with detailed documentation. esd/ : ESD algorithm components. README.md : Documentation specific to the ESD algorithm. algorithm.py : Core implementation of ESD. configs/ : Configuration files for training and generation tasks. constants/const.py : Constant values used across the ESD algorithm. environment.yaml : Environment setup for ESD. model.py : Model architectures specific to ESD. sampler.py : Sampling methods used during training or inference. scripts/train.py : Training script for ESD. trainer.py : Training routines and optimization strategies. utils.py : Utility functions and helpers. ca/ : Components for the CA algorithm. README.md : Documentation specific to the CA algorithm. ...and so on for other algorithms core/ : Foundational classes and utilities. base_algorithm.py : Abstract base class for algorithm implementations. base_data_handler.py : Base class for data handling. base_model.py : Base class for model definitions. base_sampler.py : Base class for sampling methods. base_trainer.py : Base class for training routines. datasets/ : Dataset management and utilities. __init__.py : Initializes the dataset package. dataset.py : Dataset classes and methods. helpers/ : Helper functions for data processing. unlearning_canvas_dataset.py : Specific dataset class for unlearning tasks. helpers/ : Utility functions and helpers. helper.py : General-purpose helper functions. logger.py : Logging utilities to standardize logging practices. path_setup.py : Path configurations and environment setup. tests/ : Test suites for ensuring code reliability. stable_diffusion/ : Components for stable diffusion. lora_diffusion/ : Components for the LoRA Diffusion.","title":"Project architecture"},{"location":"project_architecture/#project-architecture","text":"The project is organized to facilitate scalability and maintainability. data/ : Stores data-related files. i2p-dataset/ : contains i2p-dataset sample/ : Sample dataset full/ : Full dataset quick-canvas-dataset/ : contains quick canvas dataset sample/ : Sample dataset full/ : Full dataset docs/ : Documentation, including API references and user guides. outputs/ : Outputs of the trained algorithms. examples/ : Sample code and notebooks demonstrating usage. logs/ : Log files for debugging and auditing. models/ : Repository of trained models and checkpoints. mu/ : Core source code. algorithms/ : Implementation of various algorithms. Each algorithm has its own subdirectory containing code and a README.md with detailed documentation. esd/ : ESD algorithm components. README.md : Documentation specific to the ESD algorithm. algorithm.py : Core implementation of ESD. configs/ : Configuration files for training and generation tasks. constants/const.py : Constant values used across the ESD algorithm. environment.yaml : Environment setup for ESD. model.py : Model architectures specific to ESD. sampler.py : Sampling methods used during training or inference. scripts/train.py : Training script for ESD. trainer.py : Training routines and optimization strategies. utils.py : Utility functions and helpers. ca/ : Components for the CA algorithm. README.md : Documentation specific to the CA algorithm. ...and so on for other algorithms core/ : Foundational classes and utilities. base_algorithm.py : Abstract base class for algorithm implementations. base_data_handler.py : Base class for data handling. base_model.py : Base class for model definitions. base_sampler.py : Base class for sampling methods. base_trainer.py : Base class for training routines. datasets/ : Dataset management and utilities. __init__.py : Initializes the dataset package. dataset.py : Dataset classes and methods. helpers/ : Helper functions for data processing. unlearning_canvas_dataset.py : Specific dataset class for unlearning tasks. helpers/ : Utility functions and helpers. helper.py : General-purpose helper functions. logger.py : Logging utilities to standardize logging practices. path_setup.py : Path configurations and environment setup. tests/ : Test suites for ensuring code reliability. stable_diffusion/ : Components for stable diffusion. lora_diffusion/ : Components for the LoRA Diffusion.","title":"Project Architecture"},{"location":"evaluation/","text":"","title":"Index"},{"location":"evaluation/adv_unlearn_evaluation/","text":"Evaluation for mu_defense This section provides instructions for running the evaluation framework for the unlearned Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying adversial unlearning. Running the Evaluation Framework Create a file, eg, evaluate.py and use examples and modify your configs to run the file. Example code Run with default config from mu_defense.algorithms.adv_unlearn import MUDefenseEvaluator from mu_defense.algorithms.adv_unlearn.configs import mu_defense_evaluation_config from mu.algorithms.erase_diff.configs import erase_diff_train_mu from evaluation.metrics.clip import clip_score from evaluation.metrics.fid import fid_score target_ckpt = \"outputs/results_with_retaining/nudity/coco_object/pgd/AttackLr_0.001/text_encoder_full/all/prefix_k/AdvUnlearn-nudity-method_text_encoder_full_all-Attack_pgd-Retain_coco_object_iter_1.0-lr_1e-05-AttackLr_0.001-prefix_k_adv_num_1-word_embd-attack_init_latest-attack_step_30-adv_update_1-warmup_iter_200/models/Diffusers-UNet-noxattn-epoch_0.pt\" evaluator = MUDefenseEvaluator(config=mu_defense_evaluation_config) #default config gen_image_path = evaluator.generate_images() #generate images for evaluation print(gen_image_path) prompt_path = \"data/prompts/sample_prompt.csv\" ref_image_path = \"coco_dataset/extracted_files/coco_sample\" device = \"0\" clip_val = clip_score(gen_image_path, prompt_path, device) print(clip_val) fid_val, _ = fid_score(gen_image_path, ref_image_path) print(fid_val) Run with your configs Check the config descriptions to use your own confgs. from mu_defense.algorithms.adv_unlearn import MUDefenseEvaluator from mu_defense.algorithms.adv_unlearn.configs import mu_defense_evaluation_config from mu.algorithms.erase_diff.configs import erase_diff_train_mu from evaluation.metrics.clip import clip_score from evaluation.metrics.fid import fid_score target_ckpt = \"outputs/results_with_retaining/nudity/coco_object/pgd/AttackLr_0.001/text_encoder_full/all/prefix_k/AdvUnlearn-nudity-method_text_encoder_full_all-Attack_pgd-Retain_coco_object_iter_1.0-lr_1e-05-AttackLr_0.001-prefix_k_adv_num_1-word_embd-attack_init_latest-attack_step_30-adv_update_1-warmup_iter_200/models/Diffusers-UNet-noxattn-epoch_0.pt\" evaluator = MUDefenseEvaluator(config=mu_defense_evaluation_config, target_ckpt = target_ckpt, model_config_path = erase_diff_train_mu.model_config_path, save_path = \"outputs/adv_unlearn/results\", prompts_path = \"data/prompts/sample_prompt.csv\", num_samples = 1, folder_suffix = \"imagenette\", devices = \"0\",) gen_image_path = evaluator.generate_images() #generates images for evaluation print(gen_image_path) prompt_path = \"data/prompts/sample_prompt.csv\" ref_image_path = \"coco_dataset/extracted_files/coco_sample\" device = \"0\" clip_val = clip_score(gen_image_path, prompt_path, device) print(clip_val) fid_val, _ = fid_score(gen_image_path, ref_image_path) print(fid_val) Running the image generation Script in Offline Mode WANDB_MODE=offline python evaluate.py How It Works Default Values: The script first loads default values from the evluation config file as in configs section. Parameter Overrides: Any parameters passed directly to the algorithm, overrides these configs. Final Configuration: The script merges the configs and convert them into dictionary to proceed with the evaluation.","title":"Usage"},{"location":"evaluation/adv_unlearn_evaluation/#evaluation-for-mu_defense","text":"This section provides instructions for running the evaluation framework for the unlearned Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying adversial unlearning.","title":"Evaluation for mu_defense"},{"location":"evaluation/adv_unlearn_evaluation/#running-the-evaluation-framework","text":"Create a file, eg, evaluate.py and use examples and modify your configs to run the file. Example code Run with default config from mu_defense.algorithms.adv_unlearn import MUDefenseEvaluator from mu_defense.algorithms.adv_unlearn.configs import mu_defense_evaluation_config from mu.algorithms.erase_diff.configs import erase_diff_train_mu from evaluation.metrics.clip import clip_score from evaluation.metrics.fid import fid_score target_ckpt = \"outputs/results_with_retaining/nudity/coco_object/pgd/AttackLr_0.001/text_encoder_full/all/prefix_k/AdvUnlearn-nudity-method_text_encoder_full_all-Attack_pgd-Retain_coco_object_iter_1.0-lr_1e-05-AttackLr_0.001-prefix_k_adv_num_1-word_embd-attack_init_latest-attack_step_30-adv_update_1-warmup_iter_200/models/Diffusers-UNet-noxattn-epoch_0.pt\" evaluator = MUDefenseEvaluator(config=mu_defense_evaluation_config) #default config gen_image_path = evaluator.generate_images() #generate images for evaluation print(gen_image_path) prompt_path = \"data/prompts/sample_prompt.csv\" ref_image_path = \"coco_dataset/extracted_files/coco_sample\" device = \"0\" clip_val = clip_score(gen_image_path, prompt_path, device) print(clip_val) fid_val, _ = fid_score(gen_image_path, ref_image_path) print(fid_val) Run with your configs Check the config descriptions to use your own confgs. from mu_defense.algorithms.adv_unlearn import MUDefenseEvaluator from mu_defense.algorithms.adv_unlearn.configs import mu_defense_evaluation_config from mu.algorithms.erase_diff.configs import erase_diff_train_mu from evaluation.metrics.clip import clip_score from evaluation.metrics.fid import fid_score target_ckpt = \"outputs/results_with_retaining/nudity/coco_object/pgd/AttackLr_0.001/text_encoder_full/all/prefix_k/AdvUnlearn-nudity-method_text_encoder_full_all-Attack_pgd-Retain_coco_object_iter_1.0-lr_1e-05-AttackLr_0.001-prefix_k_adv_num_1-word_embd-attack_init_latest-attack_step_30-adv_update_1-warmup_iter_200/models/Diffusers-UNet-noxattn-epoch_0.pt\" evaluator = MUDefenseEvaluator(config=mu_defense_evaluation_config, target_ckpt = target_ckpt, model_config_path = erase_diff_train_mu.model_config_path, save_path = \"outputs/adv_unlearn/results\", prompts_path = \"data/prompts/sample_prompt.csv\", num_samples = 1, folder_suffix = \"imagenette\", devices = \"0\",) gen_image_path = evaluator.generate_images() #generates images for evaluation print(gen_image_path) prompt_path = \"data/prompts/sample_prompt.csv\" ref_image_path = \"coco_dataset/extracted_files/coco_sample\" device = \"0\" clip_val = clip_score(gen_image_path, prompt_path, device) print(clip_val) fid_val, _ = fid_score(gen_image_path, ref_image_path) print(fid_val) Running the image generation Script in Offline Mode WANDB_MODE=offline python evaluate.py How It Works Default Values: The script first loads default values from the evluation config file as in configs section. Parameter Overrides: Any parameters passed directly to the algorithm, overrides these configs. Final Configuration: The script merges the configs and convert them into dictionary to proceed with the evaluation.","title":"Running the Evaluation Framework"},{"location":"evaluation/concept_ablation/","text":"Concept ablation Evaluation Framework This section provides instructions for running the evaluation framework for the Concept ablation algorithm on Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying machine unlearning. Running the Evaluation Framework Create a file, eg, evaluate.py and use examples and modify your configs to run the file. Work within the same environment used to perform unlearning for evaluation as well. Before running evaluation, download the classifier ckpt from here: https://drive.google.com/drive/folders/1AoazlvDgWgc3bAyHDpqlafqltmn4vm61 Example Code from mu.algorithms.concept_ablation import ConceptAblationEvaluator from mu.algorithms.concept_ablation.configs import ( concept_ablation_evaluation_config ) from evaluation.metrics.accuracy import accuracy_score from evaluation.metrics.fid import fid_score evaluator = ConceptAblationEvaluator( concept_ablation_evaluation_config, ckpt_path=\"outputs/concept_ablation/checkpoints/last.ckpt\", ) generated_images_path = evaluator.generate_images() reference_image_dir = \"data/quick-canvas-dataset/sample\" accuracy = accuracy_score(gen_image_dir=generated_images_path, dataset_type = \"unlearncanvas\", classifier_ckpt_path = \"models/classifier_ckpt_path/style50_cls.pth\", reference_dir=reference_image_dir, forget_theme=\"Bricks\", seed_list = [\"188\"] ) print(accuracy['acc']) print(accuracy['loss']) fid, _ = fid_score(generated_image_dir=generated_images_path, reference_image_dir=reference_image_dir ) print(fid) Running the Training Script in Offline Mode WANDB_MODE=offline python evaluate.py How It Works * Default Values: The script first loads default values from the evluation config file as in configs section. Parameter Overrides: Any parameters passed directly to the algorithm, overrides these configs. Final Configuration: The script merges the configs and convert them into dictionary to proceed with the evaluation.","title":"Usage"},{"location":"evaluation/concept_ablation/#concept-ablation-evaluation-framework","text":"This section provides instructions for running the evaluation framework for the Concept ablation algorithm on Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying machine unlearning.","title":"Concept ablation Evaluation Framework"},{"location":"evaluation/concept_ablation/#running-the-evaluation-framework","text":"Create a file, eg, evaluate.py and use examples and modify your configs to run the file. Work within the same environment used to perform unlearning for evaluation as well. Before running evaluation, download the classifier ckpt from here: https://drive.google.com/drive/folders/1AoazlvDgWgc3bAyHDpqlafqltmn4vm61 Example Code from mu.algorithms.concept_ablation import ConceptAblationEvaluator from mu.algorithms.concept_ablation.configs import ( concept_ablation_evaluation_config ) from evaluation.metrics.accuracy import accuracy_score from evaluation.metrics.fid import fid_score evaluator = ConceptAblationEvaluator( concept_ablation_evaluation_config, ckpt_path=\"outputs/concept_ablation/checkpoints/last.ckpt\", ) generated_images_path = evaluator.generate_images() reference_image_dir = \"data/quick-canvas-dataset/sample\" accuracy = accuracy_score(gen_image_dir=generated_images_path, dataset_type = \"unlearncanvas\", classifier_ckpt_path = \"models/classifier_ckpt_path/style50_cls.pth\", reference_dir=reference_image_dir, forget_theme=\"Bricks\", seed_list = [\"188\"] ) print(accuracy['acc']) print(accuracy['loss']) fid, _ = fid_score(generated_image_dir=generated_images_path, reference_image_dir=reference_image_dir ) print(fid) Running the Training Script in Offline Mode WANDB_MODE=offline python evaluate.py How It Works * Default Values: The script first loads default values from the evluation config file as in configs section. Parameter Overrides: Any parameters passed directly to the algorithm, overrides these configs. Final Configuration: The script merges the configs and convert them into dictionary to proceed with the evaluation.","title":"Running the Evaluation Framework"},{"location":"evaluation/contributing/","text":"Contributing to Unlearn Diff Thank you for your interest in contributing to Unlearn ! This document outlines the steps and best practices for adding new algorithms, making pull requests, filing issues, and more. Table of Contents Introduction Code of Conduct Project Structure How to Contribute Reporting Issues Suggesting Enhancements Submitting Pull Requests Adding a New Algorithm Folder Structure Creating an Environment Documentation Code Style Contact Introduction Unlearn is an open-source Python package designed for unlearning algorithms in diffusion models. We welcome contributions from the community, whether it\u2019s a new feature, bug fix, or an entirely new unlearning algorithm. Code of Conduct Please note that we expect all contributors to abide by our code of conduct. Be respectful and considerate of others, and help keep our community healthy and inclusive. Project Architecture The project is organized to facilitate scalability and maintainability. data/ : Stores data-related files. i2p-dataset/ : contains i2p-dataset sample/ : Sample dataset full/ : Full dataset quick-canvas-dataset/ : contains quick canvas dataset sample/ : Sample dataset full/ : Full dataset docs/ : Documentation, including API references and user guides. outputs/ : Outputs of the trained algorithms. examples/ : Sample code and notebooks demonstrating usage. logs/ : Log files for debugging and auditing. models/ : Repository of lora_diffusion and stable_diffusion. evaluation/ : Contains metrics for evalaution. core/ :Foundational classes. base_evaluator.py : Base class for evaluation. mu_defense_base_image_generator.py : Base class for image generation. helpers/ : Utility functions and helpers. parser.py : Parse attack logs for evaluation. utils.py : Utility function. metrics/ : Contains metrics for evalaution. accuracy.py asr.py clip.py fid.py mu/ : Core source code. algorithms/ : Implementation of various algorithms. Each algorithm has its own subdirectory containing code and a README.md with detailed documentation. esd/ : ESD algorithm components. README.md : Documentation specific to the ESD algorithm. algorithm.py : Core implementation of ESD. configs/ : Configuration files for training and generation tasks. constants/const.py : Constant values used across the ESD algorithm. environment.yaml : Environment setup for ESD. model.py : Model architectures specific to ESD. sampler.py : Sampling methods used during training or inference. scripts/train.py : Training script for ESD. evaluator.py : Script that generates necessary outputs for evaluation. trainer.py : Training routines and optimization strategies. utils.py : Utility functions and helpers. ca/ : Components for the CA algorithm. README.md : Documentation specific to the CA algorithm. ...and so on for other algorithms core/ : Foundational classes and utilities. base_algorithm.py : Abstract base class for algorithm implementations. base_data_handler.py : Base class for data handling. base_model.py : Base class for model definitions. base_sampler.py : Base class for sampling methods. base_trainer.py : Base class for training routines. datasets/ : Dataset management and utilities. __init__.py : Initializes the dataset package. dataset.py : Dataset classes and methods. helpers/ : Helper functions for data processing. unlearning_canvas_dataset.py : Specific dataset class for unlearning tasks. helpers/ : Utility functions and helpers. helper.py : General-purpose helper functions. logger.py : Logging utilities to standardize logging practices. path_setup.py : Path configurations and environment setup. tests/ : Test suites for ensuring code reliability. mu_attack/ : Implementation of attack algorithms. attackers/ : Contains different types of attackers. configs/ : Configurations file. illegal/ : config for illegal task. nudity/ : config for nudity task. object/ : config for object task. style/ : config for style task. violence/ : config for violence task. core/ : Foundational classes. datasets/ : script to generate dataset. exces/ : Script to run attack tasks/ : Implementation of tasks helpers/ : Utility functions mu_defense/ : Implementation of Advunlearn algorithms. algorithms/ : Implementation of various defense algorithms. Each algorithm has its own subdirectory containing code and a README.md with detailed documentation. adv_unlearn/ : Adversial Unlearn algorithm components. README.md : Documentation specific to the advunlearn algorithm. algorithm.py : Core implementation of advunlearn. configs/ : Configuration files for training and generation tasks. model.py : Model architectures specific to advunlearn. image_generator.py : Image generator methods for generating sample images for evaluation. evaluator.py : Script that generates necessary outputs for evaluation. dataset_handler.py : Dataset handler for advunlearn algorithm. compvis_trainer.py : training loop for CompVis models. diffuser_trainer.py : training loop for diffuser models. trainer.py : Trainer class orchestrates the adversarial unlearning training process. utils.py : Utility functions and helpers. scripts.py : Commands to generate datasets and download models. notebooks/ : Contains example implementation. tests/ : Contains pytests. How to Contribute Reporting Issues Check the to see if your bug or feature request has already been reported. If not, open a new issue. Provide a clear title and description, including steps to reproduce if it\u2019s a bug. Label your issue appropriately (e.g., \u201cbug\u201d, \u201cenhancement\u201d, \u201cdocumentation\u201d). Suggesting Enhancements If you have an idea for a new feature or improvement, open a GitHub issue labeled \u201cenhancement\u201d or \u201cfeature request.\u201d Include any relevant use-cases, examples, or background information to help the community understand your proposal. Submitting Pull Requests Fork the repository and create your feature branch from main (or the appropriate branch if instructed otherwise): bash git checkout -b feature/my-awesome-feature Make your changes, including tests if applicable. Commit and push your changes: bash git push origin feature/my-awesome-feature Open a Pull Request from your fork into the main repo. Provide a clear description of what your PR does, referencing any related issues. Please ensure your PR follows the Code Style guidelines and includes updated or new tests if needed. Adding a New Algorithm One of the core goals of Unlearn Diff is to make it easy to add and benchmark new unlearning algorithms. Follow these steps when introducing a new method. Folder Structure Create a new subfolder in mu/algorithms/ with a clear, descriptive name (e.g., my_new_algo ). Inside this subfolder, include the following (at minimum): algorithm.py trainer.py scripts/train.py (and optionally scripts/evaluate.py if your training and evaluation are separate) configs/ containing YAML config files for training, evaluation, etc. environment.yaml specifying dependencies for conda Readme.md explaining how to install, run, and configure your new algorithm (Optional) data_handler.py , datasets/ , model.py , sampler.py , utils.py , etc., depending on your algorithm\u2019s design. Extend or import from base classes in mu/core/ if your algorithm logic aligns with any existing abstract classes (e.g., BaseAlgorithm , BaseTrainer , etc.). This ensures code consistency and easier maintenance. Adding a New Attack To add a new attack method, follow these guidelines: Folder Structure : Create a new file or subfolder under mu_attack/attackers/ with a clear, descriptive name (e.g., my_new_attack.py or mu_attack/attackers/my_new_attack/ ). If creating a subfolder, include essential files specific to attack implementation: Attacks : The main implementation file for the attack logic (e.g., attack.py ). Execs : Scripts or modules that execute the attack routines. Tasks : Task definitions for integrating and testing the attack. Any helper modules or configuration files specific to your attack. Update or add corresponding YAML configuration files and config class under mu_attack/configs/ if your attack requires custom settings. Implementation : Extend or import from the base class BaseAttacker (located in mu_attack/core/base_attacker.py ) if applicable. Ensure that your attack method adheres to the input-output standards defined by the project. Documentation & Testing : Add detailed documentation within your new attack module and update the main documentation if needed. Include tests covering your new attack method under the appropriate test directories. Environment : If your attack method has unique dependencies, update the environment.yaml file within the relevant directory or provide instructions in your documentation on how to create a dedicated environment. Also update the common environment file that is located in the project's root directory. Adding a New Defense To integrate a new defense mechanism, please follow these steps: Folder Structure : Create a new subfolder under mu_defense/algorithms/ with a descriptive name (e.g., my_new_defense ). Within this subfolder, include essential files such as: algorithm.py : Contains the core logic of your defense method. trainer.py : Contains training routines and optimization strategies. configs/ : Include configuration class for training and evaluation. environment.yaml : Specify dependencies unique to your defense method. Readme.md : Document usage instructions, configuration details, and any other relevant information. Implementation : Extend or use base classes provided in mu_defense/algorithms/core/ (e.g., base_algorithm.py , base_trainer.py ) to ensure consistency with existing methods. Implement any unique evaluation metrics or procedures if your defense requires them. Documentation & Testing : Document your defense method thoroughly within its Readme.md and update the global documentation if necessary. Provide tests for your defense implementation to ensure its reliability and compatibility with the rest of the system. Environment : If your defense algorithm has specific dependencies, use the provided environment.yaml file as a template and adjust it accordingly. Include clear instructions for users to create and activate the environment. Also update the common environment file that is located in the project's root directory. Creating an Environment The default environment file is located in the project root directory ( requirements.txt ). Contributors should update this file as needed to include any new packages required by their contributions making sure it doesnot effect other algorthms. If your module or algorithm requires unique dependencies, you may add a dedicated environment file in its respective directory, but be sure to update and maintain the default environment in the root. Optionally, to keep dependencies organized, each algorithm has its own environment.yaml . Contributors are encouraged to: Specify all required packages (e.g., torch , torchvision , diffusers , etc.). Name the environment after the algorithm, e.g., mu_<algorithm_name> . Update instructions in your algorithm\u2019s Readme.md on how to install and activate this environment. Ensure you run the pytests in the tests/ directory after adding a new algorithm to verify that the updated environment remains compatible with existing algorithms. Run pytests Before integrating new algorithms into the repository, please run the existing tests using the modified environment file to ensure that your changes do not break any functionality. Steps to Run the Tests Prepare the Environment Configuration: Make sure the environment is set up using the provided tests/test_config.yaml file. This file contains configuration settings for various components of the project, such as data directories, model paths, and algorithm-specific parameters. It includes: common_config_unlearn_canvas_mu: Contains settings for the UnlearnCanvas dataset, including paths for the data directory, CompVis and Diffuser model checkpoints, template information, and dataset type. common_config_i2p: Provides configuration for the i2p dataset with similar settings as above. evaluator_config: Specifies the classifier checkpoint path and sample usage flag for evaluation. Algorithm-Specific Configurations: Settings for various algorithms like concept_ablation, forget_me_not, saliency_unlearning, semipermeable, unified_concept_editing, selective_amnesia, and attack/defense configurations. Run the Tests: Execute the following commands from the root of your repository: pytest tests/test_mu.py pytest tests/test_mu_attack.py pytest tests/test_mu_defense.py These commands will run tests for: tests/test_mu.py: Core machine unlearning functionalities including evaluation. tests/test_mu_attack.py: Attack-related functionalities including evaluation. tests/test_mu_defense.py: Defense-related functionalities including evaluation. Verify the Results: Ensure that all tests pass without errors. If any test fails, fix the issues before adding new algorithms to maintain compatibility with the existing codebase. Documentation Each algorithm folder must contain a Readme.md that documents: High-level description of the algorithm (goals, references to papers, etc.). How to install any special dependencies (if not covered by environment.yaml ). How to run training and evaluation (with example commands). Explanation of the config files in configs/ . Any unique parameters or hyperparameters. If you introduce new metrics or logs, please outline them in your algorithm\u2019s README and update any relevant docs in the main docs/ folder as needed. If you have incorporated code or drawn inspiration from external codebases, please ensure that you properly credit and cite the original work within each script. Code Style We recommend following PEP 8 for Python code. Use descriptive variable names, docstrings, and type hints where possible. Keep functions and methods short and focused\u2014avoid large, monolithic methods. Use Python\u2019s built-in logging or the logging utilities in helpers/logger.py for consistent log outputs. Use wandb global wandb logger if required. Additionally, we prefer: - 4 spaces for indentation (no tabs). - Meaningful commit messages: \u201cFix dataset loader bug for large images\u201d vs. \u201cFix stuff.\u201d Contact If you have any questions, feel free to open an issue or reach out to us via email. We appreciate your contributions and look forward to collaborating with you on Unlearn Diff ! Thank you again for your interest in contributing. We\u2019re excited to see your ideas and improvements!","title":"How to contribute"},{"location":"evaluation/contributing/#table-of-contents","text":"Introduction Code of Conduct Project Structure How to Contribute Reporting Issues Suggesting Enhancements Submitting Pull Requests Adding a New Algorithm Folder Structure Creating an Environment Documentation Code Style Contact Introduction Unlearn is an open-source Python package designed for unlearning algorithms in diffusion models. We welcome contributions from the community, whether it\u2019s a new feature, bug fix, or an entirely new unlearning algorithm.","title":"Table of Contents"},{"location":"evaluation/contributing/#code-of-conduct","text":"Please note that we expect all contributors to abide by our code of conduct. Be respectful and considerate of others, and help keep our community healthy and inclusive.","title":"Code of Conduct"},{"location":"evaluation/contributing/#project-architecture","text":"The project is organized to facilitate scalability and maintainability. data/ : Stores data-related files. i2p-dataset/ : contains i2p-dataset sample/ : Sample dataset full/ : Full dataset quick-canvas-dataset/ : contains quick canvas dataset sample/ : Sample dataset full/ : Full dataset docs/ : Documentation, including API references and user guides. outputs/ : Outputs of the trained algorithms. examples/ : Sample code and notebooks demonstrating usage. logs/ : Log files for debugging and auditing. models/ : Repository of lora_diffusion and stable_diffusion. evaluation/ : Contains metrics for evalaution. core/ :Foundational classes. base_evaluator.py : Base class for evaluation. mu_defense_base_image_generator.py : Base class for image generation. helpers/ : Utility functions and helpers. parser.py : Parse attack logs for evaluation. utils.py : Utility function. metrics/ : Contains metrics for evalaution. accuracy.py asr.py clip.py fid.py mu/ : Core source code. algorithms/ : Implementation of various algorithms. Each algorithm has its own subdirectory containing code and a README.md with detailed documentation. esd/ : ESD algorithm components. README.md : Documentation specific to the ESD algorithm. algorithm.py : Core implementation of ESD. configs/ : Configuration files for training and generation tasks. constants/const.py : Constant values used across the ESD algorithm. environment.yaml : Environment setup for ESD. model.py : Model architectures specific to ESD. sampler.py : Sampling methods used during training or inference. scripts/train.py : Training script for ESD. evaluator.py : Script that generates necessary outputs for evaluation. trainer.py : Training routines and optimization strategies. utils.py : Utility functions and helpers. ca/ : Components for the CA algorithm. README.md : Documentation specific to the CA algorithm. ...and so on for other algorithms core/ : Foundational classes and utilities. base_algorithm.py : Abstract base class for algorithm implementations. base_data_handler.py : Base class for data handling. base_model.py : Base class for model definitions. base_sampler.py : Base class for sampling methods. base_trainer.py : Base class for training routines. datasets/ : Dataset management and utilities. __init__.py : Initializes the dataset package. dataset.py : Dataset classes and methods. helpers/ : Helper functions for data processing. unlearning_canvas_dataset.py : Specific dataset class for unlearning tasks. helpers/ : Utility functions and helpers. helper.py : General-purpose helper functions. logger.py : Logging utilities to standardize logging practices. path_setup.py : Path configurations and environment setup. tests/ : Test suites for ensuring code reliability. mu_attack/ : Implementation of attack algorithms. attackers/ : Contains different types of attackers. configs/ : Configurations file. illegal/ : config for illegal task. nudity/ : config for nudity task. object/ : config for object task. style/ : config for style task. violence/ : config for violence task. core/ : Foundational classes. datasets/ : script to generate dataset. exces/ : Script to run attack tasks/ : Implementation of tasks helpers/ : Utility functions mu_defense/ : Implementation of Advunlearn algorithms. algorithms/ : Implementation of various defense algorithms. Each algorithm has its own subdirectory containing code and a README.md with detailed documentation. adv_unlearn/ : Adversial Unlearn algorithm components. README.md : Documentation specific to the advunlearn algorithm. algorithm.py : Core implementation of advunlearn. configs/ : Configuration files for training and generation tasks. model.py : Model architectures specific to advunlearn. image_generator.py : Image generator methods for generating sample images for evaluation. evaluator.py : Script that generates necessary outputs for evaluation. dataset_handler.py : Dataset handler for advunlearn algorithm. compvis_trainer.py : training loop for CompVis models. diffuser_trainer.py : training loop for diffuser models. trainer.py : Trainer class orchestrates the adversarial unlearning training process. utils.py : Utility functions and helpers. scripts.py : Commands to generate datasets and download models. notebooks/ : Contains example implementation. tests/ : Contains pytests.","title":"Project Architecture"},{"location":"evaluation/contributing/#how-to-contribute","text":"","title":"How to Contribute"},{"location":"evaluation/contributing/#reporting-issues","text":"Check the to see if your bug or feature request has already been reported. If not, open a new issue. Provide a clear title and description, including steps to reproduce if it\u2019s a bug. Label your issue appropriately (e.g., \u201cbug\u201d, \u201cenhancement\u201d, \u201cdocumentation\u201d).","title":"Reporting Issues"},{"location":"evaluation/contributing/#suggesting-enhancements","text":"If you have an idea for a new feature or improvement, open a GitHub issue labeled \u201cenhancement\u201d or \u201cfeature request.\u201d Include any relevant use-cases, examples, or background information to help the community understand your proposal.","title":"Suggesting Enhancements"},{"location":"evaluation/contributing/#submitting-pull-requests","text":"Fork the repository and create your feature branch from main (or the appropriate branch if instructed otherwise): bash git checkout -b feature/my-awesome-feature Make your changes, including tests if applicable. Commit and push your changes: bash git push origin feature/my-awesome-feature Open a Pull Request from your fork into the main repo. Provide a clear description of what your PR does, referencing any related issues. Please ensure your PR follows the Code Style guidelines and includes updated or new tests if needed.","title":"Submitting Pull Requests"},{"location":"evaluation/contributing/#adding-a-new-algorithm","text":"One of the core goals of Unlearn Diff is to make it easy to add and benchmark new unlearning algorithms. Follow these steps when introducing a new method.","title":"Adding a New Algorithm"},{"location":"evaluation/contributing/#folder-structure","text":"Create a new subfolder in mu/algorithms/ with a clear, descriptive name (e.g., my_new_algo ). Inside this subfolder, include the following (at minimum): algorithm.py trainer.py scripts/train.py (and optionally scripts/evaluate.py if your training and evaluation are separate) configs/ containing YAML config files for training, evaluation, etc. environment.yaml specifying dependencies for conda Readme.md explaining how to install, run, and configure your new algorithm (Optional) data_handler.py , datasets/ , model.py , sampler.py , utils.py , etc., depending on your algorithm\u2019s design. Extend or import from base classes in mu/core/ if your algorithm logic aligns with any existing abstract classes (e.g., BaseAlgorithm , BaseTrainer , etc.). This ensures code consistency and easier maintenance.","title":"Folder Structure"},{"location":"evaluation/contributing/#adding-a-new-attack","text":"To add a new attack method, follow these guidelines: Folder Structure : Create a new file or subfolder under mu_attack/attackers/ with a clear, descriptive name (e.g., my_new_attack.py or mu_attack/attackers/my_new_attack/ ). If creating a subfolder, include essential files specific to attack implementation: Attacks : The main implementation file for the attack logic (e.g., attack.py ). Execs : Scripts or modules that execute the attack routines. Tasks : Task definitions for integrating and testing the attack. Any helper modules or configuration files specific to your attack. Update or add corresponding YAML configuration files and config class under mu_attack/configs/ if your attack requires custom settings. Implementation : Extend or import from the base class BaseAttacker (located in mu_attack/core/base_attacker.py ) if applicable. Ensure that your attack method adheres to the input-output standards defined by the project. Documentation & Testing : Add detailed documentation within your new attack module and update the main documentation if needed. Include tests covering your new attack method under the appropriate test directories. Environment : If your attack method has unique dependencies, update the environment.yaml file within the relevant directory or provide instructions in your documentation on how to create a dedicated environment. Also update the common environment file that is located in the project's root directory.","title":"Adding a New Attack"},{"location":"evaluation/contributing/#adding-a-new-defense","text":"To integrate a new defense mechanism, please follow these steps: Folder Structure : Create a new subfolder under mu_defense/algorithms/ with a descriptive name (e.g., my_new_defense ). Within this subfolder, include essential files such as: algorithm.py : Contains the core logic of your defense method. trainer.py : Contains training routines and optimization strategies. configs/ : Include configuration class for training and evaluation. environment.yaml : Specify dependencies unique to your defense method. Readme.md : Document usage instructions, configuration details, and any other relevant information. Implementation : Extend or use base classes provided in mu_defense/algorithms/core/ (e.g., base_algorithm.py , base_trainer.py ) to ensure consistency with existing methods. Implement any unique evaluation metrics or procedures if your defense requires them. Documentation & Testing : Document your defense method thoroughly within its Readme.md and update the global documentation if necessary. Provide tests for your defense implementation to ensure its reliability and compatibility with the rest of the system. Environment : If your defense algorithm has specific dependencies, use the provided environment.yaml file as a template and adjust it accordingly. Include clear instructions for users to create and activate the environment. Also update the common environment file that is located in the project's root directory.","title":"Adding a New Defense"},{"location":"evaluation/contributing/#creating-an-environment","text":"The default environment file is located in the project root directory ( requirements.txt ). Contributors should update this file as needed to include any new packages required by their contributions making sure it doesnot effect other algorthms. If your module or algorithm requires unique dependencies, you may add a dedicated environment file in its respective directory, but be sure to update and maintain the default environment in the root. Optionally, to keep dependencies organized, each algorithm has its own environment.yaml . Contributors are encouraged to: Specify all required packages (e.g., torch , torchvision , diffusers , etc.). Name the environment after the algorithm, e.g., mu_<algorithm_name> . Update instructions in your algorithm\u2019s Readme.md on how to install and activate this environment. Ensure you run the pytests in the tests/ directory after adding a new algorithm to verify that the updated environment remains compatible with existing algorithms.","title":"Creating an Environment"},{"location":"evaluation/contributing/#run-pytests","text":"Before integrating new algorithms into the repository, please run the existing tests using the modified environment file to ensure that your changes do not break any functionality. Steps to Run the Tests Prepare the Environment Configuration: Make sure the environment is set up using the provided tests/test_config.yaml file. This file contains configuration settings for various components of the project, such as data directories, model paths, and algorithm-specific parameters. It includes: common_config_unlearn_canvas_mu: Contains settings for the UnlearnCanvas dataset, including paths for the data directory, CompVis and Diffuser model checkpoints, template information, and dataset type. common_config_i2p: Provides configuration for the i2p dataset with similar settings as above. evaluator_config: Specifies the classifier checkpoint path and sample usage flag for evaluation. Algorithm-Specific Configurations: Settings for various algorithms like concept_ablation, forget_me_not, saliency_unlearning, semipermeable, unified_concept_editing, selective_amnesia, and attack/defense configurations. Run the Tests: Execute the following commands from the root of your repository: pytest tests/test_mu.py pytest tests/test_mu_attack.py pytest tests/test_mu_defense.py These commands will run tests for: tests/test_mu.py: Core machine unlearning functionalities including evaluation. tests/test_mu_attack.py: Attack-related functionalities including evaluation. tests/test_mu_defense.py: Defense-related functionalities including evaluation. Verify the Results: Ensure that all tests pass without errors. If any test fails, fix the issues before adding new algorithms to maintain compatibility with the existing codebase.","title":"Run pytests"},{"location":"evaluation/contributing/#documentation","text":"Each algorithm folder must contain a Readme.md that documents: High-level description of the algorithm (goals, references to papers, etc.). How to install any special dependencies (if not covered by environment.yaml ). How to run training and evaluation (with example commands). Explanation of the config files in configs/ . Any unique parameters or hyperparameters. If you introduce new metrics or logs, please outline them in your algorithm\u2019s README and update any relevant docs in the main docs/ folder as needed. If you have incorporated code or drawn inspiration from external codebases, please ensure that you properly credit and cite the original work within each script.","title":"Documentation"},{"location":"evaluation/contributing/#code-style","text":"We recommend following PEP 8 for Python code. Use descriptive variable names, docstrings, and type hints where possible. Keep functions and methods short and focused\u2014avoid large, monolithic methods. Use Python\u2019s built-in logging or the logging utilities in helpers/logger.py for consistent log outputs. Use wandb global wandb logger if required. Additionally, we prefer: - 4 spaces for indentation (no tabs). - Meaningful commit messages: \u201cFix dataset loader bug for large images\u201d vs. \u201cFix stuff.\u201d","title":"Code Style"},{"location":"evaluation/contributing/#contact","text":"If you have any questions, feel free to open an issue or reach out to us via email. We appreciate your contributions and look forward to collaborating with you on Unlearn Diff ! Thank you again for your interest in contributing. We\u2019re excited to see your ideas and improvements!","title":"Contact"},{"location":"evaluation/erase_diff/","text":"Erase Diff Evaluation Framework This section provides instructions for running the evaluation framework for the Erase Diff algorithm on Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying machine unlearning. Running the Evaluation Framework You can run the evaluation framework using the evaluate.py script located in the mu/algorithms/erase_diff/scripts/ directory. Work within the same environment used to perform unlearning for evaluation as well. Basic Command to Run Evaluation: Before running evaluation, download the classifier ckpt from here: https://drive.google.com/drive/folders/1AoazlvDgWgc3bAyHDpqlafqltmn4vm61 Add the following code to evaluate.py . from mu.algorithms.erase_diff import EraseDiffEvaluator from mu.algorithms.erase_diff.configs import ( erase_diff_evaluation_config ) from evaluation.metrics.accuracy import accuracy_score from evaluation.metrics.fid import fid_score evaluator = EraseDiffEvaluator( erase_diff_evaluation_config, ckpt_path=\"outputs/erase_diff/finetuned_models/erase_diff_self-harm_model.pth\", ) generated_images_path = evaluator.generate_images() accuracy = accuracy_score(gen_image_dir=generated_images_path, dataset_type = \"unlearncanvas\", classifier_ckpt_path = \"/home/ubuntu/Projects/models/classifier_ckpt_path/style50_cls.pth\", forget_theme=\"Bricks\", seed_list = [\"188\"] ) print(accuracy['acc']) print(accuracy['loss']) reference_image_dir = \"data/quick-canvas-dataset/sample\" fid, _ = fid_score(generated_image_dir=generated_images_path, reference_image_dir=reference_image_dir ) print(fid) Run the script python evaluate.py","title":"Usage"},{"location":"evaluation/erase_diff/#erase-diff-evaluation-framework","text":"This section provides instructions for running the evaluation framework for the Erase Diff algorithm on Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying machine unlearning.","title":"Erase Diff Evaluation Framework"},{"location":"evaluation/erase_diff/#running-the-evaluation-framework","text":"You can run the evaluation framework using the evaluate.py script located in the mu/algorithms/erase_diff/scripts/ directory. Work within the same environment used to perform unlearning for evaluation as well.","title":"Running the Evaluation Framework"},{"location":"evaluation/erase_diff/#basic-command-to-run-evaluation","text":"Before running evaluation, download the classifier ckpt from here: https://drive.google.com/drive/folders/1AoazlvDgWgc3bAyHDpqlafqltmn4vm61 Add the following code to evaluate.py . from mu.algorithms.erase_diff import EraseDiffEvaluator from mu.algorithms.erase_diff.configs import ( erase_diff_evaluation_config ) from evaluation.metrics.accuracy import accuracy_score from evaluation.metrics.fid import fid_score evaluator = EraseDiffEvaluator( erase_diff_evaluation_config, ckpt_path=\"outputs/erase_diff/finetuned_models/erase_diff_self-harm_model.pth\", ) generated_images_path = evaluator.generate_images() accuracy = accuracy_score(gen_image_dir=generated_images_path, dataset_type = \"unlearncanvas\", classifier_ckpt_path = \"/home/ubuntu/Projects/models/classifier_ckpt_path/style50_cls.pth\", forget_theme=\"Bricks\", seed_list = [\"188\"] ) print(accuracy['acc']) print(accuracy['loss']) reference_image_dir = \"data/quick-canvas-dataset/sample\" fid, _ = fid_score(generated_image_dir=generated_images_path, reference_image_dir=reference_image_dir ) print(fid) Run the script python evaluate.py","title":"Basic Command to Run Evaluation:"},{"location":"evaluation/esd/","text":"ESD Evaluation Framework This section provides instructions for running the evaluation framework for the ESD algorithm on Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying machine unlearning. Running the Evaluation Framework You can run the evaluation framework using the evaluate.py script located in the mu/algorithms/esd/scripts/ directory. Work within the same environment used to perform unlearning for evaluation as well. Basic Command to Run Evaluation: Before running evaluation, download the classifier ckpt from here: https://drive.google.com/drive/folders/1AoazlvDgWgc3bAyHDpqlafqltmn4vm61 Add the following code to evaluate.py . from mu.algorithms.esd import ESDAlgorithm from mu.algorithms.esd.configs import ( esd_evaluation_config ) from evaluation.metrics.accuracy import accuracy_score from evaluation.metrics.fid import fid_score evaluator = ESDAlgorithm( esd_evaluation_config, ckpt_path=\"outputs/esd/finetuned_models/esd_Bricks_model.pth\", ) generated_images_path = evaluator.generate_images() accuracy = accuracy_score(gen_image_dir=generated_images_path, dataset_type = \"unlearncanvas\", classifier_ckpt_path = \"/home/ubuntu/Projects/models/classifier_ckpt_path/style50_cls.pth\", forget_theme=\"Bricks\", seed_list = [\"188\"] ) print(accuracy['acc']) print(accuracy['loss']) reference_image_dir = \"data/quick-canvas-dataset/sample\" fid, _ = fid_score(generated_image_dir=generated_images_path, reference_image_dir=reference_image_dir ) print(fid) Run the script python evaluate.py","title":"Usage"},{"location":"evaluation/esd/#esd-evaluation-framework","text":"This section provides instructions for running the evaluation framework for the ESD algorithm on Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying machine unlearning.","title":"ESD Evaluation Framework"},{"location":"evaluation/esd/#running-the-evaluation-framework","text":"You can run the evaluation framework using the evaluate.py script located in the mu/algorithms/esd/scripts/ directory. Work within the same environment used to perform unlearning for evaluation as well.","title":"Running the Evaluation Framework"},{"location":"evaluation/esd/#basic-command-to-run-evaluation","text":"Before running evaluation, download the classifier ckpt from here: https://drive.google.com/drive/folders/1AoazlvDgWgc3bAyHDpqlafqltmn4vm61 Add the following code to evaluate.py . from mu.algorithms.esd import ESDAlgorithm from mu.algorithms.esd.configs import ( esd_evaluation_config ) from evaluation.metrics.accuracy import accuracy_score from evaluation.metrics.fid import fid_score evaluator = ESDAlgorithm( esd_evaluation_config, ckpt_path=\"outputs/esd/finetuned_models/esd_Bricks_model.pth\", ) generated_images_path = evaluator.generate_images() accuracy = accuracy_score(gen_image_dir=generated_images_path, dataset_type = \"unlearncanvas\", classifier_ckpt_path = \"/home/ubuntu/Projects/models/classifier_ckpt_path/style50_cls.pth\", forget_theme=\"Bricks\", seed_list = [\"188\"] ) print(accuracy['acc']) print(accuracy['loss']) reference_image_dir = \"data/quick-canvas-dataset/sample\" fid, _ = fid_score(generated_image_dir=generated_images_path, reference_image_dir=reference_image_dir ) print(fid) Run the script python evaluate.py","title":"Basic Command to Run Evaluation:"},{"location":"evaluation/forget_me_not/","text":"forget_me_not Evaluation Framework This section provides instructions for running the evaluation framework for the forget_me_not algorithm on Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying machine unlearning. Running the Evaluation Framework You can run the evaluation framework using the evaluate.py script located in the mu/algorithms/forget_me_not/scripts/ directory. Work within the same environment used to perform unlearning for evaluation as well. Basic Command to Run Evaluation: Before running evaluation, download the classifier ckpt from here: https://drive.google.com/drive/folders/1AoazlvDgWgc3bAyHDpqlafqltmn4vm61 Add the following code to evaluate.py from mu.algorithms.forget_me_not import ForgetMeNotEvaluator from mu.algorithms.forget_me_not.configs import ( forget_me_not_evaluation_config ) from evaluation.metrics.accuracy import accuracy_score from evaluation.metrics.fid import fid_score evaluator = ForgetMeNotEvaluator( forget_me_not_evaluation_config, ckpt_path=\"outputs/forget_me_not/finetuned_models/Abstractionism\", ) generated_images_path = evaluator.generate_images() reference_image_dir = \"/home/ubuntu/Projects/Palistha/testing/data/quick-canvas-dataset/sample\" accuracy = accuracy_score(gen_image_dir=generated_images_path, dataset_type = \"unlearncanvas\", classifier_ckpt_path = \"models/classifier_ckpt_path/style50_cls.pth\", reference_dir=reference_image_dir, forget_theme=\"Bricks\", seed_list = [\"188\"] ) print(accuracy['acc']) print(accuracy['loss']) fid, _ = fid_score(generated_image_dir=generated_images_path, reference_image_dir=reference_image_dir ) print(fid) Run the script python evaluate.py","title":"Usage"},{"location":"evaluation/forget_me_not/#forget_me_not-evaluation-framework","text":"This section provides instructions for running the evaluation framework for the forget_me_not algorithm on Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying machine unlearning.","title":"forget_me_not Evaluation Framework"},{"location":"evaluation/forget_me_not/#running-the-evaluation-framework","text":"You can run the evaluation framework using the evaluate.py script located in the mu/algorithms/forget_me_not/scripts/ directory. Work within the same environment used to perform unlearning for evaluation as well.","title":"Running the Evaluation Framework"},{"location":"evaluation/forget_me_not/#basic-command-to-run-evaluation","text":"Before running evaluation, download the classifier ckpt from here: https://drive.google.com/drive/folders/1AoazlvDgWgc3bAyHDpqlafqltmn4vm61 Add the following code to evaluate.py from mu.algorithms.forget_me_not import ForgetMeNotEvaluator from mu.algorithms.forget_me_not.configs import ( forget_me_not_evaluation_config ) from evaluation.metrics.accuracy import accuracy_score from evaluation.metrics.fid import fid_score evaluator = ForgetMeNotEvaluator( forget_me_not_evaluation_config, ckpt_path=\"outputs/forget_me_not/finetuned_models/Abstractionism\", ) generated_images_path = evaluator.generate_images() reference_image_dir = \"/home/ubuntu/Projects/Palistha/testing/data/quick-canvas-dataset/sample\" accuracy = accuracy_score(gen_image_dir=generated_images_path, dataset_type = \"unlearncanvas\", classifier_ckpt_path = \"models/classifier_ckpt_path/style50_cls.pth\", reference_dir=reference_image_dir, forget_theme=\"Bricks\", seed_list = [\"188\"] ) print(accuracy['acc']) print(accuracy['loss']) fid, _ = fid_score(generated_image_dir=generated_images_path, reference_image_dir=reference_image_dir ) print(fid) Run the script python evaluate.py","title":"Basic Command to Run Evaluation:"},{"location":"evaluation/saliency/","text":"Saliency Unlearning Evaluation Framework This section provides instructions for running the evaluation framework for the Saliency Unlearning algorithm on Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying machine unlearning. Running the Evaluation Framework You can run the evaluation framework using the evaluate.py script located in the mu/algorithms/saliency_unlearning/scripts/ directory. Work within the same environment used to perform unlearning for evaluation as well. Basic Command to Run Evaluation: Before running evaluation, download the classifier ckpt from here: https://drive.google.com/drive/folders/1AoazlvDgWgc3bAyHDpqlafqltmn4vm61 Add the following code to evaluate.py from mu.algorithms.saliency_unlearning import SaliencyUnlearningEvaluator from mu.algorithms.saliency_unlearning.configs import ( saliency_unlearning_evaluation_config ) from evaluation.metrics.accuracy import accuracy_score from evaluation.metrics.fid import fid_score evaluator = SaliencyUnlearningEvaluator( saliency_unlearning_evaluation_config, ckpt_path=\"outputs/saliency_unlearning/saliency_unlearning_Abstractionism_model.pth\", ) generated_images_path = evaluator.generate_images() reference_image_dir = \"data/quick-canvas-dataset/sample\" accuracy = accuracy_score(gen_image_dir=generated_images_path, dataset_type = \"unlearncanvas\", classifier_ckpt_path = \"models/classifier_ckpt_path/style50_cls.pth\", reference_dir=reference_image_dir, forget_theme=\"Bricks\", seed_list = [\"188\"] ) print(accuracy['acc']) print(accuracy['loss']) fid, _ = fid_score(generated_image_dir=generated_images_path, reference_image_dir=reference_image_dir ) print(fid) Run the script python evaluate.py","title":"Usage"},{"location":"evaluation/saliency/#saliency-unlearning-evaluation-framework","text":"This section provides instructions for running the evaluation framework for the Saliency Unlearning algorithm on Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying machine unlearning.","title":"Saliency Unlearning Evaluation Framework"},{"location":"evaluation/saliency/#running-the-evaluation-framework","text":"You can run the evaluation framework using the evaluate.py script located in the mu/algorithms/saliency_unlearning/scripts/ directory. Work within the same environment used to perform unlearning for evaluation as well.","title":"Running the Evaluation Framework"},{"location":"evaluation/saliency/#basic-command-to-run-evaluation","text":"Before running evaluation, download the classifier ckpt from here: https://drive.google.com/drive/folders/1AoazlvDgWgc3bAyHDpqlafqltmn4vm61 Add the following code to evaluate.py from mu.algorithms.saliency_unlearning import SaliencyUnlearningEvaluator from mu.algorithms.saliency_unlearning.configs import ( saliency_unlearning_evaluation_config ) from evaluation.metrics.accuracy import accuracy_score from evaluation.metrics.fid import fid_score evaluator = SaliencyUnlearningEvaluator( saliency_unlearning_evaluation_config, ckpt_path=\"outputs/saliency_unlearning/saliency_unlearning_Abstractionism_model.pth\", ) generated_images_path = evaluator.generate_images() reference_image_dir = \"data/quick-canvas-dataset/sample\" accuracy = accuracy_score(gen_image_dir=generated_images_path, dataset_type = \"unlearncanvas\", classifier_ckpt_path = \"models/classifier_ckpt_path/style50_cls.pth\", reference_dir=reference_image_dir, forget_theme=\"Bricks\", seed_list = [\"188\"] ) print(accuracy['acc']) print(accuracy['loss']) fid, _ = fid_score(generated_image_dir=generated_images_path, reference_image_dir=reference_image_dir ) print(fid) Run the script python evaluate.py","title":"Basic Command to Run Evaluation:"},{"location":"evaluation/scissorhands/","text":"Scissorshands Evaluation Framework This section provides instructions for running the evaluation framework for the Scissorshands algorithm on Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying machine unlearning. Running the Evaluation Framework You can run the evaluation framework using the evaluate.py script located in the mu/algorithms/scissorshands/scripts/ directory. Work within the same environment used to perform unlearning for evaluation as well. Basic Command to Run Evaluation: Before running evaluation, download the classifier ckpt from here: https://drive.google.com/drive/folders/1AoazlvDgWgc3bAyHDpqlafqltmn4vm61 Add the following code to evaluate.py from mu.algorithms.scissorhands import ScissorHandsEvaluator from mu.algorithms.scissorhands.configs import ( scissorhands_evaluation_config ) from evaluation.metrics.accuracy import accuracy_score from evaluation.metrics.fid import fid_score evaluator = ScissorHandsEvaluator( scissorhands_evaluation_config, ckpt_path=\"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", ) generated_images_path = evaluator.generate_images() reference_image_dir = \"data/quick-canvas-dataset/sample\" accuracy = accuracy_score(gen_image_dir=generated_images_path, dataset_type = \"unlearncanvas\", classifier_ckpt_path = \"models/classifier_ckpt_path/style50_cls.pth\", reference_dir=reference_image_dir, forget_theme=\"Bricks\", seed_list = [\"188\"] ) print(accuracy['acc']) print(accuracy['loss']) fid, _ = fid_score(generated_image_dir=generated_images_path, reference_image_dir=reference_image_dir ) print(fid) Run the script python evaluate.py","title":"Usage"},{"location":"evaluation/scissorhands/#scissorshands-evaluation-framework","text":"This section provides instructions for running the evaluation framework for the Scissorshands algorithm on Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying machine unlearning.","title":"Scissorshands Evaluation Framework"},{"location":"evaluation/scissorhands/#running-the-evaluation-framework","text":"You can run the evaluation framework using the evaluate.py script located in the mu/algorithms/scissorshands/scripts/ directory. Work within the same environment used to perform unlearning for evaluation as well.","title":"Running the Evaluation Framework"},{"location":"evaluation/scissorhands/#basic-command-to-run-evaluation","text":"Before running evaluation, download the classifier ckpt from here: https://drive.google.com/drive/folders/1AoazlvDgWgc3bAyHDpqlafqltmn4vm61 Add the following code to evaluate.py from mu.algorithms.scissorhands import ScissorHandsEvaluator from mu.algorithms.scissorhands.configs import ( scissorhands_evaluation_config ) from evaluation.metrics.accuracy import accuracy_score from evaluation.metrics.fid import fid_score evaluator = ScissorHandsEvaluator( scissorhands_evaluation_config, ckpt_path=\"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", ) generated_images_path = evaluator.generate_images() reference_image_dir = \"data/quick-canvas-dataset/sample\" accuracy = accuracy_score(gen_image_dir=generated_images_path, dataset_type = \"unlearncanvas\", classifier_ckpt_path = \"models/classifier_ckpt_path/style50_cls.pth\", reference_dir=reference_image_dir, forget_theme=\"Bricks\", seed_list = [\"188\"] ) print(accuracy['acc']) print(accuracy['loss']) fid, _ = fid_score(generated_image_dir=generated_images_path, reference_image_dir=reference_image_dir ) print(fid) Run the script python evaluate.py","title":"Basic Command to Run Evaluation:"},{"location":"evaluation/semipermeable_membrane/","text":"Semipermeable membrane Evaluation Framework This section provides instructions for running the evaluation framework for the Semipermeable membrane algorithm on Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying machine unlearning. Running the Evaluation Framework You can run the evaluation framework using the evaluate.py script located in the mu/algorithms/semipermeable_membrane/scripts/ directory. Work within the same environment used to perform unlearning for evaluation as well. Basic Command to Run Evaluation: Before running evaluation, download the classifier ckpt from here: https://drive.google.com/drive/folders/1AoazlvDgWgc3bAyHDpqlafqltmn4vm61 Add the following code to evaluate.py from mu.algorithms.semipermeable_membrane import SemipermeableMembraneEvaluator from mu.algorithms.semipermeable_membrane.configs import ( semipermeable_membrane_eval_config ) from evaluation.metrics.accuracy import accuracy_score from evaluation.metrics.fid import fid_score evaluator = SemipermeableMembraneEvaluator( semipermeable_membrane_eval_config, spm_path = [\"outputs/semipermiable/semipermeable_membrane_Abstractionism_last.safetensors\"], ) generated_images_path = evaluator.generate_images() reference_image_dir = \"/home/ubuntu/Projects/Palistha/testing/data/quick-canvas-dataset/sample\" accuracy = accuracy_score(gen_image_dir=generated_images_path, dataset_type = \"unlearncanvas\", classifier_ckpt_path = \"/home/ubuntu/Projects/models/classifier_ckpt_path/style50_cls.pth\", reference_dir=reference_image_dir, forget_theme=\"Bricks\", seed_list = [\"188\"] ) print(accuracy['acc']) print(accuracy['loss']) fid, _ = fid_score(generated_image_dir=generated_images_path, reference_image_dir=reference_image_dir ) print(fid) Run the script python evaluate.py","title":"Usage"},{"location":"evaluation/semipermeable_membrane/#semipermeable-membrane-evaluation-framework","text":"This section provides instructions for running the evaluation framework for the Semipermeable membrane algorithm on Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying machine unlearning.","title":"Semipermeable membrane Evaluation Framework"},{"location":"evaluation/semipermeable_membrane/#running-the-evaluation-framework","text":"You can run the evaluation framework using the evaluate.py script located in the mu/algorithms/semipermeable_membrane/scripts/ directory. Work within the same environment used to perform unlearning for evaluation as well.","title":"Running the Evaluation Framework"},{"location":"evaluation/semipermeable_membrane/#basic-command-to-run-evaluation","text":"Before running evaluation, download the classifier ckpt from here: https://drive.google.com/drive/folders/1AoazlvDgWgc3bAyHDpqlafqltmn4vm61 Add the following code to evaluate.py from mu.algorithms.semipermeable_membrane import SemipermeableMembraneEvaluator from mu.algorithms.semipermeable_membrane.configs import ( semipermeable_membrane_eval_config ) from evaluation.metrics.accuracy import accuracy_score from evaluation.metrics.fid import fid_score evaluator = SemipermeableMembraneEvaluator( semipermeable_membrane_eval_config, spm_path = [\"outputs/semipermiable/semipermeable_membrane_Abstractionism_last.safetensors\"], ) generated_images_path = evaluator.generate_images() reference_image_dir = \"/home/ubuntu/Projects/Palistha/testing/data/quick-canvas-dataset/sample\" accuracy = accuracy_score(gen_image_dir=generated_images_path, dataset_type = \"unlearncanvas\", classifier_ckpt_path = \"/home/ubuntu/Projects/models/classifier_ckpt_path/style50_cls.pth\", reference_dir=reference_image_dir, forget_theme=\"Bricks\", seed_list = [\"188\"] ) print(accuracy['acc']) print(accuracy['loss']) fid, _ = fid_score(generated_image_dir=generated_images_path, reference_image_dir=reference_image_dir ) print(fid) Run the script python evaluate.py","title":"Basic Command to Run Evaluation:"},{"location":"evaluation/uce/","text":"unified_concept_editing Evaluation Framework This section provides instructions for running the evaluation framework for the unified_concept_editing algorithm on Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying machine unlearning. Running the Evaluation Framework You can run the evaluation framework using the evaluate.py script located in the mu/algorithms/unified_concept_editing/scripts/ directory. Work within the same environment used to perform unlearning for evaluation as well. Before running evaluation, download the classifier ckpt from here: https://drive.google.com/drive/folders/1AoazlvDgWgc3bAyHDpqlafqltmn4vm61 Then, Add the following code to evaluate.py from mu.algorithms.unified_concept_editing import UnifiedConceptEditingEvaluator from mu.algorithms.unified_concept_editing.configs import ( uce_evaluation_config ) from evaluation.metrics.accuracy import accuracy_score from evaluation.metrics.fid import fid_score # reference_image_dir = \"data/generic\" evaluator = UnifiedConceptEditingEvaluator( uce_evaluation_config, ckpt_path=\"outputs/uce/uce_Abstractionism_model\", ) # model = evaluator.load_model() generated_images_path = evaluator.generate_images() reference_image_dir = \"/home/ubuntu/Projects/Palistha/testing/data/quick-canvas-dataset/sample\" accuracy = accuracy_score(gen_image_dir=generated_images_path, dataset_type = \"unlearncanvas\", classifier_ckpt_path = \"/home/ubuntu/Projects/models/classifier_ckpt_path/style50_cls.pth\", reference_dir=reference_image_dir, forget_theme=\"Bricks\", seed_list = [\"188\"] ) print(accuracy['acc']) print(accuracy['loss']) fid, _ = fid_score(generated_image_dir=generated_images_path, reference_image_dir=reference_image_dir ) print(fid) Running in Offline Mode: WANDB_MODE=offline python evaluate.py","title":"Usage"},{"location":"evaluation/uce/#unified_concept_editing-evaluation-framework","text":"This section provides instructions for running the evaluation framework for the unified_concept_editing algorithm on Stable Diffusion models. The evaluation framework is used to assess the performance of models after applying machine unlearning.","title":"unified_concept_editing Evaluation Framework"},{"location":"evaluation/uce/#running-the-evaluation-framework","text":"You can run the evaluation framework using the evaluate.py script located in the mu/algorithms/unified_concept_editing/scripts/ directory. Work within the same environment used to perform unlearning for evaluation as well. Before running evaluation, download the classifier ckpt from here: https://drive.google.com/drive/folders/1AoazlvDgWgc3bAyHDpqlafqltmn4vm61 Then, Add the following code to evaluate.py from mu.algorithms.unified_concept_editing import UnifiedConceptEditingEvaluator from mu.algorithms.unified_concept_editing.configs import ( uce_evaluation_config ) from evaluation.metrics.accuracy import accuracy_score from evaluation.metrics.fid import fid_score # reference_image_dir = \"data/generic\" evaluator = UnifiedConceptEditingEvaluator( uce_evaluation_config, ckpt_path=\"outputs/uce/uce_Abstractionism_model\", ) # model = evaluator.load_model() generated_images_path = evaluator.generate_images() reference_image_dir = \"/home/ubuntu/Projects/Palistha/testing/data/quick-canvas-dataset/sample\" accuracy = accuracy_score(gen_image_dir=generated_images_path, dataset_type = \"unlearncanvas\", classifier_ckpt_path = \"/home/ubuntu/Projects/models/classifier_ckpt_path/style50_cls.pth\", reference_dir=reference_image_dir, forget_theme=\"Bricks\", seed_list = [\"188\"] ) print(accuracy['acc']) print(accuracy['loss']) fid, _ = fid_score(generated_image_dir=generated_images_path, reference_image_dir=reference_image_dir ) print(fid) Running in Offline Mode: WANDB_MODE=offline python evaluate.py","title":"Running the Evaluation Framework"},{"location":"evaluation/configs/adv_unlearn_evaluation/","text":"Description of Evaluation Configuration Parameters model_name: Type: str Description: Name of the model to use. Options include \"SD-v1-4\" , \"SD-V2\" , \"SD-V2-1\" , etc. required: False encoder_model_name_or_path Description : Model name or path for the encoder. Type : str Example : CompVis/stable-diffusion-v1-4 target_ckpt: Type: str Description: Path to the target checkpoint. If empty, the script will load the default model weights. If provided, it supports both Diffusers-format checkpoints (directory) and CompVis checkpoints (file ending with .pt ). For CompVis, use the checkpoint of the model saved as Diffuser format. save_path: Type: str Description: Directory where the generated images will be saved. prompts_path: Type: str Description: Path to the CSV file containing prompts, evaluation seeds, and case numbers. Default: \"data/prompts/visualization_example.csv\" guidance_scale: Type: float Description: Parameter that controls the classifier-free guidance during generation. Default: 7.5 image_size: Type: int Description: Dimensions of the generated images (height and width). Default: 512 ddim_steps: Type: int Description: Number of denoising steps (used in the diffusion process). Default: 100 num_samples: Type: int Description: Number of samples generated for each prompt. Default: 1 from_case: Type: int Description: Minimum case number from which to start generating images. Default: 0 folder_suffix: Type: str Description: Suffix added to the output folder name for visualizations. prompt_path: Type: str Description: Path to the CSV file containing prompts for evaluation. Example: \"data/prompts/coco_10k.csv\" devices: Type: str Description: Comma-separated list of device IDs to be used during evaluation. Example: \"0,0\"","title":"Config"},{"location":"evaluation/configs/adv_unlearn_evaluation/#description-of-evaluation-configuration-parameters","text":"model_name: Type: str Description: Name of the model to use. Options include \"SD-v1-4\" , \"SD-V2\" , \"SD-V2-1\" , etc. required: False encoder_model_name_or_path Description : Model name or path for the encoder. Type : str Example : CompVis/stable-diffusion-v1-4 target_ckpt: Type: str Description: Path to the target checkpoint. If empty, the script will load the default model weights. If provided, it supports both Diffusers-format checkpoints (directory) and CompVis checkpoints (file ending with .pt ). For CompVis, use the checkpoint of the model saved as Diffuser format. save_path: Type: str Description: Directory where the generated images will be saved. prompts_path: Type: str Description: Path to the CSV file containing prompts, evaluation seeds, and case numbers. Default: \"data/prompts/visualization_example.csv\" guidance_scale: Type: float Description: Parameter that controls the classifier-free guidance during generation. Default: 7.5 image_size: Type: int Description: Dimensions of the generated images (height and width). Default: 512 ddim_steps: Type: int Description: Number of denoising steps (used in the diffusion process). Default: 100 num_samples: Type: int Description: Number of samples generated for each prompt. Default: 1 from_case: Type: int Description: Minimum case number from which to start generating images. Default: 0 folder_suffix: Type: str Description: Suffix added to the output folder name for visualizations. prompt_path: Type: str Description: Path to the CSV file containing prompts for evaluation. Example: \"data/prompts/coco_10k.csv\" devices: Type: str Description: Comma-separated list of device IDs to be used during evaluation. Example: \"0,0\"","title":"Description of Evaluation Configuration Parameters"},{"location":"evaluation/configs/concept_ablation/","text":"Description of parameters in evaluation_config The evaluation_config contains the necessary parameters for running the Concept ablation evaluation framework. Below is a detailed description of each parameter along with examples. Model Configuration: ckpt_path : Path to the finetuned Stable Diffusion checkpoint file to be evaluated. Type: str Example: \"outputs/concept_ablation/finetuned_models/concept_ablation_Abstractionism_model.pth\" classification_model : Specifies the classification model used for evaluating the generated outputs. Type: str Example: \"vit_large_patch16_224\" Training and Sampling Parameters: devices : CUDA device IDs to be used for the evaluation process. Type: str Example: \"0\" cfg_text : Classifier-free guidance scale value for image generation. Higher values increase the strength of the conditioning prompt. Type: float Example: 9.0 seed : Random seed for reproducibility of results. Type: int Example: 188 ddim_steps : Number of steps for the DDIM (Denoising Diffusion Implicit Models) sampling process. Type: int Example: 100 ddim_eta : DDIM eta value for controlling the amount of randomness during sampling. Set to 0 for deterministic sampling. Type: float Example: 0.0 image_height : Height of the generated images in pixels. Type: int Example: 512 image_width : Width of the generated images in pixels. Type: int Example: 512 Output and Logging Parameters: sampler_output_dir : Directory where generated images will be saved during evaluation. Type: str Example: \"outputs/eval_results/mu_results/concept_ablation/\" use_sample: If you want to just run on sample dataset then set it as True. By default it is True. Type: bool Example: True dataset_type: Type of dataset you are using: Type: str Example: unlearncanvas","title":"Config"},{"location":"evaluation/configs/concept_ablation/#description-of-parameters-in-evaluation_config","text":"The evaluation_config contains the necessary parameters for running the Concept ablation evaluation framework. Below is a detailed description of each parameter along with examples.","title":"Description of parameters in evaluation_config"},{"location":"evaluation/configs/concept_ablation/#model-configuration","text":"ckpt_path : Path to the finetuned Stable Diffusion checkpoint file to be evaluated. Type: str Example: \"outputs/concept_ablation/finetuned_models/concept_ablation_Abstractionism_model.pth\" classification_model : Specifies the classification model used for evaluating the generated outputs. Type: str Example: \"vit_large_patch16_224\"","title":"Model Configuration:"},{"location":"evaluation/configs/concept_ablation/#training-and-sampling-parameters","text":"devices : CUDA device IDs to be used for the evaluation process. Type: str Example: \"0\" cfg_text : Classifier-free guidance scale value for image generation. Higher values increase the strength of the conditioning prompt. Type: float Example: 9.0 seed : Random seed for reproducibility of results. Type: int Example: 188 ddim_steps : Number of steps for the DDIM (Denoising Diffusion Implicit Models) sampling process. Type: int Example: 100 ddim_eta : DDIM eta value for controlling the amount of randomness during sampling. Set to 0 for deterministic sampling. Type: float Example: 0.0 image_height : Height of the generated images in pixels. Type: int Example: 512 image_width : Width of the generated images in pixels. Type: int Example: 512","title":"Training and Sampling Parameters:"},{"location":"evaluation/configs/concept_ablation/#output-and-logging-parameters","text":"sampler_output_dir : Directory where generated images will be saved during evaluation. Type: str Example: \"outputs/eval_results/mu_results/concept_ablation/\" use_sample: If you want to just run on sample dataset then set it as True. By default it is True. Type: bool Example: True dataset_type: Type of dataset you are using: Type: str Example: unlearncanvas","title":"Output and Logging Parameters:"},{"location":"evaluation/configs/erase_diff/","text":"Description of parameters in evaluation_config.yaml The evaluation_config.yaml file contains the necessary parameters for running the Erase Diff evaluation framework. Below is a detailed description of each parameter along with examples. Model Configuration: ckpt_path : Path to the finetuned Stable Diffusion checkpoint file to be evaluated. Type: str Example: \"outputs/erase_diff/finetuned_models/erase_diff_Abstractionism_model.pth\" classification_model : Specifies the classification model used for evaluating the generated outputs. Type: str Example: \"vit_large_patch16_224\" Training and Sampling Parameters: devices : CUDA device IDs to be used for the evaluation process. Type: str Example: \"0\" cfg_text : Classifier-free guidance scale value for image generation. Higher values increase the strength of the conditioning prompt. Type: float Example: 9.0 seed : Random seed for reproducibility of results. Type: int Example: 188 ddim_steps : Number of steps for the DDIM (Denoising Diffusion Implicit Models) sampling process. Type: int Example: 100 ddim_eta : DDIM eta value for controlling the amount of randomness during sampling. Set to 0 for deterministic sampling. Type: float Example: 0.0 image_height : Height of the generated images in pixels. Type: int Example: 512 image_width : Width of the generated images in pixels. Type: int Example: 512 Output and Logging Parameters: sampler_output_dir : Directory where generated images will be saved during evaluation. Type: str Example: \"outputs/eval_results/mu_results/erase_diff/\" Optimization Parameters: forget_theme : Concept or style intended for removal in the evaluation process. Type: str Example: \"Bricks\" use_sample: If you want to just run on sample dataset then set it as True. By default it is True. Type: bool Example: True","title":"Config"},{"location":"evaluation/configs/erase_diff/#description-of-parameters-in-evaluation_configyaml","text":"The evaluation_config.yaml file contains the necessary parameters for running the Erase Diff evaluation framework. Below is a detailed description of each parameter along with examples.","title":"Description of parameters in evaluation_config.yaml"},{"location":"evaluation/configs/erase_diff/#model-configuration","text":"ckpt_path : Path to the finetuned Stable Diffusion checkpoint file to be evaluated. Type: str Example: \"outputs/erase_diff/finetuned_models/erase_diff_Abstractionism_model.pth\" classification_model : Specifies the classification model used for evaluating the generated outputs. Type: str Example: \"vit_large_patch16_224\"","title":"Model Configuration:"},{"location":"evaluation/configs/erase_diff/#training-and-sampling-parameters","text":"devices : CUDA device IDs to be used for the evaluation process. Type: str Example: \"0\" cfg_text : Classifier-free guidance scale value for image generation. Higher values increase the strength of the conditioning prompt. Type: float Example: 9.0 seed : Random seed for reproducibility of results. Type: int Example: 188 ddim_steps : Number of steps for the DDIM (Denoising Diffusion Implicit Models) sampling process. Type: int Example: 100 ddim_eta : DDIM eta value for controlling the amount of randomness during sampling. Set to 0 for deterministic sampling. Type: float Example: 0.0 image_height : Height of the generated images in pixels. Type: int Example: 512 image_width : Width of the generated images in pixels. Type: int Example: 512","title":"Training and Sampling Parameters:"},{"location":"evaluation/configs/erase_diff/#output-and-logging-parameters","text":"sampler_output_dir : Directory where generated images will be saved during evaluation. Type: str Example: \"outputs/eval_results/mu_results/erase_diff/\"","title":"Output and Logging Parameters:"},{"location":"evaluation/configs/erase_diff/#optimization-parameters","text":"forget_theme : Concept or style intended for removal in the evaluation process. Type: str Example: \"Bricks\" use_sample: If you want to just run on sample dataset then set it as True. By default it is True. Type: bool Example: True","title":"Optimization Parameters:"},{"location":"evaluation/configs/esd/","text":"Description of parameters in evaluation_config.yaml The evaluation_config.yaml file contains the necessary parameters for running the ESD evaluation framework. Below is a detailed description of each parameter along with examples. Model Configuration: ckpt_path : Path to the finetuned Stable Diffusion checkpoint file to be evaluated. Type: str Example: \"outputs/esd/finetuned_models/esd_Abstractionism_model.pth\" classification_model : Specifies the classification model used for evaluating the generated outputs. Type: str Example: \"vit_large_patch16_224\" Training and Sampling Parameters: devices : CUDA device IDs to be used for the evaluation process. Type: str Example: \"0\" cfg_text : Classifier-free guidance scale value for image generation. Higher values increase the strength of the conditioning prompt. Type: float Example: 9.0 seed : Random seed for reproducibility of results. Type: int Example: 188 ddim_steps : Number of steps for the DDIM (Denoising Diffusion Implicit Models) sampling process. Type: int Example: 100 ddim_eta : DDIM eta value for controlling the amount of randomness during sampling. Set to 0 for deterministic sampling. Type: float Example: 0.0 image_height : Height of the generated images in pixels. Type: int Example: 512 image_width : Width of the generated images in pixels. Type: int Example: 512 Output and Logging Parameters: sampler_output_dir : Directory where generated images will be saved during evaluation. Type: str Example: \"outputs/eval_results/mu_results/esd/\" Optimization Parameters: forget_theme : Concept or style intended for removal in the evaluation process. Type: str Example: \"Bricks\" seed_list : List of random seeds for performing multiple evaluations with different randomness levels. Type: list Example: [\"188\"] use_sample: If you want to just run on sample dataset then set it as True. By default it is True. Type: bool Example: True","title":"Config"},{"location":"evaluation/configs/esd/#description-of-parameters-in-evaluation_configyaml","text":"The evaluation_config.yaml file contains the necessary parameters for running the ESD evaluation framework. Below is a detailed description of each parameter along with examples.","title":"Description of parameters in evaluation_config.yaml"},{"location":"evaluation/configs/esd/#model-configuration","text":"ckpt_path : Path to the finetuned Stable Diffusion checkpoint file to be evaluated. Type: str Example: \"outputs/esd/finetuned_models/esd_Abstractionism_model.pth\" classification_model : Specifies the classification model used for evaluating the generated outputs. Type: str Example: \"vit_large_patch16_224\"","title":"Model Configuration:"},{"location":"evaluation/configs/esd/#training-and-sampling-parameters","text":"devices : CUDA device IDs to be used for the evaluation process. Type: str Example: \"0\" cfg_text : Classifier-free guidance scale value for image generation. Higher values increase the strength of the conditioning prompt. Type: float Example: 9.0 seed : Random seed for reproducibility of results. Type: int Example: 188 ddim_steps : Number of steps for the DDIM (Denoising Diffusion Implicit Models) sampling process. Type: int Example: 100 ddim_eta : DDIM eta value for controlling the amount of randomness during sampling. Set to 0 for deterministic sampling. Type: float Example: 0.0 image_height : Height of the generated images in pixels. Type: int Example: 512 image_width : Width of the generated images in pixels. Type: int Example: 512","title":"Training and Sampling Parameters:"},{"location":"evaluation/configs/esd/#output-and-logging-parameters","text":"sampler_output_dir : Directory where generated images will be saved during evaluation. Type: str Example: \"outputs/eval_results/mu_results/esd/\"","title":"Output and Logging Parameters:"},{"location":"evaluation/configs/esd/#optimization-parameters","text":"forget_theme : Concept or style intended for removal in the evaluation process. Type: str Example: \"Bricks\" seed_list : List of random seeds for performing multiple evaluations with different randomness levels. Type: list Example: [\"188\"] use_sample: If you want to just run on sample dataset then set it as True. By default it is True. Type: bool Example: True","title":"Optimization Parameters:"},{"location":"evaluation/configs/forget_me_not/","text":"Description of parameters in evaluation_config.yaml The evaluation_config.yaml file contains the necessary parameters for running the forget_me_not evaluation framework. Below is a detailed description of each parameter along with examples. Model Configuration: ckpt_path : Path to the finetuned Stable Diffusion checkpoint file to be evaluated. Type: str Example: \"outputs/forget_me_not/finetuned_models/forget_me_not_Abstractionism_model.pth\" classification_model : Specifies the classification model used for evaluating the generated outputs. Type: str Example: \"vit_large_patch16_224\" Training and Sampling Parameters: theme : Specifies the theme or concept being evaluated for removal from the model's outputs. Type: str Example: \"Bricks\" devices : CUDA device IDs to be used for the evaluation process. Type: str Example: \"0\" cfg_text_list : Classifier-free guidance scale value for image generation. Higher values increase the strength of the conditioning prompt. Type: list Example: [9.0] seed : Random seed for reproducibility of results. Type: int Example: 188 ddim_steps : Number of steps for the DDIM (Denoising Diffusion Implicit Models) sampling process. Type: int Example: 100 ddim_eta : DDIM eta value for controlling the amount of randomness during sampling. Set to 0 for deterministic sampling. Type: float Example: 0.0 image_height : Height of the generated images in pixels. Type: int Example: 512 image_width : Width of the generated images in pixels. Type: int Example: 512 Output and Logging Parameters: sampler_output_dir : Directory where generated images will be saved during evaluation. Type: str Example: \"outputs/eval_results/mu_results/forget_me_not/\" Optimization Parameters: forget_theme : Concept or style intended for removal in the evaluation process. Type: str Example: \"Bricks\" seed_list : List of random seeds for performing multiple evaluations with different randomness levels. Type: list Example: [\"188\"] use_sample: If you want to just run on sample dataset then set it as True. By default it is True. Type: bool Example: True","title":"Config"},{"location":"evaluation/configs/forget_me_not/#description-of-parameters-in-evaluation_configyaml","text":"The evaluation_config.yaml file contains the necessary parameters for running the forget_me_not evaluation framework. Below is a detailed description of each parameter along with examples.","title":"Description of parameters in evaluation_config.yaml"},{"location":"evaluation/configs/forget_me_not/#model-configuration","text":"ckpt_path : Path to the finetuned Stable Diffusion checkpoint file to be evaluated. Type: str Example: \"outputs/forget_me_not/finetuned_models/forget_me_not_Abstractionism_model.pth\" classification_model : Specifies the classification model used for evaluating the generated outputs. Type: str Example: \"vit_large_patch16_224\"","title":"Model Configuration:"},{"location":"evaluation/configs/forget_me_not/#training-and-sampling-parameters","text":"theme : Specifies the theme or concept being evaluated for removal from the model's outputs. Type: str Example: \"Bricks\" devices : CUDA device IDs to be used for the evaluation process. Type: str Example: \"0\" cfg_text_list : Classifier-free guidance scale value for image generation. Higher values increase the strength of the conditioning prompt. Type: list Example: [9.0] seed : Random seed for reproducibility of results. Type: int Example: 188 ddim_steps : Number of steps for the DDIM (Denoising Diffusion Implicit Models) sampling process. Type: int Example: 100 ddim_eta : DDIM eta value for controlling the amount of randomness during sampling. Set to 0 for deterministic sampling. Type: float Example: 0.0 image_height : Height of the generated images in pixels. Type: int Example: 512 image_width : Width of the generated images in pixels. Type: int Example: 512","title":"Training and Sampling Parameters:"},{"location":"evaluation/configs/forget_me_not/#output-and-logging-parameters","text":"sampler_output_dir : Directory where generated images will be saved during evaluation. Type: str Example: \"outputs/eval_results/mu_results/forget_me_not/\"","title":"Output and Logging Parameters:"},{"location":"evaluation/configs/forget_me_not/#optimization-parameters","text":"forget_theme : Concept or style intended for removal in the evaluation process. Type: str Example: \"Bricks\" seed_list : List of random seeds for performing multiple evaluations with different randomness levels. Type: list Example: [\"188\"] use_sample: If you want to just run on sample dataset then set it as True. By default it is True. Type: bool Example: True","title":"Optimization Parameters:"},{"location":"evaluation/configs/saliency/","text":"Description of parameters in evaluation_config.yaml The evaluation_config.yaml file contains the necessary parameters for running the Saliency Unlearning evaluation framework. Below is a detailed description of each parameter along with examples. Model Configuration: ckpt_path : Path to the finetuned Stable Diffusion checkpoint file to be evaluated. Type: str Example: \"outputs/saliency_unlearning/finetuned_models/saliency_unlearning_Abstractionism_model.pth\" classification_model : Specifies the classification model used for evaluating the generated outputs. Type: str Example: \"vit_large_patch16_224\" Training and Sampling Parameters: forget_theme : Specifies the theme or concept being evaluated for removal from the model's outputs. Type: str Example: \"Bricks\" devices : CUDA device IDs to be used for the evaluation process. Type: str Example: \"0\" cfg_text : Classifier-free guidance scale value for image generation. Higher values increase the strength of the conditioning prompt. Type: float Example: 9.0 seed : Random seed for reproducibility of results. Type: int Example: 188 ddim_steps : Number of steps for the DDIM (Denoising Diffusion Implicit Models) sampling process. Type: int Example: 100 ddim_eta : DDIM eta value for controlling the amount of randomness during sampling. Set to 0 for deterministic sampling. Type: float Example: 0.0 image_height : Height of the generated images in pixels. Type: int Example: 512 image_width : Width of the generated images in pixels. Type: int Example: 512 Output and Logging Parameters: sampler_output_dir : Directory where generated images will be saved during evaluation. Type: str Example: \"outputs/eval_results/mu_results/saliency_unlearning/\" Performance and Efficiency Parameters: multiprocessing : Enables multiprocessing for faster evaluation for FID score. Recommended for large datasets. Type: bool Example: False batch_size : Batch size used during FID computation and evaluation. Type: int Example: 16 Optimization Parameters: forget_theme : Concept or style intended for removal in the evaluation process. Type: str Example: \"Bricks\" seed_list : List of random seeds for performing multiple evaluations with different randomness levels. Type: list Example: [\"188\"] use_sample: If you want to just run on sample dataset then set it as True. By default it is True. Type: bool Example: True","title":"Config"},{"location":"evaluation/configs/saliency/#description-of-parameters-in-evaluation_configyaml","text":"The evaluation_config.yaml file contains the necessary parameters for running the Saliency Unlearning evaluation framework. Below is a detailed description of each parameter along with examples.","title":"Description of parameters in evaluation_config.yaml"},{"location":"evaluation/configs/saliency/#model-configuration","text":"ckpt_path : Path to the finetuned Stable Diffusion checkpoint file to be evaluated. Type: str Example: \"outputs/saliency_unlearning/finetuned_models/saliency_unlearning_Abstractionism_model.pth\" classification_model : Specifies the classification model used for evaluating the generated outputs. Type: str Example: \"vit_large_patch16_224\"","title":"Model Configuration:"},{"location":"evaluation/configs/saliency/#training-and-sampling-parameters","text":"forget_theme : Specifies the theme or concept being evaluated for removal from the model's outputs. Type: str Example: \"Bricks\" devices : CUDA device IDs to be used for the evaluation process. Type: str Example: \"0\" cfg_text : Classifier-free guidance scale value for image generation. Higher values increase the strength of the conditioning prompt. Type: float Example: 9.0 seed : Random seed for reproducibility of results. Type: int Example: 188 ddim_steps : Number of steps for the DDIM (Denoising Diffusion Implicit Models) sampling process. Type: int Example: 100 ddim_eta : DDIM eta value for controlling the amount of randomness during sampling. Set to 0 for deterministic sampling. Type: float Example: 0.0 image_height : Height of the generated images in pixels. Type: int Example: 512 image_width : Width of the generated images in pixels. Type: int Example: 512","title":"Training and Sampling Parameters:"},{"location":"evaluation/configs/saliency/#output-and-logging-parameters","text":"sampler_output_dir : Directory where generated images will be saved during evaluation. Type: str Example: \"outputs/eval_results/mu_results/saliency_unlearning/\"","title":"Output and Logging Parameters:"},{"location":"evaluation/configs/saliency/#performance-and-efficiency-parameters","text":"multiprocessing : Enables multiprocessing for faster evaluation for FID score. Recommended for large datasets. Type: bool Example: False batch_size : Batch size used during FID computation and evaluation. Type: int Example: 16","title":"Performance and Efficiency Parameters:"},{"location":"evaluation/configs/saliency/#optimization-parameters","text":"forget_theme : Concept or style intended for removal in the evaluation process. Type: str Example: \"Bricks\" seed_list : List of random seeds for performing multiple evaluations with different randomness levels. Type: list Example: [\"188\"] use_sample: If you want to just run on sample dataset then set it as True. By default it is True. Type: bool Example: True","title":"Optimization Parameters:"},{"location":"evaluation/configs/scissorhands/","text":"Description of parameters in evaluation_config.yaml The evaluation_config.yaml file contains the necessary parameters for running the Scissorshands evaluation framework. Below is a detailed description of each parameter along with examples. Model Configuration: ckpt_path : Path to the finetuned Stable Diffusion checkpoint file to be evaluated. Type: str Example: \"outputs/scissorshands/finetuned_models/scissorshands_Abstractionism_model.pth\" classification_model : Specifies the classification model used for evaluating the generated outputs. Type: str Example: \"vit_large_patch16_224\" Training and Sampling Parameters: devices : CUDA device IDs to be used for the evaluation process. Type: str Example: \"0\" cfg_text : Classifier-free guidance scale value for image generation. Higher values increase the strength of the conditioning prompt. Type: float Example: 9.0 seed : Random seed for reproducibility of results. Type: int Example: 188 ddim_steps : Number of steps for the DDIM (Denoising Diffusion Implicit Models) sampling process. Type: int Example: 100 ddim_eta : DDIM eta value for controlling the amount of randomness during sampling. Set to 0 for deterministic sampling. Type: float Example: 0.0 image_height : Height of the generated images in pixels. Type: int Example: 512 image_width : Width of the generated images in pixels. Type: int Example: 512 Output and Logging Parameters: sampler_output_dir : Directory where generated images will be saved during evaluation. Type: str Example: \"outputs/eval_results/mu_results/scissorshands/\" eval_output_dir : Directory where evaluation metrics and results will be stored. Type: str Example: \"outputs/eval_results/mu_results/scissorshands/\" reference_dir : Directory containing original images for comparison during evaluation. Type: str Example: \"msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\" Performance and Efficiency Parameters: multiprocessing : Enables multiprocessing for faster evaluation for FID score. Recommended for large datasets. Type: bool Example: False batch_size : Batch size used during FID computation and evaluation. Type: int Example: 16 Optimization Parameters: forget_theme : Concept or style intended for removal in the evaluation process. Type: str Example: \"Bricks\" seed_list : List of random seeds for performing multiple evaluations with different randomness levels. Type: list Example: [\"188\"] use_sample: If you want to just run on sample dataset then set it as True. By default it is True. Type: bool Example: True","title":"Config"},{"location":"evaluation/configs/scissorhands/#description-of-parameters-in-evaluation_configyaml","text":"The evaluation_config.yaml file contains the necessary parameters for running the Scissorshands evaluation framework. Below is a detailed description of each parameter along with examples.","title":"Description of parameters in evaluation_config.yaml"},{"location":"evaluation/configs/scissorhands/#model-configuration","text":"ckpt_path : Path to the finetuned Stable Diffusion checkpoint file to be evaluated. Type: str Example: \"outputs/scissorshands/finetuned_models/scissorshands_Abstractionism_model.pth\" classification_model : Specifies the classification model used for evaluating the generated outputs. Type: str Example: \"vit_large_patch16_224\"","title":"Model Configuration:"},{"location":"evaluation/configs/scissorhands/#training-and-sampling-parameters","text":"devices : CUDA device IDs to be used for the evaluation process. Type: str Example: \"0\" cfg_text : Classifier-free guidance scale value for image generation. Higher values increase the strength of the conditioning prompt. Type: float Example: 9.0 seed : Random seed for reproducibility of results. Type: int Example: 188 ddim_steps : Number of steps for the DDIM (Denoising Diffusion Implicit Models) sampling process. Type: int Example: 100 ddim_eta : DDIM eta value for controlling the amount of randomness during sampling. Set to 0 for deterministic sampling. Type: float Example: 0.0 image_height : Height of the generated images in pixels. Type: int Example: 512 image_width : Width of the generated images in pixels. Type: int Example: 512","title":"Training and Sampling Parameters:"},{"location":"evaluation/configs/scissorhands/#output-and-logging-parameters","text":"sampler_output_dir : Directory where generated images will be saved during evaluation. Type: str Example: \"outputs/eval_results/mu_results/scissorshands/\" eval_output_dir : Directory where evaluation metrics and results will be stored. Type: str Example: \"outputs/eval_results/mu_results/scissorshands/\" reference_dir : Directory containing original images for comparison during evaluation. Type: str Example: \"msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\"","title":"Output and Logging Parameters:"},{"location":"evaluation/configs/scissorhands/#performance-and-efficiency-parameters","text":"multiprocessing : Enables multiprocessing for faster evaluation for FID score. Recommended for large datasets. Type: bool Example: False batch_size : Batch size used during FID computation and evaluation. Type: int Example: 16","title":"Performance and Efficiency Parameters:"},{"location":"evaluation/configs/scissorhands/#optimization-parameters","text":"forget_theme : Concept or style intended for removal in the evaluation process. Type: str Example: \"Bricks\" seed_list : List of random seeds for performing multiple evaluations with different randomness levels. Type: list Example: [\"188\"] use_sample: If you want to just run on sample dataset then set it as True. By default it is True. Type: bool Example: True","title":"Optimization Parameters:"},{"location":"evaluation/configs/semipermeable_membrane/","text":"Description of parameters in evaluation_config.yaml The evaluation_config.yaml file contains the necessary parameters for running the Semipermeable membrane evaluation framework. Below is a detailed description of each parameter along with examples. Model Configuration Parameters: spm_path: paths to finetuned model checkpoint. Type: list Example: outputs/semipermeable_membrane/finetuned_models/semipermeable_membrane_Abstractionism_last.safetensors precision : Specifies the numerical precision for model computation. Type: str Options: \"fp32\" , \"fp16\" , \"bf16\" Example: \"fp32\" spm_multiplier: Specifies the multiplier for Semipermeable Membrane (SPM) model. Type: float Example: 1.0 v2 : Specifies whether to use version 2.x of the model. Type: bool Example: false matching_metric : Metric used for evaluating the similarity between generated prompts and erased concepts. Type: str Options: \"clipcos\" , \"clipcos_tokenuni\" , \"tokenuni\" Example: \"clipcos_tokenuni\" model_config : Path to the model configuration YAML file. Type: str Example: \"mu/algorithms/semipermeable_membrane/config\" model_ckpt_path: Path to pretrained Stable Diffusion model. Type : str Example : models/diffuser/style50 Sampling Parameters: seed : Random seed for reproducibility of the evaluation process. Type: int Example: 188 devices : Specifies the CUDA devices for running the model. Type: str (Comma-separated for multiple devices) Example: \"0\" task : Specifies the task type for the evaluation process. Type: str Options: \"class\" , \"style\" Example: \"class\" Output Parameters: sampler_output_dir : Directory where generated images will be saved during the sampling process. Type: str Example: \"outputs/eval_results/mu_results/semipermeable_membrane/\" Dataset and Classification Parameters: reference_dir : Path to the reference dataset used for evaluation and comparison. Type: str Example: \"msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\" classification_model : Specifies the classification model used for the evaluation. Type: str Example: \"vit_large_patch16_224\" forget_theme : Specifies the theme to be forgotten during the unlearning process. Type: str Example: \"Bricks\" Performance Parameters: seed_list : List of random seeds for multiple evaluation trials. Type: list Example: [\"188\"] use_sample: If you want to just run on sample dataset then set it as True. By default it is True. Type: bool Example: True","title":"Config"},{"location":"evaluation/configs/semipermeable_membrane/#description-of-parameters-in-evaluation_configyaml","text":"The evaluation_config.yaml file contains the necessary parameters for running the Semipermeable membrane evaluation framework. Below is a detailed description of each parameter along with examples.","title":"Description of parameters in evaluation_config.yaml"},{"location":"evaluation/configs/semipermeable_membrane/#model-configuration-parameters","text":"spm_path: paths to finetuned model checkpoint. Type: list Example: outputs/semipermeable_membrane/finetuned_models/semipermeable_membrane_Abstractionism_last.safetensors precision : Specifies the numerical precision for model computation. Type: str Options: \"fp32\" , \"fp16\" , \"bf16\" Example: \"fp32\" spm_multiplier: Specifies the multiplier for Semipermeable Membrane (SPM) model. Type: float Example: 1.0 v2 : Specifies whether to use version 2.x of the model. Type: bool Example: false matching_metric : Metric used for evaluating the similarity between generated prompts and erased concepts. Type: str Options: \"clipcos\" , \"clipcos_tokenuni\" , \"tokenuni\" Example: \"clipcos_tokenuni\" model_config : Path to the model configuration YAML file. Type: str Example: \"mu/algorithms/semipermeable_membrane/config\" model_ckpt_path: Path to pretrained Stable Diffusion model. Type : str Example : models/diffuser/style50","title":"Model Configuration Parameters:"},{"location":"evaluation/configs/semipermeable_membrane/#sampling-parameters","text":"seed : Random seed for reproducibility of the evaluation process. Type: int Example: 188 devices : Specifies the CUDA devices for running the model. Type: str (Comma-separated for multiple devices) Example: \"0\" task : Specifies the task type for the evaluation process. Type: str Options: \"class\" , \"style\" Example: \"class\"","title":"Sampling Parameters:"},{"location":"evaluation/configs/semipermeable_membrane/#output-parameters","text":"sampler_output_dir : Directory where generated images will be saved during the sampling process. Type: str Example: \"outputs/eval_results/mu_results/semipermeable_membrane/\"","title":"Output Parameters:"},{"location":"evaluation/configs/semipermeable_membrane/#dataset-and-classification-parameters","text":"reference_dir : Path to the reference dataset used for evaluation and comparison. Type: str Example: \"msu_unlearningalgorithm/data/quick-canvas-dataset/sample/\" classification_model : Specifies the classification model used for the evaluation. Type: str Example: \"vit_large_patch16_224\" forget_theme : Specifies the theme to be forgotten during the unlearning process. Type: str Example: \"Bricks\"","title":"Dataset and Classification Parameters:"},{"location":"evaluation/configs/semipermeable_membrane/#performance-parameters","text":"seed_list : List of random seeds for multiple evaluation trials. Type: list Example: [\"188\"] use_sample: If you want to just run on sample dataset then set it as True. By default it is True. Type: bool Example: True","title":"Performance Parameters:"},{"location":"evaluation/configs/uce/","text":"Description of parameters in evaluation_config.yaml The evaluation_config.yaml file contains the necessary parameters for running the unified_concept_editing evaluation framework. Below is a detailed description of each parameter along with examples. Model Configuration: ckpt_path : Path to the finetuned Stable Diffusion checkpoint file to be evaluated. Type: str Example: \"outputs/unified_concept_editing/finetuned_models/unified_concept_editing_Abstractionism_model.pth\" classification_model : Specifies the classification model used for evaluating the generated outputs. Type: str Example: \"vit_large_patch16_224\" Training and Sampling Parameters: theme : Specifies the theme or concept being evaluated for removal from the model's outputs. Type: str Example: \"Bricks\" devices : CUDA device IDs to be used for the evaluation process. Type: str Example: \"0\" cfg_text : Classifier-free guidance scale value for image generation. Higher values increase the strength of the conditioning prompt. Type: float Example: 9.0 seed : Random seed for reproducibility of results. Type: int Example: 188 ddim_steps : Number of steps for the DDIM (Denoising Diffusion Implicit Models) sampling process. Type: int Example: 100 ddim_eta : DDIM eta value for controlling the amount of randomness during sampling. Set to 0 for deterministic sampling. Type: float Example: 0.0 image_height : Height of the generated images in pixels. Type: int Example: 512 image_width : Width of the generated images in pixels. Type: int Example: 512 Output and Logging Parameters: sampler_output_dir : Directory where generated images will be saved during evaluation. Type: str Example: \"outputs/eval_results/mu_results/unified_concept_editing/\" Optimization Parameters: forget_theme : Concept or style intended for removal in the evaluation process. Type: str Example: \"Bricks\" seed_list : List of random seeds for performing multiple evaluations with different randomness levels. Type: list Example: [\"188\"] use_sample: If you want to just run on sample dataset then set it as True. By default it is True. Type: bool Example: True","title":"Config"},{"location":"evaluation/configs/uce/#description-of-parameters-in-evaluation_configyaml","text":"The evaluation_config.yaml file contains the necessary parameters for running the unified_concept_editing evaluation framework. Below is a detailed description of each parameter along with examples.","title":"Description of parameters in evaluation_config.yaml"},{"location":"evaluation/configs/uce/#model-configuration","text":"ckpt_path : Path to the finetuned Stable Diffusion checkpoint file to be evaluated. Type: str Example: \"outputs/unified_concept_editing/finetuned_models/unified_concept_editing_Abstractionism_model.pth\" classification_model : Specifies the classification model used for evaluating the generated outputs. Type: str Example: \"vit_large_patch16_224\"","title":"Model Configuration:"},{"location":"evaluation/configs/uce/#training-and-sampling-parameters","text":"theme : Specifies the theme or concept being evaluated for removal from the model's outputs. Type: str Example: \"Bricks\" devices : CUDA device IDs to be used for the evaluation process. Type: str Example: \"0\" cfg_text : Classifier-free guidance scale value for image generation. Higher values increase the strength of the conditioning prompt. Type: float Example: 9.0 seed : Random seed for reproducibility of results. Type: int Example: 188 ddim_steps : Number of steps for the DDIM (Denoising Diffusion Implicit Models) sampling process. Type: int Example: 100 ddim_eta : DDIM eta value for controlling the amount of randomness during sampling. Set to 0 for deterministic sampling. Type: float Example: 0.0 image_height : Height of the generated images in pixels. Type: int Example: 512 image_width : Width of the generated images in pixels. Type: int Example: 512","title":"Training and Sampling Parameters:"},{"location":"evaluation/configs/uce/#output-and-logging-parameters","text":"sampler_output_dir : Directory where generated images will be saved during evaluation. Type: str Example: \"outputs/eval_results/mu_results/unified_concept_editing/\"","title":"Output and Logging Parameters:"},{"location":"evaluation/configs/uce/#optimization-parameters","text":"forget_theme : Concept or style intended for removal in the evaluation process. Type: str Example: \"Bricks\" seed_list : List of random seeds for performing multiple evaluations with different randomness levels. Type: list Example: [\"188\"] use_sample: If you want to just run on sample dataset then set it as True. By default it is True. Type: bool Example: True","title":"Optimization Parameters:"},{"location":"mu_attack/Readme_MU_Attack/","text":"UnlearnDiffAttak This repository contains the implementation of UnlearnDiffAtk, a framework for evaluating the robustness of safety-driven unlearned Diffusion Models using adversarial prompts. Usage This section contains the usage guide for the package. Installation Prerequisities Ensure conda is installed on your system. You can install Miniconda or Anaconda: Miniconda (recommended): https://docs.conda.io/en/latest/miniconda.html Anaconda : https://www.anaconda.com/products/distribution After installing conda , ensure it is available in your PATH by running. You may require to restart the terminal session: Before installing the unlearn_diff package, follow these steps to set up your environment correctly. These instructions ensure compatibility with the required dependencies, including Python, PyTorch, and ONNX Runtime. Step-by-Step Setup: Step 1. Create a Conda Environment Create a new Conda environment named myenv with Python 3.8.5: conda create -n myenv python=3.8.5 Step 2. Activate the Environment Activate the environment to work within it: conda activate myenv Step 3. Install Core Dependencies Install PyTorch, torchvision, CUDA Toolkit, and ONNX Runtime with specific versions: conda install pytorch==1.11.0 torchvision==0.12.0 cudatoolkit=11.3 onnxruntime==1.16.3 -c pytorch -c conda-forge Step 4. Install our unlearn_diff Package using pip: pip install unlearn_diff Step 5. Install Additional Git Dependencies: After installing unlearn_diff, install the following Git-based dependencies in the same Conda environment to ensure full functionality: pip install git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers pip install git+https://github.com/openai/CLIP.git@main#egg=clip pip install git+https://github.com/crowsonkb/k-diffusion.git pip install git+https://github.com/cocodataset/panopticapi.git pip install git+https://github.com/Phoveran/fastargs.git@main#egg=fastargs pip install git+https://github.com/boomb0om/text2image-benchmark Optional(Create environment for specific algorithm): If you want to create algorithm specific environment then use command given below: create_env erase_diff The has to be one of the folders in the mu/algorithms folder. Generate Dataset python -m scripts.generate_dataset --prompts_path data/prompts/prompts.csv --concept i2p_nude --save_path outputs/dataset Description of fields in config json file for diffuser overall This section defines the high-level configuration for the attack. task : The name of the task being performed. Type: str Example: P4D, classifer attacker: Specifies the attack type. Type: str Example: hard_prompt, no_attack logger: Defines the logging mechanism. Type: str Example: JSON resume: Option to resume from previous checkpoint. task concept: The concept targeted by the attack. Type: str Example: nudity, harm diffusers_model_name_or_path: Path to the pre-trained checkpoint of the diffuser model. Type: str Example: \"outputs/semipermeable_membrane/finetuned_models/\" target_ckpt: Path to the target model checkpoint used in the attack. Type: str Example: \"files/pretrained/SD-1-4/ESD_ckpt/Nudity-ESDx1-UNET-SD.pt\" cache_path: Directory to cache intermediate results. Type: str Example: \".cache\" dataset_path: Path to the dataset used for the attack. Type: str Example: \"outputs/dataset/i2p_nude\" criterion: The loss function or criterion used during the attack. Type: str Example: \"l2\" classifier_dir: Directory for the classifier, if applicable. null if not used. Type: str Example: \"/path/classifier_dir\" sampling_step_num: Number of sampling steps during the attack. Type: int Example: 1 sld: Strength of latent disentanglement. Type: str Example: \"weak\" sld_concept: Concept tied to latent disentanglement. Type: str Example: \"nudity\" negative_prompt: The negative prompt used to steer the generation. Type: str Example: \"\" backend: Specifies the backend model i.e \"diffuser\". attacker insertion_location: The point of insertion for the prompt. Type: str Example: \"prefix_k\" k: The value of k for the prompt insertion point. Type: int Example: 5 iteration: Number of iterations for the attack. Type: int Example: 1 seed_iteration: Random seed for the iterative process. Type: int Example: 1 attack_idx: Index of the attack for evaluation purposes. Type: int Example: 0 eval_seed: Seed value used for evaluation. Type: int Example: 0 universal: Whether the attack is universal (true or false). Type: bool Example: false sequential: Whether the attack is applied sequentially. Type: bool Example: true lr: Learning rate for the attack optimization process. Type: float Example: 0.01 weight_decay: Weight decay applied during optimization. Type: float Example: 0.1 logger json: Logging configuration. root: Path to the directory where logs will be saved. Type: str Example: \"results/hard_prompt_esd_nudity_P4D\" name: Name for the log file or experiment. - Type: str - Example: \"P4D\" Description of fields in config json file for compvis overall This section defines the high-level configuration for the attack. task : The name of the task being performed. Type: str Example: P4D, classifer attacker: Specifies the attack type. Type: str Example: hard_prompt, no_attack logger: Defines the logging mechanism. Type: str Example: JSON resume: Option to resume from previous checkpoint. task concept: The concept targeted by the attack. Type: str Example: nudity, harm compvis_ckpt_path: Path to the pre-trained checkpoint of the CompVis model. Type: str Example: \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\" compvis_config_path: Path to the configuration file for the CompVis model. Type: str Example: \"configs/scissorhands/model_config.yaml\" cache_path: Directory to cache intermediate results. Type: str Example: \".cache\" dataset_path: Path to the dataset used for the attack. Type: str Example: \"outputs/dataset/i2p_nude\" criterion: The loss function or criterion used during the attack. Type: str Example: \"l2\" classifier_dir: Directory for the classifier, if applicable. null if not used. Type: str Example: \"/path/classifier_dir\" sampling_step_num: Number of sampling steps during the attack. Type: int Example: 1 sld: Strength of latent disentanglement. Type: str Example: \"weak\" sld_concept: Concept tied to latent disentanglement. Type: str Example: \"nudity\" negative_prompt: The negative prompt used to steer the generation. Type: str Example: \"\" backend: Specifies the backend model i.e \"compvis\". attacker insertion_location: The point of insertion for the prompt. Type: str Example: \"prefix_k\" k: The value of k for the prompt insertion point. Type: int Example: 5 iteration: Number of iterations for the attack. Type: int Example: 1 seed_iteration: Random seed for the iterative process. Type: int Example: 1 attack_idx: Index of the attack for evaluation purposes. Type: int Example: 0 eval_seed: Seed value used for evaluation. Type: int Example: 0 universal: Whether the attack is universal (true or false). Type: bool Example: false sequential: Whether the attack is applied sequentially. Type: bool Example: true lr: Learning rate for the attack optimization process. Type: float Example: 0.01 weight_decay: Weight decay applied during optimization. Type: float Example: 0.1 logger json: Logging configuration. root: Path to the directory where logs will be saved. Type: str Example: \"results/hard_prompt_esd_nudity_P4D\" name: Name for the log file or experiment. - Type: str - Example: \"P4D\" Evaluation: In this section, we assess the performance and robustness of the results generated by the attack algorithms Running the Evaluation Framework Create a file, eg, evaluate.py and use examples and modify your configs to run the file. Example Code from mu_attack.evaluators.asr import ASREvaluator from mu_attack.evaluators.clip_score import ClipScoreEvaluator from mu_attack.evaluators.fid import FIDEvaluator from mu_attack.configs.evaluation import attack_evaluation_config def main(): # Initialize the configuration config = attack_evaluation_config config.asr.root = \"/home/ubuntu/Projects/Palistha/unlearn_diff_attack/results/hard_prompt_esd_nudity_P4D_concept_ablation/P4d\" config.asr.root_no_attack = \"/home/ubuntu/Projects/Palistha/unlearn_diff_attack/results/no_attack_esd_nudity/NoAttackEsdNudity\" config.clip.devices = \"0\" config.clip.image_path = \"/home/ubuntu/Projects/Palistha/unlearn_diff_attack/results/hard_prompt_esd_nudity_P4D_concept_ablation/P4d/images\" config.clip.log_path = \"/home/ubuntu/Projects/Palistha/unlearn_diff_attack/results/hard_prompt_esd_nudity_P4D_concept_ablation/P4d/log.json\" config.fid.ref_batch_path = \"/home/ubuntu/Projects/Palistha/unlearn_diff_attack/results/hard_prompt_esd_nudity_P4D_concept_ablation/P4d/images\" config.fid.sample_batch_path = \"/home/ubuntu/Projects/balaram/unlearn_diff_attack/outputs/dataset/i2p_nude/imgs\" # Common output path config.output_path = \"/home/ubuntu/Projects/Palistha/unlearn_diff_attack/results/evaluation/results.json\" # Initialize and run the ASR evaluator asr_evaluator = ASREvaluator( config = attack_evaluation_config, root=config.asr.root, root_no_attack=config.asr.root_no_attack, output_path=config.output_path ) print(\"Running ASR Evaluator...\") asr_evaluator.run() # Initialize and run the CLIP Score evaluator clip_evaluator = ClipScoreEvaluator( config = attack_evaluation_config, image_path=config.clip.image_path, log_path=config.clip.log_path, output_path=config.output_path, devices = config.clip.devices ) print(\"Running CLIP Score Evaluator...\") clip_evaluator.run() # Initialize and run the FID evaluator fid_evaluator = FIDEvaluator( config = attack_evaluation_config, ref_batch_path=config.fid.ref_batch_path, sample_batch_path=config.fid.sample_batch_path, output_path=config.output_path ) print(\"Running FID Evaluator...\") fid_evaluator.run() if __name__ == \"__main__\": main() ] Running the Training Script in Offline Mode WANDB_MODE=offline python evaluate.py How It Works * Default Values: The script first loads default values from the evluation config file as in configs section. Parameter Overrides: Any parameters passed directly to the algorithm, overrides these configs. Final Configuration: The script merges the configs and convert them into dictionary to proceed with the evaluation. Evaluation Metrics: Attack Succes Rate (ASR) Fr\u00e9chet inception distance(FID): evaluate distributional quality of image generations, lower is better. CLIP score : measure contextual alignment with prompt descriptions, higher is better. Configuration File Structure for Evaluator ASR Evaluator Configuration root: Directory containing results with attack. root-no-attack: Directory containing results without attack. Clip Evaluator Configuration image_path: Path to the directory containing generated images to evaluate. devices: Device ID(s) to use for evaluation. Example: \"0\" for the first GPU or \"0,1\" for multiple GPUs. log_path: Path to the log file containing prompt for the generated images. model_name_or_path: Path or model name for the pre-trained CLIP model. Default is \"openai/clip-vit-base-patch32\". FID Evaluator Configuration ref_batch_path: Path to the directory containing reference images. sample_batch_path: Path to the directory containing generated/sample images. Global Configuration output_path: Path to save the evaluation results as a JSON file.","title":"Readme MU Attack"},{"location":"mu_attack/Readme_MU_Attack/#unlearndiffattak","text":"This repository contains the implementation of UnlearnDiffAtk, a framework for evaluating the robustness of safety-driven unlearned Diffusion Models using adversarial prompts.","title":"UnlearnDiffAttak"},{"location":"mu_attack/Readme_MU_Attack/#usage","text":"This section contains the usage guide for the package.","title":"Usage"},{"location":"mu_attack/Readme_MU_Attack/#installation","text":"","title":"Installation"},{"location":"mu_attack/Readme_MU_Attack/#prerequisities","text":"Ensure conda is installed on your system. You can install Miniconda or Anaconda: Miniconda (recommended): https://docs.conda.io/en/latest/miniconda.html Anaconda : https://www.anaconda.com/products/distribution After installing conda , ensure it is available in your PATH by running. You may require to restart the terminal session: Before installing the unlearn_diff package, follow these steps to set up your environment correctly. These instructions ensure compatibility with the required dependencies, including Python, PyTorch, and ONNX Runtime. Step-by-Step Setup: Step 1. Create a Conda Environment Create a new Conda environment named myenv with Python 3.8.5: conda create -n myenv python=3.8.5 Step 2. Activate the Environment Activate the environment to work within it: conda activate myenv Step 3. Install Core Dependencies Install PyTorch, torchvision, CUDA Toolkit, and ONNX Runtime with specific versions: conda install pytorch==1.11.0 torchvision==0.12.0 cudatoolkit=11.3 onnxruntime==1.16.3 -c pytorch -c conda-forge Step 4. Install our unlearn_diff Package using pip: pip install unlearn_diff Step 5. Install Additional Git Dependencies: After installing unlearn_diff, install the following Git-based dependencies in the same Conda environment to ensure full functionality: pip install git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers pip install git+https://github.com/openai/CLIP.git@main#egg=clip pip install git+https://github.com/crowsonkb/k-diffusion.git pip install git+https://github.com/cocodataset/panopticapi.git pip install git+https://github.com/Phoveran/fastargs.git@main#egg=fastargs pip install git+https://github.com/boomb0om/text2image-benchmark","title":"Prerequisities"},{"location":"mu_attack/Readme_MU_Attack/#optionalcreate-environment-for-specific-algorithm","text":"If you want to create algorithm specific environment then use command given below: create_env erase_diff The has to be one of the folders in the mu/algorithms folder.","title":"Optional(Create environment for specific algorithm):"},{"location":"mu_attack/Readme_MU_Attack/#generate-dataset","text":"python -m scripts.generate_dataset --prompts_path data/prompts/prompts.csv --concept i2p_nude --save_path outputs/dataset","title":"Generate Dataset"},{"location":"mu_attack/Readme_MU_Attack/#description-of-fields-in-config-json-file-for-diffuser","text":"overall This section defines the high-level configuration for the attack. task : The name of the task being performed. Type: str Example: P4D, classifer attacker: Specifies the attack type. Type: str Example: hard_prompt, no_attack logger: Defines the logging mechanism. Type: str Example: JSON resume: Option to resume from previous checkpoint. task concept: The concept targeted by the attack. Type: str Example: nudity, harm diffusers_model_name_or_path: Path to the pre-trained checkpoint of the diffuser model. Type: str Example: \"outputs/semipermeable_membrane/finetuned_models/\" target_ckpt: Path to the target model checkpoint used in the attack. Type: str Example: \"files/pretrained/SD-1-4/ESD_ckpt/Nudity-ESDx1-UNET-SD.pt\" cache_path: Directory to cache intermediate results. Type: str Example: \".cache\" dataset_path: Path to the dataset used for the attack. Type: str Example: \"outputs/dataset/i2p_nude\" criterion: The loss function or criterion used during the attack. Type: str Example: \"l2\" classifier_dir: Directory for the classifier, if applicable. null if not used. Type: str Example: \"/path/classifier_dir\" sampling_step_num: Number of sampling steps during the attack. Type: int Example: 1 sld: Strength of latent disentanglement. Type: str Example: \"weak\" sld_concept: Concept tied to latent disentanglement. Type: str Example: \"nudity\" negative_prompt: The negative prompt used to steer the generation. Type: str Example: \"\" backend: Specifies the backend model i.e \"diffuser\". attacker insertion_location: The point of insertion for the prompt. Type: str Example: \"prefix_k\" k: The value of k for the prompt insertion point. Type: int Example: 5 iteration: Number of iterations for the attack. Type: int Example: 1 seed_iteration: Random seed for the iterative process. Type: int Example: 1 attack_idx: Index of the attack for evaluation purposes. Type: int Example: 0 eval_seed: Seed value used for evaluation. Type: int Example: 0 universal: Whether the attack is universal (true or false). Type: bool Example: false sequential: Whether the attack is applied sequentially. Type: bool Example: true lr: Learning rate for the attack optimization process. Type: float Example: 0.01 weight_decay: Weight decay applied during optimization. Type: float Example: 0.1 logger json: Logging configuration. root: Path to the directory where logs will be saved. Type: str Example: \"results/hard_prompt_esd_nudity_P4D\" name: Name for the log file or experiment. - Type: str - Example: \"P4D\"","title":"Description of fields in config json file for diffuser"},{"location":"mu_attack/Readme_MU_Attack/#description-of-fields-in-config-json-file-for-compvis","text":"overall This section defines the high-level configuration for the attack. task : The name of the task being performed. Type: str Example: P4D, classifer attacker: Specifies the attack type. Type: str Example: hard_prompt, no_attack logger: Defines the logging mechanism. Type: str Example: JSON resume: Option to resume from previous checkpoint. task concept: The concept targeted by the attack. Type: str Example: nudity, harm compvis_ckpt_path: Path to the pre-trained checkpoint of the CompVis model. Type: str Example: \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\" compvis_config_path: Path to the configuration file for the CompVis model. Type: str Example: \"configs/scissorhands/model_config.yaml\" cache_path: Directory to cache intermediate results. Type: str Example: \".cache\" dataset_path: Path to the dataset used for the attack. Type: str Example: \"outputs/dataset/i2p_nude\" criterion: The loss function or criterion used during the attack. Type: str Example: \"l2\" classifier_dir: Directory for the classifier, if applicable. null if not used. Type: str Example: \"/path/classifier_dir\" sampling_step_num: Number of sampling steps during the attack. Type: int Example: 1 sld: Strength of latent disentanglement. Type: str Example: \"weak\" sld_concept: Concept tied to latent disentanglement. Type: str Example: \"nudity\" negative_prompt: The negative prompt used to steer the generation. Type: str Example: \"\" backend: Specifies the backend model i.e \"compvis\". attacker insertion_location: The point of insertion for the prompt. Type: str Example: \"prefix_k\" k: The value of k for the prompt insertion point. Type: int Example: 5 iteration: Number of iterations for the attack. Type: int Example: 1 seed_iteration: Random seed for the iterative process. Type: int Example: 1 attack_idx: Index of the attack for evaluation purposes. Type: int Example: 0 eval_seed: Seed value used for evaluation. Type: int Example: 0 universal: Whether the attack is universal (true or false). Type: bool Example: false sequential: Whether the attack is applied sequentially. Type: bool Example: true lr: Learning rate for the attack optimization process. Type: float Example: 0.01 weight_decay: Weight decay applied during optimization. Type: float Example: 0.1 logger json: Logging configuration. root: Path to the directory where logs will be saved. Type: str Example: \"results/hard_prompt_esd_nudity_P4D\" name: Name for the log file or experiment. - Type: str - Example: \"P4D\"","title":"Description of fields in config json file for compvis"},{"location":"mu_attack/Readme_MU_Attack/#evaluation","text":"In this section, we assess the performance and robustness of the results generated by the attack algorithms","title":"Evaluation:"},{"location":"mu_attack/Readme_MU_Attack/#running-the-evaluation-framework","text":"Create a file, eg, evaluate.py and use examples and modify your configs to run the file. Example Code from mu_attack.evaluators.asr import ASREvaluator from mu_attack.evaluators.clip_score import ClipScoreEvaluator from mu_attack.evaluators.fid import FIDEvaluator from mu_attack.configs.evaluation import attack_evaluation_config def main(): # Initialize the configuration config = attack_evaluation_config config.asr.root = \"/home/ubuntu/Projects/Palistha/unlearn_diff_attack/results/hard_prompt_esd_nudity_P4D_concept_ablation/P4d\" config.asr.root_no_attack = \"/home/ubuntu/Projects/Palistha/unlearn_diff_attack/results/no_attack_esd_nudity/NoAttackEsdNudity\" config.clip.devices = \"0\" config.clip.image_path = \"/home/ubuntu/Projects/Palistha/unlearn_diff_attack/results/hard_prompt_esd_nudity_P4D_concept_ablation/P4d/images\" config.clip.log_path = \"/home/ubuntu/Projects/Palistha/unlearn_diff_attack/results/hard_prompt_esd_nudity_P4D_concept_ablation/P4d/log.json\" config.fid.ref_batch_path = \"/home/ubuntu/Projects/Palistha/unlearn_diff_attack/results/hard_prompt_esd_nudity_P4D_concept_ablation/P4d/images\" config.fid.sample_batch_path = \"/home/ubuntu/Projects/balaram/unlearn_diff_attack/outputs/dataset/i2p_nude/imgs\" # Common output path config.output_path = \"/home/ubuntu/Projects/Palistha/unlearn_diff_attack/results/evaluation/results.json\" # Initialize and run the ASR evaluator asr_evaluator = ASREvaluator( config = attack_evaluation_config, root=config.asr.root, root_no_attack=config.asr.root_no_attack, output_path=config.output_path ) print(\"Running ASR Evaluator...\") asr_evaluator.run() # Initialize and run the CLIP Score evaluator clip_evaluator = ClipScoreEvaluator( config = attack_evaluation_config, image_path=config.clip.image_path, log_path=config.clip.log_path, output_path=config.output_path, devices = config.clip.devices ) print(\"Running CLIP Score Evaluator...\") clip_evaluator.run() # Initialize and run the FID evaluator fid_evaluator = FIDEvaluator( config = attack_evaluation_config, ref_batch_path=config.fid.ref_batch_path, sample_batch_path=config.fid.sample_batch_path, output_path=config.output_path ) print(\"Running FID Evaluator...\") fid_evaluator.run() if __name__ == \"__main__\": main() ] Running the Training Script in Offline Mode WANDB_MODE=offline python evaluate.py How It Works * Default Values: The script first loads default values from the evluation config file as in configs section. Parameter Overrides: Any parameters passed directly to the algorithm, overrides these configs. Final Configuration: The script merges the configs and convert them into dictionary to proceed with the evaluation. Evaluation Metrics: Attack Succes Rate (ASR) Fr\u00e9chet inception distance(FID): evaluate distributional quality of image generations, lower is better. CLIP score : measure contextual alignment with prompt descriptions, higher is better. Configuration File Structure for Evaluator ASR Evaluator Configuration root: Directory containing results with attack. root-no-attack: Directory containing results without attack. Clip Evaluator Configuration image_path: Path to the directory containing generated images to evaluate. devices: Device ID(s) to use for evaluation. Example: \"0\" for the first GPU or \"0,1\" for multiple GPUs. log_path: Path to the log file containing prompt for the generated images. model_name_or_path: Path or model name for the pre-trained CLIP model. Default is \"openai/clip-vit-base-patch32\". FID Evaluator Configuration ref_batch_path: Path to the directory containing reference images. sample_batch_path: Path to the directory containing generated/sample images. Global Configuration output_path: Path to save the evaluation results as a JSON file.","title":"Running the Evaluation Framework"},{"location":"mu_attack/mu_attack_evaluation/","text":"Evaluation: In this section, we assess the performance and robustness of the results generated by the attack algorithms Activate Environment You can either use default environment or mu_attack specific environment. Use Default Step-by-Step Setup: Step 1. Create a Conda Environment Create a new Conda environment named myenv with Python 3.8.5: conda create -n myenv python=3.8.5 Step 2. Activate the Environment Activate the environment to work within it: conda activate myenv Step 3. Install Core Dependencies Install PyTorch, torchvision, CUDA Toolkit, and ONNX Runtime with specific versions: conda install pytorch==1.11.0 torchvision==0.12.0 cudatoolkit=11.3 onnxruntime==1.16.3 -c pytorch -c conda-forge Step 4. Install our unlearn_diff Package using pip: pip install unlearn_diff Step 5. Install Additional Git Dependencies: After installing unlearn_diff, install the following Git-based dependencies in the same Conda environment to ensure full functionality: pip install git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers pip install git+https://github.com/openai/CLIP.git@main#egg=clip pip install git+https://github.com/crowsonkb/k-diffusion.git pip install git+https://github.com/cocodataset/panopticapi.git pip install git+https://github.com/Phoveran/fastargs.git@main#egg=fastargs pip install git+https://github.com/boomb0om/text2image-benchmark Running the Evaluation Framework Create a file, eg, evaluate.py and use examples and modify your configs to run the evalautions. Example Code from evaluation.metrics.asr import asr_score from evaluation.metrics.clip import clip_score from evaluation.metrics.fid import fid_score root = \"/home/ubuntu/Projects/Palistha/testing/results/hard_prompt_esd_nudity_P4D_erase_diff/P4d\" root_no_attack =\"/home/ubuntu/Projects/Palistha/testing/results/no_attack_esd_nudity_P4D_abstrctionism/NoAttackEsdNudity\" asr_val = asr_score(root, root_no_attack) print(asr_val) images = \"/home/ubuntu/Projects/Palistha/testing/results/hard_prompt_esd_nudity_P4D_erase_diff_compvis_to_diffuser/P4d/images\" prompt_path = \"/home/ubuntu/Projects/Palistha/testing/results/hard_prompt_esd_nudity_P4D_erase_diff_compvis_to_diffuser/P4d/log.json\" device = \"0\" clip_val = clip_score(images, prompt_path, device) print(clip_val) gen_path = \"/home/ubuntu/Projects/Palistha/testing/results/hard_prompt_esd_nudity_P4D_erase_diff/P4d/images\" ref_path = \"data/i2p/nude\" fid_val = fid_score(gen_path,ref_path) print(fid_val) Running the Training Script in Offline Mode WANDB_MODE=offline python evaluate.py Evaluation Metrics: Attack Succes Rate (ASR) Fr\u00e9chet inception distance(FID): evaluate distributional quality of image generations, lower is better. CLIP score : measure contextual alignment with prompt descriptions, higher is better. Configuration File Structure for Evaluator ASR Evaluator Configuration root: Directory containing results with attack. root-no-attack: Directory containing results without attack. Clip Evaluator Configuration image_path: Path to the directory containing generated images to evaluate. devices: Device ID(s) to use for evaluation. Example: \"0\" for the first GPU or \"0,1\" for multiple GPUs. log_path: Path to the log file containing prompt for the generated images. model_name_or_path: Path or model name for the pre-trained CLIP model. Default is \"openai/clip-vit-base-patch32\". FID Evaluator Configuration ref_batch_path: Path to the directory containing reference images. sample_batch_path: Path to the directory containing generated/sample images. Global Configuration output_path: Path to save the evaluation results as a JSON file.","title":"Mu attack evaluation"},{"location":"mu_attack/mu_attack_evaluation/#evaluation","text":"In this section, we assess the performance and robustness of the results generated by the attack algorithms","title":"Evaluation:"},{"location":"mu_attack/mu_attack_evaluation/#activate-environment","text":"You can either use default environment or mu_attack specific environment. Use Default Step-by-Step Setup: Step 1. Create a Conda Environment Create a new Conda environment named myenv with Python 3.8.5: conda create -n myenv python=3.8.5 Step 2. Activate the Environment Activate the environment to work within it: conda activate myenv Step 3. Install Core Dependencies Install PyTorch, torchvision, CUDA Toolkit, and ONNX Runtime with specific versions: conda install pytorch==1.11.0 torchvision==0.12.0 cudatoolkit=11.3 onnxruntime==1.16.3 -c pytorch -c conda-forge Step 4. Install our unlearn_diff Package using pip: pip install unlearn_diff Step 5. Install Additional Git Dependencies: After installing unlearn_diff, install the following Git-based dependencies in the same Conda environment to ensure full functionality: pip install git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers pip install git+https://github.com/openai/CLIP.git@main#egg=clip pip install git+https://github.com/crowsonkb/k-diffusion.git pip install git+https://github.com/cocodataset/panopticapi.git pip install git+https://github.com/Phoveran/fastargs.git@main#egg=fastargs pip install git+https://github.com/boomb0om/text2image-benchmark","title":"Activate Environment"},{"location":"mu_attack/mu_attack_evaluation/#running-the-evaluation-framework","text":"Create a file, eg, evaluate.py and use examples and modify your configs to run the evalautions. Example Code from evaluation.metrics.asr import asr_score from evaluation.metrics.clip import clip_score from evaluation.metrics.fid import fid_score root = \"/home/ubuntu/Projects/Palistha/testing/results/hard_prompt_esd_nudity_P4D_erase_diff/P4d\" root_no_attack =\"/home/ubuntu/Projects/Palistha/testing/results/no_attack_esd_nudity_P4D_abstrctionism/NoAttackEsdNudity\" asr_val = asr_score(root, root_no_attack) print(asr_val) images = \"/home/ubuntu/Projects/Palistha/testing/results/hard_prompt_esd_nudity_P4D_erase_diff_compvis_to_diffuser/P4d/images\" prompt_path = \"/home/ubuntu/Projects/Palistha/testing/results/hard_prompt_esd_nudity_P4D_erase_diff_compvis_to_diffuser/P4d/log.json\" device = \"0\" clip_val = clip_score(images, prompt_path, device) print(clip_val) gen_path = \"/home/ubuntu/Projects/Palistha/testing/results/hard_prompt_esd_nudity_P4D_erase_diff/P4d/images\" ref_path = \"data/i2p/nude\" fid_val = fid_score(gen_path,ref_path) print(fid_val) Running the Training Script in Offline Mode WANDB_MODE=offline python evaluate.py Evaluation Metrics: Attack Succes Rate (ASR) Fr\u00e9chet inception distance(FID): evaluate distributional quality of image generations, lower is better. CLIP score : measure contextual alignment with prompt descriptions, higher is better. Configuration File Structure for Evaluator ASR Evaluator Configuration root: Directory containing results with attack. root-no-attack: Directory containing results without attack. Clip Evaluator Configuration image_path: Path to the directory containing generated images to evaluate. devices: Device ID(s) to use for evaluation. Example: \"0\" for the first GPU or \"0,1\" for multiple GPUs. log_path: Path to the log file containing prompt for the generated images. model_name_or_path: Path or model name for the pre-trained CLIP model. Default is \"openai/clip-vit-base-patch32\". FID Evaluator Configuration ref_batch_path: Path to the directory containing reference images. sample_batch_path: Path to the directory containing generated/sample images. Global Configuration output_path: Path to save the evaluation results as a JSON file.","title":"Running the Evaluation Framework"},{"location":"mu_attack/attack/hard_prompt/","text":"UnlearnDiffAttak This repository contains the implementation of UnlearnDiffAttack for hard prompt attack, a framework for evaluating the robustness of safety-driven unlearned Models using adversarial prompts. usage Generate Dataset Before running attacks you need to generate dataset. Run the following command into the terminal. generate_attack_dataset --prompts_path data/prompts/nudity_sample.csv --concept i2p_nude --save_path outputs/dataset --num_samples 1 Note: If you want to generate image using full prompt then use data/prompts/nudity.csv as prompts_path. Run Attack Hard Prompt Attack - compvis Use the following code if you wish to run the hard prompt attack using the CompVis model directly (without converting it into Diffusers format): from mu_attack.configs.nudity import hard_prompt_esd_nudity_P4D_compvis_config from mu_attack.execs.attack import MUAttack from mu.algorithms.scissorhands.configs import scissorhands_train_mu def run_attack_for_nudity(): overridable_params = { \"task.compvis_ckpt_path\":\"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\": scissorhands_train_mu.model_config_path, \"task.dataset_path\":\"outputs/dataset/i2p_nude\", \"logger.json.root\":\"results/hard_prompt_esd_nudity_P4D_scissorhands\", } MUAttack( config=hard_prompt_esd_nudity_P4D_compvis_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() Hard Prompt Attack \u2013 CompVis to Diffusers Conversion If you want to convert the CompVis model into the Diffusers format before running the attack, use the following code. Note: For the conversion to take place, set task.save_diffuser to True and to use the converted model task.sld should be set to None. from mu_attack.configs.nudity import hard_prompt_esd_nudity_P4D_compvis_config from mu_attack.execs.attack import MUAttack from mu.algorithms.scissorhands.configs import scissorhands_train_mu def run_attack_for_nudity(): overridable_params = { \"task.compvis_ckpt_path\":\"/home/ubuntu/Projects/dipesh/unlearn_diff/outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\": scissorhands_train_mu.model_config_path, \"task.dataset_path\":\"/home/ubuntu/Projects/Palistha/unlearn_diff_attack/outputs/dataset/i2p_nude\", \"logger.json.root\":\"results/hard_prompt_esd_nudity_P4D_scissorhands\", \"task.save_diffuser\": True, # This flag triggers conversion \"task.sld\": None, # Set sld to None for conversion \"task.model_name\": \"SD-v1-4\" } MUAttack( config=hard_prompt_esd_nudity_P4D_compvis_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() For Conversion: When converting a CompVis model to the Diffusers format, ensure that task.save_diffuser is set to True and task.sld is set to None. This instructs the pipeline to perform the conversion during initialization and then load the converted checkpoint. Code Explanation & Important Notes from mu_attack.configs.nudity import hard_prompt_esd_nudity_P4D_compvis_config \u2192 This imports the predefined Hard Prompt Attack configuration for nudity unlearning in the CompVis model. It sets up the attack parameters and methodologies. from mu.algorithms.scissorhands.configs import scissorhands_train_mu \u2192 Imports the Scissorhands model configuration, required to set the task.compvis_config_path parameter correctly. Overriding Parameters in JSON Configuration The overridable_params dictionary allows dynamic modification of parameters defined in the JSON configuration. This enables users to override default values by passing them as arguments. Example usage overridable_params = { \"task.compvis_ckpt_path\": \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\": scissorhands_train_mu.model_config_path, # Overrides model config \"task.dataset_path\": \"outputs/dataset/i2p_nude\", # Overrides dataset path \"logger.json.root\": \"results/hard_prompt_esd_nudity_P4D_scissorhands\", # Overrides logging path \"attacker.k\" = 3, } Hard Prompt Attack - diffuser from mu_attack.configs.nudity import hard_prompt_esd_nudity_P4D_diffusers_config from mu_attack.execs.attack import MUAttack def run_attack_for_nudity(): overridable_params = { \"task.diffusers_model_name_or_path\" : \"outputs/forget_me_not/finetuned_models/Abstractionism\", \"task.dataset_path\" : \"outputs/dataset/i2p_nude\", \"logger.json.root\" :\"results/hard_prompt_esd_nudity_P4D_abstractionism\" } MUAttack( config=hard_prompt_esd_nudity_P4D_diffusers_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() Code Explanation & Important Notes from mu_attack.configs.nudity import hard_prompt_esd_nudity_P4D_diffusers_config \u2192 This imports the predefined Hard Prompt Attack configuration for nudity unlearning in the diffusers model. It sets up the attack parameters and methodologies. Running the Evaluation Framework Create a file, eg, evaluate.py and use examples and modify your configs to run the evalautions. Example Code from evaluation.metrics.asr import asr_score from evaluation.metrics.clip import clip_score from evaluation.metrics.fid import fid_score root = \"results/hard_prompt_esd_nudity_P4D_erase_diff/P4d\" root_no_attack =\"results/no_attack_esd_nudity_P4D_abstrctionism/NoAttackEsdNudity\" asr_val = asr_score(root, root_no_attack) print(asr_val) images = \"results/hard_prompt_esd_nudity_P4D_erase_diff_compvis_to_diffuser/P4d/images\" prompt_path = \"results/hard_prompt_esd_nudity_P4D_erase_diff_compvis_to_diffuser/P4d/log.json\" device = \"0\" clip_val = clip_score(images, prompt_path, device) print(clip_val) gen_path = \"results/hard_prompt_esd_nudity_P4D_erase_diff/P4d/images\" ref_path = \"data/i2p/nude\" fid_val = fid_score(gen_path,ref_path) print(fid_val) Running the Training Script in Offline Mode WANDB_MODE=offline python evaluate.py Evaluation Metrics: Attack Succes Rate (ASR) Fr\u00e9chet inception distance(FID): evaluate distributional quality of image generations, lower is better. CLIP score : measure contextual alignment with prompt descriptions, higher is better. Configuration File Structure for Evaluator ASR Evaluator Configuration root: Directory containing results with attack. root-no-attack: Directory containing results without attack. Clip Evaluator Configuration image_path: Path to the directory containing generated images to evaluate. devices: Device ID(s) to use for evaluation. Example: \"0\" for the first GPU or \"0,1\" for multiple GPUs. log_path: Path to the log file containing prompt for the generated images. model_name_or_path: Path or model name for the pre-trained CLIP model. Default is \"openai/clip-vit-base-patch32\". FID Evaluator Configuration ref_batch_path: Path to the directory containing reference images. sample_batch_path: Path to the directory containing generated/sample images. Global Configuration output_path: Path to save the evaluation results as a JSON file.","title":"Usage"},{"location":"mu_attack/attack/hard_prompt/#unlearndiffattak","text":"This repository contains the implementation of UnlearnDiffAttack for hard prompt attack, a framework for evaluating the robustness of safety-driven unlearned Models using adversarial prompts.","title":"UnlearnDiffAttak"},{"location":"mu_attack/attack/hard_prompt/#usage","text":"","title":"usage"},{"location":"mu_attack/attack/hard_prompt/#generate-dataset","text":"Before running attacks you need to generate dataset. Run the following command into the terminal. generate_attack_dataset --prompts_path data/prompts/nudity_sample.csv --concept i2p_nude --save_path outputs/dataset --num_samples 1 Note: If you want to generate image using full prompt then use data/prompts/nudity.csv as prompts_path.","title":"Generate Dataset"},{"location":"mu_attack/attack/hard_prompt/#run-attack","text":"Hard Prompt Attack - compvis Use the following code if you wish to run the hard prompt attack using the CompVis model directly (without converting it into Diffusers format): from mu_attack.configs.nudity import hard_prompt_esd_nudity_P4D_compvis_config from mu_attack.execs.attack import MUAttack from mu.algorithms.scissorhands.configs import scissorhands_train_mu def run_attack_for_nudity(): overridable_params = { \"task.compvis_ckpt_path\":\"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\": scissorhands_train_mu.model_config_path, \"task.dataset_path\":\"outputs/dataset/i2p_nude\", \"logger.json.root\":\"results/hard_prompt_esd_nudity_P4D_scissorhands\", } MUAttack( config=hard_prompt_esd_nudity_P4D_compvis_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() Hard Prompt Attack \u2013 CompVis to Diffusers Conversion If you want to convert the CompVis model into the Diffusers format before running the attack, use the following code. Note: For the conversion to take place, set task.save_diffuser to True and to use the converted model task.sld should be set to None. from mu_attack.configs.nudity import hard_prompt_esd_nudity_P4D_compvis_config from mu_attack.execs.attack import MUAttack from mu.algorithms.scissorhands.configs import scissorhands_train_mu def run_attack_for_nudity(): overridable_params = { \"task.compvis_ckpt_path\":\"/home/ubuntu/Projects/dipesh/unlearn_diff/outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\": scissorhands_train_mu.model_config_path, \"task.dataset_path\":\"/home/ubuntu/Projects/Palistha/unlearn_diff_attack/outputs/dataset/i2p_nude\", \"logger.json.root\":\"results/hard_prompt_esd_nudity_P4D_scissorhands\", \"task.save_diffuser\": True, # This flag triggers conversion \"task.sld\": None, # Set sld to None for conversion \"task.model_name\": \"SD-v1-4\" } MUAttack( config=hard_prompt_esd_nudity_P4D_compvis_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() For Conversion: When converting a CompVis model to the Diffusers format, ensure that task.save_diffuser is set to True and task.sld is set to None. This instructs the pipeline to perform the conversion during initialization and then load the converted checkpoint. Code Explanation & Important Notes from mu_attack.configs.nudity import hard_prompt_esd_nudity_P4D_compvis_config \u2192 This imports the predefined Hard Prompt Attack configuration for nudity unlearning in the CompVis model. It sets up the attack parameters and methodologies. from mu.algorithms.scissorhands.configs import scissorhands_train_mu \u2192 Imports the Scissorhands model configuration, required to set the task.compvis_config_path parameter correctly. Overriding Parameters in JSON Configuration The overridable_params dictionary allows dynamic modification of parameters defined in the JSON configuration. This enables users to override default values by passing them as arguments. Example usage overridable_params = { \"task.compvis_ckpt_path\": \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\": scissorhands_train_mu.model_config_path, # Overrides model config \"task.dataset_path\": \"outputs/dataset/i2p_nude\", # Overrides dataset path \"logger.json.root\": \"results/hard_prompt_esd_nudity_P4D_scissorhands\", # Overrides logging path \"attacker.k\" = 3, } Hard Prompt Attack - diffuser from mu_attack.configs.nudity import hard_prompt_esd_nudity_P4D_diffusers_config from mu_attack.execs.attack import MUAttack def run_attack_for_nudity(): overridable_params = { \"task.diffusers_model_name_or_path\" : \"outputs/forget_me_not/finetuned_models/Abstractionism\", \"task.dataset_path\" : \"outputs/dataset/i2p_nude\", \"logger.json.root\" :\"results/hard_prompt_esd_nudity_P4D_abstractionism\" } MUAttack( config=hard_prompt_esd_nudity_P4D_diffusers_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() Code Explanation & Important Notes from mu_attack.configs.nudity import hard_prompt_esd_nudity_P4D_diffusers_config \u2192 This imports the predefined Hard Prompt Attack configuration for nudity unlearning in the diffusers model. It sets up the attack parameters and methodologies.","title":"Run Attack"},{"location":"mu_attack/attack/hard_prompt/#running-the-evaluation-framework","text":"Create a file, eg, evaluate.py and use examples and modify your configs to run the evalautions. Example Code from evaluation.metrics.asr import asr_score from evaluation.metrics.clip import clip_score from evaluation.metrics.fid import fid_score root = \"results/hard_prompt_esd_nudity_P4D_erase_diff/P4d\" root_no_attack =\"results/no_attack_esd_nudity_P4D_abstrctionism/NoAttackEsdNudity\" asr_val = asr_score(root, root_no_attack) print(asr_val) images = \"results/hard_prompt_esd_nudity_P4D_erase_diff_compvis_to_diffuser/P4d/images\" prompt_path = \"results/hard_prompt_esd_nudity_P4D_erase_diff_compvis_to_diffuser/P4d/log.json\" device = \"0\" clip_val = clip_score(images, prompt_path, device) print(clip_val) gen_path = \"results/hard_prompt_esd_nudity_P4D_erase_diff/P4d/images\" ref_path = \"data/i2p/nude\" fid_val = fid_score(gen_path,ref_path) print(fid_val) Running the Training Script in Offline Mode WANDB_MODE=offline python evaluate.py Evaluation Metrics: Attack Succes Rate (ASR) Fr\u00e9chet inception distance(FID): evaluate distributional quality of image generations, lower is better. CLIP score : measure contextual alignment with prompt descriptions, higher is better. Configuration File Structure for Evaluator ASR Evaluator Configuration root: Directory containing results with attack. root-no-attack: Directory containing results without attack. Clip Evaluator Configuration image_path: Path to the directory containing generated images to evaluate. devices: Device ID(s) to use for evaluation. Example: \"0\" for the first GPU or \"0,1\" for multiple GPUs. log_path: Path to the log file containing prompt for the generated images. model_name_or_path: Path or model name for the pre-trained CLIP model. Default is \"openai/clip-vit-base-patch32\". FID Evaluator Configuration ref_batch_path: Path to the directory containing reference images. sample_batch_path: Path to the directory containing generated/sample images. Global Configuration output_path: Path to save the evaluation results as a JSON file.","title":"Running the Evaluation Framework"},{"location":"mu_attack/attack/no_attack/","text":"UnlearnDiffAttak This repository contains the implementation of UnlearnDiffAttack for No-attack, a framework for evaluating the robustness of safety-driven unlearned Models using adversarial prompts. usage Generate Dataset Before running attacks you need to generate dataset. Run the following command in terminal. generate_attack_dataset --prompts_path data/prompts/nudity_sample.csv --concept i2p_nude --save_path outputs/dataset --num_samples 1 Note: If you want to generate image using full prompt then use data/prompts/nudity.csv as prompts_path. Run Attack No Attack - compvis Use the following code if you wish to run the no attack using the CompVis model directly (without converting it into Diffusers format): from mu_attack.configs.nudity import no_attack_esd_nudity_classifier_compvis_config from mu_attack.execs.attack import MUAttack from mu.algorithms.scissorhands.configs import scissorhands_train_mu def run_attack_for_nudity(): overridable_params = { \"task.compvis_ckpt_path\" : \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\" : scissorhands_train_mu.model_config_path, \"task.dataset_path\" : \"outputs/dataset/i2p_nude\", \"logger.json.root\" : \"results/no_attack_esd_nudity_P4D_scissorhands\", \"attacker.no_attack.dataset_path\" : \"outputs/dataset/i2p_nude\" } MUAttack( config=no_attack_esd_nudity_classifier_compvis_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() No Attack \u2013 CompVis to Diffusers Conversion If you want to convert the CompVis model into the Diffusers format before running the attack, use the following code. Note: For the conversion to take place, set task.save_diffuser to True and to use the converted model task.sld should be set to None. from mu_attack.configs.nudity import no_attack_esd_nudity_classifier_compvis_config from mu_attack.execs.attack import MUAttack from mu.algorithms.scissorhands.configs import scissorhands_train_mu def run_attack_for_nudity(): overridable_params = { \"task.compvis_ckpt_path\" : \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\" : scissorhands_train_mu.model_config_path, \"task.dataset_path\" : \"outputs/dataset/i2p_nude\", \"logger.json.root\" : \"results/no_attack_esd_nudity_P4D_scissorhands\", \"attacker.no_attack.dataset_path\" : \"outputs/dataset/i2p_nude\", \"task.save_diffuser\": True, # This flag triggers conversion \"task.sld\": None, # Set sld to None for conversion \"task.model_name\": \"SD-v1-4\" } MUAttack( config=no_attack_esd_nudity_classifier_compvis_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() For Conversion: When converting a CompVis model to the Diffusers format, ensure that task.save_diffuser is set to True and task.sld is set to None. This instructs the pipeline to perform the conversion during initialization and then load the converted checkpoint. Code Explanation & Important Notes from mu_attack.configs.nudity import no_attack_esd_nudity_P4D_compvis_config \u2192 This imports the predefined No Attack configuration for nudity unlearning in the CompVis model. It sets up the attack parameters and methodologies. from mu.algorithms.scissorhands.configs import scissorhands_train_mu \u2192 Imports the Scissorhands model configuration, required to set the task.compvis_config_path parameter correctly. Overriding Parameters in JSON Configuration The overridable_params dictionary allows dynamic modification of parameters defined in the JSON configuration. This enables users to override default values by passing them as arguments. Example usage overridable_params = { \"task.compvis_ckpt_path\": \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\": scissorhands_train_mu.model_config_path, # Overrides model config \"task.dataset_path\": \"outputs/dataset/i2p_nude\", # Overrides dataset path \"logger.json.root\": \"results/no_attack_esd_nudity_P4D_scissorhands\", # Overrides logging path \"attacker.k\" = 3, \"attacker.no_attack.dataset_path\" = \"path/to/dataset\" #overrides the datset path for no attack } No Attack - diffuser from mu_attack.configs.nudity import no_attack_esd_nudity_classifier_diffusers_config from mu_attack.execs.attack import MUAttack def run_no_attack_for_nudity(): overridable_params = { \"task.diffusers_model_name_or_path\" :\"outputs/forget_me_not/finetuned_models/Abstractionism\", \"task.dataset_path\" : \"outputs/dataset/i2p_nude\", \"logger.json.root\" : \"results/no_attack_esd_nudity_P4D_abstrc\", \"attacker.no_attack.dataset_path\" : \"outputs/dataset/i2p_nude\" } MUAttack( config=no_attack_esd_nudity_classifier_diffusers_config, **overridable_params ) if __name__ == \"__main__\": run_no_attack_for_nudity() Code Explanation & Important Notes from mu_attack.configs.nudity import no_attack_esd_nudity_P4D_diffusers_config \u2192 This imports the predefined no attack Attack configuration for nudity unlearning in the diffusers model. It sets up the attack parameters and methodologies. Running the Evaluation Framework Create a file, eg, evaluate.py and use examples and modify your configs to run the evalautions. Example Code from evaluation.metrics.asr import asr_score from evaluation.metrics.clip import clip_score from evaluation.metrics.fid import fid_score root = \"results/hard_prompt_esd_nudity_P4D_erase_diff/P4d\" root_no_attack =\"results/no_attack_esd_nudity_P4D_abstrctionism/NoAttackEsdNudity\" asr_val = asr_score(root, root_no_attack) print(asr_val) images = \"results/hard_prompt_esd_nudity_P4D_erase_diff_compvis_to_diffuser/P4d/images\" prompt_path = \"results/hard_prompt_esd_nudity_P4D_erase_diff_compvis_to_diffuser/P4d/log.json\" device = \"0\" clip_val = clip_score(images, prompt_path, device) print(clip_val) gen_path = \"results/hard_prompt_esd_nudity_P4D_erase_diff/P4d/images\" ref_path = \"data/i2p/nude\" fid_val = fid_score(gen_path,ref_path) print(fid_val) Running the Training Script in Offline Mode WANDB_MODE=offline python evaluate.py Evaluation Metrics: Attack Succes Rate (ASR) Fr\u00e9chet inception distance(FID): evaluate distributional quality of image generations, lower is better. CLIP score : measure contextual alignment with prompt descriptions, higher is better. Configuration File Structure for Evaluator ASR Evaluator Configuration root: Directory containing results with attack. root-no-attack: Directory containing results without attack. Clip Evaluator Configuration image_path: Path to the directory containing generated images to evaluate. devices: Device ID(s) to use for evaluation. Example: \"0\" for the first GPU or \"0,1\" for multiple GPUs. log_path: Path to the log file containing prompt for the generated images. model_name_or_path: Path or model name for the pre-trained CLIP model. Default is \"openai/clip-vit-base-patch32\". FID Evaluator Configuration ref_batch_path: Path to the directory containing reference images. sample_batch_path: Path to the directory containing generated/sample images. Global Configuration output_path: Path to save the evaluation results as a JSON file.","title":"Usage"},{"location":"mu_attack/attack/no_attack/#unlearndiffattak","text":"This repository contains the implementation of UnlearnDiffAttack for No-attack, a framework for evaluating the robustness of safety-driven unlearned Models using adversarial prompts.","title":"UnlearnDiffAttak"},{"location":"mu_attack/attack/no_attack/#usage","text":"","title":"usage"},{"location":"mu_attack/attack/no_attack/#generate-dataset","text":"Before running attacks you need to generate dataset. Run the following command in terminal. generate_attack_dataset --prompts_path data/prompts/nudity_sample.csv --concept i2p_nude --save_path outputs/dataset --num_samples 1 Note: If you want to generate image using full prompt then use data/prompts/nudity.csv as prompts_path.","title":"Generate Dataset"},{"location":"mu_attack/attack/no_attack/#run-attack","text":"No Attack - compvis Use the following code if you wish to run the no attack using the CompVis model directly (without converting it into Diffusers format): from mu_attack.configs.nudity import no_attack_esd_nudity_classifier_compvis_config from mu_attack.execs.attack import MUAttack from mu.algorithms.scissorhands.configs import scissorhands_train_mu def run_attack_for_nudity(): overridable_params = { \"task.compvis_ckpt_path\" : \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\" : scissorhands_train_mu.model_config_path, \"task.dataset_path\" : \"outputs/dataset/i2p_nude\", \"logger.json.root\" : \"results/no_attack_esd_nudity_P4D_scissorhands\", \"attacker.no_attack.dataset_path\" : \"outputs/dataset/i2p_nude\" } MUAttack( config=no_attack_esd_nudity_classifier_compvis_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() No Attack \u2013 CompVis to Diffusers Conversion If you want to convert the CompVis model into the Diffusers format before running the attack, use the following code. Note: For the conversion to take place, set task.save_diffuser to True and to use the converted model task.sld should be set to None. from mu_attack.configs.nudity import no_attack_esd_nudity_classifier_compvis_config from mu_attack.execs.attack import MUAttack from mu.algorithms.scissorhands.configs import scissorhands_train_mu def run_attack_for_nudity(): overridable_params = { \"task.compvis_ckpt_path\" : \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\" : scissorhands_train_mu.model_config_path, \"task.dataset_path\" : \"outputs/dataset/i2p_nude\", \"logger.json.root\" : \"results/no_attack_esd_nudity_P4D_scissorhands\", \"attacker.no_attack.dataset_path\" : \"outputs/dataset/i2p_nude\", \"task.save_diffuser\": True, # This flag triggers conversion \"task.sld\": None, # Set sld to None for conversion \"task.model_name\": \"SD-v1-4\" } MUAttack( config=no_attack_esd_nudity_classifier_compvis_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() For Conversion: When converting a CompVis model to the Diffusers format, ensure that task.save_diffuser is set to True and task.sld is set to None. This instructs the pipeline to perform the conversion during initialization and then load the converted checkpoint. Code Explanation & Important Notes from mu_attack.configs.nudity import no_attack_esd_nudity_P4D_compvis_config \u2192 This imports the predefined No Attack configuration for nudity unlearning in the CompVis model. It sets up the attack parameters and methodologies. from mu.algorithms.scissorhands.configs import scissorhands_train_mu \u2192 Imports the Scissorhands model configuration, required to set the task.compvis_config_path parameter correctly. Overriding Parameters in JSON Configuration The overridable_params dictionary allows dynamic modification of parameters defined in the JSON configuration. This enables users to override default values by passing them as arguments. Example usage overridable_params = { \"task.compvis_ckpt_path\": \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\": scissorhands_train_mu.model_config_path, # Overrides model config \"task.dataset_path\": \"outputs/dataset/i2p_nude\", # Overrides dataset path \"logger.json.root\": \"results/no_attack_esd_nudity_P4D_scissorhands\", # Overrides logging path \"attacker.k\" = 3, \"attacker.no_attack.dataset_path\" = \"path/to/dataset\" #overrides the datset path for no attack } No Attack - diffuser from mu_attack.configs.nudity import no_attack_esd_nudity_classifier_diffusers_config from mu_attack.execs.attack import MUAttack def run_no_attack_for_nudity(): overridable_params = { \"task.diffusers_model_name_or_path\" :\"outputs/forget_me_not/finetuned_models/Abstractionism\", \"task.dataset_path\" : \"outputs/dataset/i2p_nude\", \"logger.json.root\" : \"results/no_attack_esd_nudity_P4D_abstrc\", \"attacker.no_attack.dataset_path\" : \"outputs/dataset/i2p_nude\" } MUAttack( config=no_attack_esd_nudity_classifier_diffusers_config, **overridable_params ) if __name__ == \"__main__\": run_no_attack_for_nudity() Code Explanation & Important Notes from mu_attack.configs.nudity import no_attack_esd_nudity_P4D_diffusers_config \u2192 This imports the predefined no attack Attack configuration for nudity unlearning in the diffusers model. It sets up the attack parameters and methodologies.","title":"Run Attack"},{"location":"mu_attack/attack/no_attack/#running-the-evaluation-framework","text":"Create a file, eg, evaluate.py and use examples and modify your configs to run the evalautions. Example Code from evaluation.metrics.asr import asr_score from evaluation.metrics.clip import clip_score from evaluation.metrics.fid import fid_score root = \"results/hard_prompt_esd_nudity_P4D_erase_diff/P4d\" root_no_attack =\"results/no_attack_esd_nudity_P4D_abstrctionism/NoAttackEsdNudity\" asr_val = asr_score(root, root_no_attack) print(asr_val) images = \"results/hard_prompt_esd_nudity_P4D_erase_diff_compvis_to_diffuser/P4d/images\" prompt_path = \"results/hard_prompt_esd_nudity_P4D_erase_diff_compvis_to_diffuser/P4d/log.json\" device = \"0\" clip_val = clip_score(images, prompt_path, device) print(clip_val) gen_path = \"results/hard_prompt_esd_nudity_P4D_erase_diff/P4d/images\" ref_path = \"data/i2p/nude\" fid_val = fid_score(gen_path,ref_path) print(fid_val) Running the Training Script in Offline Mode WANDB_MODE=offline python evaluate.py Evaluation Metrics: Attack Succes Rate (ASR) Fr\u00e9chet inception distance(FID): evaluate distributional quality of image generations, lower is better. CLIP score : measure contextual alignment with prompt descriptions, higher is better. Configuration File Structure for Evaluator ASR Evaluator Configuration root: Directory containing results with attack. root-no-attack: Directory containing results without attack. Clip Evaluator Configuration image_path: Path to the directory containing generated images to evaluate. devices: Device ID(s) to use for evaluation. Example: \"0\" for the first GPU or \"0,1\" for multiple GPUs. log_path: Path to the log file containing prompt for the generated images. model_name_or_path: Path or model name for the pre-trained CLIP model. Default is \"openai/clip-vit-base-patch32\". FID Evaluator Configuration ref_batch_path: Path to the directory containing reference images. sample_batch_path: Path to the directory containing generated/sample images. Global Configuration output_path: Path to save the evaluation results as a JSON file.","title":"Running the Evaluation Framework"},{"location":"mu_attack/attack/random/","text":"UnlearnDiffAttak This repository contains the implementation of UnlearnDiffAttack for random, a framework for evaluating the robustness of safety-driven unlearned Models using adversarial prompts. usage Generate Dataset Before running attacks you need to generate dataset. Run the following command into the terminal. generate_attack_dataset --prompts_path data/prompts/nudity_sample.csv --concept i2p_nude --save_path outputs/dataset --num_samples 1 Note: If you want to generate image using full prompt then use data/prompts/nudity.csv as prompts_path. Run Attack Random Attack - compvis Use the following code if you wish to run the random attack attack using the CompVis model directly (without converting it into Diffusers format): from mu_attack.configs.nudity import random_esd_nudity_compvis_config from mu_attack.execs.attack import MUAttack from mu.algorithms.scissorhands.configs import scissorhands_train_mu def run_attack_for_nudity(): overridable_params = { \"task.compvis_ckpt_path\" :\"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\" : scissorhands_train_mu.model_config_path , \"task.dataset_path\" : \"outputs/dataset/i2p_nude\", \"logger.json.root\" : \"results/random_attack_esd_nudity_P4D_scissorhands\" } MUAttack( config=random_esd_nudity_compvis_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() Rnadom Attack \u2013 CompVis to Diffusers Conversion If you want to convert the CompVis model into the Diffusers format before running the attack, use the following code. Note: For the conversion to take place, set task.save_diffuser to True and to use the converted model task.sld should be set to None. from mu_attack.configs.nudity import random_esd_nudity_compvis_config from mu_attack.execs.attack import MUAttack from mu.algorithms.scissorhands.configs import scissorhands_train_mu def run_attack_for_nudity(): overridable_params = { \"task.compvis_ckpt_path\" :\"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\" : scissorhands_train_mu.model_config_path , \"task.dataset_path\" : \"outputs/dataset/i2p_nude\", \"logger.json.root\" : \"results/random_attack_esd_nudity_P4D_scissorhands\", \"task.save_diffuser\": True, # This flag triggers conversion \"task.sld\": None, # Set sld to None for conversion \"task.model_name\": \"SD-v1-4\" } MUAttack( config=random_esd_nudity_compvis_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() For Conversion: When converting a CompVis model to the Diffusers format, ensure that task.save_diffuser is set to True and task.sld is set to None. This instructs the pipeline to perform the conversion during initialization and then load the converted checkpoint. Code Explanation & Important Notes from mu_attack.configs.nudity import random_attack_esd_nudity_P4D_compvis_config \u2192 This imports the predefined random attack Attack configuration for nudity unlearning in the CompVis model. It sets up the attack parameters and methodologies. from mu.algorithms.scissorhands.configs import scissorhands_train_mu \u2192 Imports the Scissorhands model configuration, required to set the task.compvis_config_path parameter correctly. Overriding Parameters in JSON Configuration The overridable_params dictionary allows dynamic modification of parameters defined in the JSON configuration. This enables users to override default values by passing them as arguments. Example usage overridable_params = { \"task.compvis_ckpt_path\": \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\": scissorhands_train_mu.model_config_path, # Overrides model config \"task.dataset_path\": \"outputs/dataset/i2p_nude\", # Overrides dataset path \"logger.json.root\": \"results/random_attack_esd_nudity_P4D_scissorhands\", # Overrides logging path \"attacker.k\" = 3, \"attacker.random_attack.dataset_path\" = \"path/to/dataset\" #overrides the datset path for random attack } Random Attack - diffuser from mu_attack.configs.nudity import random_esd_nudity_diffuser_config from mu_attack.execs.attack import MUAttack def run_attack_for_nudity(): overridable_params = { \"task.diffusers_model_name_or_path\" : \"outputs/forget_me_not/finetuned_models/Abstractionism\", \"task.dataset_path\" : \"outputs/dataset/i2p_nude\", \"logger.json.root\" : \"results/random_esd_nudity_diffuser_uce\" } MUAttack( config=random_esd_nudity_diffuser_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() Code Explanation & Important Notes from mu_attack.configs.nudity import random_esd_nudity_P4D_diffusers_config \u2192 This imports the predefined random Attack configuration for nudity unlearning in the diffusers model. It sets up the attack parameters and methodologies. Running the Evaluation Framework Create a file, eg, evaluate.py and use examples and modify your configs to run the evalautions. Example Code from evaluation.metrics.asr import asr_score from evaluation.metrics.clip import clip_score from evaluation.metrics.fid import fid_score root = \"results/hard_prompt_esd_nudity_P4D_erase_diff/P4d\" root_no_attack =\"results/no_attack_esd_nudity_P4D_abstrctionism/NoAttackEsdNudity\" asr_val = asr_score(root, root_no_attack) print(asr_val) images = \"results/hard_prompt_esd_nudity_P4D_erase_diff_compvis_to_diffuser/P4d/images\" prompt_path = \"results/hard_prompt_esd_nudity_P4D_erase_diff_compvis_to_diffuser/P4d/log.json\" device = \"0\" clip_val = clip_score(images, prompt_path, device) print(clip_val) gen_path = \"results/hard_prompt_esd_nudity_P4D_erase_diff/P4d/images\" ref_path = \"data/i2p/nude\" fid_val = fid_score(gen_path,ref_path) print(fid_val) Running the Training Script in Offline Mode WANDB_MODE=offline python evaluate.py Evaluation Metrics: Attack Succes Rate (ASR) Fr\u00e9chet inception distance(FID): evaluate distributional quality of image generations, lower is better. CLIP score : measure contextual alignment with prompt descriptions, higher is better. Configuration File Structure for Evaluator ASR Evaluator Configuration root: Directory containing results with attack. root-no-attack: Directory containing results without attack. Clip Evaluator Configuration image_path: Path to the directory containing generated images to evaluate. devices: Device ID(s) to use for evaluation. Example: \"0\" for the first GPU or \"0,1\" for multiple GPUs. log_path: Path to the log file containing prompt for the generated images. model_name_or_path: Path or model name for the pre-trained CLIP model. Default is \"openai/clip-vit-base-patch32\". FID Evaluator Configuration ref_batch_path: Path to the directory containing reference images. sample_batch_path: Path to the directory containing generated/sample images. Global Configuration output_path: Path to save the evaluation results as a JSON file.","title":"Usage"},{"location":"mu_attack/attack/random/#unlearndiffattak","text":"This repository contains the implementation of UnlearnDiffAttack for random, a framework for evaluating the robustness of safety-driven unlearned Models using adversarial prompts.","title":"UnlearnDiffAttak"},{"location":"mu_attack/attack/random/#usage","text":"","title":"usage"},{"location":"mu_attack/attack/random/#generate-dataset","text":"Before running attacks you need to generate dataset. Run the following command into the terminal. generate_attack_dataset --prompts_path data/prompts/nudity_sample.csv --concept i2p_nude --save_path outputs/dataset --num_samples 1 Note: If you want to generate image using full prompt then use data/prompts/nudity.csv as prompts_path.","title":"Generate Dataset"},{"location":"mu_attack/attack/random/#run-attack","text":"Random Attack - compvis Use the following code if you wish to run the random attack attack using the CompVis model directly (without converting it into Diffusers format): from mu_attack.configs.nudity import random_esd_nudity_compvis_config from mu_attack.execs.attack import MUAttack from mu.algorithms.scissorhands.configs import scissorhands_train_mu def run_attack_for_nudity(): overridable_params = { \"task.compvis_ckpt_path\" :\"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\" : scissorhands_train_mu.model_config_path , \"task.dataset_path\" : \"outputs/dataset/i2p_nude\", \"logger.json.root\" : \"results/random_attack_esd_nudity_P4D_scissorhands\" } MUAttack( config=random_esd_nudity_compvis_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() Rnadom Attack \u2013 CompVis to Diffusers Conversion If you want to convert the CompVis model into the Diffusers format before running the attack, use the following code. Note: For the conversion to take place, set task.save_diffuser to True and to use the converted model task.sld should be set to None. from mu_attack.configs.nudity import random_esd_nudity_compvis_config from mu_attack.execs.attack import MUAttack from mu.algorithms.scissorhands.configs import scissorhands_train_mu def run_attack_for_nudity(): overridable_params = { \"task.compvis_ckpt_path\" :\"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\" : scissorhands_train_mu.model_config_path , \"task.dataset_path\" : \"outputs/dataset/i2p_nude\", \"logger.json.root\" : \"results/random_attack_esd_nudity_P4D_scissorhands\", \"task.save_diffuser\": True, # This flag triggers conversion \"task.sld\": None, # Set sld to None for conversion \"task.model_name\": \"SD-v1-4\" } MUAttack( config=random_esd_nudity_compvis_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() For Conversion: When converting a CompVis model to the Diffusers format, ensure that task.save_diffuser is set to True and task.sld is set to None. This instructs the pipeline to perform the conversion during initialization and then load the converted checkpoint. Code Explanation & Important Notes from mu_attack.configs.nudity import random_attack_esd_nudity_P4D_compvis_config \u2192 This imports the predefined random attack Attack configuration for nudity unlearning in the CompVis model. It sets up the attack parameters and methodologies. from mu.algorithms.scissorhands.configs import scissorhands_train_mu \u2192 Imports the Scissorhands model configuration, required to set the task.compvis_config_path parameter correctly. Overriding Parameters in JSON Configuration The overridable_params dictionary allows dynamic modification of parameters defined in the JSON configuration. This enables users to override default values by passing them as arguments. Example usage overridable_params = { \"task.compvis_ckpt_path\": \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\": scissorhands_train_mu.model_config_path, # Overrides model config \"task.dataset_path\": \"outputs/dataset/i2p_nude\", # Overrides dataset path \"logger.json.root\": \"results/random_attack_esd_nudity_P4D_scissorhands\", # Overrides logging path \"attacker.k\" = 3, \"attacker.random_attack.dataset_path\" = \"path/to/dataset\" #overrides the datset path for random attack } Random Attack - diffuser from mu_attack.configs.nudity import random_esd_nudity_diffuser_config from mu_attack.execs.attack import MUAttack def run_attack_for_nudity(): overridable_params = { \"task.diffusers_model_name_or_path\" : \"outputs/forget_me_not/finetuned_models/Abstractionism\", \"task.dataset_path\" : \"outputs/dataset/i2p_nude\", \"logger.json.root\" : \"results/random_esd_nudity_diffuser_uce\" } MUAttack( config=random_esd_nudity_diffuser_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() Code Explanation & Important Notes from mu_attack.configs.nudity import random_esd_nudity_P4D_diffusers_config \u2192 This imports the predefined random Attack configuration for nudity unlearning in the diffusers model. It sets up the attack parameters and methodologies.","title":"Run Attack"},{"location":"mu_attack/attack/random/#running-the-evaluation-framework","text":"Create a file, eg, evaluate.py and use examples and modify your configs to run the evalautions. Example Code from evaluation.metrics.asr import asr_score from evaluation.metrics.clip import clip_score from evaluation.metrics.fid import fid_score root = \"results/hard_prompt_esd_nudity_P4D_erase_diff/P4d\" root_no_attack =\"results/no_attack_esd_nudity_P4D_abstrctionism/NoAttackEsdNudity\" asr_val = asr_score(root, root_no_attack) print(asr_val) images = \"results/hard_prompt_esd_nudity_P4D_erase_diff_compvis_to_diffuser/P4d/images\" prompt_path = \"results/hard_prompt_esd_nudity_P4D_erase_diff_compvis_to_diffuser/P4d/log.json\" device = \"0\" clip_val = clip_score(images, prompt_path, device) print(clip_val) gen_path = \"results/hard_prompt_esd_nudity_P4D_erase_diff/P4d/images\" ref_path = \"data/i2p/nude\" fid_val = fid_score(gen_path,ref_path) print(fid_val) Running the Training Script in Offline Mode WANDB_MODE=offline python evaluate.py Evaluation Metrics: Attack Succes Rate (ASR) Fr\u00e9chet inception distance(FID): evaluate distributional quality of image generations, lower is better. CLIP score : measure contextual alignment with prompt descriptions, higher is better. Configuration File Structure for Evaluator ASR Evaluator Configuration root: Directory containing results with attack. root-no-attack: Directory containing results without attack. Clip Evaluator Configuration image_path: Path to the directory containing generated images to evaluate. devices: Device ID(s) to use for evaluation. Example: \"0\" for the first GPU or \"0,1\" for multiple GPUs. log_path: Path to the log file containing prompt for the generated images. model_name_or_path: Path or model name for the pre-trained CLIP model. Default is \"openai/clip-vit-base-patch32\". FID Evaluator Configuration ref_batch_path: Path to the directory containing reference images. sample_batch_path: Path to the directory containing generated/sample images. Global Configuration output_path: Path to save the evaluation results as a JSON file.","title":"Running the Evaluation Framework"},{"location":"mu_attack/attack/seed_search/","text":"UnlearnDiffAttak This repository contains the implementation of UnlearnDiffAttack for seed search, a framework for evaluating the robustness of safety-driven unlearned Models using adversarial prompts. Usage Generate Dataset Before running attacks you need to generate dataset. Run the following command into the terminal. generate_attack_dataset --prompts_path data/prompts/nudity_sample.csv --concept i2p_nude --save_path outputs/dataset --num_samples 1 Note: If you want to generate image using full prompt then use data/prompts/nudity.csv as prompts_path. Downloading best.onnx model. After you install the package, you can use the following commands to download. If you have already downloaded then skip this step. ```bash download_best_onnx ``` Run Attack Seed search Attack - compvis Use the following code if you wish to run the seed search attack using the CompVis model directly (without converting it into Diffusers format): from mu_attack.configs.nudity import seed_search_esd_nudity_classifier_compvis_config from mu_attack.execs.attack import MUAttack from mu.algorithms.scissorhands.configs import scissorhands_train_mu def run_attack_for_nudity(): overridable_params = { \"task.compvis_ckpt_path\" :\"/home/ubuntu/Projects/dipesh/unlearn_diff/outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\" : scissorhands_train_mu.model_config_path, \"task.dataset_path\" : \"/home/ubuntu/Projects/Palistha/unlearn_diff_attack/outputs/dataset/i2p_nude\", \"logger.json.root\" : \"results/seed_search_esd_nudity_P4D_scissorhands\" } MUAttack( config=seed_search_esd_nudity_classifier_compvis_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() Seed search Attack \u2013 CompVis to Diffusers Conversion If you want to convert the CompVis model into the Diffusers format before running the attack, use the following code. Note: For the conversion to take place, set task.save_diffuser to True and to use the converted model task.sld should be set to None. from mu_attack.configs.nudity import seed_search_esd_nudity_classifier_compvis_config from mu_attack.execs.attack import MUAttack from mu.algorithms.scissorhands.configs import scissorhands_train_mu def run_attack_for_nudity(): overridable_params = { \"task.compvis_ckpt_path\" :\"/home/ubuntu/Projects/dipesh/unlearn_diff/outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\" : scissorhands_train_mu.model_config_path, \"task.dataset_path\" : \"/home/ubuntu/Projects/Palistha/unlearn_diff_attack/outputs/dataset/i2p_nude\", \"logger.json.root\" : \"results/seed_search_esd_nudity_P4D_scissorhands\", \"task.save_diffuser\": True, # This flag triggers conversion \"task.sld\": None, # Set sld to None for conversion \"task.model_name\": \"SD-v1-4\" } MUAttack( config=seed_search_esd_nudity_classifier_compvis_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() For Conversion: When converting a CompVis model to the Diffusers format, ensure that task.save_diffuser is set to True and task.sld is set to None. This instructs the pipeline to perform the conversion during initialization and then load the converted checkpoint. Code Explanation & Important Notes from mu_attack.configs.nudity import seed_search_esd_nudity_P4D_compvis_config \u2192 This imports the predefined seed search Attack configuration for nudity unlearning in the CompVis model. It sets up the attack parameters and methodologies. from mu.algorithms.scissorhands.configs import scissorhands_train_mu \u2192 Imports the Scissorhands model configuration, required to set the task.compvis_config_path parameter correctly. Overriding Parameters in JSON Configuration The overridable_params dictionary allows dynamic modification of parameters defined in the JSON configuration. This enables users to override default values by passing them as arguments. Example usage overridable_params = { \"task.compvis_ckpt_path\": \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\": scissorhands_train_mu.model_config_path, # Overrides model config \"task.dataset_path\": \"outputs/dataset/i2p_nude\", # Overrides dataset path \"logger.json.root\": \"results/seed_search_esd_nudity_P4D_scissorhands\", # Overrides logging path \"attacker.k\" = 3, \"attacker.seed_search.dataset_path\" = \"path/to/dataset\" #overrides the datset path for no attack } Random Attack - diffuser from mu_attack.configs.nudity import seed_search_esd_nudity_classifier_diffuser_config from mu_attack.execs.attack import MUAttack def run_attack_for_nudity(): overridable_params = { \"task.diffusers_model_name_or_path\" : \"outputs/forget_me_not/finetuned_models/Abstractionism\", \"task.dataset_path\" : \"outputs/dataset/i2p_nude\", \"logger.json.root\" : \"results/random_esd_nudity_diffuser_uce\" } MUAttack( config=seed_search_esd_nudity_classifier_diffuser_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() Code Explanation & Important Notes from mu_attack.configs.nudity import seed_search_esd_nudity_P4D_diffusers_config \u2192 This imports the predefined seed search Attack configuration for nudity unlearning in the diffusers model. It sets up the attack parameters and methodologies. Running the Evaluation Framework Create a file, eg, evaluate.py and use examples and modify your configs to run the evalautions. Example Code from evaluation.metrics.asr import asr_score from evaluation.metrics.clip import clip_score from evaluation.metrics.fid import fid_score root = \"results/hard_prompt_esd_nudity_P4D_erase_diff/P4d\" root_no_attack =\"results/no_attack_esd_nudity_P4D_abstrctionism/NoAttackEsdNudity\" asr_val = asr_score(root, root_no_attack) print(asr_val) images = \"results/hard_prompt_esd_nudity_P4D_erase_diff_compvis_to_diffuser/P4d/images\" prompt_path = \"results/hard_prompt_esd_nudity_P4D_erase_diff_compvis_to_diffuser/P4d/log.json\" device = \"0\" clip_val = clip_score(images, prompt_path, device) print(clip_val) gen_path = \"results/hard_prompt_esd_nudity_P4D_erase_diff/P4d/images\" ref_path = \"data/i2p/nude\" fid_val = fid_score(gen_path,ref_path) print(fid_val) Running the Training Script in Offline Mode WANDB_MODE=offline python evaluate.py Evaluation Metrics: Attack Succes Rate (ASR) Fr\u00e9chet inception distance(FID): evaluate distributional quality of image generations, lower is better. CLIP score : measure contextual alignment with prompt descriptions, higher is better. Configuration File Structure for Evaluator ASR Evaluator Configuration root: Directory containing results with attack. root-no-attack: Directory containing results without attack. Clip Evaluator Configuration image_path: Path to the directory containing generated images to evaluate. devices: Device ID(s) to use for evaluation. Example: \"0\" for the first GPU or \"0,1\" for multiple GPUs. log_path: Path to the log file containing prompt for the generated images. model_name_or_path: Path or model name for the pre-trained CLIP model. Default is \"openai/clip-vit-base-patch32\". FID Evaluator Configuration ref_batch_path: Path to the directory containing reference images. sample_batch_path: Path to the directory containing generated/sample images. Global Configuration output_path: Path to save the evaluation results as a JSON file.","title":"Usage"},{"location":"mu_attack/attack/seed_search/#unlearndiffattak","text":"This repository contains the implementation of UnlearnDiffAttack for seed search, a framework for evaluating the robustness of safety-driven unlearned Models using adversarial prompts.","title":"UnlearnDiffAttak"},{"location":"mu_attack/attack/seed_search/#usage","text":"","title":"Usage"},{"location":"mu_attack/attack/seed_search/#generate-dataset","text":"Before running attacks you need to generate dataset. Run the following command into the terminal. generate_attack_dataset --prompts_path data/prompts/nudity_sample.csv --concept i2p_nude --save_path outputs/dataset --num_samples 1 Note: If you want to generate image using full prompt then use data/prompts/nudity.csv as prompts_path.","title":"Generate Dataset"},{"location":"mu_attack/attack/seed_search/#downloading-bestonnx-model","text":"After you install the package, you can use the following commands to download. If you have already downloaded then skip this step. ```bash download_best_onnx ```","title":"Downloading best.onnx model."},{"location":"mu_attack/attack/seed_search/#run-attack","text":"Seed search Attack - compvis Use the following code if you wish to run the seed search attack using the CompVis model directly (without converting it into Diffusers format): from mu_attack.configs.nudity import seed_search_esd_nudity_classifier_compvis_config from mu_attack.execs.attack import MUAttack from mu.algorithms.scissorhands.configs import scissorhands_train_mu def run_attack_for_nudity(): overridable_params = { \"task.compvis_ckpt_path\" :\"/home/ubuntu/Projects/dipesh/unlearn_diff/outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\" : scissorhands_train_mu.model_config_path, \"task.dataset_path\" : \"/home/ubuntu/Projects/Palistha/unlearn_diff_attack/outputs/dataset/i2p_nude\", \"logger.json.root\" : \"results/seed_search_esd_nudity_P4D_scissorhands\" } MUAttack( config=seed_search_esd_nudity_classifier_compvis_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() Seed search Attack \u2013 CompVis to Diffusers Conversion If you want to convert the CompVis model into the Diffusers format before running the attack, use the following code. Note: For the conversion to take place, set task.save_diffuser to True and to use the converted model task.sld should be set to None. from mu_attack.configs.nudity import seed_search_esd_nudity_classifier_compvis_config from mu_attack.execs.attack import MUAttack from mu.algorithms.scissorhands.configs import scissorhands_train_mu def run_attack_for_nudity(): overridable_params = { \"task.compvis_ckpt_path\" :\"/home/ubuntu/Projects/dipesh/unlearn_diff/outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\" : scissorhands_train_mu.model_config_path, \"task.dataset_path\" : \"/home/ubuntu/Projects/Palistha/unlearn_diff_attack/outputs/dataset/i2p_nude\", \"logger.json.root\" : \"results/seed_search_esd_nudity_P4D_scissorhands\", \"task.save_diffuser\": True, # This flag triggers conversion \"task.sld\": None, # Set sld to None for conversion \"task.model_name\": \"SD-v1-4\" } MUAttack( config=seed_search_esd_nudity_classifier_compvis_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() For Conversion: When converting a CompVis model to the Diffusers format, ensure that task.save_diffuser is set to True and task.sld is set to None. This instructs the pipeline to perform the conversion during initialization and then load the converted checkpoint. Code Explanation & Important Notes from mu_attack.configs.nudity import seed_search_esd_nudity_P4D_compvis_config \u2192 This imports the predefined seed search Attack configuration for nudity unlearning in the CompVis model. It sets up the attack parameters and methodologies. from mu.algorithms.scissorhands.configs import scissorhands_train_mu \u2192 Imports the Scissorhands model configuration, required to set the task.compvis_config_path parameter correctly. Overriding Parameters in JSON Configuration The overridable_params dictionary allows dynamic modification of parameters defined in the JSON configuration. This enables users to override default values by passing them as arguments. Example usage overridable_params = { \"task.compvis_ckpt_path\": \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\": scissorhands_train_mu.model_config_path, # Overrides model config \"task.dataset_path\": \"outputs/dataset/i2p_nude\", # Overrides dataset path \"logger.json.root\": \"results/seed_search_esd_nudity_P4D_scissorhands\", # Overrides logging path \"attacker.k\" = 3, \"attacker.seed_search.dataset_path\" = \"path/to/dataset\" #overrides the datset path for no attack } Random Attack - diffuser from mu_attack.configs.nudity import seed_search_esd_nudity_classifier_diffuser_config from mu_attack.execs.attack import MUAttack def run_attack_for_nudity(): overridable_params = { \"task.diffusers_model_name_or_path\" : \"outputs/forget_me_not/finetuned_models/Abstractionism\", \"task.dataset_path\" : \"outputs/dataset/i2p_nude\", \"logger.json.root\" : \"results/random_esd_nudity_diffuser_uce\" } MUAttack( config=seed_search_esd_nudity_classifier_diffuser_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() Code Explanation & Important Notes from mu_attack.configs.nudity import seed_search_esd_nudity_P4D_diffusers_config \u2192 This imports the predefined seed search Attack configuration for nudity unlearning in the diffusers model. It sets up the attack parameters and methodologies.","title":"Run Attack"},{"location":"mu_attack/attack/seed_search/#running-the-evaluation-framework","text":"Create a file, eg, evaluate.py and use examples and modify your configs to run the evalautions. Example Code from evaluation.metrics.asr import asr_score from evaluation.metrics.clip import clip_score from evaluation.metrics.fid import fid_score root = \"results/hard_prompt_esd_nudity_P4D_erase_diff/P4d\" root_no_attack =\"results/no_attack_esd_nudity_P4D_abstrctionism/NoAttackEsdNudity\" asr_val = asr_score(root, root_no_attack) print(asr_val) images = \"results/hard_prompt_esd_nudity_P4D_erase_diff_compvis_to_diffuser/P4d/images\" prompt_path = \"results/hard_prompt_esd_nudity_P4D_erase_diff_compvis_to_diffuser/P4d/log.json\" device = \"0\" clip_val = clip_score(images, prompt_path, device) print(clip_val) gen_path = \"results/hard_prompt_esd_nudity_P4D_erase_diff/P4d/images\" ref_path = \"data/i2p/nude\" fid_val = fid_score(gen_path,ref_path) print(fid_val) Running the Training Script in Offline Mode WANDB_MODE=offline python evaluate.py Evaluation Metrics: Attack Succes Rate (ASR) Fr\u00e9chet inception distance(FID): evaluate distributional quality of image generations, lower is better. CLIP score : measure contextual alignment with prompt descriptions, higher is better. Configuration File Structure for Evaluator ASR Evaluator Configuration root: Directory containing results with attack. root-no-attack: Directory containing results without attack. Clip Evaluator Configuration image_path: Path to the directory containing generated images to evaluate. devices: Device ID(s) to use for evaluation. Example: \"0\" for the first GPU or \"0,1\" for multiple GPUs. log_path: Path to the log file containing prompt for the generated images. model_name_or_path: Path or model name for the pre-trained CLIP model. Default is \"openai/clip-vit-base-patch32\". FID Evaluator Configuration ref_batch_path: Path to the directory containing reference images. sample_batch_path: Path to the directory containing generated/sample images. Global Configuration output_path: Path to save the evaluation results as a JSON file.","title":"Running the Evaluation Framework"},{"location":"mu_attack/attack/soft_prompt/","text":"UnlearnDiffAttak This project implements a novel adversarial unlearning framework designed to perform soft prompt attacks on diffusion models. The primary objective is to subtly perturb the latent conditioning (or prompt) in order to manipulate the generated outputs, such as images, in a controlled and adversarial manner. Usage This section contains the usage guide for the package. Installation Prerequisities Ensure conda is installed on your system. You can install Miniconda or Anaconda: Miniconda (recommended): https://docs.conda.io/en/latest/miniconda.html Anaconda : https://www.anaconda.com/products/distribution After installing conda , ensure it is available in your PATH by running. You may require to restart the terminal session: Before installing the unlearn_diff package, follow these steps to set up your environment correctly. These instructions ensure compatibility with the required dependencies, including Python, PyTorch, and ONNX Runtime. Step-by-Step Setup: Step 1. Create a Conda Environment Create a new Conda environment named myenv with Python 3.8.5: conda create -n myenv python=3.8.5 Step 2. Activate the Environment Activate the environment to work within it: conda activate myenv Step 3. Install Core Dependencies Install PyTorch, torchvision, CUDA Toolkit, and ONNX Runtime with specific versions: conda install pytorch==1.11.0 torchvision==0.12.0 cudatoolkit=11.3 onnxruntime==1.16.3 -c pytorch -c conda-forge Step 4. Install our unlearn_diff Package using pip: pip install unlearn_diff Step 5. Install Additional Git Dependencies: After installing unlearn_diff, install the following Git-based dependencies in the same Conda environment to ensure full functionality: pip install git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers pip install git+https://github.com/openai/CLIP.git@main#egg=clip pip install git+https://github.com/crowsonkb/k-diffusion.git pip install git+https://github.com/cocodataset/panopticapi.git pip install git+https://github.com/Phoveran/fastargs.git@main#egg=fastargs pip install git+https://github.com/boomb0om/text2image-benchmark Downloading best.onnx model. After you install the package, you can use the following commands to download. If you have already downloaded then skip this step. ```bash download_best_onnx ``` Generate Dataset python -m scripts.generate_dataset --prompts_path data/prompts/prompts.csv --concept i2p_nude --save_path outputs/dataset Run Soft Prompt Attack Soft Prompt Attack - compvis from mu_attack.execs.adv_attack import AdvAttack from mu_attack.configs.adv_unlearn import adv_attack_config from mu.algorithms.esd.configs import esd_train_mu def mu_defense(): adv_unlearn = AdvAttack( config=adv_attack_config, compvis_ckpt_path = \"/home/ubuntu/Projects/dipesh/unlearn_diff/models/sd-v1-4-full-ema.ckpt\", attack_step = 2, backend = \"compvis\", config_path = esd_train_mu.model_config_path ) adv_unlearn.attack() if __name__ == \"__main__\": mu_defense() Soft Prompt Attack - diffuser from mu_attack.execs.adv_attack import AdvAttack from mu_attack.configs.adv_unlearn import adv_attack_config def mu_defense(): adv_unlearn = AdvAttack( config=adv_attack_config, diffusers_model_name_or_path = \"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/diffuser/style50\", attack_step = 2, backend = \"diffusers\" ) adv_unlearn.attack() if __name__ == \"__main__\": mu_defense() Run the python file in offline mode WANDB_MODE=offline python_file.py Code Explanation & Important Notes from mu_attack.configs.adv_unlearn import adv_unlearn_config \u2192 This imports the predefined Soft Prompt Attack configuration. It sets up the attack parameters and methodologies. How It Works * Default Values: The script first loads default values from the train config file as in configs section. Parameter Overrides: Any parameters passed directly to the algorithm, overrides these configs. Final Configuration: The script merges the configs and convert them into dictionary to proceed with the training. Description of fields in soft prompt attack config Model setup config_path : Path to the inference configuration file for Stable Diffusion v1.4. Type: str Default: \"model_config.yaml\" compvis_ckpt_path : Path to the Stable Diffusion v1.4 checkpoint file. Type: str Default: \"models/sd-v1-4-full-ema.ckpt\" encoder_model_name_or_path : Path to the pre-trained encoder model used for text-to-image training. Type: str Default: \"CompVis/stable-diffusion-v1-4\" diffusers_model_name_or_path : Path to the Diffusers-based implementation of the model. Type: str Default: \"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/diffuser/style50\" target_ckpt : Checkpoint path for sampling. If None, it uses the default model. Type: str Default: None Devices & I/O devices : Specifies the CUDA devices used for training. Type: str Default: \"0,0\" seperator : Defines the separator used when processing multiple words for unlearning. Type: str Default: None cache_path : Path where intermediate results and cache files are stored. Type: str Default: \".cache\" Image & Diffusion Sampling start_guidance : Guidance scale used for generating the initial image. Type: float Default: 3.0 ddim_steps : Number of DDIM sampling steps used for inference. Type: int Default: 50 image_size : The resolution of images generated during training. Type: int Default: 512 ddim_eta : Noise scaling factor for DDIM inference. Type: float Default: 0 prompt: The text prompt associated with the concept to erase. Type: str Default: \"nudity\" attack_method: The adversarial attack method used during training. Type: str Choices: [\"pgd\", \"multi_pgd\", \"fast_at\", \"free_at\"] Default: \"pgd\" ddim_eta: The DDIM sampling noise parameter. Type: float Default: 0 Adversarial Attack Hyperparameters adv_prompt_num: Number of prompt tokens used for adversarial learning. Type: int Default: 1 attack_embd_type: Type of embedding targeted for attack. Type: str Choices: [\"word_embd\", \"condition_embd\"] Default: \"word_embd\" attack_type: The type of attack applied. Type: str Choices: [\"replace_k\", \"add\", \"prefix_k\", \"suffix_k\", \"mid_k\", \"insert_k\", \"per_k_words\"] Default: \"prefix_k\" attack_init: Method for initializing adversarial attacks. Type: str Choices: [\"random\", \"latest\"] Default: \"latest\" attack_step: Number of attack optimization steps. Type: int Default: 30 attack_lr: Learning rate for adversarial attack updates. Type: float Default: 1e-3 Backend & Logging backend: Specifies the backend for diffusion-based training. Type: str Default: \"diffusers\" project_name: Name of the WandB project for logging. Type: str Default: \"quick-canvas-machine-unlearning\"","title":"Soft prompt"},{"location":"mu_attack/attack/soft_prompt/#unlearndiffattak","text":"This project implements a novel adversarial unlearning framework designed to perform soft prompt attacks on diffusion models. The primary objective is to subtly perturb the latent conditioning (or prompt) in order to manipulate the generated outputs, such as images, in a controlled and adversarial manner.","title":"UnlearnDiffAttak"},{"location":"mu_attack/attack/soft_prompt/#usage","text":"This section contains the usage guide for the package.","title":"Usage"},{"location":"mu_attack/attack/soft_prompt/#installation","text":"","title":"Installation"},{"location":"mu_attack/attack/soft_prompt/#prerequisities","text":"Ensure conda is installed on your system. You can install Miniconda or Anaconda: Miniconda (recommended): https://docs.conda.io/en/latest/miniconda.html Anaconda : https://www.anaconda.com/products/distribution After installing conda , ensure it is available in your PATH by running. You may require to restart the terminal session: Before installing the unlearn_diff package, follow these steps to set up your environment correctly. These instructions ensure compatibility with the required dependencies, including Python, PyTorch, and ONNX Runtime. Step-by-Step Setup: Step 1. Create a Conda Environment Create a new Conda environment named myenv with Python 3.8.5: conda create -n myenv python=3.8.5 Step 2. Activate the Environment Activate the environment to work within it: conda activate myenv Step 3. Install Core Dependencies Install PyTorch, torchvision, CUDA Toolkit, and ONNX Runtime with specific versions: conda install pytorch==1.11.0 torchvision==0.12.0 cudatoolkit=11.3 onnxruntime==1.16.3 -c pytorch -c conda-forge Step 4. Install our unlearn_diff Package using pip: pip install unlearn_diff Step 5. Install Additional Git Dependencies: After installing unlearn_diff, install the following Git-based dependencies in the same Conda environment to ensure full functionality: pip install git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers pip install git+https://github.com/openai/CLIP.git@main#egg=clip pip install git+https://github.com/crowsonkb/k-diffusion.git pip install git+https://github.com/cocodataset/panopticapi.git pip install git+https://github.com/Phoveran/fastargs.git@main#egg=fastargs pip install git+https://github.com/boomb0om/text2image-benchmark","title":"Prerequisities"},{"location":"mu_attack/attack/soft_prompt/#downloading-bestonnx-model","text":"After you install the package, you can use the following commands to download. If you have already downloaded then skip this step. ```bash download_best_onnx ```","title":"Downloading best.onnx model."},{"location":"mu_attack/attack/soft_prompt/#generate-dataset","text":"python -m scripts.generate_dataset --prompts_path data/prompts/prompts.csv --concept i2p_nude --save_path outputs/dataset","title":"Generate Dataset"},{"location":"mu_attack/attack/soft_prompt/#run-soft-prompt-attack","text":"Soft Prompt Attack - compvis from mu_attack.execs.adv_attack import AdvAttack from mu_attack.configs.adv_unlearn import adv_attack_config from mu.algorithms.esd.configs import esd_train_mu def mu_defense(): adv_unlearn = AdvAttack( config=adv_attack_config, compvis_ckpt_path = \"/home/ubuntu/Projects/dipesh/unlearn_diff/models/sd-v1-4-full-ema.ckpt\", attack_step = 2, backend = \"compvis\", config_path = esd_train_mu.model_config_path ) adv_unlearn.attack() if __name__ == \"__main__\": mu_defense() Soft Prompt Attack - diffuser from mu_attack.execs.adv_attack import AdvAttack from mu_attack.configs.adv_unlearn import adv_attack_config def mu_defense(): adv_unlearn = AdvAttack( config=adv_attack_config, diffusers_model_name_or_path = \"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/diffuser/style50\", attack_step = 2, backend = \"diffusers\" ) adv_unlearn.attack() if __name__ == \"__main__\": mu_defense() Run the python file in offline mode WANDB_MODE=offline python_file.py Code Explanation & Important Notes from mu_attack.configs.adv_unlearn import adv_unlearn_config \u2192 This imports the predefined Soft Prompt Attack configuration. It sets up the attack parameters and methodologies. How It Works * Default Values: The script first loads default values from the train config file as in configs section. Parameter Overrides: Any parameters passed directly to the algorithm, overrides these configs. Final Configuration: The script merges the configs and convert them into dictionary to proceed with the training.","title":"Run Soft Prompt Attack"},{"location":"mu_attack/attack/soft_prompt/#description-of-fields-in-soft-prompt-attack-config","text":"Model setup config_path : Path to the inference configuration file for Stable Diffusion v1.4. Type: str Default: \"model_config.yaml\" compvis_ckpt_path : Path to the Stable Diffusion v1.4 checkpoint file. Type: str Default: \"models/sd-v1-4-full-ema.ckpt\" encoder_model_name_or_path : Path to the pre-trained encoder model used for text-to-image training. Type: str Default: \"CompVis/stable-diffusion-v1-4\" diffusers_model_name_or_path : Path to the Diffusers-based implementation of the model. Type: str Default: \"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/diffuser/style50\" target_ckpt : Checkpoint path for sampling. If None, it uses the default model. Type: str Default: None Devices & I/O devices : Specifies the CUDA devices used for training. Type: str Default: \"0,0\" seperator : Defines the separator used when processing multiple words for unlearning. Type: str Default: None cache_path : Path where intermediate results and cache files are stored. Type: str Default: \".cache\" Image & Diffusion Sampling start_guidance : Guidance scale used for generating the initial image. Type: float Default: 3.0 ddim_steps : Number of DDIM sampling steps used for inference. Type: int Default: 50 image_size : The resolution of images generated during training. Type: int Default: 512 ddim_eta : Noise scaling factor for DDIM inference. Type: float Default: 0 prompt: The text prompt associated with the concept to erase. Type: str Default: \"nudity\" attack_method: The adversarial attack method used during training. Type: str Choices: [\"pgd\", \"multi_pgd\", \"fast_at\", \"free_at\"] Default: \"pgd\" ddim_eta: The DDIM sampling noise parameter. Type: float Default: 0 Adversarial Attack Hyperparameters adv_prompt_num: Number of prompt tokens used for adversarial learning. Type: int Default: 1 attack_embd_type: Type of embedding targeted for attack. Type: str Choices: [\"word_embd\", \"condition_embd\"] Default: \"word_embd\" attack_type: The type of attack applied. Type: str Choices: [\"replace_k\", \"add\", \"prefix_k\", \"suffix_k\", \"mid_k\", \"insert_k\", \"per_k_words\"] Default: \"prefix_k\" attack_init: Method for initializing adversarial attacks. Type: str Choices: [\"random\", \"latest\"] Default: \"latest\" attack_step: Number of attack optimization steps. Type: int Default: 30 attack_lr: Learning rate for adversarial attack updates. Type: float Default: 1e-3 Backend & Logging backend: Specifies the backend for diffusion-based training. Type: str Default: \"diffusers\" project_name: Name of the WandB project for logging. Type: str Default: \"quick-canvas-machine-unlearning\"","title":"Description of fields in soft prompt attack config"},{"location":"mu_attack/attack/text_grad/","text":"UnlearnDiffAttak This repository contains the implementation of UnlearnDiffAttack for text grad, a framework for evaluating the robustness of safety-driven unlearned Models using adversarial prompts. Usage Generate Dataset Before running attacks you need to generate dataset. Run the following command into the terminal. generate_attack_dataset --prompts_path data/prompts/nudity_sample.csv --concept i2p_nude --save_path outputs/dataset --num_samples 1 Note: If you want to generate image using full prompt then use data/prompts/nudity.csv as prompts_path. Run Attack Text Grad Attack - compvis Use the following code if you wish to run the seed search attack using the CompVis model directly (without converting it into Diffusers format): from mu_attack.configs.nudity import text_grad_esd_nudity_classifier_compvis_config from mu_attack.execs.attack import MUAttack from mu.algorithms.scissorhands.configs import scissorhands_train_mu def run_attack_for_nudity(): overridable_params = { \"task.compvis_ckpt_path\" :\"/home/ubuntu/Projects/dipesh/unlearn_diff/outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\" : scissorhands_train_mu.model_config_path, \"task.dataset_path\" : \"/home/ubuntu/Projects/Palistha/unlearn_diff_attack/outputs/dataset/i2p_nude\", \"attacker.text_grad.lr\": 0.02, \"logger.json.root\" : \"results/seed_search_esd_nudity_P4D_scissorhands\" } MUAttack( config=text_grad_esd_nudity_classifier_compvis_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() Text Grad Attack \u2013 CompVis to Diffusers Conversion If you want to convert the CompVis model into the Diffusers format before running the attack, use the following code. Note: For the conversion to take place, set task.save_diffuser to True and to use the converted model task.sld should be set to None. from mu_attack.configs.nudity import text_grad_esd_nudity_classifier_compvis_config from mu_attack.execs.attack import MUAttack from mu.algorithms.scissorhands.configs import scissorhands_train_mu def run_attack_for_nudity(): overridable_params = { \"task.compvis_ckpt_path\" :\"/home/ubuntu/Projects/dipesh/unlearn_diff/outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\" : scissorhands_train_mu.model_config_path, \"task.dataset_path\" : \"/home/ubuntu/Projects/Palistha/unlearn_diff_attack/outputs/dataset/i2p_nude\", \"attacker.text_grad.lr\": 0.02, \"logger.json.root\" : \"results/seed_search_esd_nudity_P4D_scissorhands\", \"task.save_diffuser\": True, # This flag triggers conversion \"task.sld\": None, # Set sld to None for conversion \"task.model_name\": \"SD-v1-4\" } MUAttack( config=text_grad_esd_nudity_classifier_compvis_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() For Conversion: When converting a CompVis model to the Diffusers format, ensure that task.save_diffuser is set to True and task.sld is set to None. This instructs the pipeline to perform the conversion during initialization and then load the converted checkpoint. Code Explanation & Important Notes from mu_attack.configs.nudity import text_grad_esd_nudity_P4D_compvis_config \u2192 This imports the predefined text grad Attack configuration for nudity unlearning in the CompVis model. It sets up the attack parameters and methodologies. from mu.algorithms.scissorhands.configs import scissorhands_train_mu \u2192 Imports the Scissorhands model configuration, required to set the task.compvis_config_path parameter correctly. Overriding Parameters in JSON Configuration The overridable_params dictionary allows dynamic modification of parameters defined in the JSON configuration. This enables users to override default values by passing them as arguments. Example usage overridable_params = { \"task.compvis_ckpt_path\": \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\": scissorhands_train_mu.model_config_path, # Overrides model config \"task.dataset_path\": \"outputs/dataset/i2p_nude\", # Overrides dataset path \"logger.json.root\": \"results/seed_search_esd_nudity_P4D_scissorhands\", # Overrides logging path \"attacker.k\" = 3, \"attacker.no_attack.dataset_path\" = \"path/to/dataset\" #overrides the datset path for no attack } Text Grad Attack - diffuser from mu_attack.configs.nudity import text_grad_esd_nudity_classifier_diffuser_config from mu_attack.execs.attack import MUAttack def run_attack_for_nudity(): overridable_params = { \"task.diffusers_model_name_or_path\" : \"outputs/forget_me_not/finetuned_models/Abstractionism\", \"task.dataset_path\" : \"outputs/dataset/i2p_nude\", \"logger.json.root\" : \"results/random_esd_nudity_diffuser_uce\" } MUAttack( config=text_grad_esd_nudity_classifier_diffuser_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() Code Explanation & Important Notes from mu_attack.configs.nudity import text_grad_esd_nudity_P4D_diffusers_config \u2192 This imports the predefined Text Grad Attack configuration for nudity unlearning in the diffusers model. It sets up the attack parameters and methodologies. Running the Evaluation Framework Create a file, eg, evaluate.py and use examples and modify your configs to run the evalautions. Example Code from evaluation.metrics.asr import asr_score from evaluation.metrics.clip import clip_score from evaluation.metrics.fid import fid_score root = \"results/hard_prompt_esd_nudity_P4D_erase_diff/P4d\" root_no_attack =\"results/no_attack_esd_nudity_P4D_abstrctionism/NoAttackEsdNudity\" asr_val = asr_score(root, root_no_attack) print(asr_val) images = \"results/hard_prompt_esd_nudity_P4D_erase_diff_compvis_to_diffuser/P4d/images\" prompt_path = \"results/hard_prompt_esd_nudity_P4D_erase_diff_compvis_to_diffuser/P4d/log.json\" device = \"0\" clip_val = clip_score(images, prompt_path, device) print(clip_val) gen_path = \"results/hard_prompt_esd_nudity_P4D_erase_diff/P4d/images\" ref_path = \"data/i2p/nude\" fid_val = fid_score(gen_path,ref_path) print(fid_val) Running the Training Script in Offline Mode WANDB_MODE=offline python evaluate.py Evaluation Metrics: Attack Succes Rate (ASR) Fr\u00e9chet inception distance(FID): evaluate distributional quality of image generations, lower is better. CLIP score : measure contextual alignment with prompt descriptions, higher is better. Configuration File Structure for Evaluator ASR Evaluator Configuration root: Directory containing results with attack. root-no-attack: Directory containing results without attack. Clip Evaluator Configuration image_path: Path to the directory containing generated images to evaluate. devices: Device ID(s) to use for evaluation. Example: \"0\" for the first GPU or \"0,1\" for multiple GPUs. log_path: Path to the log file containing prompt for the generated images. model_name_or_path: Path or model name for the pre-trained CLIP model. Default is \"openai/clip-vit-base-patch32\". FID Evaluator Configuration ref_batch_path: Path to the directory containing reference images. sample_batch_path: Path to the directory containing generated/sample images. Global Configuration output_path: Path to save the evaluation results as a JSON file.","title":"Usage"},{"location":"mu_attack/attack/text_grad/#unlearndiffattak","text":"This repository contains the implementation of UnlearnDiffAttack for text grad, a framework for evaluating the robustness of safety-driven unlearned Models using adversarial prompts.","title":"UnlearnDiffAttak"},{"location":"mu_attack/attack/text_grad/#usage","text":"","title":"Usage"},{"location":"mu_attack/attack/text_grad/#generate-dataset","text":"Before running attacks you need to generate dataset. Run the following command into the terminal. generate_attack_dataset --prompts_path data/prompts/nudity_sample.csv --concept i2p_nude --save_path outputs/dataset --num_samples 1 Note: If you want to generate image using full prompt then use data/prompts/nudity.csv as prompts_path.","title":"Generate Dataset"},{"location":"mu_attack/attack/text_grad/#run-attack","text":"Text Grad Attack - compvis Use the following code if you wish to run the seed search attack using the CompVis model directly (without converting it into Diffusers format): from mu_attack.configs.nudity import text_grad_esd_nudity_classifier_compvis_config from mu_attack.execs.attack import MUAttack from mu.algorithms.scissorhands.configs import scissorhands_train_mu def run_attack_for_nudity(): overridable_params = { \"task.compvis_ckpt_path\" :\"/home/ubuntu/Projects/dipesh/unlearn_diff/outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\" : scissorhands_train_mu.model_config_path, \"task.dataset_path\" : \"/home/ubuntu/Projects/Palistha/unlearn_diff_attack/outputs/dataset/i2p_nude\", \"attacker.text_grad.lr\": 0.02, \"logger.json.root\" : \"results/seed_search_esd_nudity_P4D_scissorhands\" } MUAttack( config=text_grad_esd_nudity_classifier_compvis_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() Text Grad Attack \u2013 CompVis to Diffusers Conversion If you want to convert the CompVis model into the Diffusers format before running the attack, use the following code. Note: For the conversion to take place, set task.save_diffuser to True and to use the converted model task.sld should be set to None. from mu_attack.configs.nudity import text_grad_esd_nudity_classifier_compvis_config from mu_attack.execs.attack import MUAttack from mu.algorithms.scissorhands.configs import scissorhands_train_mu def run_attack_for_nudity(): overridable_params = { \"task.compvis_ckpt_path\" :\"/home/ubuntu/Projects/dipesh/unlearn_diff/outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\" : scissorhands_train_mu.model_config_path, \"task.dataset_path\" : \"/home/ubuntu/Projects/Palistha/unlearn_diff_attack/outputs/dataset/i2p_nude\", \"attacker.text_grad.lr\": 0.02, \"logger.json.root\" : \"results/seed_search_esd_nudity_P4D_scissorhands\", \"task.save_diffuser\": True, # This flag triggers conversion \"task.sld\": None, # Set sld to None for conversion \"task.model_name\": \"SD-v1-4\" } MUAttack( config=text_grad_esd_nudity_classifier_compvis_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() For Conversion: When converting a CompVis model to the Diffusers format, ensure that task.save_diffuser is set to True and task.sld is set to None. This instructs the pipeline to perform the conversion during initialization and then load the converted checkpoint. Code Explanation & Important Notes from mu_attack.configs.nudity import text_grad_esd_nudity_P4D_compvis_config \u2192 This imports the predefined text grad Attack configuration for nudity unlearning in the CompVis model. It sets up the attack parameters and methodologies. from mu.algorithms.scissorhands.configs import scissorhands_train_mu \u2192 Imports the Scissorhands model configuration, required to set the task.compvis_config_path parameter correctly. Overriding Parameters in JSON Configuration The overridable_params dictionary allows dynamic modification of parameters defined in the JSON configuration. This enables users to override default values by passing them as arguments. Example usage overridable_params = { \"task.compvis_ckpt_path\": \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"task.compvis_config_path\": scissorhands_train_mu.model_config_path, # Overrides model config \"task.dataset_path\": \"outputs/dataset/i2p_nude\", # Overrides dataset path \"logger.json.root\": \"results/seed_search_esd_nudity_P4D_scissorhands\", # Overrides logging path \"attacker.k\" = 3, \"attacker.no_attack.dataset_path\" = \"path/to/dataset\" #overrides the datset path for no attack } Text Grad Attack - diffuser from mu_attack.configs.nudity import text_grad_esd_nudity_classifier_diffuser_config from mu_attack.execs.attack import MUAttack def run_attack_for_nudity(): overridable_params = { \"task.diffusers_model_name_or_path\" : \"outputs/forget_me_not/finetuned_models/Abstractionism\", \"task.dataset_path\" : \"outputs/dataset/i2p_nude\", \"logger.json.root\" : \"results/random_esd_nudity_diffuser_uce\" } MUAttack( config=text_grad_esd_nudity_classifier_diffuser_config, **overridable_params ) if __name__ == \"__main__\": run_attack_for_nudity() Code Explanation & Important Notes from mu_attack.configs.nudity import text_grad_esd_nudity_P4D_diffusers_config \u2192 This imports the predefined Text Grad Attack configuration for nudity unlearning in the diffusers model. It sets up the attack parameters and methodologies.","title":"Run Attack"},{"location":"mu_attack/attack/text_grad/#running-the-evaluation-framework","text":"Create a file, eg, evaluate.py and use examples and modify your configs to run the evalautions. Example Code from evaluation.metrics.asr import asr_score from evaluation.metrics.clip import clip_score from evaluation.metrics.fid import fid_score root = \"results/hard_prompt_esd_nudity_P4D_erase_diff/P4d\" root_no_attack =\"results/no_attack_esd_nudity_P4D_abstrctionism/NoAttackEsdNudity\" asr_val = asr_score(root, root_no_attack) print(asr_val) images = \"results/hard_prompt_esd_nudity_P4D_erase_diff_compvis_to_diffuser/P4d/images\" prompt_path = \"results/hard_prompt_esd_nudity_P4D_erase_diff_compvis_to_diffuser/P4d/log.json\" device = \"0\" clip_val = clip_score(images, prompt_path, device) print(clip_val) gen_path = \"results/hard_prompt_esd_nudity_P4D_erase_diff/P4d/images\" ref_path = \"data/i2p/nude\" fid_val = fid_score(gen_path,ref_path) print(fid_val) Running the Training Script in Offline Mode WANDB_MODE=offline python evaluate.py Evaluation Metrics: Attack Succes Rate (ASR) Fr\u00e9chet inception distance(FID): evaluate distributional quality of image generations, lower is better. CLIP score : measure contextual alignment with prompt descriptions, higher is better. Configuration File Structure for Evaluator ASR Evaluator Configuration root: Directory containing results with attack. root-no-attack: Directory containing results without attack. Clip Evaluator Configuration image_path: Path to the directory containing generated images to evaluate. devices: Device ID(s) to use for evaluation. Example: \"0\" for the first GPU or \"0,1\" for multiple GPUs. log_path: Path to the log file containing prompt for the generated images. model_name_or_path: Path or model name for the pre-trained CLIP model. Default is \"openai/clip-vit-base-patch32\". FID Evaluator Configuration ref_batch_path: Path to the directory containing reference images. sample_batch_path: Path to the directory containing generated/sample images. Global Configuration output_path: Path to save the evaluation results as a JSON file.","title":"Running the Evaluation Framework"},{"location":"mu_attack/configs/hard_prompt/","text":"Sample hard_prompt_config for compvis # mu_attack/configs/nudity/hard_prompt_esd_nudity_P4D_compvis.py import os from mu_attack.core import BaseConfig, OverallConfig, TaskConfig, AttackerConfig, LoggerConfig class HardPromptESDNudityP4DConfigCompvis(BaseConfig): overall: OverallConfig = OverallConfig( task=\"P4D\", attacker=\"hard_prompt\", logger=\"json\", resume=None ) task: TaskConfig = TaskConfig( classifier_dir=None, sampling_step_num=1, sld=\"weak\", sld_concept=\"nudity\", negative_prompt=\"\", backend=\"compvis\", diffusers_config_file = None, save_diffuser = False, converted_model_folder_path = \"outputs\" ) attacker: AttackerConfig = AttackerConfig( sequential = True, lr=0.01, weight_decay=0.1 ) logger: LoggerConfig = LoggerConfig( json={\"root\": \"results/hard_prompt_esd_nudity_P4D_scissorhands\", \"name\": \"P4d\"} ) hard_prompt_esd_nudity_P4D_compvis_config = HardPromptESDNudityP4DConfigCompvis() Sample hard_prompt config json for compvis { \"overall\": { \"task\": \"P4D\", \"attacker\": \"hard_prompt\", \"logger\": \"json\", \"resume\": null }, \"task\": { \"concept\": \"nudity\", \"compvis_ckpt_path\": \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"compvis_config_path\":\"mu/algorithms/scissorhands/configs/model_config.yaml\", \"cache_path\": \".cache\", \"dataset_path\": \"outputs/dataset/i2p_nude\", \"criterion\": \"l2\", \"classifier_dir\": null, \"sampling_step_num\": 1, \"sld\": \"weak\", \"sld_concept\": \"nudity\", \"negative_prompt\": \"\", \"backend\":\"compvis\" }, \"attacker\": { \"insertion_location\": \"prefix_k\", \"k\": 5, \"iteration\": 1, \"seed_iteration\": 1, \"attack_idx\": 0, \"eval_seed\": 0, \"universal\": false, \"sequential\": true, \"lr\": 0.01, \"weight_decay\": 0.1 }, \"logger\": { \"json\": { \"root\": \"results/hard_prompt_esd_nudity_P4D_scissorhands\", \"name\": \"P4d\" } } } Sample hard_prompt_config for diffusers class HardPromptESDNudityP4DConfigDiffusers(BaseConfig): overall: OverallConfig = OverallConfig( task=\"P4D\", attacker=\"hard_prompt\", logger=\"json\", resume=None ) task: TaskConfig = TaskConfig( classifier_dir=None, sampling_step_num=1, sld=\"weak\", sld_concept=\"nudity\", negative_prompt=\"\", backend=\"diffusers\" ) attacker: AttackerConfig = AttackerConfig( sequential = True, lr=0.01, weight_decay=0.1 ) logger: LoggerConfig = LoggerConfig( json={ \"root\": \"results/hard_prompt_esd_nudity_P4D_scissorhands\", \"name\": \"P4d\" } ) Sample hard_prompt config json for diffusers { \"overall\": { \"task\": \"P4D\", \"attacker\": \"hard_prompt\", \"logger\": \"json\", \"resume\": null }, \"task\": { \"concept\": \"nudity\", \"diffusers_model_name_or_path\": \"/home/ubuntu/Projects/dipesh/unlearn_diff/outputs/forget_me_not/finetuned_models/Abstractionism\", \"target_ckpt\": \"files/pretrained/SD-1-4/ESD_ckpt/Nudity-ESDx1-UNET-SD.pt\", \"cache_path\": \".cache\", \"dataset_path\": \"/home/ubuntu/Projects/Palistha/unlearn_diff_attack/outputs/dataset/i2p_nude\", \"criterion\": \"l2\", \"classifier_dir\": null, \"sampling_step_num\": 1, \"sld\": \"weak\", \"sld_concept\": \"nudity\", \"negative_prompt\": \"\", \"backend\":\"diffusers\" }, \"attacker\": { \"insertion_location\": \"prefix_k\", \"k\": 5, \"iteration\": 1, \"seed_iteration\": 1, \"attack_idx\": 0, \"eval_seed\": 0, \"universal\": false, \"sequential\": true, \"lr\": 0.01, \"weight_decay\": 0.1 }, \"logger\": { \"json\": { \"root\": \"results/hard_prompt_esd_nudity_P4D_semipermeable_membrane\", \"name\": \"P4d\" } } } Description of fields in config file overall This section defines the high-level configuration for the attack. task : The name of the task being performed. Type: str Example: classifer attacker: Specifies the attack type. Type: str Example: no_attack logger: Defines the logging mechanism. Type: str Example: JSON resume: Option to resume from previous checkpoint. task concept: The concept targeted by the attack. Type: str Example: nudity, harm diffusers_model_name_or_path: Path to the pre-trained checkpoint of the diffuser model. (For diffuser) Type: str Example: \"outputs/semipermeable_membrane/finetuned_models/\" target_ckpt: Path to the target model checkpoint used in the attack. (For diffuser) Type: str Example: \"files/pretrained/SD-1-4/ESD_ckpt/Nudity-ESDx1-UNET-SD.pt\" compvis_ckpt_path: Path to the pre-trained checkpoint of the CompVis model. (For compvis) Type: str Example: \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\" compvis_config_path: Path to the configuration file for the CompVis model. (For compvis) Type: str Example: \"configs/scissorhands/model_config.yaml\" cache_path: Directory to cache intermediate results. Type: str Example: \".cache\" dataset_path: Path to the dataset used for the attack. Type: str Example: \"outputs/dataset/i2p_nude\" criterion: The loss function or criterion used during the attack. Type: str Example: \"l2\" classifier_dir: Directory for the classifier, if applicable. null if not used. Type: str Example: \"/path/classifier_dir\" sampling_step_num: Number of sampling steps during the attack. Type: int Example: 1 sld: Strength of latent disentanglement. Type: str Example: \"weak\" sld_concept: Concept tied to latent disentanglement. Type: str Example: \"nudity\" negative_prompt: The negative prompt used to steer the generation. Type: str Example: \"\" model_name: Name of the model. The model_name parameter determines which base Stable Diffusion model is used by the pipeline. Type: str Example: \"SD-v1-4\" Choices: \"SD-v1-4\", \"SD-V2\", \"SD-V2-1\" save_diffuser: A Boolean flag that determines whether the CompVis model should be converted into the Diffusers format before being used. Type: str Example: True Behavior: * If set to True, the pipeline will perform a conversion of the CompVis model into the Diffusers format and then load the converted checkpoint. If set to False, the conversion is skipped and the model remains in its original CompVis format for use and uses compvis based implementation. converted_model_folder_path: Folder path to save the converted compvis model to diffuser. Type: str Example: \"outputs\" backend: Specifies the backend model i.e \"diffusers\". Type: str Options: \"diffusers\" or \"compvis\" attacker insertion_location: The point of insertion for the prompt. Type: str Example: \"prefix_k\" k: The value of k for the prompt insertion point. Type: int Example: 5 iteration: Number of iterations for the attack. Type: int Example: 1 seed_iteration: Random seed for the iterative process. Type: int Example: 1 attack_idx: Index of the attack for evaluation purposes. Type: int Example: 0 eval_seed: Seed value used for evaluation. Type: int Example: 0 universal: Whether the attack is universal (true or false). Type: bool Example: false sequential: Whether the attack is applied sequentially. Type: bool Example: true lr: Learning rate for the attack optimization process. Type: float Example: 0.01 weight_decay: Weight decay applied during optimization. Type: float Example: 0.1 logger json: Logging configuration. root: Path to the directory where logs will be saved. Type: str Example: \"results/hard_prompt_esd_nudity_P4D\" name: Name for the log file or experiment. Type: str Example: \"P4D\" Example usage: { \"json\": { \"root\": \"results/no_attack_esd_nudity_esd\", \"name\": \"P4D\" } }","title":"Config"},{"location":"mu_attack/configs/hard_prompt/#description-of-fields-in-config-file","text":"overall This section defines the high-level configuration for the attack. task : The name of the task being performed. Type: str Example: classifer attacker: Specifies the attack type. Type: str Example: no_attack logger: Defines the logging mechanism. Type: str Example: JSON resume: Option to resume from previous checkpoint. task concept: The concept targeted by the attack. Type: str Example: nudity, harm diffusers_model_name_or_path: Path to the pre-trained checkpoint of the diffuser model. (For diffuser) Type: str Example: \"outputs/semipermeable_membrane/finetuned_models/\" target_ckpt: Path to the target model checkpoint used in the attack. (For diffuser) Type: str Example: \"files/pretrained/SD-1-4/ESD_ckpt/Nudity-ESDx1-UNET-SD.pt\" compvis_ckpt_path: Path to the pre-trained checkpoint of the CompVis model. (For compvis) Type: str Example: \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\" compvis_config_path: Path to the configuration file for the CompVis model. (For compvis) Type: str Example: \"configs/scissorhands/model_config.yaml\" cache_path: Directory to cache intermediate results. Type: str Example: \".cache\" dataset_path: Path to the dataset used for the attack. Type: str Example: \"outputs/dataset/i2p_nude\" criterion: The loss function or criterion used during the attack. Type: str Example: \"l2\" classifier_dir: Directory for the classifier, if applicable. null if not used. Type: str Example: \"/path/classifier_dir\" sampling_step_num: Number of sampling steps during the attack. Type: int Example: 1 sld: Strength of latent disentanglement. Type: str Example: \"weak\" sld_concept: Concept tied to latent disentanglement. Type: str Example: \"nudity\" negative_prompt: The negative prompt used to steer the generation. Type: str Example: \"\" model_name: Name of the model. The model_name parameter determines which base Stable Diffusion model is used by the pipeline. Type: str Example: \"SD-v1-4\" Choices: \"SD-v1-4\", \"SD-V2\", \"SD-V2-1\" save_diffuser: A Boolean flag that determines whether the CompVis model should be converted into the Diffusers format before being used. Type: str Example: True Behavior: * If set to True, the pipeline will perform a conversion of the CompVis model into the Diffusers format and then load the converted checkpoint. If set to False, the conversion is skipped and the model remains in its original CompVis format for use and uses compvis based implementation. converted_model_folder_path: Folder path to save the converted compvis model to diffuser. Type: str Example: \"outputs\" backend: Specifies the backend model i.e \"diffusers\". Type: str Options: \"diffusers\" or \"compvis\" attacker insertion_location: The point of insertion for the prompt. Type: str Example: \"prefix_k\" k: The value of k for the prompt insertion point. Type: int Example: 5 iteration: Number of iterations for the attack. Type: int Example: 1 seed_iteration: Random seed for the iterative process. Type: int Example: 1 attack_idx: Index of the attack for evaluation purposes. Type: int Example: 0 eval_seed: Seed value used for evaluation. Type: int Example: 0 universal: Whether the attack is universal (true or false). Type: bool Example: false sequential: Whether the attack is applied sequentially. Type: bool Example: true lr: Learning rate for the attack optimization process. Type: float Example: 0.01 weight_decay: Weight decay applied during optimization. Type: float Example: 0.1 logger json: Logging configuration. root: Path to the directory where logs will be saved. Type: str Example: \"results/hard_prompt_esd_nudity_P4D\" name: Name for the log file or experiment. Type: str Example: \"P4D\" Example usage: { \"json\": { \"root\": \"results/no_attack_esd_nudity_esd\", \"name\": \"P4D\" } }","title":"Description of fields in config file"},{"location":"mu_attack/configs/no_attack/","text":"Sample no attack config for compvis # mu_attack/configs/nudity/no_attack_esd_nudity_classifier_compvis.py from mu_attack.core import BaseConfig, OverallConfig, TaskConfig, AttackerConfig, LoggerConfig class NoAttackESDNudityClassifierConfigCompvis(BaseConfig): overall: OverallConfig = OverallConfig( task=\"classifier\", attacker=\"no_attack\", logger=\"json\", resume=None ) task: TaskConfig = TaskConfig( classifier_dir=None, sampling_step_num=1, criterion=\"l1\", sld=\"weak\", sld_concept=\"nudity\", negative_prompt=\"sth\", backend=\"compvis\", diffusers_config_file = None, save_diffuser = False ) attacker: AttackerConfig = AttackerConfig( iteration=1, attack_idx=1, no_attack = { \"dataset_path\": \"outputs/dataset/i2p_nude\" } ) logger: LoggerConfig = LoggerConfig( json={\"root\": \"results/no_attack_esd_nudity_esd\", \"name\": \"NoAttackEsdNudity\"} ) no_attack_esd_nudity_classifier_compvis_config = NoAttackESDNudityClassifierConfigCompvis() Sample compvis config json for no attack { \"overall\": { \"task\": \"classifier\", \"attacker\": \"no_attack\", \"logger\": \"json\", \"resume\": null }, \"task\": { \"concept\": \"nudity\", \"compvis_ckpt_path\": \"outputs/esd/esd_Abstractionism_model.pth\", \"compvis_config_path\":\"mu/algorithms/esd/configs/model_config.yaml\", \"cache_path\": \".cache\", \"dataset_path\": \"outputs/dataset/i2p_nude\", \"criterion\": \"l1\", \"sampling_step_num\": 1, \"sld\": \"weak\", \"sld_concept\": \"nudity\", \"negative_prompt\": \"sth\", \"backend\":\"compvis\" }, \"attacker\": { \"insertion_location\": \"prefix_k\", \"k\": 5, \"seed_iteration\": 1, \"sequential\": true, \"iteration\": 40, \"attack_idx\": 1, \"eval_seed\": 0, \"universal\": false, \"no_attack\": { \"dataset_path\": \"outputs/dataset/i2p_nude\" } }, \"logger\": { \"json\": { \"root\": \"results/no_attack_esd_nudity_esd\", \"name\": \"NoAttackEsdNudity\" } } } Sample config for no attack for diffuser class NoAttackESDNudityClassifierConfigDiffusers(BaseConfig): overall: OverallConfig = OverallConfig( task=\"classifier\", attacker=\"no_attack\", logger=\"json\", resume=None ) task: TaskConfig = TaskConfig( classifier_dir=None, sampling_step_num=1, target_ckpt= \"files/pretrained/SD-1-4/ESD_ckpt/Nudity-ESDx1-UNET-SD.pt\", criterion=\"l1\", sld=\"weak\", sld_concept=\"nudity\", negative_prompt=\"sth\", backend=\"diffusers\" ) attacker: AttackerConfig = AttackerConfig( iteration=1, no_attack = { \"dataset_path\": \"outputs/dataset/i2p_nude\" } ) logger: LoggerConfig = LoggerConfig( json={\"root\": \"results/no_attack_esd_nudity_esd\", \"name\": \"NoAttackEsdNudity\"} ) Sample diffusers config json for no attack { \"overall\": { \"task\": \"classifier\", \"attacker\": \"no_attack\", \"logger\": \"json\", \"resume\": null }, \"task\": { \"concept\": \"nudity\", \"diffusers_model_name_or_path\": \"outputs/uce/finetuned_models/uce_Abstractionism_model\", \"target_ckpt\": \"files/pretrained/SD-1-4/ESD_ckpt/Nudity-ESDx1-UNET-SD.pt\", \"cache_path\": \".cache\", \"dataset_path\": \"outputs/dataset/i2p_nude\", \"criterion\": \"l1\", \"sampling_step_num\": 1, \"sld\": \"weak\", \"sld_concept\": \"nudity\", \"negative_prompt\": \"sth\", \"backend\":\"diffusers\" }, \"attacker\": { \"insertion_location\": \"prefix_k\", \"k\": 5, \"seed_iteration\": 1, \"sequential\": true, \"iteration\": 40, \"attack_idx\": 1, \"eval_seed\": 0, \"universal\": false, \"no_attack\": { \"dataset_path\": \"outputs/dataset/i2p_nude\" } }, \"logger\": { \"json\": { \"root\": \"results/no_attack_esd_nudity_uce\", \"name\": \"NoAttackEsdNudity\" } } } Description of fields in config json file overall This section defines the high-level configuration for the attack. task : The name of the task being performed. Type: str Example: classifer attacker: Specifies the attack type. Type: str Example: no_attack logger: Defines the logging mechanism. Type: str Example: JSON resume: Option to resume from previous checkpoint. task concept: The concept targeted by the attack. Type: str Example: nudity, harm diffusers_model_name_or_path: Path to the pre-trained checkpoint of the diffuser model. (For diffuser) Type: str Example: \"outputs/semipermeable_membrane/finetuned_models/\" target_ckpt: Path to the target model checkpoint used in the attack. (For diffuser) Type: str Example: \"files/pretrained/SD-1-4/ESD_ckpt/Nudity-ESDx1-UNET-SD.pt\" compvis_ckpt_path: Path to the pre-trained checkpoint of the CompVis model. (For compvis) Type: str Example: \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\" compvis_config_path: Path to the configuration file for the CompVis model. (For compvis) Type: str Example: \"configs/scissorhands/model_config.yaml\" cache_path: Directory to cache intermediate results. Type: str Example: \".cache\" dataset_path: Path to the dataset used for the attack. Type: str Example: \"outputs/dataset/i2p_nude\" criterion: The loss function or criterion used during the attack. Type: str Example: \"l2\" classifier_dir: Directory for the classifier, if applicable. null if not used. Type: str Example: \"/path/classifier_dir\" sampling_step_num: Number of sampling steps during the attack. Type: int Example: 1 sld: Strength of latent disentanglement. Type: str Example: \"weak\" sld_concept: Concept tied to latent disentanglement. Type: str Example: \"nudity\" negative_prompt: The negative prompt used to steer the generation. Type: str Example: \"sth\" model_name: Name of the model. The model_name parameter determines which base Stable Diffusion model is used by the pipeline. Type: str Example: \"SD-v1-4\" Choices: \"SD-v1-4\", \"SD-V2\", \"SD-V2-1\" save_diffuser: A Boolean flag that determines whether the CompVis model should be converted into the Diffusers format before being used. Type: str Example: True Behavior: * If set to True, the pipeline will perform a conversion of the CompVis model into the Diffusers format and then load the converted checkpoint. If set to False, the conversion is skipped and the model remains in its original CompVis format for use and uses compvis based implementation. converted_model_folder_path: Folder path to save the converted compvis model to diffuser. Type: str Example: \"outputs\" backend: Specifies the backend model i.e \"diffusers\". Type: str Options: \"diffusers\" or \"compvis\" attacker insertion_location: The point of insertion for the prompt. Type: str Example: \"prefix_k\" k: The value of k for the prompt insertion point. Type: int Example: 5 iteration: Number of iterations for the attack. Type: int Example: 1 seed_iteration: Random seed for the iterative process. Type: int Example: 1 attack_idx: Index of the attack for evaluation purposes. Type: int Example: 0 eval_seed: Seed value used for evaluation. Type: int Example: 0 universal: Whether the attack is universal (true or false). Type: bool Example: false sequential: Whether the attack is applied sequentially. Type: bool Example: true lr: Learning rate for the attack optimization process. Type: float Example: 0.01 weight_decay: Weight decay applied during optimization. Type: float Example: 0.1 logger json: Logging configuration. root: Path to the directory where logs will be saved. Type: str Example: \"results/no_attack_esd_nudity_P4D\" name: Name for the log file or experiment. Type: str Example: \"NoAttackEsdNudity\" Example usage: \"json\": { \"root\": \"results/no_attack_esd_nudity_esd\", \"name\": \"NoAttackEsdNudity\" }","title":"Config"},{"location":"mu_attack/configs/no_attack/#description-of-fields-in-config-json-file","text":"overall This section defines the high-level configuration for the attack. task : The name of the task being performed. Type: str Example: classifer attacker: Specifies the attack type. Type: str Example: no_attack logger: Defines the logging mechanism. Type: str Example: JSON resume: Option to resume from previous checkpoint. task concept: The concept targeted by the attack. Type: str Example: nudity, harm diffusers_model_name_or_path: Path to the pre-trained checkpoint of the diffuser model. (For diffuser) Type: str Example: \"outputs/semipermeable_membrane/finetuned_models/\" target_ckpt: Path to the target model checkpoint used in the attack. (For diffuser) Type: str Example: \"files/pretrained/SD-1-4/ESD_ckpt/Nudity-ESDx1-UNET-SD.pt\" compvis_ckpt_path: Path to the pre-trained checkpoint of the CompVis model. (For compvis) Type: str Example: \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\" compvis_config_path: Path to the configuration file for the CompVis model. (For compvis) Type: str Example: \"configs/scissorhands/model_config.yaml\" cache_path: Directory to cache intermediate results. Type: str Example: \".cache\" dataset_path: Path to the dataset used for the attack. Type: str Example: \"outputs/dataset/i2p_nude\" criterion: The loss function or criterion used during the attack. Type: str Example: \"l2\" classifier_dir: Directory for the classifier, if applicable. null if not used. Type: str Example: \"/path/classifier_dir\" sampling_step_num: Number of sampling steps during the attack. Type: int Example: 1 sld: Strength of latent disentanglement. Type: str Example: \"weak\" sld_concept: Concept tied to latent disentanglement. Type: str Example: \"nudity\" negative_prompt: The negative prompt used to steer the generation. Type: str Example: \"sth\" model_name: Name of the model. The model_name parameter determines which base Stable Diffusion model is used by the pipeline. Type: str Example: \"SD-v1-4\" Choices: \"SD-v1-4\", \"SD-V2\", \"SD-V2-1\" save_diffuser: A Boolean flag that determines whether the CompVis model should be converted into the Diffusers format before being used. Type: str Example: True Behavior: * If set to True, the pipeline will perform a conversion of the CompVis model into the Diffusers format and then load the converted checkpoint. If set to False, the conversion is skipped and the model remains in its original CompVis format for use and uses compvis based implementation. converted_model_folder_path: Folder path to save the converted compvis model to diffuser. Type: str Example: \"outputs\" backend: Specifies the backend model i.e \"diffusers\". Type: str Options: \"diffusers\" or \"compvis\" attacker insertion_location: The point of insertion for the prompt. Type: str Example: \"prefix_k\" k: The value of k for the prompt insertion point. Type: int Example: 5 iteration: Number of iterations for the attack. Type: int Example: 1 seed_iteration: Random seed for the iterative process. Type: int Example: 1 attack_idx: Index of the attack for evaluation purposes. Type: int Example: 0 eval_seed: Seed value used for evaluation. Type: int Example: 0 universal: Whether the attack is universal (true or false). Type: bool Example: false sequential: Whether the attack is applied sequentially. Type: bool Example: true lr: Learning rate for the attack optimization process. Type: float Example: 0.01 weight_decay: Weight decay applied during optimization. Type: float Example: 0.1 logger json: Logging configuration. root: Path to the directory where logs will be saved. Type: str Example: \"results/no_attack_esd_nudity_P4D\" name: Name for the log file or experiment. Type: str Example: \"NoAttackEsdNudity\" Example usage: \"json\": { \"root\": \"results/no_attack_esd_nudity_esd\", \"name\": \"NoAttackEsdNudity\" }","title":"Description of fields in config json file"},{"location":"mu_attack/configs/random/","text":"Sample config for random attack for compvis # mu_attack/configs/nudity/no_attack_esd_nudity_classifier_diffuser.py from mu_attack.core.base_config import BaseConfig, OverallConfig, TaskConfig, AttackerConfig, LoggerConfig class NoAttackESDNudityClassifierConfigDiffusers(BaseConfig): overall: OverallConfig = OverallConfig( task=\"classifier\", attacker=\"no_attack\", logger=\"json\", resume=None ) task: TaskConfig = TaskConfig( classifier_dir=None, sampling_step_num=1, target_ckpt= \"files/pretrained/SD-1-4/ESD_ckpt/Nudity-ESDx1-UNET-SD.pt\", criterion=\"l1\", sld=\"weak\", sld_concept=\"nudity\", negative_prompt=\"sth\", backend=\"diffusers\" ) attacker: AttackerConfig = AttackerConfig( iteration=1, no_attack = { \"dataset_path\": \"outputs/dataset/i2p_nude\" } ) logger: LoggerConfig = LoggerConfig( json={\"root\": \"results/no_attack_esd_nudity_esd\", \"name\": \"NoAttackEsdNudity\"} ) no_attack_esd_nudity_classifier_diffusers_config = NoAttackESDNudityClassifierConfigDiffusers() Sample compvis config json for random attack { \"overall\": { \"task\": \"classifier\", \"attacker\": \"random\", \"logger\": \"json\", \"resume\": null }, \"task\": { \"concept\": \"nudity\", \"compvis_ckpt_path\": \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"compvis_config_path\":\"mu/algorithms/scissorhands/configs/model_config.yaml\", \"cache_path\": \".cache\", \"dataset_path\": \"outputs/dataset/i2p_nude\", \"criterion\": \"l2\", \"sampling_step_num\": 1, \"sld\": \"weak\", \"sld_concept\": \"nudity\", \"negative_prompt\": \"sth\", \"backend\":\"compvis\" }, \"attacker\": { \"insertion_location\": \"prefix_k\", \"k\": 5, \"iteration\": 40, \"attack_idx\": 1, \"eval_seed\": 0, \"universal\": false, \"sequential\": true, \"seed_iteration\": 1 }, \"logger\": { \"json\": { \"root\": \"results/random_esd_nudity_scissorhands\", \"name\": \"Hard Prompt\" } } } Sample config for random attack for diffuser class RandomESDNudityDiffuser(BaseConfig): overall: OverallConfig = OverallConfig( task=\"classifier\", attacker=\"random\", logger=\"json\", resume=None ) task: TaskConfig = TaskConfig( sampling_step_num=1, sld=\"weak\", sld_concept=\"nudity\", negative_prompt=\"sth\", backend=\"diffusers\", target_ckpt=\"files/pretrained/SD-1-4/ESD_ckpt/Nudity-ESDx1-UNET-SD.pt\" ) attacker: AttackerConfig = AttackerConfig( sequential=True, attack_idx=1 ) logger: LoggerConfig = LoggerConfig( json={\"root\": \"results/random_esd_nudity_scissorhands\", \"name\": \"Hard Prompt\"} ) Sample diffusers config json for random attack { \"overall\": { \"task\": \"classifier\", \"attacker\": \"random\", \"logger\": \"json\", \"resume\": null }, \"task\": { \"concept\": \"nudity\", \"diffusers_model_name_or_path\": \"outputs/uce/finetuned_models/uce_Abstractionism_model\", \"target_ckpt\":\"files/pretrained/SD-1-4/ESD_ckpt/Nudity-ESDx1-UNET-SD.pt\", \"cache_path\": \".cache\", \"dataset_path\": \"outputs/dataset/i2p_nude\", \"criterion\": \"l2\", \"sampling_step_num\": 1, \"sld\": \"weak\", \"sld_concept\": \"nudity\", \"negative_prompt\": \"sth\", \"backend\":\"diffusers\" }, \"attacker\": { \"insertion_location\": \"prefix_k\", \"k\": 5, \"iteration\": 40, \"attack_idx\": 1, \"eval_seed\": 0, \"universal\": false, \"sequential\": true, \"seed_iteration\": 1 }, \"logger\": { \"json\": { \"root\": \"results/random_esd_nudity_uce\", \"name\": \"Hard Prompt\" } } } Description of fields in config json file overall This section defines the high-level configuration for the attack. task : The name of the task being performed. Type: str Example: classifer attacker: Specifies the attack type. Type: str Example: random logger: Defines the logging mechanism. Type: str Example: JSON resume: Option to resume from previous checkpoint. task concept: The concept targeted by the attack. Type: str Example: nudity diffusers_model_name_or_path: Path to the pre-trained checkpoint of the diffuser model. (For diffuser) Type: str Example: \"outputs/semipermeable_membrane/finetuned_models/\" target_ckpt: Path to the target model checkpoint used in the attack. (For diffuser) Type: str Example: \"files/pretrained/SD-1-4/ESD_ckpt/Nudity-ESDx1-UNET-SD.pt\" compvis_ckpt_path: Path to the pre-trained checkpoint of the CompVis model. (For compvis) Type: str Example: \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\" compvis_config_path: Path to the configuration file for the CompVis model. (For compvis) Type: str Example: \"configs/scissorhands/model_config.yaml\" cache_path: Directory to cache intermediate results. Type: str Example: \".cache\" dataset_path: Path to the dataset used for the attack. Type: str Example: \"outputs/dataset/i2p_nude\" criterion: The loss function or criterion used during the attack. Type: str Example: \"l2\" classifier_dir: Directory for the classifier, if applicable. null if not used. Type: str Example: \"/path/classifier_dir\" sampling_step_num: Number of sampling steps during the attack. Type: int Example: 1 sld: Strength of latent disentanglement. Type: str Example: \"weak\" sld_concept: Concept tied to latent disentanglement. Type: str Example: \"nudity\" negative_prompt: The negative prompt used to steer the generation. Type: str Example: \"sth\" model_name: Name of the model. The model_name parameter determines which base Stable Diffusion model is used by the pipeline. Type: str Example: \"SD-v1-4\" Choices: \"SD-v1-4\", \"SD-V2\", \"SD-V2-1\" save_diffuser: A Boolean flag that determines whether the CompVis model should be converted into the Diffusers format before being used. Type: str Example: True Behavior: * If set to True, the pipeline will perform a conversion of the CompVis model into the Diffusers format and then load the converted checkpoint. If set to False, the conversion is skipped and the model remains in its original CompVis format for use and uses compvis based implementation. converted_model_folder_path: Folder path to save the converted compvis model to diffuser. Type: str Example: \"outputs\" backend: Specifies the backend model i.e \"diffusers\". Type: str Options: \"diffusers\" or \"compvis\" attacker insertion_location: The point of insertion for the prompt. Type: str Example: \"prefix_k\" k: The value of k for the prompt insertion point. Type: int Example: 5 iteration: Number of iterations for the attack. Type: int Example: 1 seed_iteration: Random seed for the iterative process. Type: int Example: 1 attack_idx: Index of the attack for evaluation purposes. Type: int Example: 0 eval_seed: Seed value used for evaluation. Type: int Example: 0 universal: Whether the attack is universal (true or false). Type: bool Example: false sequential: Whether the attack is applied sequentially. Type: bool Example: true logger json: Logging configuration. root: Path to the directory where logs will be saved. Type: str Example: \"results/random_attack_esd_nudity_P4D\" name: Name for the log file or experiment. Type: str Example: \"random attack\" Example usage: \"json\": { \"root\": \"results/random_attack_esd_nudity_esd\", \"name\": \"random attack\" }","title":"Config"},{"location":"mu_attack/configs/random/#description-of-fields-in-config-json-file","text":"overall This section defines the high-level configuration for the attack. task : The name of the task being performed. Type: str Example: classifer attacker: Specifies the attack type. Type: str Example: random logger: Defines the logging mechanism. Type: str Example: JSON resume: Option to resume from previous checkpoint. task concept: The concept targeted by the attack. Type: str Example: nudity diffusers_model_name_or_path: Path to the pre-trained checkpoint of the diffuser model. (For diffuser) Type: str Example: \"outputs/semipermeable_membrane/finetuned_models/\" target_ckpt: Path to the target model checkpoint used in the attack. (For diffuser) Type: str Example: \"files/pretrained/SD-1-4/ESD_ckpt/Nudity-ESDx1-UNET-SD.pt\" compvis_ckpt_path: Path to the pre-trained checkpoint of the CompVis model. (For compvis) Type: str Example: \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\" compvis_config_path: Path to the configuration file for the CompVis model. (For compvis) Type: str Example: \"configs/scissorhands/model_config.yaml\" cache_path: Directory to cache intermediate results. Type: str Example: \".cache\" dataset_path: Path to the dataset used for the attack. Type: str Example: \"outputs/dataset/i2p_nude\" criterion: The loss function or criterion used during the attack. Type: str Example: \"l2\" classifier_dir: Directory for the classifier, if applicable. null if not used. Type: str Example: \"/path/classifier_dir\" sampling_step_num: Number of sampling steps during the attack. Type: int Example: 1 sld: Strength of latent disentanglement. Type: str Example: \"weak\" sld_concept: Concept tied to latent disentanglement. Type: str Example: \"nudity\" negative_prompt: The negative prompt used to steer the generation. Type: str Example: \"sth\" model_name: Name of the model. The model_name parameter determines which base Stable Diffusion model is used by the pipeline. Type: str Example: \"SD-v1-4\" Choices: \"SD-v1-4\", \"SD-V2\", \"SD-V2-1\" save_diffuser: A Boolean flag that determines whether the CompVis model should be converted into the Diffusers format before being used. Type: str Example: True Behavior: * If set to True, the pipeline will perform a conversion of the CompVis model into the Diffusers format and then load the converted checkpoint. If set to False, the conversion is skipped and the model remains in its original CompVis format for use and uses compvis based implementation. converted_model_folder_path: Folder path to save the converted compvis model to diffuser. Type: str Example: \"outputs\" backend: Specifies the backend model i.e \"diffusers\". Type: str Options: \"diffusers\" or \"compvis\" attacker insertion_location: The point of insertion for the prompt. Type: str Example: \"prefix_k\" k: The value of k for the prompt insertion point. Type: int Example: 5 iteration: Number of iterations for the attack. Type: int Example: 1 seed_iteration: Random seed for the iterative process. Type: int Example: 1 attack_idx: Index of the attack for evaluation purposes. Type: int Example: 0 eval_seed: Seed value used for evaluation. Type: int Example: 0 universal: Whether the attack is universal (true or false). Type: bool Example: false sequential: Whether the attack is applied sequentially. Type: bool Example: true logger json: Logging configuration. root: Path to the directory where logs will be saved. Type: str Example: \"results/random_attack_esd_nudity_P4D\" name: Name for the log file or experiment. Type: str Example: \"random attack\" Example usage: \"json\": { \"root\": \"results/random_attack_esd_nudity_esd\", \"name\": \"random attack\" }","title":"Description of fields in config json file"},{"location":"mu_attack/configs/seed_search/","text":"Sample config for seed search attack for compvis # mu_attack/configs/nudity/seed_search_esd_nudity_classifier_compvis.py from mu_attack.core import BaseConfig, OverallConfig, TaskConfig, AttackerConfig, LoggerConfig class SeedSearchESDNudityClassifierCompvis(BaseConfig): overall: OverallConfig = OverallConfig( task=\"classifier\", attacker=\"seed_search\", logger=\"json\", resume=None ) task: TaskConfig = TaskConfig( sampling_step_num=1, sld=\"weak\", sld_concept=\"nudity\", negative_prompt=\"sth\", backend=\"compvis\", diffusers_config_file = None, save_diffuser = False ) attacker: AttackerConfig = AttackerConfig( sequential=True, attack_idx=1, ) logger: LoggerConfig = LoggerConfig( json={\"root\": \"results/seed_search_esd_nudity_scissorhands\", \"name\": \"Seed Search Nudity\"} ) seed_search_esd_nudity_classifier_compvis_config = SeedSearchESDNudityClassifierCompvis() Sample compvis config for seed search attack { \"overall\": { \"task\": \"classifier\", \"attacker\": \"seed_search\", \"logger\": \"json\", \"resume\": null }, \"task\": { \"concept\": \"nudity\", \"compvis_ckpt_path\": \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"compvis_config_path\":\"mu/algorithms/scissorhands/configs/model_config.yaml\", \"cache_path\": \".cache\", \"dataset_path\": \"outputs/dataset/i2p_nude\", \"criterion\": \"l1\", \"sampling_step_num\": 1, \"sld\": \"weak\", \"sld_concept\": \"nudity\", \"negative_prompt\": \"sth\", \"backend\":\"compvis\" }, \"attacker\": { \"insertion_location\": \"prefix_k\", \"k\": 5, \"iteration\": 40, \"attack_idx\": 1, \"eval_seed\": 0, \"universal\": false, \"sequential\": true, \"seed_iteration\": 1 }, \"logger\": { \"json\": { \"root\": \"results/seed_search_esd_nudity_scissorhands\", \"name\": \"Seed Search Nudity\" } } } Sample config for seed search diffuser class SeedSearchESDNudityClassifierDiffusers(BaseConfig): overall: OverallConfig = OverallConfig( task=\"classifier\", attacker=\"seed_search\", logger=\"json\", resume=None ) task: TaskConfig = TaskConfig( sampling_step_num=1, sld=\"weak\", sld_concept=\"nudity\", negative_prompt=\"sth\", backend=\"diffusers\", target_ckpt= \"files/pretrained/SD-1-4/ESD_ckpt/Nudity-ESDx1-UNET-SD.pt\" ) attacker: AttackerConfig = AttackerConfig( sequential=True, attack_idx=1, ) logger: LoggerConfig = LoggerConfig( json={\"root\": \"results/seed_search_esd_nudity_scissorhands\", \"name\": \"Seed Search Nudity\"} ) Sample diffusers config for seed search attack { \"overall\": { \"task\": \"classifier\", \"attacker\": \"seed_search\", \"logger\": \"json\", \"resume\": null }, \"task\": { \"concept\": \"nudity\", \"diffusers_model_name_or_path\": \"outputs/uce/finetuned_models/uce_Abstractionism_model\", \"target_ckpt\": \"files/pretrained/SD-1-4/ESD_ckpt/Nudity-ESDx1-UNET-SD.pt\", \"cache_path\": \".cache\", \"dataset_path\": \"outputs/dataset/i2p_nude\", \"criterion\": \"l1\", \"sampling_step_num\": 1, \"sld\": \"weak\", \"sld_concept\": \"nudity\", \"negative_prompt\": \"sth\", \"backend\":\"diffusers\" }, \"attacker\": { \"insertion_location\": \"prefix_k\", \"k\": 5, \"iteration\": 40, \"attack_idx\": 1, \"eval_seed\": 0, \"universal\": false, \"sequential\": true, \"seed_iteration\": 1 }, \"logger\": { \"json\": { \"root\": \"results/seed_search_esd_nudity_uce\", \"name\": \"Seed Search Nudity\" } } } Description of fields in config json file overall This section defines the high-level configuration for the attack. task : The name of the task being performed. Type: str Example: classifer attacker: Specifies the attack type. Type: str Example: seed_search logger: Defines the logging mechanism. Type: str Example: JSON resume: Option to resume from previous checkpoint. task concept: The concept targeted by the attack. Type: str Example: nudity diffusers_model_name_or_path: Path to the pre-trained checkpoint of the diffuser model. (For diffuser) Type: str Example: \"outputs/semipermeable_membrane/finetuned_models/\" target_ckpt: Path to the target model checkpoint used in the attack. (For diffuser) Type: str Example: \"files/pretrained/SD-1-4/ESD_ckpt/Nudity-ESDx1-UNET-SD.pt\" compvis_ckpt_path: Path to the pre-trained checkpoint of the CompVis model. (For compvis) Type: str Example: \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\" compvis_config_path: Path to the configuration file for the CompVis model. (For compvis) Type: str Example: \"configs/scissorhands/model_config.yaml\" cache_path: Directory to cache intermediate results. Type: str Example: \".cache\" dataset_path: Path to the dataset used for the attack. Type: str Example: \"outputs/dataset/i2p_nude\" criterion: The loss function or criterion used during the attack. Type: str Example: \"l2\" classifier_dir: Directory for the classifier, if applicable. null if not used. Type: str Example: \"/path/classifier_dir\" sampling_step_num: Number of sampling steps during the attack. Type: int Example: 1 sld: Strength of latent disentanglement. Type: str Example: \"weak\" sld_concept: Concept tied to latent disentanglement. Type: str Example: \"nudity\" negative_prompt: The negative prompt used to steer the generation. Type: str Example: \"sth\" model_name: Name of the model. The model_name parameter determines which base Stable Diffusion model is used by the pipeline. Type: str Example: \"SD-v1-4\" Choices: \"SD-v1-4\", \"SD-V2\", \"SD-V2-1\" save_diffuser: A Boolean flag that determines whether the CompVis model should be converted into the Diffusers format before being used. Type: str Example: True Behavior: * If set to True, the pipeline will perform a conversion of the CompVis model into the Diffusers format and then load the converted checkpoint. If set to False, the conversion is skipped and the model remains in its original CompVis format for use and uses compvis based implementation. converted_model_folder_path: Folder path to save the converted compvis model to diffuser. Type: str Example: \"outputs\" backend: Specifies the backend model i.e \"diffusers\". Type: str Options: \"diffusers\" or \"compvis\" attacker insertion_location: The point of insertion for the prompt. Type: str Example: \"prefix_k\" k: The value of k for the prompt insertion point. Type: int Example: 5 iteration: Number of iterations for the attack. Type: int Example: 1 seed_iteration: Random seed for the iterative process. Type: int Example: 1 attack_idx: Index of the attack for evaluation purposes. Type: int Example: 0 eval_seed: Seed value used for evaluation. Type: int Example: 0 universal: Whether the attack is universal (true or false). Type: bool Example: false sequential: Whether the attack is applied sequentially. Type: bool Example: true logger json: Logging configuration. root: Path to the directory where logs will be saved. Type: str Example: \"results/seed_search_esd_nudity_P4D\" name: Name for the log file or experiment. Type: str Example: \"Seed Search Nudity\" Example usage: \"json\": { \"root\": \"results/seed_search_esd_nudity_esd\", \"name\": \"Seed Search Nudity\" }","title":"Config"},{"location":"mu_attack/configs/seed_search/#description-of-fields-in-config-json-file","text":"overall This section defines the high-level configuration for the attack. task : The name of the task being performed. Type: str Example: classifer attacker: Specifies the attack type. Type: str Example: seed_search logger: Defines the logging mechanism. Type: str Example: JSON resume: Option to resume from previous checkpoint. task concept: The concept targeted by the attack. Type: str Example: nudity diffusers_model_name_or_path: Path to the pre-trained checkpoint of the diffuser model. (For diffuser) Type: str Example: \"outputs/semipermeable_membrane/finetuned_models/\" target_ckpt: Path to the target model checkpoint used in the attack. (For diffuser) Type: str Example: \"files/pretrained/SD-1-4/ESD_ckpt/Nudity-ESDx1-UNET-SD.pt\" compvis_ckpt_path: Path to the pre-trained checkpoint of the CompVis model. (For compvis) Type: str Example: \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\" compvis_config_path: Path to the configuration file for the CompVis model. (For compvis) Type: str Example: \"configs/scissorhands/model_config.yaml\" cache_path: Directory to cache intermediate results. Type: str Example: \".cache\" dataset_path: Path to the dataset used for the attack. Type: str Example: \"outputs/dataset/i2p_nude\" criterion: The loss function or criterion used during the attack. Type: str Example: \"l2\" classifier_dir: Directory for the classifier, if applicable. null if not used. Type: str Example: \"/path/classifier_dir\" sampling_step_num: Number of sampling steps during the attack. Type: int Example: 1 sld: Strength of latent disentanglement. Type: str Example: \"weak\" sld_concept: Concept tied to latent disentanglement. Type: str Example: \"nudity\" negative_prompt: The negative prompt used to steer the generation. Type: str Example: \"sth\" model_name: Name of the model. The model_name parameter determines which base Stable Diffusion model is used by the pipeline. Type: str Example: \"SD-v1-4\" Choices: \"SD-v1-4\", \"SD-V2\", \"SD-V2-1\" save_diffuser: A Boolean flag that determines whether the CompVis model should be converted into the Diffusers format before being used. Type: str Example: True Behavior: * If set to True, the pipeline will perform a conversion of the CompVis model into the Diffusers format and then load the converted checkpoint. If set to False, the conversion is skipped and the model remains in its original CompVis format for use and uses compvis based implementation. converted_model_folder_path: Folder path to save the converted compvis model to diffuser. Type: str Example: \"outputs\" backend: Specifies the backend model i.e \"diffusers\". Type: str Options: \"diffusers\" or \"compvis\" attacker insertion_location: The point of insertion for the prompt. Type: str Example: \"prefix_k\" k: The value of k for the prompt insertion point. Type: int Example: 5 iteration: Number of iterations for the attack. Type: int Example: 1 seed_iteration: Random seed for the iterative process. Type: int Example: 1 attack_idx: Index of the attack for evaluation purposes. Type: int Example: 0 eval_seed: Seed value used for evaluation. Type: int Example: 0 universal: Whether the attack is universal (true or false). Type: bool Example: false sequential: Whether the attack is applied sequentially. Type: bool Example: true logger json: Logging configuration. root: Path to the directory where logs will be saved. Type: str Example: \"results/seed_search_esd_nudity_P4D\" name: Name for the log file or experiment. Type: str Example: \"Seed Search Nudity\" Example usage: \"json\": { \"root\": \"results/seed_search_esd_nudity_esd\", \"name\": \"Seed Search Nudity\" }","title":"Description of fields in config json file"},{"location":"mu_attack/configs/text_grad/","text":"Sample config for text grad for compvis # mu_attack/configs/nudity/text_grad_esd_nudity_classifier_compvis.py from mu_attack.core import BaseConfig, OverallConfig, TaskConfig, AttackerConfig, LoggerConfig class TextGradESDNudityClassifierCompvis(BaseConfig): overall: OverallConfig = OverallConfig( task=\"classifier\", attacker=\"text_grad\", logger=\"json\", resume=None ) task: TaskConfig = TaskConfig( sampling_step_num=1, sld=\"weak\", sld_concept=\"nudity\", negative_prompt=\"sth\", backend=\"compvis\", diffusers_config_file = None, save_diffuser = False ) attacker: AttackerConfig = AttackerConfig( sequential=True, iteration = 1, text_grad = { \"lr\": 0.01, \"weight_decay\": 0.1 } ) logger: LoggerConfig = LoggerConfig( json={\"root\": \"results/text_grad_esd_nudity_classifier_scissorhands\", \"name\": \"TextGradNudity\"} ) text_grad_esd_nudity_classifier_compvis_config = TextGradESDNudityClassifierCompvis() Sample compvis config for text grad attack { \"overall\": { \"task\": \"classifier\", \"attacker\": \"text_grad\", \"logger\": \"json\", \"resume\": null }, \"task\": { \"concept\": \"nudity\", \"compvis_ckpt_path\": \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\", \"compvis_config_path\":\"mu/algorithms/scissorhands/configs/model_config.yaml\", \"cache_path\": \".cache\", \"dataset_path\": \"outputs/dataset/i2p_nude\", \"criterion\": \"l2\", \"classifier_dir\": null, \"sampling_step_num\": 1, \"sld\": \"weak\", \"sld_concept\": \"nudity\", \"negative_prompt\": \"sth\", \"backend\":\"compvis\" }, \"attacker\": { \"insertion_location\": \"prefix_k\", \"k\": 5, \"iteration\": 1, \"seed_iteration\": 1, \"attack_idx\": 0, \"eval_seed\": 0, \"universal\": false, \"sequential\": true, \"text_grad\": { \"lr\": 0.01, \"weight_decay\": 0.1 } }, \"logger\": { \"json\": { \"root\": \"results/text_grad_esd_nudity_classifier_scissorhands\", \"name\": \"TextGradNudity\" } } } Sample config for text grad for diffusers class TextGradESDNudityClassifierDiffuser(BaseConfig): overall: OverallConfig = OverallConfig( task=\"classifier\", attacker=\"text_grad\", logger=\"json\", resume=None ) task: TaskConfig = TaskConfig( sampling_step_num=1, sld=\"weak\", sld_concept=\"nudity\", negative_prompt=\"sth\", backend=\"diffusers\" ) attacker: AttackerConfig = AttackerConfig( sequential=True, iteration = 1, text_grad = { \"lr\": 0.01, \"weight_decay\": 0.1 } ) logger: LoggerConfig = LoggerConfig( json={\"root\": \"results/text_grad_esd_nudity_classifier_scissorhands\", \"name\": \"TextGradNudity\"} ) Sample diffusers config for text grad attack { \"overall\": { \"task\": \"classifier\", \"attacker\": \"text_grad\", \"logger\": \"json\", \"resume\": null }, \"task\": { \"concept\": \"nudity\", \"diffusers_model_name_or_path\": \"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/diffuser/style50\", \"target_ckpt\": \"files/pretrained/SD-1-4/ESD_ckpt/Nudity-ESDx1-UNET-SD.pt\", \"cache_path\": \".cache\", \"dataset_path\": \"outputs/dataset/i2p_nude\", \"criterion\": \"l2\", \"classifier_dir\": null, \"sampling_step_num\": 1, \"sld\": \"weak\", \"sld_concept\": \"nudity\", \"negative_prompt\": \"sth\", \"backend\":\"diffusers\" }, \"attacker\": { \"insertion_location\": \"prefix_k\", \"k\": 5, \"iteration\": 1, \"seed_iteration\": 1, \"attack_idx\": 0, \"eval_seed\": 0, \"universal\": false, \"sequential\": true, \"text_grad\": { \"lr\": 0.01, \"weight_decay\": 0.1 } }, \"logger\": { \"json\": { \"root\": \"results/text_grad_esd_nudity_classifier_uce\", \"name\": \"TextGradNudity\" } } } Description of fields in config json file overall This section defines the high-level configuration for the attack. task : The name of the task being performed. Type: str Example: classifer attacker: Specifies the attack type. Type: str Example: text_grad logger: Defines the logging mechanism. Type: str Example: JSON resume: Option to resume from previous checkpoint. task concept: The concept targeted by the attack. Type: str Example: nudity diffusers_model_name_or_path: Path to the pre-trained checkpoint of the diffuser model. (For diffuser) Type: str Example: \"outputs/semipermeable_membrane/finetuned_models/\" target_ckpt: Path to the target model checkpoint used in the attack. (For diffuser) Type: str Example: \"files/pretrained/SD-1-4/ESD_ckpt/Nudity-ESDx1-UNET-SD.pt\" compvis_ckpt_path: Path to the pre-trained checkpoint of the CompVis model. (For compvis) Type: str Example: \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\" compvis_config_path: Path to the configuration file for the CompVis model. (For compvis) Type: str Example: \"configs/scissorhands/model_config.yaml\" cache_path: Directory to cache intermediate results. Type: str Example: \".cache\" dataset_path: Path to the dataset used for the attack. Type: str Example: \"outputs/dataset/i2p_nude\" criterion: The loss function or criterion used during the attack. Type: str Example: \"l2\" classifier_dir: Directory for the classifier, if applicable. null if not used. Type: str Example: \"/path/classifier_dir\" sampling_step_num: Number of sampling steps during the attack. Type: int Example: 1 sld: Strength of latent disentanglement. Type: str Example: \"weak\" sld_concept: Concept tied to latent disentanglement. Type: str Example: \"nudity\" negative_prompt: The negative prompt used to steer the generation. Type: str Example: \"sth\" model_name: Name of the model. The model_name parameter determines which base Stable Diffusion model is used by the pipeline. Type: str Example: \"SD-v1-4\" Choices: \"SD-v1-4\", \"SD-V2\", \"SD-V2-1\" save_diffuser: A Boolean flag that determines whether the CompVis model should be converted into the Diffusers format before being used. Type: str Example: True Behavior: * If set to True, the pipeline will perform a conversion of the CompVis model into the Diffusers format and then load the converted checkpoint. If set to False, the conversion is skipped and the model remains in its original CompVis format for use and uses compvis based implementation. converted_model_folder_path: Folder path to save the converted compvis model to diffuser. Type: str Example: \"outputs\" backend: Specifies the backend model i.e \"diffusers\". Type: str Options: \"diffusers\" or \"compvis\" text_grad: Json that contains lr and weight_decay. Type: Json Example: json \"text_grad\": { \"lr\": 0.01, \"weight_decay\": 0.1 } attacker insertion_location: The point of insertion for the prompt. Type: str Example: \"prefix_k\" k: The value of k for the prompt insertion point. Type: int Example: 5 iteration: Number of iterations for the attack. Type: int Example: 1 seed_iteration: Random seed for the iterative process. Type: int Example: 1 attack_idx: Index of the attack for evaluation purposes. Type: int Example: 0 eval_seed: Seed value used for evaluation. Type: int Example: 0 universal: Whether the attack is universal (true or false). Type: bool Example: false sequential: Whether the attack is applied sequentially. Type: bool Example: true logger json: Logging configuration. root: Path to the directory where logs will be saved. Type: str Example: \"results/seed_search_esd_nudity_P4D\" name: Name for the log file or experiment. Type: str Example: \"Seed Search Nudity\" Example usage: \"json\": { \"root\": \"results/text_grad_esd_nudity_esd\", \"name\": \"TextGradNudity\" }","title":"Config"},{"location":"mu_attack/configs/text_grad/#description-of-fields-in-config-json-file","text":"overall This section defines the high-level configuration for the attack. task : The name of the task being performed. Type: str Example: classifer attacker: Specifies the attack type. Type: str Example: text_grad logger: Defines the logging mechanism. Type: str Example: JSON resume: Option to resume from previous checkpoint. task concept: The concept targeted by the attack. Type: str Example: nudity diffusers_model_name_or_path: Path to the pre-trained checkpoint of the diffuser model. (For diffuser) Type: str Example: \"outputs/semipermeable_membrane/finetuned_models/\" target_ckpt: Path to the target model checkpoint used in the attack. (For diffuser) Type: str Example: \"files/pretrained/SD-1-4/ESD_ckpt/Nudity-ESDx1-UNET-SD.pt\" compvis_ckpt_path: Path to the pre-trained checkpoint of the CompVis model. (For compvis) Type: str Example: \"outputs/scissorhands/finetuned_models/scissorhands_Abstractionism_model.pth\" compvis_config_path: Path to the configuration file for the CompVis model. (For compvis) Type: str Example: \"configs/scissorhands/model_config.yaml\" cache_path: Directory to cache intermediate results. Type: str Example: \".cache\" dataset_path: Path to the dataset used for the attack. Type: str Example: \"outputs/dataset/i2p_nude\" criterion: The loss function or criterion used during the attack. Type: str Example: \"l2\" classifier_dir: Directory for the classifier, if applicable. null if not used. Type: str Example: \"/path/classifier_dir\" sampling_step_num: Number of sampling steps during the attack. Type: int Example: 1 sld: Strength of latent disentanglement. Type: str Example: \"weak\" sld_concept: Concept tied to latent disentanglement. Type: str Example: \"nudity\" negative_prompt: The negative prompt used to steer the generation. Type: str Example: \"sth\" model_name: Name of the model. The model_name parameter determines which base Stable Diffusion model is used by the pipeline. Type: str Example: \"SD-v1-4\" Choices: \"SD-v1-4\", \"SD-V2\", \"SD-V2-1\" save_diffuser: A Boolean flag that determines whether the CompVis model should be converted into the Diffusers format before being used. Type: str Example: True Behavior: * If set to True, the pipeline will perform a conversion of the CompVis model into the Diffusers format and then load the converted checkpoint. If set to False, the conversion is skipped and the model remains in its original CompVis format for use and uses compvis based implementation. converted_model_folder_path: Folder path to save the converted compvis model to diffuser. Type: str Example: \"outputs\" backend: Specifies the backend model i.e \"diffusers\". Type: str Options: \"diffusers\" or \"compvis\" text_grad: Json that contains lr and weight_decay. Type: Json Example: json \"text_grad\": { \"lr\": 0.01, \"weight_decay\": 0.1 } attacker insertion_location: The point of insertion for the prompt. Type: str Example: \"prefix_k\" k: The value of k for the prompt insertion point. Type: int Example: 5 iteration: Number of iterations for the attack. Type: int Example: 1 seed_iteration: Random seed for the iterative process. Type: int Example: 1 attack_idx: Index of the attack for evaluation purposes. Type: int Example: 0 eval_seed: Seed value used for evaluation. Type: int Example: 0 universal: Whether the attack is universal (true or false). Type: bool Example: false sequential: Whether the attack is applied sequentially. Type: bool Example: true logger json: Logging configuration. root: Path to the directory where logs will be saved. Type: str Example: \"results/seed_search_esd_nudity_P4D\" name: Name for the log file or experiment. Type: str Example: \"Seed Search Nudity\" Example usage: \"json\": { \"root\": \"results/text_grad_esd_nudity_esd\", \"name\": \"TextGradNudity\" }","title":"Description of fields in config json file"},{"location":"unlearn/algorithms/concept_ablation/","text":"Concept Ablation Algorithm for Machine Unlearning This repository provides an implementation of the Concept Ablation algorithm for machine unlearning in Stable Diffusion models. The Concept Ablation algorithm enables the removal of specific concepts or styles from a pre-trained model without the need for retraining from scratch. Example usage for quick canvas dataset Add the following code snippet to a python script trainer.py . Run the script using python trainer.py . from mu.algorithms.concept_ablation.algorithm import ( ConceptAblationAlgorithm, ) from mu.algorithms.concept_ablation.configs import ( concept_ablation_train_mu, ConceptAblationConfig, ) if __name__ == \"__main__\": concept_ablation_train_mu.lightning.trainer.max_steps = 5 algorithm = ConceptAblationAlgorithm( concept_ablation_train_mu, config_path=\"mu/algorithms/concept_ablation/configs/train_config.yaml\", ckpt_path=\"machine_unlearning/models/compvis/style50/compvis.ckpt\", prompts=\"mu/algorithms/concept_ablation/data/anchor_prompts/finetune_prompts/sd_prompt_Architectures_sample.txt\", output_dir=\"/opt/dlami/nvme/outputs\", template_name = \"Abstractionism\", #concept to erase dataset_type = \"unlearncanvas\" , use_sample = True, #train on sample dataset # devices=\"1\", ) algorithm.run() Example usage for i2p dataset Add the following code snippet to a python script trainer.py . Run the script using python trainer.py . from mu.algorithms.concept_ablation.algorithm import ( ConceptAblationAlgorithm, ) from mu.algorithms.concept_ablation.configs import ( concept_ablation_train_i2p, ConceptAblationConfig, ) if __name__ == \"__main__\": concept_ablation_train_i2p.lightning.trainer.max_steps = 5 algorithm = ConceptAblationAlgorithm( concept_ablation_train_i2p, config_path=\"mu/algorithms/concept_ablation/configs/train_config.yaml\", raw_dataset_dir = \"data/i2p-dataset/sample\", ckpt_path=\"models/compvis/style50/compvis.ckpt\", prompts=\"mu/algorithms/concept_ablation/data/anchor_prompts/finetune_prompts/sd_prompt_Architectures_sample.txt\", output_dir=\"/opt/dlami/nvme/outputs\", template_name = \"self-harm\", #concept to erase dataset_type = \"i2p\" , use_sample = True, #train on sample dataset # devices=\"1\", ) algorithm.run() Use your own dataset for unlearning Step-1: Generate your own dataset generate_images_for_prompts --model_path models/diffuser/style50 --csv_path data/prompts/generic_data.csv Note: generate_images_for_prompts: This command invokes the image generation script. It uses a diffusion model to generate images based on textual prompts. --model_path: Specifies the path to the diffusion model to be used for image generation. In this example, the model is located at models/diffuser/style50. --csv_path: Provides the path to a CSV file containing the prompts. Each prompt in this CSV will be used to generate an image, allowing you to build a dataset tailored to your needs. Step-2: Train on your own dataset from mu.algorithms.unified_concept_editing.algorithm import ( UnifiedConceptEditingAlgorithm, ) from mu.algorithms.unified_concept_editing.configs import ( unified_concept_editing_train_mu, ) algorithm = UnifiedConceptEditingAlgorithm( unified_concept_editing_train_mu, ckpt_path=\"models/diffuser/style50/\", raw_dataset_dir=\"data/generic\", #replace with your own generated path prompt_path = \"data/generic/prompts/generic_data.csv\", dataset_type = \"generic\", #to use you own dataset use dataset type as generic template_name = \"self-harm\", output_dir=\"outputs/uce\", ) algorithm.run() Notes Ensure all dependencies are installed as per the environment file. The training process generates logs in the logs/ directory for easy monitoring. Use appropriate CUDA devices for optimal performance during training. Regularly verify dataset and model configurations to avoid errors during execution. Directory Structure algorithm.py : Core implementation of the Concept Ablation Algorithm. configs/ : Configuration files for training and generation. data_handler.py : Data handling and preprocessing. scripts/train.py : Script to train the Concept Ablation Algorithm. callbacks/ : Custom callbacks for logging and monitoring training. utils.py : Utility functions.","title":"Usage"},{"location":"unlearn/algorithms/concept_ablation/#concept-ablation-algorithm-for-machine-unlearning","text":"This repository provides an implementation of the Concept Ablation algorithm for machine unlearning in Stable Diffusion models. The Concept Ablation algorithm enables the removal of specific concepts or styles from a pre-trained model without the need for retraining from scratch.","title":"Concept Ablation Algorithm for Machine Unlearning"},{"location":"unlearn/algorithms/concept_ablation/#example-usage-for-quick-canvas-dataset","text":"Add the following code snippet to a python script trainer.py . Run the script using python trainer.py . from mu.algorithms.concept_ablation.algorithm import ( ConceptAblationAlgorithm, ) from mu.algorithms.concept_ablation.configs import ( concept_ablation_train_mu, ConceptAblationConfig, ) if __name__ == \"__main__\": concept_ablation_train_mu.lightning.trainer.max_steps = 5 algorithm = ConceptAblationAlgorithm( concept_ablation_train_mu, config_path=\"mu/algorithms/concept_ablation/configs/train_config.yaml\", ckpt_path=\"machine_unlearning/models/compvis/style50/compvis.ckpt\", prompts=\"mu/algorithms/concept_ablation/data/anchor_prompts/finetune_prompts/sd_prompt_Architectures_sample.txt\", output_dir=\"/opt/dlami/nvme/outputs\", template_name = \"Abstractionism\", #concept to erase dataset_type = \"unlearncanvas\" , use_sample = True, #train on sample dataset # devices=\"1\", ) algorithm.run()","title":"Example usage for quick canvas dataset"},{"location":"unlearn/algorithms/concept_ablation/#example-usage-for-i2p-dataset","text":"Add the following code snippet to a python script trainer.py . Run the script using python trainer.py . from mu.algorithms.concept_ablation.algorithm import ( ConceptAblationAlgorithm, ) from mu.algorithms.concept_ablation.configs import ( concept_ablation_train_i2p, ConceptAblationConfig, ) if __name__ == \"__main__\": concept_ablation_train_i2p.lightning.trainer.max_steps = 5 algorithm = ConceptAblationAlgorithm( concept_ablation_train_i2p, config_path=\"mu/algorithms/concept_ablation/configs/train_config.yaml\", raw_dataset_dir = \"data/i2p-dataset/sample\", ckpt_path=\"models/compvis/style50/compvis.ckpt\", prompts=\"mu/algorithms/concept_ablation/data/anchor_prompts/finetune_prompts/sd_prompt_Architectures_sample.txt\", output_dir=\"/opt/dlami/nvme/outputs\", template_name = \"self-harm\", #concept to erase dataset_type = \"i2p\" , use_sample = True, #train on sample dataset # devices=\"1\", ) algorithm.run() Use your own dataset for unlearning Step-1: Generate your own dataset generate_images_for_prompts --model_path models/diffuser/style50 --csv_path data/prompts/generic_data.csv Note: generate_images_for_prompts: This command invokes the image generation script. It uses a diffusion model to generate images based on textual prompts. --model_path: Specifies the path to the diffusion model to be used for image generation. In this example, the model is located at models/diffuser/style50. --csv_path: Provides the path to a CSV file containing the prompts. Each prompt in this CSV will be used to generate an image, allowing you to build a dataset tailored to your needs. Step-2: Train on your own dataset from mu.algorithms.unified_concept_editing.algorithm import ( UnifiedConceptEditingAlgorithm, ) from mu.algorithms.unified_concept_editing.configs import ( unified_concept_editing_train_mu, ) algorithm = UnifiedConceptEditingAlgorithm( unified_concept_editing_train_mu, ckpt_path=\"models/diffuser/style50/\", raw_dataset_dir=\"data/generic\", #replace with your own generated path prompt_path = \"data/generic/prompts/generic_data.csv\", dataset_type = \"generic\", #to use you own dataset use dataset type as generic template_name = \"self-harm\", output_dir=\"outputs/uce\", ) algorithm.run()","title":"Example usage for i2p dataset"},{"location":"unlearn/algorithms/concept_ablation/#notes","text":"Ensure all dependencies are installed as per the environment file. The training process generates logs in the logs/ directory for easy monitoring. Use appropriate CUDA devices for optimal performance during training. Regularly verify dataset and model configurations to avoid errors during execution.","title":"Notes"},{"location":"unlearn/algorithms/concept_ablation/#directory-structure","text":"algorithm.py : Core implementation of the Concept Ablation Algorithm. configs/ : Configuration files for training and generation. data_handler.py : Data handling and preprocessing. scripts/train.py : Script to train the Concept Ablation Algorithm. callbacks/ : Custom callbacks for logging and monitoring training. utils.py : Utility functions.","title":"Directory Structure"},{"location":"unlearn/algorithms/contributing/","text":"Contributing to Unlearn Diff Thank you for your interest in contributing to Unlearn ! This document outlines the steps and best practices for adding new algorithms, making pull requests, filing issues, and more. Table of Contents Introduction Code of Conduct Project Structure How to Contribute Reporting Issues Suggesting Enhancements Submitting Pull Requests Adding a New Algorithm Folder Structure Creating an Environment Documentation Code Style Contact Introduction Unlearn is an open-source Python package designed for unlearning algorithms in diffusion models. We welcome contributions from the community, whether it\u2019s a new feature, bug fix, or an entirely new unlearning algorithm. Code of Conduct Please note that we expect all contributors to abide by our code of conduct. Be respectful and considerate of others, and help keep our community healthy and inclusive. Project Structure A quick overview of relevant folders: mu/algorithms/ This is where all algorithms reside. Each algorithm has its own subfolder containing: algorithm.py (core logic) data_handler.py (if data-processing is needed) datasets/ (special dataset classes) environment.yaml (conda environment details) evaluator.py (evaluation routines) model.py (model definitions) sampler.py (sampling methods) scripts/ (training, evaluation scripts, etc.) trainer.py (training loop, optimization) utils.py (auxiliary functions) Readme.md (algorithm-specific documentation) mu/core/ Contains abstract/base classes ( base_algorithm.py , base_trainer.py , etc.) that algorithms can extend or override. mu/datasets/ Common dataset classes and utilities used by multiple algorithms. helpers/ Shared utility functions (e.g., logging, path setup, config loading). mu_attack/ & mu_defense/ : These directories contain the implementation of attack strategies and defensive unlearning mechanisms, respectively. Each contains: Attack Modules ( mu_attack/ ) : attackers/ : Different attack implementations (e.g., hard/soft prompts, seed search). configs/ : YAML configuration files for attack routines. tasks/ : Task definitions to evaluate attack efficacy. Defense Modules ( mu_defense/ ) : algorithms/ : Defensive algorithms including adversarial unlearning methods. configs/ : Configurations for training and evaluating defenses. Other Directories : data/ : Contains datasets. docs/ : Project documentation and API references. outputs/ : Generated outputs from algorithms. logs/ : Log files for debugging. models/ : Trained models and checkpoints. tests/ : Test suites to ensure code reliability. stable_diffusion/ & lora_diffusion/ : Diffusion components. How to Contribute Reporting Issues Check the to see if your bug or feature request has already been reported. If not, open a new issue. Provide a clear title and description, including steps to reproduce if it\u2019s a bug. Label your issue appropriately (e.g., \u201cbug\u201d, \u201cenhancement\u201d, \u201cdocumentation\u201d). Suggesting Enhancements If you have an idea for a new feature or improvement, open a GitHub issue labeled \u201cenhancement\u201d or \u201cfeature request.\u201d Include any relevant use-cases, examples, or background information to help the community understand your proposal. Submitting Pull Requests Fork the repository and create your feature branch from main (or the appropriate branch if instructed otherwise): bash git checkout -b feature/my-awesome-feature Make your changes, including tests if applicable. Commit and push your changes: bash git push origin feature/my-awesome-feature Open a Pull Request from your fork into the main repo. Provide a clear description of what your PR does, referencing any related issues. Please ensure your PR follows the Code Style guidelines and includes updated or new tests if needed. Adding a New Algorithm One of the core goals of Unlearn Diff is to make it easy to add and benchmark new unlearning algorithms. Follow these steps when introducing a new method. Folder Structure Create a new subfolder in mu/algorithms/ with a clear, descriptive name (e.g., my_new_algo ). Inside this subfolder, include the following (at minimum): algorithm.py trainer.py scripts/train.py (and optionally scripts/evaluate.py if your training and evaluation are separate) configs/ containing YAML config files for training, evaluation, etc. environment.yaml specifying dependencies for conda Readme.md explaining how to install, run, and configure your new algorithm (Optional) data_handler.py , datasets/ , model.py , sampler.py , utils.py , etc., depending on your algorithm\u2019s design. Extend or import from base classes in mu/core/ if your algorithm logic aligns with any existing abstract classes (e.g., BaseAlgorithm , BaseTrainer , etc.). This ensures code consistency and easier maintenance. Adding a New Attack To add a new attack method, follow these guidelines: Folder Structure : Create a new file or subfolder under mu_attack/attackers/ with a clear, descriptive name (e.g., my_new_attack.py or mu_attack/attackers/my_new_attack/ ). If creating a subfolder, include essential files specific to attack implementation: Attacks : The main implementation file for the attack logic (e.g., attack.py ). Execs : Scripts or modules that execute the attack routines. Tasks : Task definitions for integrating and testing the attack. Any helper modules or configuration files specific to your attack. Update or add corresponding YAML configuration files and config class under mu_attack/configs/ if your attack requires custom settings. Implementation : Extend or import from the base class BaseAttacker (located in mu_attack/core/base_attacker.py ) if applicable. Ensure that your attack method adheres to the input-output standards defined by the project. Documentation & Testing : Add detailed documentation within your new attack module and update the main documentation if needed. Include tests covering your new attack method under the appropriate test directories. Environment : If your attack method has unique dependencies, update the environment.yaml file within the relevant directory or provide instructions in your documentation on how to create a dedicated environment. Also update the common environment file that is located in the project's root directory. Adding a New Defense To integrate a new defense mechanism, please follow these steps: Folder Structure : Create a new subfolder under mu_defense/algorithms/ with a descriptive name (e.g., my_new_defense ). Within this subfolder, include essential files such as: algorithm.py : Contains the core logic of your defense method. trainer.py : Contains training routines and optimization strategies. configs/ : Include configuration class for training and evaluation. environment.yaml : Specify dependencies unique to your defense method. Readme.md : Document usage instructions, configuration details, and any other relevant information. Implementation : Extend or use base classes provided in mu_defense/algorithms/core/ (e.g., base_algorithm.py , base_trainer.py ) to ensure consistency with existing methods. Implement any unique evaluation metrics or procedures if your defense requires them. Documentation & Testing : Document your defense method thoroughly within its Readme.md and update the global documentation if necessary. Provide tests for your defense implementation to ensure its reliability and compatibility with the rest of the system. Environment : If your defense algorithm has specific dependencies, use the provided environment.yaml file as a template and adjust it accordingly. Include clear instructions for users to create and activate the environment. Also update the common environment file that is located in the project's root directory. Creating an Environment The default environment file is located in the project root directory ( environment.yaml ). Contributors should update this file as needed to include any new packages required by their contributions. If your module or algorithm requires unique dependencies, you may add a dedicated environment file in its respective directory, but be sure to update and maintain the default environment in the root. Optionally, to keep dependencies organized, each algorithm has its own environment.yaml . Contributors are encouraged to: Specify all required packages (e.g., torch , torchvision , diffusers , etc.). Name the environment after the algorithm, e.g., mu_<algorithm_name> . Update instructions in your algorithm\u2019s Readme.md on how to install and activate this environment. Documentation Each algorithm folder must contain a Readme.md that documents: High-level description of the algorithm (goals, references to papers, etc.). How to install any special dependencies (if not covered by environment.yaml ). How to run training and evaluation (with example commands). Explanation of the config files in configs/ . Any unique parameters or hyperparameters. If you introduce new metrics or logs, please outline them in your algorithm\u2019s README and update any relevant docs in the main docs/ folder as needed. Code Style We recommend following PEP 8 for Python code. Use descriptive variable names, docstrings, and type hints where possible. Keep functions and methods short and focused\u2014avoid large, monolithic methods. Use Python\u2019s built-in logging or the logging utilities in helpers/logger.py for consistent log outputs. Use wandb global wandb logger if required. Additionally, we prefer: - 4 spaces for indentation (no tabs). - Meaningful commit messages: \u201cFix dataset loader bug for large images\u201d vs. \u201cFix stuff.\u201d Contact If you have any questions, feel free to open an issue or reach out to us via email. We appreciate your contributions and look forward to collaborating with you on Unlearn Diff ! Thank you again for your interest in contributing. We\u2019re excited to see your ideas and improvements!","title":"Contributing to Unlearn Diff"},{"location":"unlearn/algorithms/contributing/#contributing-to-unlearn-diff","text":"Thank you for your interest in contributing to Unlearn ! This document outlines the steps and best practices for adding new algorithms, making pull requests, filing issues, and more.","title":"Contributing to Unlearn Diff"},{"location":"unlearn/algorithms/contributing/#table-of-contents","text":"Introduction Code of Conduct Project Structure How to Contribute Reporting Issues Suggesting Enhancements Submitting Pull Requests Adding a New Algorithm Folder Structure Creating an Environment Documentation Code Style Contact","title":"Table of Contents"},{"location":"unlearn/algorithms/contributing/#introduction","text":"Unlearn is an open-source Python package designed for unlearning algorithms in diffusion models. We welcome contributions from the community, whether it\u2019s a new feature, bug fix, or an entirely new unlearning algorithm.","title":"Introduction"},{"location":"unlearn/algorithms/contributing/#code-of-conduct","text":"Please note that we expect all contributors to abide by our code of conduct. Be respectful and considerate of others, and help keep our community healthy and inclusive.","title":"Code of Conduct"},{"location":"unlearn/algorithms/contributing/#project-structure","text":"A quick overview of relevant folders: mu/algorithms/ This is where all algorithms reside. Each algorithm has its own subfolder containing: algorithm.py (core logic) data_handler.py (if data-processing is needed) datasets/ (special dataset classes) environment.yaml (conda environment details) evaluator.py (evaluation routines) model.py (model definitions) sampler.py (sampling methods) scripts/ (training, evaluation scripts, etc.) trainer.py (training loop, optimization) utils.py (auxiliary functions) Readme.md (algorithm-specific documentation) mu/core/ Contains abstract/base classes ( base_algorithm.py , base_trainer.py , etc.) that algorithms can extend or override. mu/datasets/ Common dataset classes and utilities used by multiple algorithms. helpers/ Shared utility functions (e.g., logging, path setup, config loading). mu_attack/ & mu_defense/ : These directories contain the implementation of attack strategies and defensive unlearning mechanisms, respectively. Each contains: Attack Modules ( mu_attack/ ) : attackers/ : Different attack implementations (e.g., hard/soft prompts, seed search). configs/ : YAML configuration files for attack routines. tasks/ : Task definitions to evaluate attack efficacy. Defense Modules ( mu_defense/ ) : algorithms/ : Defensive algorithms including adversarial unlearning methods. configs/ : Configurations for training and evaluating defenses. Other Directories : data/ : Contains datasets. docs/ : Project documentation and API references. outputs/ : Generated outputs from algorithms. logs/ : Log files for debugging. models/ : Trained models and checkpoints. tests/ : Test suites to ensure code reliability. stable_diffusion/ & lora_diffusion/ : Diffusion components.","title":"Project Structure"},{"location":"unlearn/algorithms/contributing/#how-to-contribute","text":"","title":"How to Contribute"},{"location":"unlearn/algorithms/contributing/#reporting-issues","text":"Check the to see if your bug or feature request has already been reported. If not, open a new issue. Provide a clear title and description, including steps to reproduce if it\u2019s a bug. Label your issue appropriately (e.g., \u201cbug\u201d, \u201cenhancement\u201d, \u201cdocumentation\u201d).","title":"Reporting Issues"},{"location":"unlearn/algorithms/contributing/#suggesting-enhancements","text":"If you have an idea for a new feature or improvement, open a GitHub issue labeled \u201cenhancement\u201d or \u201cfeature request.\u201d Include any relevant use-cases, examples, or background information to help the community understand your proposal.","title":"Suggesting Enhancements"},{"location":"unlearn/algorithms/contributing/#submitting-pull-requests","text":"Fork the repository and create your feature branch from main (or the appropriate branch if instructed otherwise): bash git checkout -b feature/my-awesome-feature Make your changes, including tests if applicable. Commit and push your changes: bash git push origin feature/my-awesome-feature Open a Pull Request from your fork into the main repo. Provide a clear description of what your PR does, referencing any related issues. Please ensure your PR follows the Code Style guidelines and includes updated or new tests if needed.","title":"Submitting Pull Requests"},{"location":"unlearn/algorithms/contributing/#adding-a-new-algorithm","text":"One of the core goals of Unlearn Diff is to make it easy to add and benchmark new unlearning algorithms. Follow these steps when introducing a new method.","title":"Adding a New Algorithm"},{"location":"unlearn/algorithms/contributing/#folder-structure","text":"Create a new subfolder in mu/algorithms/ with a clear, descriptive name (e.g., my_new_algo ). Inside this subfolder, include the following (at minimum): algorithm.py trainer.py scripts/train.py (and optionally scripts/evaluate.py if your training and evaluation are separate) configs/ containing YAML config files for training, evaluation, etc. environment.yaml specifying dependencies for conda Readme.md explaining how to install, run, and configure your new algorithm (Optional) data_handler.py , datasets/ , model.py , sampler.py , utils.py , etc., depending on your algorithm\u2019s design. Extend or import from base classes in mu/core/ if your algorithm logic aligns with any existing abstract classes (e.g., BaseAlgorithm , BaseTrainer , etc.). This ensures code consistency and easier maintenance.","title":"Folder Structure"},{"location":"unlearn/algorithms/contributing/#adding-a-new-attack","text":"To add a new attack method, follow these guidelines: Folder Structure : Create a new file or subfolder under mu_attack/attackers/ with a clear, descriptive name (e.g., my_new_attack.py or mu_attack/attackers/my_new_attack/ ). If creating a subfolder, include essential files specific to attack implementation: Attacks : The main implementation file for the attack logic (e.g., attack.py ). Execs : Scripts or modules that execute the attack routines. Tasks : Task definitions for integrating and testing the attack. Any helper modules or configuration files specific to your attack. Update or add corresponding YAML configuration files and config class under mu_attack/configs/ if your attack requires custom settings. Implementation : Extend or import from the base class BaseAttacker (located in mu_attack/core/base_attacker.py ) if applicable. Ensure that your attack method adheres to the input-output standards defined by the project. Documentation & Testing : Add detailed documentation within your new attack module and update the main documentation if needed. Include tests covering your new attack method under the appropriate test directories. Environment : If your attack method has unique dependencies, update the environment.yaml file within the relevant directory or provide instructions in your documentation on how to create a dedicated environment. Also update the common environment file that is located in the project's root directory.","title":"Adding a New Attack"},{"location":"unlearn/algorithms/contributing/#adding-a-new-defense","text":"To integrate a new defense mechanism, please follow these steps: Folder Structure : Create a new subfolder under mu_defense/algorithms/ with a descriptive name (e.g., my_new_defense ). Within this subfolder, include essential files such as: algorithm.py : Contains the core logic of your defense method. trainer.py : Contains training routines and optimization strategies. configs/ : Include configuration class for training and evaluation. environment.yaml : Specify dependencies unique to your defense method. Readme.md : Document usage instructions, configuration details, and any other relevant information. Implementation : Extend or use base classes provided in mu_defense/algorithms/core/ (e.g., base_algorithm.py , base_trainer.py ) to ensure consistency with existing methods. Implement any unique evaluation metrics or procedures if your defense requires them. Documentation & Testing : Document your defense method thoroughly within its Readme.md and update the global documentation if necessary. Provide tests for your defense implementation to ensure its reliability and compatibility with the rest of the system. Environment : If your defense algorithm has specific dependencies, use the provided environment.yaml file as a template and adjust it accordingly. Include clear instructions for users to create and activate the environment. Also update the common environment file that is located in the project's root directory.","title":"Adding a New Defense"},{"location":"unlearn/algorithms/contributing/#creating-an-environment","text":"The default environment file is located in the project root directory ( environment.yaml ). Contributors should update this file as needed to include any new packages required by their contributions. If your module or algorithm requires unique dependencies, you may add a dedicated environment file in its respective directory, but be sure to update and maintain the default environment in the root. Optionally, to keep dependencies organized, each algorithm has its own environment.yaml . Contributors are encouraged to: Specify all required packages (e.g., torch , torchvision , diffusers , etc.). Name the environment after the algorithm, e.g., mu_<algorithm_name> . Update instructions in your algorithm\u2019s Readme.md on how to install and activate this environment.","title":"Creating an Environment"},{"location":"unlearn/algorithms/contributing/#documentation","text":"Each algorithm folder must contain a Readme.md that documents: High-level description of the algorithm (goals, references to papers, etc.). How to install any special dependencies (if not covered by environment.yaml ). How to run training and evaluation (with example commands). Explanation of the config files in configs/ . Any unique parameters or hyperparameters. If you introduce new metrics or logs, please outline them in your algorithm\u2019s README and update any relevant docs in the main docs/ folder as needed.","title":"Documentation"},{"location":"unlearn/algorithms/contributing/#code-style","text":"We recommend following PEP 8 for Python code. Use descriptive variable names, docstrings, and type hints where possible. Keep functions and methods short and focused\u2014avoid large, monolithic methods. Use Python\u2019s built-in logging or the logging utilities in helpers/logger.py for consistent log outputs. Use wandb global wandb logger if required. Additionally, we prefer: - 4 spaces for indentation (no tabs). - Meaningful commit messages: \u201cFix dataset loader bug for large images\u201d vs. \u201cFix stuff.\u201d","title":"Code Style"},{"location":"unlearn/algorithms/contributing/#contact","text":"If you have any questions, feel free to open an issue or reach out to us via email. We appreciate your contributions and look forward to collaborating with you on Unlearn Diff ! Thank you again for your interest in contributing. We\u2019re excited to see your ideas and improvements!","title":"Contact"},{"location":"unlearn/algorithms/erase_diff/","text":"EraseDiff Algorithm for Machine Unlearning This repository provides an implementation of the erase diff algorithm for machine unlearning in Stable Diffusion models. The erasediff algorithm allows you to remove specific concepts or styles from a pre-trained model without retraining it from scratch. Run Train using quick canvas dataset Create a file, eg, my_trainer.py and use examples and modify your configs to run the file. Example Code from mu.algorithms.erase_diff.algorithm import EraseDiffAlgorithm from mu.algorithms.erase_diff.configs import ( erase_diff_train_mu, ) algorithm = EraseDiffAlgorithm( erase_diff_train_mu, ckpt_path=\"UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\", raw_dataset_dir=( \"data/quick-canvas-dataset/sample\" ), template_name = \"Abstractionism\", #concept to erase dataset_type = \"unlearncanvas\" , use_sample = True, #train on sample dataset output_dir = \"outputs/erase_diff/finetuned_models\" #output dir to save finetuned models ) algorithm.run() Run Train using i2p dataset Create a file, eg, my_trainer.py and use examples and modify your configs to run the file. Example Code from mu.algorithms.erase_diff.algorithm import EraseDiffAlgorithm from mu.algorithms.erase_diff.configs import ( erase_diff_train_i2p, ) algorithm = EraseDiffAlgorithm( erase_diff_train_i2p, ckpt_path=\"UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\", raw_dataset_dir = \"data/i2p-dataset/sample\", template_name = \"self-harm\", #concept to erase dataset_type = \"i2p\" , use_sample = True, #train on sample dataset output_dir = \"outputs/erase_diff/finetuned_models\" #output dir to save finetuned models ) algorithm.run() Use your own dataset for unlearning Step-1: Generate your own dataset generate_images_for_prompts --model_path models/diffuser/style50 --csv_path data/prompts/generic_data.csv Note: generate_images_for_prompts: This command invokes the image generation script. It uses a diffusion model to generate images based on textual prompts. --model_path: Specifies the path to the diffusion model to be used for image generation. In this example, the model is located at models/diffuser/style50. --csv_path: Provides the path to a CSV file containing the prompts. Each prompt in this CSV will be used to generate an image, allowing you to build a dataset tailored to your needs. Step-2: Train on your own dataset from mu.algorithms.erase_diff.algorithm import EraseDiffAlgorithm from mu.algorithms.erase_diff.configs import erase_diff_train_mu algorithm = EraseDiffAlgorithm( erase_diff_train_mu, ckpt_path=\"models/compvis/style50/compvis.ckpt\", raw_dataset_dir=\"data/generic\", #replace with your own generated path train_method=\"noxattn\", dataset_type=\"generic\", template_name = \"self-harm\" ) algorithm.run() Running the Training Script in Offline Mode WANDB_MODE=offline python my_trainer.py How It Works * Default Values: The script first loads default values from the train config file as in configs section. Parameter Overrides: Any parameters passed directly to the algorithm, overrides these configs. Final Configuration: The script merges the configs and convert them into dictionary to proceed with the training. Directory Structure algorithm.py : Implementation of the EraseDiffAlgorithm class. configs/ : Contains configuration files for training and generation. model.py : Implementation of the EraseDiffModel class. scripts/train.py : Script to train the EraseDiff algorithm. trainer.py : Implementation of the EraseDiffTrainer class. utils.py : Utility functions used in the project. data_handler.py : Implementation of DataHandler class","title":"Usage"},{"location":"unlearn/algorithms/erase_diff/#erasediff-algorithm-for-machine-unlearning","text":"This repository provides an implementation of the erase diff algorithm for machine unlearning in Stable Diffusion models. The erasediff algorithm allows you to remove specific concepts or styles from a pre-trained model without retraining it from scratch.","title":"EraseDiff Algorithm for Machine Unlearning"},{"location":"unlearn/algorithms/erase_diff/#run-train-using-quick-canvas-dataset","text":"Create a file, eg, my_trainer.py and use examples and modify your configs to run the file. Example Code from mu.algorithms.erase_diff.algorithm import EraseDiffAlgorithm from mu.algorithms.erase_diff.configs import ( erase_diff_train_mu, ) algorithm = EraseDiffAlgorithm( erase_diff_train_mu, ckpt_path=\"UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\", raw_dataset_dir=( \"data/quick-canvas-dataset/sample\" ), template_name = \"Abstractionism\", #concept to erase dataset_type = \"unlearncanvas\" , use_sample = True, #train on sample dataset output_dir = \"outputs/erase_diff/finetuned_models\" #output dir to save finetuned models ) algorithm.run()","title":"Run Train using quick canvas dataset"},{"location":"unlearn/algorithms/erase_diff/#run-train-using-i2p-dataset","text":"Create a file, eg, my_trainer.py and use examples and modify your configs to run the file. Example Code from mu.algorithms.erase_diff.algorithm import EraseDiffAlgorithm from mu.algorithms.erase_diff.configs import ( erase_diff_train_i2p, ) algorithm = EraseDiffAlgorithm( erase_diff_train_i2p, ckpt_path=\"UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\", raw_dataset_dir = \"data/i2p-dataset/sample\", template_name = \"self-harm\", #concept to erase dataset_type = \"i2p\" , use_sample = True, #train on sample dataset output_dir = \"outputs/erase_diff/finetuned_models\" #output dir to save finetuned models ) algorithm.run() Use your own dataset for unlearning Step-1: Generate your own dataset generate_images_for_prompts --model_path models/diffuser/style50 --csv_path data/prompts/generic_data.csv Note: generate_images_for_prompts: This command invokes the image generation script. It uses a diffusion model to generate images based on textual prompts. --model_path: Specifies the path to the diffusion model to be used for image generation. In this example, the model is located at models/diffuser/style50. --csv_path: Provides the path to a CSV file containing the prompts. Each prompt in this CSV will be used to generate an image, allowing you to build a dataset tailored to your needs. Step-2: Train on your own dataset from mu.algorithms.erase_diff.algorithm import EraseDiffAlgorithm from mu.algorithms.erase_diff.configs import erase_diff_train_mu algorithm = EraseDiffAlgorithm( erase_diff_train_mu, ckpt_path=\"models/compvis/style50/compvis.ckpt\", raw_dataset_dir=\"data/generic\", #replace with your own generated path train_method=\"noxattn\", dataset_type=\"generic\", template_name = \"self-harm\" ) algorithm.run() Running the Training Script in Offline Mode WANDB_MODE=offline python my_trainer.py How It Works * Default Values: The script first loads default values from the train config file as in configs section. Parameter Overrides: Any parameters passed directly to the algorithm, overrides these configs. Final Configuration: The script merges the configs and convert them into dictionary to proceed with the training.","title":"Run Train using i2p dataset"},{"location":"unlearn/algorithms/erase_diff/#directory-structure","text":"algorithm.py : Implementation of the EraseDiffAlgorithm class. configs/ : Contains configuration files for training and generation. model.py : Implementation of the EraseDiffModel class. scripts/train.py : Script to train the EraseDiff algorithm. trainer.py : Implementation of the EraseDiffTrainer class. utils.py : Utility functions used in the project. data_handler.py : Implementation of DataHandler class","title":"Directory Structure"},{"location":"unlearn/algorithms/esd/","text":"ESD Algorithm for Machine Unlearning This repository provides an implementation of the ESD algorithm for machine unlearning in Stable Diffusion models. The ESD algorithm allows you to remove specific concepts or styles from a pre-trained model without retraining it from scratch. Run Train using quick canvas dataset Create a file, eg, my_trainer.py and use examples and modify your configs to run the file. Example Code from mu.algorithms.esd.algorithm import ESDAlgorithm from mu.algorithms.esd.configs import ( esd_train_mu, ) algorithm = ESDAlgorithm( esd_train_mu, ckpt_path=\"machine_unlearning/models/compvis/style50/compvis.ckpt\", raw_dataset_dir=( \"data/quick-canvas-dataset/sample\" ), template_name = \"Abstractionism\", #concept to erase dataset_type = \"unlearncanvas\" , use_sample = True, #train on sample dataset output_dir = \"outputs/esd/finetuned_models\" #output dir to save finetuned models ) algorithm.run() Run Train using i2p dataset Create a file, eg, my_trainer.py and use examples and modify your configs to run the file. Example Code from mu.algorithms.esd.algorithm import ESDAlgorithm from mu.algorithms.esd.configs import ( esd_train_i2p, ) algorithm = ESDAlgorithm( esd_train_i2p, ckpt_path=\"models/compvis/style50/compvis.ckpt\", raw_dataset_dir = \"data/i2p-dataset/sample\", template_name = \"self-harm\", #concept to erase dataset_type = \"i2p\" , use_sample = True, #train on sample dataset output_dir = \"outputs/esd/finetuned_models\" #output dir to save finetuned models ) algorithm.run() Run on your own dataset Step-1: Generate your own dataset generate_images_for_prompts --model_path models/diffuser/style50 --csv_path data/prompts/generic_data.csv Note: generate_images_for_prompts: This command invokes the image generation script. It uses a diffusion model to generate images based on textual prompts. --model_path: Specifies the path to the diffusion model to be used for image generation. In this example, the model is located at models/diffuser/style50. --csv_path: Provides the path to a CSV file containing the prompts. Each prompt in this CSV will be used to generate an image, allowing you to build a dataset tailored to your needs. Step-2: Train on your own dataset from mu.algorithms.esd.algorithm import ESDAlgorithm from mu.algorithms.esd.configs import esd_train_mu algorithm = ESDAlgorithm( esd_train_mu, ckpt_path=\"models/compvis/style50/compvis.ckpt\", raw_dataset_dir = \"data/generic_data\", dataset_type = \"generic\", train_method=\"noxattn\", devices = \"0\" ) algorithm.run() Running the Training Script in Offline Mode WANDB_MODE=offline python my_trainer.py How It Works * Default Values: The script first loads default values from the train config file as in configs section. Parameter Overrides: Any parameters passed directly to the algorithm, overrides these configs. Final Configuration: The script merges the configs and convert them into dictionary to proceed with the training. Directory Structure algorithm.py : Implementation of the ESDAlgorithm class. configs/ : Contains configuration files for training and generation. constants/const.py : Constants used throughout the project. model.py : Implementation of the ESDModel class. scripts/train.py : Script to train the ESD algorithm. trainer.py : Implementation of the ESDTrainer class. utils.py : Utility functions used in the project.","title":"Usage"},{"location":"unlearn/algorithms/esd/#esd-algorithm-for-machine-unlearning","text":"This repository provides an implementation of the ESD algorithm for machine unlearning in Stable Diffusion models. The ESD algorithm allows you to remove specific concepts or styles from a pre-trained model without retraining it from scratch.","title":"ESD Algorithm for Machine Unlearning"},{"location":"unlearn/algorithms/esd/#run-train-using-quick-canvas-dataset","text":"Create a file, eg, my_trainer.py and use examples and modify your configs to run the file. Example Code from mu.algorithms.esd.algorithm import ESDAlgorithm from mu.algorithms.esd.configs import ( esd_train_mu, ) algorithm = ESDAlgorithm( esd_train_mu, ckpt_path=\"machine_unlearning/models/compvis/style50/compvis.ckpt\", raw_dataset_dir=( \"data/quick-canvas-dataset/sample\" ), template_name = \"Abstractionism\", #concept to erase dataset_type = \"unlearncanvas\" , use_sample = True, #train on sample dataset output_dir = \"outputs/esd/finetuned_models\" #output dir to save finetuned models ) algorithm.run()","title":"Run Train using quick canvas dataset"},{"location":"unlearn/algorithms/esd/#run-train-using-i2p-dataset","text":"Create a file, eg, my_trainer.py and use examples and modify your configs to run the file. Example Code from mu.algorithms.esd.algorithm import ESDAlgorithm from mu.algorithms.esd.configs import ( esd_train_i2p, ) algorithm = ESDAlgorithm( esd_train_i2p, ckpt_path=\"models/compvis/style50/compvis.ckpt\", raw_dataset_dir = \"data/i2p-dataset/sample\", template_name = \"self-harm\", #concept to erase dataset_type = \"i2p\" , use_sample = True, #train on sample dataset output_dir = \"outputs/esd/finetuned_models\" #output dir to save finetuned models ) algorithm.run() Run on your own dataset Step-1: Generate your own dataset generate_images_for_prompts --model_path models/diffuser/style50 --csv_path data/prompts/generic_data.csv Note: generate_images_for_prompts: This command invokes the image generation script. It uses a diffusion model to generate images based on textual prompts. --model_path: Specifies the path to the diffusion model to be used for image generation. In this example, the model is located at models/diffuser/style50. --csv_path: Provides the path to a CSV file containing the prompts. Each prompt in this CSV will be used to generate an image, allowing you to build a dataset tailored to your needs. Step-2: Train on your own dataset from mu.algorithms.esd.algorithm import ESDAlgorithm from mu.algorithms.esd.configs import esd_train_mu algorithm = ESDAlgorithm( esd_train_mu, ckpt_path=\"models/compvis/style50/compvis.ckpt\", raw_dataset_dir = \"data/generic_data\", dataset_type = \"generic\", train_method=\"noxattn\", devices = \"0\" ) algorithm.run() Running the Training Script in Offline Mode WANDB_MODE=offline python my_trainer.py How It Works * Default Values: The script first loads default values from the train config file as in configs section. Parameter Overrides: Any parameters passed directly to the algorithm, overrides these configs. Final Configuration: The script merges the configs and convert them into dictionary to proceed with the training.","title":"Run Train using i2p dataset"},{"location":"unlearn/algorithms/esd/#directory-structure","text":"algorithm.py : Implementation of the ESDAlgorithm class. configs/ : Contains configuration files for training and generation. constants/const.py : Constants used throughout the project. model.py : Implementation of the ESDModel class. scripts/train.py : Script to train the ESD algorithm. trainer.py : Implementation of the ESDTrainer class. utils.py : Utility functions used in the project.","title":"Directory Structure"},{"location":"unlearn/algorithms/forget_me_not/","text":"Forget Me Not Algorithm for Machine Unlearning This repository provides an implementation of the erase diff algorithm for machine unlearning in Stable Diffusion models. The Forget Me Not algorithm allows you to remove specific concepts or styles from a pre-trained model without retraining it from scratch. Installation Prerequisities Ensure conda is installed on your system. You can install Miniconda or Anaconda: Miniconda (recommended): https://docs.conda.io/en/latest/miniconda.html Anaconda : https://www.anaconda.com/products/distribution After installing conda , ensure it is available in your PATH by running. You may require to restart the terminal session: conda --version Step-by-Step Setup: Step 1. Create a Conda Environment Create a new Conda environment named myenv with Python 3.8.5: conda create -n myenv python=3.8.5 Step 2. Activate the Environment Activate the environment to work within it: conda activate myenv Step 3. Install Core Dependencies Install PyTorch, torchvision, CUDA Toolkit, and ONNX Runtime with specific versions: conda install pytorch==1.11.0 torchvision==0.12.0 cudatoolkit=11.3 onnxruntime==1.16.3 -c pytorch -c conda-forge Step 4. Install our unlearn_diff Package using pip: pip install unlearn_diff Step 5. Install Additional Git Dependencies: After installing unlearn_diff, install the following Git-based dependencies in the same Conda environment to ensure full functionality: pip install git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers pip install git+https://github.com/openai/CLIP.git@main#egg=clip pip install git+https://github.com/crowsonkb/k-diffusion.git pip install git+https://github.com/cocodataset/panopticapi.git pip install git+https://github.com/Phoveran/fastargs.git@main#egg=fastargs pip install git+https://github.com/boomb0om/text2image-benchmark Downloading data and models. After you install the package, you can use the following commands to download. Dataset : i2p : Sample : download_data sample i2p Full : download_data full i2p quick_canvas : Sample : download_data sample quick_canvas Full : download_data full quick_canvas Model : compvis : download_model compvis diffuser : download_model diffuser Download best.onnx file download_best_onnx Verify the Downloaded Files After downloading, verify that the datasets have been correctly extracted: ls -lh ./data/i2p-dataset/sample/ ls -lh ./data/quick-canvas-dataset/sample/ Run Train Create a file, eg, my_trainer.py and use examples and modify your configs to run the file. Train a Text Inversion using quick canvas dataset Before finetuning the model you need to generate safetensors. from mu.algorithms.forget_me_not.algorithm import ForgetMeNotAlgorithm from mu.algorithms.forget_me_not.configs import ( forget_me_not_train_ti_mu, ) algorithm = ForgetMeNotAlgorithm( forget_me_not_train_ti_mu, ckpt_path=\"models/diffuser/style50\", raw_dataset_dir=( \"data/quick-canvas-dataset/sample\" ), steps=10, template_name = \"Abstractionism\", #concept to erase dataset_type = \"unlearncanvas\" , use_sample = True, #train on sample dataset output_dir = \"outputs/forget_me_not/finetuned_models\" #output dir to save finetuned models ) algorithm.run(train_type=\"train_ti\") Running the Script in Offline Mode WANDB_MODE=offline python my_trainer_ti.py Perform Unlearning using quick canvas dataset Before running the train_attn script, update the ti_weights_path parameter in the configuration file to point to the output generated from the Text Inversion (train_ti.py) stage from mu.algorithms.forget_me_not.algorithm import ForgetMeNotAlgorithm from mu.algorithms.forget_me_not.configs import ( forget_me_not_train_attn_mu, ) algorithm = ForgetMeNotAlgorithm( forget_me_not_train_attn_mu, ckpt_path=\"models/diffuser/style50\", raw_dataset_dir=( \"data/quick-canvas-dataset/sample\" ), steps=10, ti_weights_path=\"outputs/forget_me_not/finetuned_models/Abstractionism/step_inv_10.safetensors\", template_name = \"Abstractionism\", #concept to erase dataset_type = \"unlearncanvas\" , use_sample = True, #train on sample dataset output_dir = \"outputs/forget_me_not/finetuned_models\" #output dir to save finetuned models ) algorithm.run(train_type=\"train_attn\") Train a Text Inversion using i2p dataset Before finetuning the model you need to generate safetensors. from mu.algorithms.forget_me_not.algorithm import ForgetMeNotAlgorithm from mu.algorithms.forget_me_not.configs import ( forget_me_not_train_ti_i2p, ) algorithm = ForgetMeNotAlgorithm( forget_me_not_train_ti_i2p, ckpt_path=\"models/diffuser/style50\", raw_dataset_dir = \"data/i2p-dataset/sample\", steps=10, template_name = \"self-harm\", #concept to erase dataset_type = \"i2p\" , use_sample = True, #train on sample dataset output_dir = \"outputs/forget_me_not/finetuned_models\" #output dir to save finetuned models ) algorithm.run(train_type=\"train_ti\") Running the Script in Offline Mode WANDB_MODE=offline python my_trainer_ti.py Perform Unlearning Before running the train_attn script, update the ti_weights_path parameter in the configuration file to point to the output generated from the Text Inversion (train_ti.py) stage from mu.algorithms.forget_me_not.algorithm import ForgetMeNotAlgorithm from mu.algorithms.forget_me_not.configs import ( forget_me_not_train_attn_mu, ) algorithm = ForgetMeNotAlgorithm( forget_me_not_train_attn_mu, ckpt_path=\"models/diffuser/style50\", raw_dataset_dir=( \"data/quick-canvas-dataset/sample\" ), steps=10, ti_weights_path=\"outputs/forget_me_not/finetuned_models/Abstractionism/step_inv_10.safetensors\", use_sample = True, #train on sample dataset output_dir = \"outputs/forget_me_not/finetuned_models\" ,#output dir to save finetuned models template_name = \"self-harm\", #concept to erase dataset_type = \"i2p\" , ) algorithm.run(train_type=\"train_attn\") Run on your own dataset Step-1: Generate your own dataset generate_images_for_prompts --model_path models/diffuser/style50 --csv_path data/prompts/generic_data.csv Note: generate_images_for_prompts: This command invokes the image generation script. It uses a diffusion model to generate images based on textual prompts. --model_path: Specifies the path to the diffusion model to be used for image generation. In this example, the model is located at models/diffuser/style50. --csv_path: Provides the path to a CSV file containing the prompts. Each prompt in this CSV will be used to generate an image, allowing you to build a dataset tailored to your needs. Step-2: Train text inversion on your own dataset from mu.algorithms.forget_me_not.algorithm import ForgetMeNotAlgorithm from mu.algorithms.forget_me_not.configs import ( forget_me_not_train_ti_mu, ) algorithm = ForgetMeNotAlgorithm( forget_me_not_train_ti_mu, ckpt_path=\"models/diffuser/style50\", raw_dataset_dir=( \"data/generic\" #replace with your own generated path ), steps=10, template_name = \"self-harm\", dataset_type = \"generic\" ) algorithm.run(train_type=\"train_ti\") Perform Unlearning on your own dataset from mu.algorithms.forget_me_not.algorithm import ForgetMeNotAlgorithm from mu.algorithms.forget_me_not.configs import ( forget_me_not_train_attn_mu, ) algorithm = ForgetMeNotAlgorithm( forget_me_not_train_attn_mu, ckpt_path=\"models/diffuser/style50\", raw_dataset_dir=( \"data/generic\" #replace with your own generated path ), steps=10, ti_weights_path=\"outputs/forget_me_not/ti_models/step_inv_10.safetensors\", template_name = \"self-harm\", dataset_type = \"generic\" ) algorithm.run(train_type=\"train_attn\") Running the Script in Offline Mode WANDB_MODE=offline python my_trainer_attn.py How It Works * Default Values: The script first loads default values from the train config file as in configs section. Parameter Overrides: Any parameters passed directly to the algorithm, overrides these configs. Final Configuration: The script merges the configs and convert them into dictionary to proceed with the training. Directory Structure algorithm.py : Implementation of the Forget Me NotAlgorithm class. configs/ : Contains configuration files for training and generation. model.py : Implementation of the Forget Me NotModel class. scripts/train.py : Script to train the Forget Me Not algorithm. trainer.py : Implementation of the Forget Me NotTrainer class. utils.py : Utility functions used in the project. data_handler.py : Implementation of DataHandler class This method involves two stages: Train a Text Inversion : The first stage involves training a Text Inversion. Refer to the script train_ti.py for details and implementation. It uses train_ti_config.yaml as config file. Perform Unlearning : The second stage uses the outputs from the first stage to perform unlearning. Refer to the script train_attn.py for details and implementation. It uses train_attn_config.yaml as config file.","title":"Usage"},{"location":"unlearn/algorithms/forget_me_not/#forget-me-not-algorithm-for-machine-unlearning","text":"This repository provides an implementation of the erase diff algorithm for machine unlearning in Stable Diffusion models. The Forget Me Not algorithm allows you to remove specific concepts or styles from a pre-trained model without retraining it from scratch.","title":"Forget Me Not Algorithm for Machine Unlearning"},{"location":"unlearn/algorithms/forget_me_not/#installation","text":"","title":"Installation"},{"location":"unlearn/algorithms/forget_me_not/#prerequisities","text":"Ensure conda is installed on your system. You can install Miniconda or Anaconda: Miniconda (recommended): https://docs.conda.io/en/latest/miniconda.html Anaconda : https://www.anaconda.com/products/distribution After installing conda , ensure it is available in your PATH by running. You may require to restart the terminal session: conda --version Step-by-Step Setup: Step 1. Create a Conda Environment Create a new Conda environment named myenv with Python 3.8.5: conda create -n myenv python=3.8.5 Step 2. Activate the Environment Activate the environment to work within it: conda activate myenv Step 3. Install Core Dependencies Install PyTorch, torchvision, CUDA Toolkit, and ONNX Runtime with specific versions: conda install pytorch==1.11.0 torchvision==0.12.0 cudatoolkit=11.3 onnxruntime==1.16.3 -c pytorch -c conda-forge Step 4. Install our unlearn_diff Package using pip: pip install unlearn_diff Step 5. Install Additional Git Dependencies: After installing unlearn_diff, install the following Git-based dependencies in the same Conda environment to ensure full functionality: pip install git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers pip install git+https://github.com/openai/CLIP.git@main#egg=clip pip install git+https://github.com/crowsonkb/k-diffusion.git pip install git+https://github.com/cocodataset/panopticapi.git pip install git+https://github.com/Phoveran/fastargs.git@main#egg=fastargs pip install git+https://github.com/boomb0om/text2image-benchmark","title":"Prerequisities"},{"location":"unlearn/algorithms/forget_me_not/#downloading-data-and-models","text":"After you install the package, you can use the following commands to download. Dataset : i2p : Sample : download_data sample i2p Full : download_data full i2p quick_canvas : Sample : download_data sample quick_canvas Full : download_data full quick_canvas Model : compvis : download_model compvis diffuser : download_model diffuser Download best.onnx file download_best_onnx Verify the Downloaded Files After downloading, verify that the datasets have been correctly extracted: ls -lh ./data/i2p-dataset/sample/ ls -lh ./data/quick-canvas-dataset/sample/","title":"Downloading data and models."},{"location":"unlearn/algorithms/forget_me_not/#run-train","text":"Create a file, eg, my_trainer.py and use examples and modify your configs to run the file. Train a Text Inversion using quick canvas dataset Before finetuning the model you need to generate safetensors. from mu.algorithms.forget_me_not.algorithm import ForgetMeNotAlgorithm from mu.algorithms.forget_me_not.configs import ( forget_me_not_train_ti_mu, ) algorithm = ForgetMeNotAlgorithm( forget_me_not_train_ti_mu, ckpt_path=\"models/diffuser/style50\", raw_dataset_dir=( \"data/quick-canvas-dataset/sample\" ), steps=10, template_name = \"Abstractionism\", #concept to erase dataset_type = \"unlearncanvas\" , use_sample = True, #train on sample dataset output_dir = \"outputs/forget_me_not/finetuned_models\" #output dir to save finetuned models ) algorithm.run(train_type=\"train_ti\") Running the Script in Offline Mode WANDB_MODE=offline python my_trainer_ti.py Perform Unlearning using quick canvas dataset Before running the train_attn script, update the ti_weights_path parameter in the configuration file to point to the output generated from the Text Inversion (train_ti.py) stage from mu.algorithms.forget_me_not.algorithm import ForgetMeNotAlgorithm from mu.algorithms.forget_me_not.configs import ( forget_me_not_train_attn_mu, ) algorithm = ForgetMeNotAlgorithm( forget_me_not_train_attn_mu, ckpt_path=\"models/diffuser/style50\", raw_dataset_dir=( \"data/quick-canvas-dataset/sample\" ), steps=10, ti_weights_path=\"outputs/forget_me_not/finetuned_models/Abstractionism/step_inv_10.safetensors\", template_name = \"Abstractionism\", #concept to erase dataset_type = \"unlearncanvas\" , use_sample = True, #train on sample dataset output_dir = \"outputs/forget_me_not/finetuned_models\" #output dir to save finetuned models ) algorithm.run(train_type=\"train_attn\") Train a Text Inversion using i2p dataset Before finetuning the model you need to generate safetensors. from mu.algorithms.forget_me_not.algorithm import ForgetMeNotAlgorithm from mu.algorithms.forget_me_not.configs import ( forget_me_not_train_ti_i2p, ) algorithm = ForgetMeNotAlgorithm( forget_me_not_train_ti_i2p, ckpt_path=\"models/diffuser/style50\", raw_dataset_dir = \"data/i2p-dataset/sample\", steps=10, template_name = \"self-harm\", #concept to erase dataset_type = \"i2p\" , use_sample = True, #train on sample dataset output_dir = \"outputs/forget_me_not/finetuned_models\" #output dir to save finetuned models ) algorithm.run(train_type=\"train_ti\") Running the Script in Offline Mode WANDB_MODE=offline python my_trainer_ti.py Perform Unlearning Before running the train_attn script, update the ti_weights_path parameter in the configuration file to point to the output generated from the Text Inversion (train_ti.py) stage from mu.algorithms.forget_me_not.algorithm import ForgetMeNotAlgorithm from mu.algorithms.forget_me_not.configs import ( forget_me_not_train_attn_mu, ) algorithm = ForgetMeNotAlgorithm( forget_me_not_train_attn_mu, ckpt_path=\"models/diffuser/style50\", raw_dataset_dir=( \"data/quick-canvas-dataset/sample\" ), steps=10, ti_weights_path=\"outputs/forget_me_not/finetuned_models/Abstractionism/step_inv_10.safetensors\", use_sample = True, #train on sample dataset output_dir = \"outputs/forget_me_not/finetuned_models\" ,#output dir to save finetuned models template_name = \"self-harm\", #concept to erase dataset_type = \"i2p\" , ) algorithm.run(train_type=\"train_attn\") Run on your own dataset Step-1: Generate your own dataset generate_images_for_prompts --model_path models/diffuser/style50 --csv_path data/prompts/generic_data.csv Note: generate_images_for_prompts: This command invokes the image generation script. It uses a diffusion model to generate images based on textual prompts. --model_path: Specifies the path to the diffusion model to be used for image generation. In this example, the model is located at models/diffuser/style50. --csv_path: Provides the path to a CSV file containing the prompts. Each prompt in this CSV will be used to generate an image, allowing you to build a dataset tailored to your needs. Step-2: Train text inversion on your own dataset from mu.algorithms.forget_me_not.algorithm import ForgetMeNotAlgorithm from mu.algorithms.forget_me_not.configs import ( forget_me_not_train_ti_mu, ) algorithm = ForgetMeNotAlgorithm( forget_me_not_train_ti_mu, ckpt_path=\"models/diffuser/style50\", raw_dataset_dir=( \"data/generic\" #replace with your own generated path ), steps=10, template_name = \"self-harm\", dataset_type = \"generic\" ) algorithm.run(train_type=\"train_ti\") Perform Unlearning on your own dataset from mu.algorithms.forget_me_not.algorithm import ForgetMeNotAlgorithm from mu.algorithms.forget_me_not.configs import ( forget_me_not_train_attn_mu, ) algorithm = ForgetMeNotAlgorithm( forget_me_not_train_attn_mu, ckpt_path=\"models/diffuser/style50\", raw_dataset_dir=( \"data/generic\" #replace with your own generated path ), steps=10, ti_weights_path=\"outputs/forget_me_not/ti_models/step_inv_10.safetensors\", template_name = \"self-harm\", dataset_type = \"generic\" ) algorithm.run(train_type=\"train_attn\") Running the Script in Offline Mode WANDB_MODE=offline python my_trainer_attn.py How It Works * Default Values: The script first loads default values from the train config file as in configs section. Parameter Overrides: Any parameters passed directly to the algorithm, overrides these configs. Final Configuration: The script merges the configs and convert them into dictionary to proceed with the training.","title":"Run Train"},{"location":"unlearn/algorithms/forget_me_not/#directory-structure","text":"algorithm.py : Implementation of the Forget Me NotAlgorithm class. configs/ : Contains configuration files for training and generation. model.py : Implementation of the Forget Me NotModel class. scripts/train.py : Script to train the Forget Me Not algorithm. trainer.py : Implementation of the Forget Me NotTrainer class. utils.py : Utility functions used in the project. data_handler.py : Implementation of DataHandler class This method involves two stages: Train a Text Inversion : The first stage involves training a Text Inversion. Refer to the script train_ti.py for details and implementation. It uses train_ti_config.yaml as config file. Perform Unlearning : The second stage uses the outputs from the first stage to perform unlearning. Refer to the script train_attn.py for details and implementation. It uses train_attn_config.yaml as config file.","title":"Directory Structure"},{"location":"unlearn/algorithms/saliency/","text":"Saliency Unlearning Algorithm for Machine Unlearning This repository provides an implementation of the Saliency Unlearning algorithm for machine unlearning in Stable Diffusion models. The Saliency Unlearning algorithm allows you to remove specific concepts or styles from a pre-trained model without retraining it from scratch. Usage Before training saliency unlearning algorithm you need to generate mask. Use the following code snippet to generate mask. Step 1: Generate mask using unlearn canvas dataset from mu.algorithms.saliency_unlearning.algorithm import MaskingAlgorithm from mu.algorithms.saliency_unlearning.configs import saliency_unlearning_generate_mask_mu generate_mask = MaskingAlgorithm( saliency_unlearning_generate_mask_mu, ckpt_path = \"models/compvis/style50/compvis.ckpt\", raw_dataset_dir = \"data/quick-canvas-dataset/sample\", dataset_type = \"unlearncanvas\", use_sample = True, #to use sample dataset output_dir = \"outputs/saliency_unlearning/masks\", #output path to save mask template_name = \"Abstractionism\", template = \"style\" ) if __name__ == \"__main__\": generate_mask.run() using i2p dataset from mu.algorithms.saliency_unlearning.algorithm import MaskingAlgorithm from mu.algorithms.saliency_unlearning.configs import saliency_unlearning_generate_mask_i2p generate_mask = MaskingAlgorithm( saliency_unlearning_generate_mask_i2p, ckpt_path = \"models/compvis/style50/compvis.ckpt\", raw_dataset_dir = \"data/quick-canvas-dataset/sample\", dataset_type = \"unlearncanvas\", use_sample = True, #to use sample dataset output_dir = \"outputs/saliency_unlearning/masks\", #output path to save mask template_name = \"self-harm\", template = \"i2p\" ) if __name__ == \"__main__\": generate_mask.run() Run Train Using quick canvas dataset To train the saliency unlearning algorithm to unlearn a specific concept or style from the Stable Diffusion model, use the train.py script located in the scripts directory. Example Code from mu.algorithms.saliency_unlearning.algorithm import ( SaliencyUnlearningAlgorithm, ) from mu.algorithms.saliency_unlearning.configs import ( saliency_unlearning_train_mu, ) algorithm = SaliencyUnlearningAlgorithm( saliency_unlearning_train_mu, output_dir=\"/opt/dlami/nvme/outputs\", dataset_type = \"unlearncanvas\" template_name = \"Abstractionism\", #concept to erase template = \"style\", use_sample = True #to run on sample dataset. ) algorithm.run() Using i2p dataset from mu.algorithms.saliency_unlearning.algorithm import ( SaliencyUnlearningAlgorithm, ) from mu.algorithms.saliency_unlearning.configs import ( saliency_unlearning_train_i2p, ) algorithm = SaliencyUnlearningAlgorithm( saliency_unlearning_train_i2p, raw_dataset_dir = \"data/i2p-dataset/sample\", output_dir=\"/opt/dlami/nvme/outputs\", template_name = \"self-harm\", #concept to erase template = \"style\", dataset_type = \"i2p\", use_sample = True #to run on sample dataset. ) algorithm.run() Run on your own dataset Step-1: Generate your own dataset generate_images_for_prompts --model_path models/diffuser/style50 --csv_path data/prompts/generic_data.csv Note: generate_images_for_prompts: This command invokes the image generation script. It uses a diffusion model to generate images based on textual prompts. --model_path: Specifies the path to the diffusion model to be used for image generation. In this example, the model is located at models/diffuser/style50. --csv_path: Provides the path to a CSV file containing the prompts. Each prompt in this CSV will be used to generate an image, allowing you to build a dataset tailored to your needs. Step-2: Train on your own dataset from mu.algorithms.saliency_unlearning.algorithm import ( SaliencyUnlearnAlgorithm, ) from mu.algorithms.saliency_unlearning.configs import ( saliency_unlearning_train_mu, ) algorithm = SaliencyUnlearnAlgorithm( saliency_unlearning_train_mu, raw_dataset_dir=( \"data/generic\" #replace with your own generated path ), ckpt_path=\"models/compvis/style50/compvis.ckpt\", output_dir=\"/opt/dlami/nvme/outputs\", dataset_type = \"generic\", template_name = \"self-harm\" ) algorithm.run() Running the Training Script in Offline Mode WANDB_MODE=offline python my_trainer.py How It Works * Default Values: The script first loads default values from the train config file as in configs section. Parameter Overrides: Any parameters passed directly to the algorithm, overrides these configs. Final Configuration: The script merges the configs and convert them into dictionary to proceed with the training. Similarly, you can pass arguments during runtime to generate mask. How It Works * Default Values: The script first loads default values from the YAML file specified by --config_path. Command-Line Overrides: Any arguments passed on the command line will override the corresponding keys in the YAML configuration file. Final Configuration: The script merges the YAML file and command-line arguments into a single configuration dictionary and uses it for training. Directory Structure algorithm.py : Implementation of the SaliencyUnlearnAlgorithm class. configs/ : Contains configuration files for training and generation. model.py : Implementation of the SaliencyUnlearnModel class. scripts/train.py : Script to train the SaliencyUnlearn algorithm. trainer.py : Implementation of the SaliencyUnlearnTrainer class. utils.py : Utility functions used in the project. data_handler.py : Implementation of DataHandler class The unlearning has two stages: Generate the mask Unlearn the weights.","title":"Usage"},{"location":"unlearn/algorithms/saliency/#saliency-unlearning-algorithm-for-machine-unlearning","text":"This repository provides an implementation of the Saliency Unlearning algorithm for machine unlearning in Stable Diffusion models. The Saliency Unlearning algorithm allows you to remove specific concepts or styles from a pre-trained model without retraining it from scratch.","title":"Saliency Unlearning Algorithm for Machine Unlearning"},{"location":"unlearn/algorithms/saliency/#usage","text":"Before training saliency unlearning algorithm you need to generate mask. Use the following code snippet to generate mask. Step 1: Generate mask using unlearn canvas dataset from mu.algorithms.saliency_unlearning.algorithm import MaskingAlgorithm from mu.algorithms.saliency_unlearning.configs import saliency_unlearning_generate_mask_mu generate_mask = MaskingAlgorithm( saliency_unlearning_generate_mask_mu, ckpt_path = \"models/compvis/style50/compvis.ckpt\", raw_dataset_dir = \"data/quick-canvas-dataset/sample\", dataset_type = \"unlearncanvas\", use_sample = True, #to use sample dataset output_dir = \"outputs/saliency_unlearning/masks\", #output path to save mask template_name = \"Abstractionism\", template = \"style\" ) if __name__ == \"__main__\": generate_mask.run() using i2p dataset from mu.algorithms.saliency_unlearning.algorithm import MaskingAlgorithm from mu.algorithms.saliency_unlearning.configs import saliency_unlearning_generate_mask_i2p generate_mask = MaskingAlgorithm( saliency_unlearning_generate_mask_i2p, ckpt_path = \"models/compvis/style50/compvis.ckpt\", raw_dataset_dir = \"data/quick-canvas-dataset/sample\", dataset_type = \"unlearncanvas\", use_sample = True, #to use sample dataset output_dir = \"outputs/saliency_unlearning/masks\", #output path to save mask template_name = \"self-harm\", template = \"i2p\" ) if __name__ == \"__main__\": generate_mask.run()","title":"Usage"},{"location":"unlearn/algorithms/saliency/#run-train","text":"Using quick canvas dataset To train the saliency unlearning algorithm to unlearn a specific concept or style from the Stable Diffusion model, use the train.py script located in the scripts directory. Example Code from mu.algorithms.saliency_unlearning.algorithm import ( SaliencyUnlearningAlgorithm, ) from mu.algorithms.saliency_unlearning.configs import ( saliency_unlearning_train_mu, ) algorithm = SaliencyUnlearningAlgorithm( saliency_unlearning_train_mu, output_dir=\"/opt/dlami/nvme/outputs\", dataset_type = \"unlearncanvas\" template_name = \"Abstractionism\", #concept to erase template = \"style\", use_sample = True #to run on sample dataset. ) algorithm.run() Using i2p dataset from mu.algorithms.saliency_unlearning.algorithm import ( SaliencyUnlearningAlgorithm, ) from mu.algorithms.saliency_unlearning.configs import ( saliency_unlearning_train_i2p, ) algorithm = SaliencyUnlearningAlgorithm( saliency_unlearning_train_i2p, raw_dataset_dir = \"data/i2p-dataset/sample\", output_dir=\"/opt/dlami/nvme/outputs\", template_name = \"self-harm\", #concept to erase template = \"style\", dataset_type = \"i2p\", use_sample = True #to run on sample dataset. ) algorithm.run() Run on your own dataset Step-1: Generate your own dataset generate_images_for_prompts --model_path models/diffuser/style50 --csv_path data/prompts/generic_data.csv Note: generate_images_for_prompts: This command invokes the image generation script. It uses a diffusion model to generate images based on textual prompts. --model_path: Specifies the path to the diffusion model to be used for image generation. In this example, the model is located at models/diffuser/style50. --csv_path: Provides the path to a CSV file containing the prompts. Each prompt in this CSV will be used to generate an image, allowing you to build a dataset tailored to your needs. Step-2: Train on your own dataset from mu.algorithms.saliency_unlearning.algorithm import ( SaliencyUnlearnAlgorithm, ) from mu.algorithms.saliency_unlearning.configs import ( saliency_unlearning_train_mu, ) algorithm = SaliencyUnlearnAlgorithm( saliency_unlearning_train_mu, raw_dataset_dir=( \"data/generic\" #replace with your own generated path ), ckpt_path=\"models/compvis/style50/compvis.ckpt\", output_dir=\"/opt/dlami/nvme/outputs\", dataset_type = \"generic\", template_name = \"self-harm\" ) algorithm.run() Running the Training Script in Offline Mode WANDB_MODE=offline python my_trainer.py How It Works * Default Values: The script first loads default values from the train config file as in configs section. Parameter Overrides: Any parameters passed directly to the algorithm, overrides these configs. Final Configuration: The script merges the configs and convert them into dictionary to proceed with the training. Similarly, you can pass arguments during runtime to generate mask. How It Works * Default Values: The script first loads default values from the YAML file specified by --config_path. Command-Line Overrides: Any arguments passed on the command line will override the corresponding keys in the YAML configuration file. Final Configuration: The script merges the YAML file and command-line arguments into a single configuration dictionary and uses it for training.","title":"Run Train"},{"location":"unlearn/algorithms/saliency/#directory-structure","text":"algorithm.py : Implementation of the SaliencyUnlearnAlgorithm class. configs/ : Contains configuration files for training and generation. model.py : Implementation of the SaliencyUnlearnModel class. scripts/train.py : Script to train the SaliencyUnlearn algorithm. trainer.py : Implementation of the SaliencyUnlearnTrainer class. utils.py : Utility functions used in the project. data_handler.py : Implementation of DataHandler class The unlearning has two stages: Generate the mask Unlearn the weights.","title":"Directory Structure"},{"location":"unlearn/algorithms/scissorhands/","text":"ScissorHands Algorithm for Machine Unlearning This repository provides an implementation of the scissor hands algorithm for machine unlearning in Stable Diffusion models. The scissor hands algorithm allows you to remove specific concepts or styles from a pre-trained model without retraining it from scratch. Usage To train the ScissorHands algorithm to unlearn a specific concept or style from the Stable Diffusion model, use the train.py script located in the scripts directory. Run Train Create a file, eg, my_trainer.py and use examples and modify your configs to run the file. Example Code Using quick canvas dataset from mu.algorithms.scissorhands.algorithm import ScissorHandsAlgorithm from mu.algorithms.scissorhands.configs import ( scissorhands_train_mu, ) algorithm = ScissorHandsAlgorithm( scissorhands_train_mu, ckpt_path=\"models/compvis/style50/compvis.ckpt\", raw_dataset_dir=( \"/home/ubuntu/Projects/balaram/packaging/data/quick-canvas-dataset/sample\" ), output_dir=\"/opt/dlami/nvme/outputs\", dataset_type = \"unlearncanvas\", template = \"style\", template_name = \"Abstractionism\", use_sample = True # to train on sample dataset ) algorithm.run() Using i2p dataset from mu.algorithms.scissorhands.algorithm import ScissorHandsAlgorithm from mu.algorithms.scissorhands.configs import ( scissorhands_train_i2p, ) algorithm = ScissorHandsAlgorithm( scissorhands_train_i2p, ckpt_path=\"models/compvis/style50/compvis.ckpt\", raw_dataset_dir = \"data/i2p-dataset/sample\", output_dir=\"/opt/dlami/nvme/outputs\", use_sample = True, # to train on sample dataset dataset_type = \"i2p\", template_name = \"self-harm\" ) algorithm.run() Use your own dataset Step-1: Generate your own dataset generate_images_for_prompts --model_path models/diffuser/style50 --csv_path data/prompts/generic_data.csv Note: generate_images_for_prompts: This command invokes the image generation script. It uses a diffusion model to generate images based on textual prompts. --model_path: Specifies the path to the diffusion model to be used for image generation. In this example, the model is located at models/diffuser/style50. --csv_path: Provides the path to a CSV file containing the prompts. Each prompt in this CSV will be used to generate an image, allowing you to build a dataset tailored to your needs. Step-2: Train on your own dataset from mu.algorithms.scissorhands.algorithm import ScissorHandsAlgorithm from mu.algorithms.scissorhands.configs import scissorhands_train_mu algorithm = ScissorHandsAlgorithm( scissorhands_train_mu, ckpt_path=\"models/compvis/style50/compvis.ckpt\", raw_dataset_dir=\"data/generic_data\", #use your own generated path output_dir=\"/opt/dlami/nvme/outputs\", dataset_type = \"generic\", template_name = \"self-harm\" ) algorithm.run() Running the Training Script in Offline Mode WANDB_MODE=offline python my_trainer.py How It Works * Default Values: The script first loads default values from the train config file as in configs section. Parameter Overrides: Any parameters passed directly to the algorithm, overrides these configs. Final Configuration: The script merges the configs and convert them into dictionary to proceed with the training. Directory Structure algorithm.py : Implementation of the ScissorHandsAlgorithm class. configs/ : Contains configuration files for training and generation. model.py : Implementation of the ScissorHandsModel class. scripts/train.py : Script to train the ScissorHands algorithm. trainer.py : Implementation of the ScissorHandsTrainer class. utils.py : Utility functions used in the project. data_handler.py : Implementation of DataHandler class","title":"Usage"},{"location":"unlearn/algorithms/scissorhands/#scissorhands-algorithm-for-machine-unlearning","text":"This repository provides an implementation of the scissor hands algorithm for machine unlearning in Stable Diffusion models. The scissor hands algorithm allows you to remove specific concepts or styles from a pre-trained model without retraining it from scratch.","title":"ScissorHands Algorithm for Machine Unlearning"},{"location":"unlearn/algorithms/scissorhands/#usage","text":"To train the ScissorHands algorithm to unlearn a specific concept or style from the Stable Diffusion model, use the train.py script located in the scripts directory.","title":"Usage"},{"location":"unlearn/algorithms/scissorhands/#run-train","text":"Create a file, eg, my_trainer.py and use examples and modify your configs to run the file. Example Code Using quick canvas dataset from mu.algorithms.scissorhands.algorithm import ScissorHandsAlgorithm from mu.algorithms.scissorhands.configs import ( scissorhands_train_mu, ) algorithm = ScissorHandsAlgorithm( scissorhands_train_mu, ckpt_path=\"models/compvis/style50/compvis.ckpt\", raw_dataset_dir=( \"/home/ubuntu/Projects/balaram/packaging/data/quick-canvas-dataset/sample\" ), output_dir=\"/opt/dlami/nvme/outputs\", dataset_type = \"unlearncanvas\", template = \"style\", template_name = \"Abstractionism\", use_sample = True # to train on sample dataset ) algorithm.run() Using i2p dataset from mu.algorithms.scissorhands.algorithm import ScissorHandsAlgorithm from mu.algorithms.scissorhands.configs import ( scissorhands_train_i2p, ) algorithm = ScissorHandsAlgorithm( scissorhands_train_i2p, ckpt_path=\"models/compvis/style50/compvis.ckpt\", raw_dataset_dir = \"data/i2p-dataset/sample\", output_dir=\"/opt/dlami/nvme/outputs\", use_sample = True, # to train on sample dataset dataset_type = \"i2p\", template_name = \"self-harm\" ) algorithm.run() Use your own dataset Step-1: Generate your own dataset generate_images_for_prompts --model_path models/diffuser/style50 --csv_path data/prompts/generic_data.csv Note: generate_images_for_prompts: This command invokes the image generation script. It uses a diffusion model to generate images based on textual prompts. --model_path: Specifies the path to the diffusion model to be used for image generation. In this example, the model is located at models/diffuser/style50. --csv_path: Provides the path to a CSV file containing the prompts. Each prompt in this CSV will be used to generate an image, allowing you to build a dataset tailored to your needs. Step-2: Train on your own dataset from mu.algorithms.scissorhands.algorithm import ScissorHandsAlgorithm from mu.algorithms.scissorhands.configs import scissorhands_train_mu algorithm = ScissorHandsAlgorithm( scissorhands_train_mu, ckpt_path=\"models/compvis/style50/compvis.ckpt\", raw_dataset_dir=\"data/generic_data\", #use your own generated path output_dir=\"/opt/dlami/nvme/outputs\", dataset_type = \"generic\", template_name = \"self-harm\" ) algorithm.run() Running the Training Script in Offline Mode WANDB_MODE=offline python my_trainer.py How It Works * Default Values: The script first loads default values from the train config file as in configs section. Parameter Overrides: Any parameters passed directly to the algorithm, overrides these configs. Final Configuration: The script merges the configs and convert them into dictionary to proceed with the training.","title":"Run Train"},{"location":"unlearn/algorithms/scissorhands/#directory-structure","text":"algorithm.py : Implementation of the ScissorHandsAlgorithm class. configs/ : Contains configuration files for training and generation. model.py : Implementation of the ScissorHandsModel class. scripts/train.py : Script to train the ScissorHands algorithm. trainer.py : Implementation of the ScissorHandsTrainer class. utils.py : Utility functions used in the project. data_handler.py : Implementation of DataHandler class","title":"Directory Structure"},{"location":"unlearn/algorithms/selective_amnesia/","text":"Selective Amnesia Algorithm for Machine Unlearning This repository provides an implementation of the Selective Amnesia algorithm for machine unlearning in Stable Diffusion models. The Selective Amnesia algorithm focuses on removing specific concepts or styles from a pre-trained model while retaining the rest of the knowledge. Usage To train the Selective Amnesia algorithm to remove specific concepts or styles from the Stable Diffusion model, use the train.py script located in the scripts directory. First download the full_fisher_dict.pkl file. wget https://huggingface.co/ajrheng/selective-amnesia/resolve/main/full_fisher_dict.pkl Run train Create a file, eg, my_trainer.py and use examples and modify your configs to run the file. Using quick canvas dataset from mu.algorithms.selective_amnesia.algorithm import SelectiveAmnesiaAlgorithm from mu.algorithms.selective_amnesia.configs import ( selective_amnesia_config_quick_canvas, ) algorithm = SelectiveAmnesiaAlgorithm( selective_amnesia_config_quick_canvas, ckpt_path=\"models/compvis/style50/compvis.ckpt\", raw_dataset_dir=( \"data/quick-canvas-dataset/sample\" ), dataset_type = \"unlearncanvas\", template = \"style\", template_name = \"Abstractionism\", use_sample = True # to run on sample dataset ) algorithm.run() Using i2p dataset from mu.algorithms.selective_amnesia.algorithm import SelectiveAmnesiaAlgorithm from mu.algorithms.selective_amnesia.configs import ( selective_amnesia_config_i2p, ) algorithm = SelectiveAmnesiaAlgorithm( selective_amnesia_config_i2p, ckpt_path=\"models/compvis/style50/compvis.ckpt\", raw_dataset_dir=( \"data/i2p/sample\" ), dataset_type = \"i2p\", template_name = \"self-harm\", use_sample = True # to run on sample dataset ) algorithm.run() Step-1: Generate your own dataset generate_images_for_prompts --model_path models/diffuser/style50 --csv_path data/prompts/generic_data.csv Note: generate_images_for_prompts: This command invokes the image generation script. It uses a diffusion model to generate images based on textual prompts. --model_path: Specifies the path to the diffusion model to be used for image generation. In this example, the model is located at models/diffuser/style50. --csv_path: Provides the path to a CSV file containing the prompts. Each prompt in this CSV will be used to generate an image, allowing you to build a dataset tailored to your needs. Step-2: Train on your own dataset from mu.algorithms.selective_amnesia.algorithm import SelectiveAmnesiaAlgorithm from mu.algorithms.selective_amnesia.configs import ( selective_amnesia_config_quick_canvas, ) algorithm = SelectiveAmnesiaAlgorithm( selective_amnesia_config_quick_canvas, ckpt_path=\"models/compvis/style50/compvis.ckpt\", raw_dataset_dir=\"data/generic\", #use your own path dataset_type = \"generic\", template_name = \"self-harm\", replay_prompt_path = \"mu/algorithms/selective_amnesia/data/fim_prompts_sample.txt\" ) algorithm.run() Run the script WANDB_MODE=offline python my_trainer.py How It Works * Default Values: The script first loads default values from the train config file as in configs section. Parameter Overrides: Any parameters passed directly to the algorithm, overrides these configs. Final Configuration: The script merges the configs and convert them into dictionary to proceed with the training. Notes Ensure all dependencies are installed as per the environment file. The training process generates logs in the logs/ directory for easy monitoring. Use appropriate CUDA devices for optimal performance during training. Regularly verify dataset and model configurations to avoid errors during execution.","title":"Usage"},{"location":"unlearn/algorithms/selective_amnesia/#selective-amnesia-algorithm-for-machine-unlearning","text":"This repository provides an implementation of the Selective Amnesia algorithm for machine unlearning in Stable Diffusion models. The Selective Amnesia algorithm focuses on removing specific concepts or styles from a pre-trained model while retaining the rest of the knowledge.","title":"Selective Amnesia Algorithm for Machine Unlearning"},{"location":"unlearn/algorithms/selective_amnesia/#usage","text":"To train the Selective Amnesia algorithm to remove specific concepts or styles from the Stable Diffusion model, use the train.py script located in the scripts directory. First download the full_fisher_dict.pkl file. wget https://huggingface.co/ajrheng/selective-amnesia/resolve/main/full_fisher_dict.pkl","title":"Usage"},{"location":"unlearn/algorithms/selective_amnesia/#run-train","text":"Create a file, eg, my_trainer.py and use examples and modify your configs to run the file. Using quick canvas dataset from mu.algorithms.selective_amnesia.algorithm import SelectiveAmnesiaAlgorithm from mu.algorithms.selective_amnesia.configs import ( selective_amnesia_config_quick_canvas, ) algorithm = SelectiveAmnesiaAlgorithm( selective_amnesia_config_quick_canvas, ckpt_path=\"models/compvis/style50/compvis.ckpt\", raw_dataset_dir=( \"data/quick-canvas-dataset/sample\" ), dataset_type = \"unlearncanvas\", template = \"style\", template_name = \"Abstractionism\", use_sample = True # to run on sample dataset ) algorithm.run() Using i2p dataset from mu.algorithms.selective_amnesia.algorithm import SelectiveAmnesiaAlgorithm from mu.algorithms.selective_amnesia.configs import ( selective_amnesia_config_i2p, ) algorithm = SelectiveAmnesiaAlgorithm( selective_amnesia_config_i2p, ckpt_path=\"models/compvis/style50/compvis.ckpt\", raw_dataset_dir=( \"data/i2p/sample\" ), dataset_type = \"i2p\", template_name = \"self-harm\", use_sample = True # to run on sample dataset ) algorithm.run() Step-1: Generate your own dataset generate_images_for_prompts --model_path models/diffuser/style50 --csv_path data/prompts/generic_data.csv Note: generate_images_for_prompts: This command invokes the image generation script. It uses a diffusion model to generate images based on textual prompts. --model_path: Specifies the path to the diffusion model to be used for image generation. In this example, the model is located at models/diffuser/style50. --csv_path: Provides the path to a CSV file containing the prompts. Each prompt in this CSV will be used to generate an image, allowing you to build a dataset tailored to your needs. Step-2: Train on your own dataset from mu.algorithms.selective_amnesia.algorithm import SelectiveAmnesiaAlgorithm from mu.algorithms.selective_amnesia.configs import ( selective_amnesia_config_quick_canvas, ) algorithm = SelectiveAmnesiaAlgorithm( selective_amnesia_config_quick_canvas, ckpt_path=\"models/compvis/style50/compvis.ckpt\", raw_dataset_dir=\"data/generic\", #use your own path dataset_type = \"generic\", template_name = \"self-harm\", replay_prompt_path = \"mu/algorithms/selective_amnesia/data/fim_prompts_sample.txt\" ) algorithm.run() Run the script WANDB_MODE=offline python my_trainer.py How It Works * Default Values: The script first loads default values from the train config file as in configs section. Parameter Overrides: Any parameters passed directly to the algorithm, overrides these configs. Final Configuration: The script merges the configs and convert them into dictionary to proceed with the training.","title":"Run train"},{"location":"unlearn/algorithms/selective_amnesia/#notes","text":"Ensure all dependencies are installed as per the environment file. The training process generates logs in the logs/ directory for easy monitoring. Use appropriate CUDA devices for optimal performance during training. Regularly verify dataset and model configurations to avoid errors during execution.","title":"Notes"},{"location":"unlearn/algorithms/semipermeable_membrane/","text":"Semi Permeable Membrane Algorithm for Machine Unlearning This repository provides an implementation of the semipermeable membrane algorithm for machine unlearning in Stable Diffusion models. The semipermeable membrane algorithm allows you to remove specific concepts or styles from a pre-trained model without retraining it from scratch. Usage To train the Semi Permeable Membrane algorithm to unlearn a specific concept or style from the Stable Diffusion model, use the train.py script located in the scripts directory. Run Train Create a file, eg, my_trainer.py and use examples and modify your configs to run the file. Example Code Using quick canvas dataset from mu.algorithms.semipermeable_membrane.algorithm import ( SemipermeableMembraneAlgorithm, ) from mu.algorithms.semipermeable_membrane.configs import ( semipermiable_membrane_train_mu, SemipermeableMembraneConfig, ) algorithm = SemipermeableMembraneAlgorithm( semipermiable_membrane_train_mu, output_dir=\"/opt/dlami/nvme/outputs\", train={\"iterations\": 2}, use_sample = True # to run on sample dataset ) algorithm.run() Using quick canvas dataset from mu.algorithms.semipermeable_membrane.algorithm import ( SemipermeableMembraneAlgorithm, ) from mu.algorithms.semipermeable_membrane.configs import ( semipermiable_membrane_train_i2p, SemipermeableMembraneConfig, ) algorithm = SemipermeableMembraneAlgorithm( semipermiable_membrane_train_i2p, output_dir=\"/opt/dlami/nvme/outputs\", train={\"iterations\": 2}, use_sample = True # to run on sample dataset dataset_type = \"i2p\", template_name = \"self-harm\", ) algorithm.run() Running the Training Script in Offline Mode WANDB_MODE=offline python my_trainer.py How It Works * Default Values: The script first loads default values from the train config file as in configs section. Parameter Overrides: Any parameters passed directly to the algorithm, overrides these configs. Final Configuration: The script merges the configs and convert them into dictionary to proceed with the training. Directory Structure algorithm.py : Implementation of the Semi Permeable MembraneAlgorithm class. configs/ : Contains configuration files for training and generation. model.py : Implementation of the Semi Permeable MembraneModel class. scripts/train.py : Script to train the Semi Permeable Membrane algorithm. trainer.py : Implementation of the Semi Permeable MembraneTrainer class. utils.py : Utility functions used in the project. data_handler.py : Implementation of DataHandler class","title":"Usage"},{"location":"unlearn/algorithms/semipermeable_membrane/#semi-permeable-membrane-algorithm-for-machine-unlearning","text":"This repository provides an implementation of the semipermeable membrane algorithm for machine unlearning in Stable Diffusion models. The semipermeable membrane algorithm allows you to remove specific concepts or styles from a pre-trained model without retraining it from scratch.","title":"Semi Permeable Membrane Algorithm for Machine Unlearning"},{"location":"unlearn/algorithms/semipermeable_membrane/#usage","text":"To train the Semi Permeable Membrane algorithm to unlearn a specific concept or style from the Stable Diffusion model, use the train.py script located in the scripts directory.","title":"Usage"},{"location":"unlearn/algorithms/semipermeable_membrane/#run-train","text":"Create a file, eg, my_trainer.py and use examples and modify your configs to run the file. Example Code Using quick canvas dataset from mu.algorithms.semipermeable_membrane.algorithm import ( SemipermeableMembraneAlgorithm, ) from mu.algorithms.semipermeable_membrane.configs import ( semipermiable_membrane_train_mu, SemipermeableMembraneConfig, ) algorithm = SemipermeableMembraneAlgorithm( semipermiable_membrane_train_mu, output_dir=\"/opt/dlami/nvme/outputs\", train={\"iterations\": 2}, use_sample = True # to run on sample dataset ) algorithm.run() Using quick canvas dataset from mu.algorithms.semipermeable_membrane.algorithm import ( SemipermeableMembraneAlgorithm, ) from mu.algorithms.semipermeable_membrane.configs import ( semipermiable_membrane_train_i2p, SemipermeableMembraneConfig, ) algorithm = SemipermeableMembraneAlgorithm( semipermiable_membrane_train_i2p, output_dir=\"/opt/dlami/nvme/outputs\", train={\"iterations\": 2}, use_sample = True # to run on sample dataset dataset_type = \"i2p\", template_name = \"self-harm\", ) algorithm.run() Running the Training Script in Offline Mode WANDB_MODE=offline python my_trainer.py How It Works * Default Values: The script first loads default values from the train config file as in configs section. Parameter Overrides: Any parameters passed directly to the algorithm, overrides these configs. Final Configuration: The script merges the configs and convert them into dictionary to proceed with the training.","title":"Run Train"},{"location":"unlearn/algorithms/semipermeable_membrane/#directory-structure","text":"algorithm.py : Implementation of the Semi Permeable MembraneAlgorithm class. configs/ : Contains configuration files for training and generation. model.py : Implementation of the Semi Permeable MembraneModel class. scripts/train.py : Script to train the Semi Permeable Membrane algorithm. trainer.py : Implementation of the Semi Permeable MembraneTrainer class. utils.py : Utility functions used in the project. data_handler.py : Implementation of DataHandler class","title":"Directory Structure"},{"location":"unlearn/algorithms/uce/","text":"Unified Concept Editing Algorithm for Machine Unlearning This repository provides an implementation of the unified concept editing algorithm for machine unlearning in Stable Diffusion models. The unified concept editing algorithm allows you to remove specific concepts or styles from a pre-trained model without retraining it from scratch. Usage To train the Unified Concept Editing algorithm to unlearn a specific concept or style from the Stable Diffusion model, use the train.py script located in the scripts directory. Run Train Create a file, eg, my_trainer.py and use examples and modify your configs to run the file. Using quick canvas dataset from mu.algorithms.unified_concept_editing.algorithm import ( UnifiedConceptEditingAlgorithm, ) from mu.algorithms.unified_concept_editing.configs import ( unified_concept_editing_train_mu, ) algorithm = UnifiedConceptEditingAlgorithm( unified_concept_editing_train_mu, ckpt_path=\"models/diffuser/style50/\", raw_dataset_dir=( \"data/quick-canvas-dataset/sample\" ), output_dir=\"/opt/dlami/nvme/outputs\", ) algorithm.run() Using i2p dataset from mu.algorithms.unified_concept_editing.algorithm import ( UnifiedConceptEditingAlgorithm, ) from mu.algorithms.unified_concept_editing.configs import ( unified_concept_editing_train_i2p, ) algorithm = UnifiedConceptEditingAlgorithm( unified_concept_editing_train_i2p, ckpt_path=\"models/diffuser/style50/\", raw_dataset_dir=( \"data/quick-canvas-dataset/sample\" ), output_dir=\"/opt/dlami/nvme/outputs\", use_sample = True # to run on sample dataset dataset_type = \"i2p\", template_name = \"self-harm\", ) algorithm.run() Using your own dataset Step-1: Generate your own dataset generate_images_for_prompts --model_path models/diffuser/style50 --csv_path data/prompts/generic_data.csv Note: generate_images_for_prompts: This command invokes the image generation script. It uses a diffusion model to generate images based on textual prompts. --model_path: Specifies the path to the diffusion model to be used for image generation. In this example, the model is located at models/diffuser/style50. --csv_path: Provides the path to a CSV file containing the prompts. Each prompt in this CSV will be used to generate an image, allowing you to build a dataset tailored to your needs. Step-2: Train on your own dataset from mu.algorithms.unified_concept_editing.algorithm import ( UnifiedConceptEditingAlgorithm, ) from mu.algorithms.unified_concept_editing.configs import ( unified_concept_editing_train_mu, ) algorithm = UnifiedConceptEditingAlgorithm( unified_concept_editing_train_mu, ckpt_path=\"models/diffuser/style50/\", raw_dataset_dir=\"data/generic\", #replace with your own generated path prompt_path = \"data/generic/prompts/generic.csv\", #replace with your own prompt path dataset_type = \"generic\", template_name = \"self-harm\", output_dir=\"outputs/uce\", ) algorithm.run() Running the Training Script in Offline Mode WANDB_MODE=offline python my_trainer.py How It Works * Default Values: The script first loads default values from the train config file as in configs section. Parameter Overrides: Any parameters passed directly to the algorithm, overrides these configs. Final Configuration: The script merges the configs and convert them into dictionary to proceed with the training. Directory Structure algorithm.py : Implementation of the ScissorHandsAlgorithm class. configs/ : Contains configuration files for training and generation. model.py : Implementation of the ScissorHandsModel class. scripts/train.py : Script to train the ScissorHands algorithm. trainer.py : Implementation of the ScissorHandsTrainer class. utils.py : Utility functions used in the project. data_handler.py : Implementation of DataHandler class","title":"Usage"},{"location":"unlearn/algorithms/uce/#unified-concept-editing-algorithm-for-machine-unlearning","text":"This repository provides an implementation of the unified concept editing algorithm for machine unlearning in Stable Diffusion models. The unified concept editing algorithm allows you to remove specific concepts or styles from a pre-trained model without retraining it from scratch.","title":"Unified Concept Editing Algorithm for Machine Unlearning"},{"location":"unlearn/algorithms/uce/#usage","text":"To train the Unified Concept Editing algorithm to unlearn a specific concept or style from the Stable Diffusion model, use the train.py script located in the scripts directory.","title":"Usage"},{"location":"unlearn/algorithms/uce/#run-train","text":"Create a file, eg, my_trainer.py and use examples and modify your configs to run the file. Using quick canvas dataset from mu.algorithms.unified_concept_editing.algorithm import ( UnifiedConceptEditingAlgorithm, ) from mu.algorithms.unified_concept_editing.configs import ( unified_concept_editing_train_mu, ) algorithm = UnifiedConceptEditingAlgorithm( unified_concept_editing_train_mu, ckpt_path=\"models/diffuser/style50/\", raw_dataset_dir=( \"data/quick-canvas-dataset/sample\" ), output_dir=\"/opt/dlami/nvme/outputs\", ) algorithm.run() Using i2p dataset from mu.algorithms.unified_concept_editing.algorithm import ( UnifiedConceptEditingAlgorithm, ) from mu.algorithms.unified_concept_editing.configs import ( unified_concept_editing_train_i2p, ) algorithm = UnifiedConceptEditingAlgorithm( unified_concept_editing_train_i2p, ckpt_path=\"models/diffuser/style50/\", raw_dataset_dir=( \"data/quick-canvas-dataset/sample\" ), output_dir=\"/opt/dlami/nvme/outputs\", use_sample = True # to run on sample dataset dataset_type = \"i2p\", template_name = \"self-harm\", ) algorithm.run() Using your own dataset Step-1: Generate your own dataset generate_images_for_prompts --model_path models/diffuser/style50 --csv_path data/prompts/generic_data.csv Note: generate_images_for_prompts: This command invokes the image generation script. It uses a diffusion model to generate images based on textual prompts. --model_path: Specifies the path to the diffusion model to be used for image generation. In this example, the model is located at models/diffuser/style50. --csv_path: Provides the path to a CSV file containing the prompts. Each prompt in this CSV will be used to generate an image, allowing you to build a dataset tailored to your needs. Step-2: Train on your own dataset from mu.algorithms.unified_concept_editing.algorithm import ( UnifiedConceptEditingAlgorithm, ) from mu.algorithms.unified_concept_editing.configs import ( unified_concept_editing_train_mu, ) algorithm = UnifiedConceptEditingAlgorithm( unified_concept_editing_train_mu, ckpt_path=\"models/diffuser/style50/\", raw_dataset_dir=\"data/generic\", #replace with your own generated path prompt_path = \"data/generic/prompts/generic.csv\", #replace with your own prompt path dataset_type = \"generic\", template_name = \"self-harm\", output_dir=\"outputs/uce\", ) algorithm.run() Running the Training Script in Offline Mode WANDB_MODE=offline python my_trainer.py How It Works * Default Values: The script first loads default values from the train config file as in configs section. Parameter Overrides: Any parameters passed directly to the algorithm, overrides these configs. Final Configuration: The script merges the configs and convert them into dictionary to proceed with the training.","title":"Run Train"},{"location":"unlearn/algorithms/uce/#directory-structure","text":"algorithm.py : Implementation of the ScissorHandsAlgorithm class. configs/ : Contains configuration files for training and generation. model.py : Implementation of the ScissorHandsModel class. scripts/train.py : Script to train the ScissorHands algorithm. trainer.py : Implementation of the ScissorHandsTrainer class. utils.py : Utility functions used in the project. data_handler.py : Implementation of DataHandler class","title":"Directory Structure"},{"location":"unlearn/configs/concept_ablation/","text":"Train Config class ConceptAblationConfig(BaseConfig): def __init__(self, **kwargs): # Training parameters self.seed = 23 # Seed for random number generators self.scale_lr = True # Flag to scale the learning rate self.caption_target = \"Abstractionism Style\" # Caption target for the training self.regularization = True # Whether to apply regularization self.n_samples = 10 # Number of samples to generate self.train_size = 200 # Number of training samples self.base_lr = 2.0e-06 # Base learning rate # Model configuration self.config_path = current_dir / \"train_config.yaml\" self.model_config_path = ( current_dir / \"model_config.yaml\" ) # Path to model config self.ckpt_path = ( \"models/compvis/style50/compvis.ckpt\" # Path to model checkpoint ) # Dataset directories self.raw_dataset_dir = ( \"data/quick-canvas-dataset/sample\" # Raw dataset directory ) self.processed_dataset_dir = ( \"mu/algorithms/concept_ablation/data\" # Processed dataset directory ) self.dataset_type = \"unlearncanvas\" # Dataset type self.template = \"style\" # Template used for training self.template_name = \"Abstractionism\" # Template name # Learning rate for training self.lr = 5e-5 # Learning rate # Output directory for saving models self.output_dir = ( \"outputs/concept_ablation/finetuned_models\" # Output directory for results ) # Device configuration self.devices = \"0\" # CUDA devices (comma-separated) # Additional flags self.use_sample = True # Whether to use the sample dataset for training # Data configuration self.data = { \"target\": \"mu.algorithms.concept_ablation.data_handler.ConceptAblationDataHandler\", \"params\": { \"batch_size\": 1, # Batch size for training \"num_workers\": 1, # Number of workers for loading data \"wrap\": False, # Whether to wrap the dataset \"train\": { \"target\": \"mu.algorithms.concept_ablation.src.finetune_data.MaskBase\", \"params\": {\"size\": 512}, # Image size for the training set }, \"train2\": { \"target\": \"mu.algorithms.concept_ablation.src.finetune_data.MaskBase\", \"params\": {\"size\": 512}, # Image size for the second training set }, }, } # Lightning configuration self.lightning = { \"callbacks\": { \"image_logger\": { \"target\": \"mu.algorithms.concept_ablation.callbacks.ImageLogger\", \"params\": { \"batch_frequency\": 20000, # Frequency to log images \"save_freq\": 10000, # Frequency to save images \"max_images\": 8, # Maximum number of images to log \"increase_log_steps\": False, # Whether to increase the logging steps }, } }, \"modelcheckpoint\": { \"params\": { \"every_n_train_steps\": 10000 # Save the model every N training steps } }, \"trainer\": {\"max_steps\": 2000}, # Maximum number of training steps } self.prompts = \"mu/algorithms/concept_ablation/data/anchor_prompts/finetune_prompts/sd_prompt_Architectures_sample.txt\" Model Config # Training parameters seed : 23 scale_lr : True caption_target : \"Abstractionism Style\" regularization : True n_samples : 10 train_size : 200 base_lr : 2.0e-06 # Model configuration model_config_path: \"mu/algorithms/concept_ablation/configs/model_config.yaml\" # Config path for Stable Diffusion ckpt_path: \"models/compvis/style50/compvis.ckpt\" # Checkpoint path for Stable Diffusion # Dataset directories raw_dataset_dir: \"data/quick-canvas-dataset/sample\" processed_dataset_dir: \"mu/algorithms/concept_ablation/data\" dataset_type : \"unlearncanvas\" template : \"style\" template_name : \"Abstractionism\" lr: 5e-5 # Output configurations output_dir: \"outputs/concept_ablation/finetuned_models\" # Output directory to save results # Sampling and image configurations # Device configuration devices: \"0,\" # CUDA devices to train on (comma-separated) # Additional flags use_sample: True # Use the sample dataset for training data: target: mu.algorithms.concept_ablation.data_handler.ConceptAblationDataHandler params: batch_size: 4 num_workers: 4 wrap: false train: target: mu.algorithms.concept_ablation.src.finetune_data.MaskBase params: size: 512 train2: target: mu.algorithms.concept_ablation.src.finetune_data.MaskBase params: size: 512 lightning: callbacks: image_logger: target: mu.algorithms.concept_ablation.callbacks.ImageLogger params: batch_frequency: 20000 save_freq: 10000 max_images: 8 increase_log_steps: False modelcheckpoint: params: every_n_train_steps: 10000 trainer: max_steps: 2000 Configuration File description Training Parameters seed: Random seed for reproducibility. Type: int Example: 23 scale_lr: Whether to scale the base learning rate. Type: bool Example: True caption_target: Target style to remove. Type: str Example: \"Abstractionism Style\" regularization: Adds regularization loss during training. Type: bool Example: True n_samples: Number of batch sizes for image generation. Type: int Example: 10 train_size: Number of generated images for training. Type: int Example: 1000 base_lr: Learning rate for the optimizer. Type: float Example: 2.0e-06 Model Configuration model_config_path: Path to the Stable Diffusion model configuration YAML file. Type: str Example: \"/path/to/model_config.yaml\" ckpt_path: Path to the Stable Diffusion model checkpoint. Type: str Example: \"/path/to/compvis.ckpt\" Dataset Directories raw_dataset_dir: Directory containing the raw dataset categorized by themes or classes. Type: str Example: \"/path/to/raw_dataset\" processed_dataset_dir: Directory to save the processed dataset. Type: str Example: \"/path/to/processed_dataset\" dataset_type: Specifies the dataset type for training. Use generic as type if you want to use your own dataset. Choices: [\"unlearncanvas\", \"i2p\",\"generic\"] Example: \"unlearncanvas\" template: Type of template to use during training. Choices: [\"object\", \"style\", \"i2p\"] Example: \"style\" template_name: Name of the concept or style to erase. Choices: [\"self-harm\", \"Abstractionism\"] Example: \"Abstractionism\" Output Configurations output_dir: Directory to save fine-tuned models and results. Type: str Example: \"outputs/concept_ablation/finetuned_models\" Device Configuration devices: CUDA devices for training (comma-separated). Type: str Example: \"0\"","title":"Configs"},{"location":"unlearn/configs/concept_ablation/#train-config","text":"class ConceptAblationConfig(BaseConfig): def __init__(self, **kwargs): # Training parameters self.seed = 23 # Seed for random number generators self.scale_lr = True # Flag to scale the learning rate self.caption_target = \"Abstractionism Style\" # Caption target for the training self.regularization = True # Whether to apply regularization self.n_samples = 10 # Number of samples to generate self.train_size = 200 # Number of training samples self.base_lr = 2.0e-06 # Base learning rate # Model configuration self.config_path = current_dir / \"train_config.yaml\" self.model_config_path = ( current_dir / \"model_config.yaml\" ) # Path to model config self.ckpt_path = ( \"models/compvis/style50/compvis.ckpt\" # Path to model checkpoint ) # Dataset directories self.raw_dataset_dir = ( \"data/quick-canvas-dataset/sample\" # Raw dataset directory ) self.processed_dataset_dir = ( \"mu/algorithms/concept_ablation/data\" # Processed dataset directory ) self.dataset_type = \"unlearncanvas\" # Dataset type self.template = \"style\" # Template used for training self.template_name = \"Abstractionism\" # Template name # Learning rate for training self.lr = 5e-5 # Learning rate # Output directory for saving models self.output_dir = ( \"outputs/concept_ablation/finetuned_models\" # Output directory for results ) # Device configuration self.devices = \"0\" # CUDA devices (comma-separated) # Additional flags self.use_sample = True # Whether to use the sample dataset for training # Data configuration self.data = { \"target\": \"mu.algorithms.concept_ablation.data_handler.ConceptAblationDataHandler\", \"params\": { \"batch_size\": 1, # Batch size for training \"num_workers\": 1, # Number of workers for loading data \"wrap\": False, # Whether to wrap the dataset \"train\": { \"target\": \"mu.algorithms.concept_ablation.src.finetune_data.MaskBase\", \"params\": {\"size\": 512}, # Image size for the training set }, \"train2\": { \"target\": \"mu.algorithms.concept_ablation.src.finetune_data.MaskBase\", \"params\": {\"size\": 512}, # Image size for the second training set }, }, } # Lightning configuration self.lightning = { \"callbacks\": { \"image_logger\": { \"target\": \"mu.algorithms.concept_ablation.callbacks.ImageLogger\", \"params\": { \"batch_frequency\": 20000, # Frequency to log images \"save_freq\": 10000, # Frequency to save images \"max_images\": 8, # Maximum number of images to log \"increase_log_steps\": False, # Whether to increase the logging steps }, } }, \"modelcheckpoint\": { \"params\": { \"every_n_train_steps\": 10000 # Save the model every N training steps } }, \"trainer\": {\"max_steps\": 2000}, # Maximum number of training steps } self.prompts = \"mu/algorithms/concept_ablation/data/anchor_prompts/finetune_prompts/sd_prompt_Architectures_sample.txt\"","title":"Train Config"},{"location":"unlearn/configs/concept_ablation/#model-config","text":"# Training parameters seed : 23 scale_lr : True caption_target : \"Abstractionism Style\" regularization : True n_samples : 10 train_size : 200 base_lr : 2.0e-06 # Model configuration model_config_path: \"mu/algorithms/concept_ablation/configs/model_config.yaml\" # Config path for Stable Diffusion ckpt_path: \"models/compvis/style50/compvis.ckpt\" # Checkpoint path for Stable Diffusion # Dataset directories raw_dataset_dir: \"data/quick-canvas-dataset/sample\" processed_dataset_dir: \"mu/algorithms/concept_ablation/data\" dataset_type : \"unlearncanvas\" template : \"style\" template_name : \"Abstractionism\" lr: 5e-5 # Output configurations output_dir: \"outputs/concept_ablation/finetuned_models\" # Output directory to save results # Sampling and image configurations # Device configuration devices: \"0,\" # CUDA devices to train on (comma-separated) # Additional flags use_sample: True # Use the sample dataset for training data: target: mu.algorithms.concept_ablation.data_handler.ConceptAblationDataHandler params: batch_size: 4 num_workers: 4 wrap: false train: target: mu.algorithms.concept_ablation.src.finetune_data.MaskBase params: size: 512 train2: target: mu.algorithms.concept_ablation.src.finetune_data.MaskBase params: size: 512 lightning: callbacks: image_logger: target: mu.algorithms.concept_ablation.callbacks.ImageLogger params: batch_frequency: 20000 save_freq: 10000 max_images: 8 increase_log_steps: False modelcheckpoint: params: every_n_train_steps: 10000 trainer: max_steps: 2000","title":"Model Config"},{"location":"unlearn/configs/concept_ablation/#configuration-file-description","text":"","title":"Configuration File description"},{"location":"unlearn/configs/concept_ablation/#training-parameters","text":"seed: Random seed for reproducibility. Type: int Example: 23 scale_lr: Whether to scale the base learning rate. Type: bool Example: True caption_target: Target style to remove. Type: str Example: \"Abstractionism Style\" regularization: Adds regularization loss during training. Type: bool Example: True n_samples: Number of batch sizes for image generation. Type: int Example: 10 train_size: Number of generated images for training. Type: int Example: 1000 base_lr: Learning rate for the optimizer. Type: float Example: 2.0e-06","title":"Training Parameters"},{"location":"unlearn/configs/concept_ablation/#model-configuration","text":"model_config_path: Path to the Stable Diffusion model configuration YAML file. Type: str Example: \"/path/to/model_config.yaml\" ckpt_path: Path to the Stable Diffusion model checkpoint. Type: str Example: \"/path/to/compvis.ckpt\"","title":"Model Configuration"},{"location":"unlearn/configs/concept_ablation/#dataset-directories","text":"raw_dataset_dir: Directory containing the raw dataset categorized by themes or classes. Type: str Example: \"/path/to/raw_dataset\" processed_dataset_dir: Directory to save the processed dataset. Type: str Example: \"/path/to/processed_dataset\" dataset_type: Specifies the dataset type for training. Use generic as type if you want to use your own dataset. Choices: [\"unlearncanvas\", \"i2p\",\"generic\"] Example: \"unlearncanvas\" template: Type of template to use during training. Choices: [\"object\", \"style\", \"i2p\"] Example: \"style\" template_name: Name of the concept or style to erase. Choices: [\"self-harm\", \"Abstractionism\"] Example: \"Abstractionism\"","title":"Dataset Directories"},{"location":"unlearn/configs/concept_ablation/#output-configurations","text":"output_dir: Directory to save fine-tuned models and results. Type: str Example: \"outputs/concept_ablation/finetuned_models\"","title":"Output Configurations"},{"location":"unlearn/configs/concept_ablation/#device-configuration","text":"devices: CUDA devices for training (comma-separated). Type: str Example: \"0\"","title":"Device Configuration"},{"location":"unlearn/configs/erase_diff/","text":"Sample Train Config class EraseDiffConfig(BaseConfig): def __init__(self, **kwargs): self.train_method = \"xattn\" self.alpha = 0.1 self.epochs = 1 self.K_steps = 2 self.lr = 5e-5 self.model_config_path = current_dir / \"model_config.yaml\" self.ckpt_path = \"models/compvis/style50/compvis.ckpt\" self.raw_dataset_dir = \"data/quick-canvas-dataset/sample\" self.processed_dataset_dir = \"mu/algorithms/erase_diff/data\" self.dataset_type = \"unlearncanvas\" self.template = \"style\" self.template_name = \"Abstractionism\" self.output_dir = \"outputs/erase_diff/finetuned_models\" self.separator = None self.image_size = 512 self.interpolation = \"bicubic\" self.ddim_steps = 50 self.ddim_eta = 0.0 self.devices = \"0\" self.use_sample = True self.num_workers = 4 self.pin_memory = True Sample Model Config model: base_learning_rate: 1.0e-04 target: stable_diffusion.ldm.models.diffusion.ddpm.LatentDiffusion params: linear_start: 0.00085 linear_end: 0.0120 num_timesteps_cond: 1 log_every_t: 200 timesteps: 1000 first_stage_key: \"edited\" cond_stage_key: \"edit\" image_size: 64 channels: 4 cond_stage_trainable: false # Note: different from the one we trained before conditioning_key: crossattn monitor: val/loss_simple_ema scale_factor: 0.18215 use_ema: False scheduler_config: # 10000 warmup steps target: stable_diffusion.ldm.lr_scheduler.LambdaLinearScheduler params: warm_up_steps: [ 10000 ] cycle_lengths: [ 10000000000000 ] # incredibly large number to prevent corner cases f_start: [ 1.e-6 ] f_max: [ 1. ] f_min: [ 1. ] unet_config: target: stable_diffusion.ldm.modules.diffusionmodules.openaimodel.UNetModel params: image_size: 32 # unused in_channels: 4 out_channels: 4 model_channels: 320 attention_resolutions: [ 4, 2, 1 ] num_res_blocks: 2 channel_mult: [ 1, 2, 4, 4 ] num_heads: 8 use_spatial_transformer: True transformer_depth: 1 context_dim: 768 use_checkpoint: True legacy: False first_stage_config: target: stable_diffusion.ldm.models.autoencoder.AutoencoderKL params: embed_dim: 4 monitor: val/rec_loss ddconfig: double_z: true z_channels: 4 resolution: 256 in_channels: 3 out_ch: 3 ch: 128 ch_mult: - 1 - 2 - 4 - 4 num_res_blocks: 2 attn_resolutions: [] dropout: 0.0 lossconfig: target: torch.nn.Identity cond_stage_config: target: stable_diffusion.ldm.modules.encoders.modules.FrozenCLIPEmbedder Description of Arguments in train_config.yaml Training Parameters train_method: Specifies the method of training for concept erasure. Choices: [\"noxattn\", \"selfattn\", \"xattn\", \"full\", \"notime\", \"xlayer\", \"selflayer\"] Example: \"xattn\" alpha: Guidance strength for the starting image during training. Type: float Example: 0.1 epochs: Number of epochs to train the model. Type: int Example: 1 K_steps: Number of K optimization steps during training. Type: int Example: 2 lr: Learning rate used for the optimizer during training. Type: float Example: 5e-5 Model Configuration model_config_path: File path to the Stable Diffusion model configuration YAML file. type: str Example: \"/path/to/model_config.yaml\" ckpt_path: File path to the checkpoint of the Stable Diffusion model. Type: str Example: \"/path/to/model_checkpoint.ckpt\" Dataset Directories raw_dataset_dir: Directory containing the raw dataset categorized by themes or classes. Type: str Example: \"/path/to/raw_dataset\" processed_dataset_dir: Directory to save the processed dataset. Type: str Example: \"/path/to/processed_dataset\" dataset_type: Specifies the dataset type for the training process. Use generic as type if you want to use your own dataset. Choices: [\"unlearncanvas\", \"i2p\", \"generic\"] Example: \"unlearncanvas\" template: Type of template to use during training. Choices: [\"object\", \"style\", \"i2p\"] Example: \"style\" template_name: Name of the specific concept or style to be erased. Choices: [\"self-harm\", \"Abstractionism\"] Example: \"Abstractionism\" Output Configurations output_dir: Directory where the fine-tuned models and results will be saved. Type: str Example: \"outputs/erase_diff/finetuned_models\" separator: String separator used to train multiple words separately, if applicable. Type: str or null Example: null Sampling and Image Configurations image_size: Size of the training images (height and width in pixels). Type: int Example: 512 interpolation: Interpolation method used for image resizing. Choices: [\"bilinear\", \"bicubic\", \"lanczos\"] Example: \"bicubic\" ddim_steps: Number of DDIM inference steps during training. Type: int Example: 50 ddim_eta: DDIM eta parameter for stochasticity during sampling. Type: float Example: 0.0 Device Configuration devices: Specifies the CUDA devices to be used for training (comma-separated). Type: str Example: \"0\" Additional Flags use_sample: Flag to indicate whether a sample dataset should be used for training. Type: bool Example: True num_workers: Number of worker threads for data loading. Type: int Example: 4 pin_memory: Flag to enable pinning memory during data loading for faster GPU transfers. Type: bool Example: true","title":"Configs"},{"location":"unlearn/configs/erase_diff/#sample-train-config","text":"class EraseDiffConfig(BaseConfig): def __init__(self, **kwargs): self.train_method = \"xattn\" self.alpha = 0.1 self.epochs = 1 self.K_steps = 2 self.lr = 5e-5 self.model_config_path = current_dir / \"model_config.yaml\" self.ckpt_path = \"models/compvis/style50/compvis.ckpt\" self.raw_dataset_dir = \"data/quick-canvas-dataset/sample\" self.processed_dataset_dir = \"mu/algorithms/erase_diff/data\" self.dataset_type = \"unlearncanvas\" self.template = \"style\" self.template_name = \"Abstractionism\" self.output_dir = \"outputs/erase_diff/finetuned_models\" self.separator = None self.image_size = 512 self.interpolation = \"bicubic\" self.ddim_steps = 50 self.ddim_eta = 0.0 self.devices = \"0\" self.use_sample = True self.num_workers = 4 self.pin_memory = True","title":"Sample Train Config"},{"location":"unlearn/configs/erase_diff/#sample-model-config","text":"model: base_learning_rate: 1.0e-04 target: stable_diffusion.ldm.models.diffusion.ddpm.LatentDiffusion params: linear_start: 0.00085 linear_end: 0.0120 num_timesteps_cond: 1 log_every_t: 200 timesteps: 1000 first_stage_key: \"edited\" cond_stage_key: \"edit\" image_size: 64 channels: 4 cond_stage_trainable: false # Note: different from the one we trained before conditioning_key: crossattn monitor: val/loss_simple_ema scale_factor: 0.18215 use_ema: False scheduler_config: # 10000 warmup steps target: stable_diffusion.ldm.lr_scheduler.LambdaLinearScheduler params: warm_up_steps: [ 10000 ] cycle_lengths: [ 10000000000000 ] # incredibly large number to prevent corner cases f_start: [ 1.e-6 ] f_max: [ 1. ] f_min: [ 1. ] unet_config: target: stable_diffusion.ldm.modules.diffusionmodules.openaimodel.UNetModel params: image_size: 32 # unused in_channels: 4 out_channels: 4 model_channels: 320 attention_resolutions: [ 4, 2, 1 ] num_res_blocks: 2 channel_mult: [ 1, 2, 4, 4 ] num_heads: 8 use_spatial_transformer: True transformer_depth: 1 context_dim: 768 use_checkpoint: True legacy: False first_stage_config: target: stable_diffusion.ldm.models.autoencoder.AutoencoderKL params: embed_dim: 4 monitor: val/rec_loss ddconfig: double_z: true z_channels: 4 resolution: 256 in_channels: 3 out_ch: 3 ch: 128 ch_mult: - 1 - 2 - 4 - 4 num_res_blocks: 2 attn_resolutions: [] dropout: 0.0 lossconfig: target: torch.nn.Identity cond_stage_config: target: stable_diffusion.ldm.modules.encoders.modules.FrozenCLIPEmbedder","title":"Sample Model Config"},{"location":"unlearn/configs/erase_diff/#description-of-arguments-in-train_configyaml","text":"Training Parameters train_method: Specifies the method of training for concept erasure. Choices: [\"noxattn\", \"selfattn\", \"xattn\", \"full\", \"notime\", \"xlayer\", \"selflayer\"] Example: \"xattn\" alpha: Guidance strength for the starting image during training. Type: float Example: 0.1 epochs: Number of epochs to train the model. Type: int Example: 1 K_steps: Number of K optimization steps during training. Type: int Example: 2 lr: Learning rate used for the optimizer during training. Type: float Example: 5e-5 Model Configuration model_config_path: File path to the Stable Diffusion model configuration YAML file. type: str Example: \"/path/to/model_config.yaml\" ckpt_path: File path to the checkpoint of the Stable Diffusion model. Type: str Example: \"/path/to/model_checkpoint.ckpt\" Dataset Directories raw_dataset_dir: Directory containing the raw dataset categorized by themes or classes. Type: str Example: \"/path/to/raw_dataset\" processed_dataset_dir: Directory to save the processed dataset. Type: str Example: \"/path/to/processed_dataset\" dataset_type: Specifies the dataset type for the training process. Use generic as type if you want to use your own dataset. Choices: [\"unlearncanvas\", \"i2p\", \"generic\"] Example: \"unlearncanvas\" template: Type of template to use during training. Choices: [\"object\", \"style\", \"i2p\"] Example: \"style\" template_name: Name of the specific concept or style to be erased. Choices: [\"self-harm\", \"Abstractionism\"] Example: \"Abstractionism\" Output Configurations output_dir: Directory where the fine-tuned models and results will be saved. Type: str Example: \"outputs/erase_diff/finetuned_models\" separator: String separator used to train multiple words separately, if applicable. Type: str or null Example: null Sampling and Image Configurations image_size: Size of the training images (height and width in pixels). Type: int Example: 512 interpolation: Interpolation method used for image resizing. Choices: [\"bilinear\", \"bicubic\", \"lanczos\"] Example: \"bicubic\" ddim_steps: Number of DDIM inference steps during training. Type: int Example: 50 ddim_eta: DDIM eta parameter for stochasticity during sampling. Type: float Example: 0.0 Device Configuration devices: Specifies the CUDA devices to be used for training (comma-separated). Type: str Example: \"0\" Additional Flags use_sample: Flag to indicate whether a sample dataset should be used for training. Type: bool Example: True num_workers: Number of worker threads for data loading. Type: int Example: 4 pin_memory: Flag to enable pinning memory during data loading for faster GPU transfers. Type: bool Example: true","title":"Description of Arguments in train_config.yaml"},{"location":"unlearn/configs/esd/","text":"Sample Train Config class ESDConfig(BaseConfig): def __init__(self, **kwargs): # Training parameters self.train_method = \"xattn\" # Choices: [\"noxattn\", \"selfattn\", \"xattn\", \"full\", \"notime\", \"xlayer\", \"selflayer\"] self.start_guidance = ( 0.1 # Optional: guidance of start image (previously alpha) ) self.negative_guidance = 0.0 # Optional: guidance of negative training self.iterations = 1 # Optional: iterations used to train (previously epochs) self.lr = 1e-5 # Optional: learning rate self.image_size = 512 # Optional: image size used to train self.ddim_steps = 50 # Optional: DDIM steps of inference # Model configuration self.model_config_path = current_dir / \"model_config.yaml\" self.ckpt_path = \"models/compvis/style50/compvis.ckpt\" # Checkpoint path for Stable Diffusion # Dataset directories self.raw_dataset_dir = \"data/quick-canvas-dataset/sample\" self.processed_dataset_dir = \"mu/algorithms/esd/data\" self.dataset_type = \"unlearncanvas\" # Choices: ['unlearncanvas', 'i2p'] self.template = \"style\" # Choices: ['object', 'style', 'i2p'] self.template_name = ( \"Abstractionism\" # Choices: ['self-harm', 'Abstractionism'] ) # Output configurations self.output_dir = \"outputs/esd/finetuned_models\" self.separator = None # Device configuration self.devices = \"0,0\" self.use_sample = True # For backward compatibility self.interpolation = \"bicubic\" # Interpolation method self.ddim_eta = 0.0 # Eta for DDIM self.num_workers = 4 # Number of workers for data loading self.pin_memory = True # Pin memory for faster transfer to GPU Sample Model Config model: base_learning_rate: 1.0e-04 target: stable_diffusion.ldm.models.diffusion.ddpm.LatentDiffusion params: linear_start: 0.00085 linear_end: 0.0120 num_timesteps_cond: 1 log_every_t: 200 timesteps: 1000 first_stage_key: \"jpg\" cond_stage_key: \"txt\" image_size: 32 channels: 4 cond_stage_trainable: false # Note: different from the one we trained before conditioning_key: crossattn monitor: val/loss_simple_ema scale_factor: 0.18215 use_ema: False scheduler_config: # 10000 warmup steps target: stable_diffusion.ldm.lr_scheduler.LambdaLinearScheduler params: warm_up_steps: [ 10000 ] cycle_lengths: [ 10000000000000 ] # incredibly large number to prevent corner cases f_start: [ 1.e-6 ] f_max: [ 1. ] f_min: [ 1. ] unet_config: target: stable_diffusion.ldm.modules.diffusionmodules.openaimodel.UNetModel params: image_size: 32 # unused in_channels: 4 out_channels: 4 model_channels: 320 attention_resolutions: [ 4, 2, 1 ] num_res_blocks: 2 channel_mult: [ 1, 2, 4, 4 ] num_heads: 8 use_spatial_transformer: True transformer_depth: 1 context_dim: 768 use_checkpoint: True legacy: False first_stage_config: target: stable_diffusion.ldm.models.autoencoder.AutoencoderKL params: embed_dim: 4 monitor: val/rec_loss ddconfig: double_z: true z_channels: 4 resolution: 256 in_channels: 3 out_ch: 3 ch: 128 ch_mult: - 1 - 2 - 4 - 4 num_res_blocks: 2 attn_resolutions: [] dropout: 0.0 lossconfig: target: torch.nn.Identity cond_stage_config: target: stable_diffusion.ldm.modules.encoders.modules.FrozenCLIPEmbedder Description of arguments being used in train_config class These are the configuration used for training a Stable Diffusion model using the ESD (Erase Stable Diffusion) method. It defines various parameters related to training, model setup, dataset handling, and output configuration. Below is a detailed description of each section and parameter: Training Parameters These parameters control the fine-tuning process, including the method of training, guidance scales, learning rate, and iteration settings. train_method: Specifies the method of training to decide which parts of the model to update. Type: str Choices: noxattn, selfattn, xattn, full, notime, xlayer, selflayer Example: xattn start_guidance: Guidance scale for generating initial images during training. Affects the diversity of the training set. Type: float Example: 0.1 negative_guidance: Guidance scale for erasing the target concept during training. Type: float Example: 0.0 iterations: Number of training iterations (similar to epochs). Type: int Example: 1 lr: Learning rate used by the optimizer for fine-tuning. Type: float Example: 5e-5 image_size: Size of images used during training and sampling (in pixels). Type: int Example: 512 ddim_steps: Number of diffusion steps used in the DDIM sampling process. Type: int Example: 50 Model Configuration These parameters specify the Stable Diffusion model checkpoint and configuration file. model_config_path: Path to the YAML file defining the model architecture and parameters. Type: str Example: mu/algorithms/esd/configs/model_config.yaml ckpt_path: Path to the finetuned Stable Diffusion model checkpoint. Type: str Example: '../models/compvis/style50/compvis.ckpt' Dataset Configuration These parameters define the dataset type and template for training, specifying whether to focus on objects, styles, or inappropriate content. dataset_type: Type of dataset used for training. Use generic as type if you want to use your own dataset. Type: str Choices: unlearncanvas, i2p, generic Example: unlearncanvas template: Type of concept or style to erase during training. Type: str Choices: object, style, i2p Example: style template_name: Specific name of the object or style to erase (e.g., \"Abstractionism\"). Type: str Example Choices: Abstractionism, self-harm Example: Abstractionism Output Configuration These parameters control where the outputs of the training process, such as fine-tuned models, are stored. output_dir: Directory where the fine-tuned model and training results will be saved. Type: str Example: outputs/esd/finetuned_models separator: Separator character used to handle multiple prompts during training. If set to null, no special handling occurs. Type: str or null Example: null Device Configuration These parameters define the compute resources for training. devices: Specifies the CUDA devices used for training. Provide a comma-separated list of device IDs. Type: str Example: 0,1 use_sample: Boolean flag indicating whether to use a sample dataset for testing or debugging. Type: bool Example: True","title":"Configs"},{"location":"unlearn/configs/esd/#sample-train-config","text":"class ESDConfig(BaseConfig): def __init__(self, **kwargs): # Training parameters self.train_method = \"xattn\" # Choices: [\"noxattn\", \"selfattn\", \"xattn\", \"full\", \"notime\", \"xlayer\", \"selflayer\"] self.start_guidance = ( 0.1 # Optional: guidance of start image (previously alpha) ) self.negative_guidance = 0.0 # Optional: guidance of negative training self.iterations = 1 # Optional: iterations used to train (previously epochs) self.lr = 1e-5 # Optional: learning rate self.image_size = 512 # Optional: image size used to train self.ddim_steps = 50 # Optional: DDIM steps of inference # Model configuration self.model_config_path = current_dir / \"model_config.yaml\" self.ckpt_path = \"models/compvis/style50/compvis.ckpt\" # Checkpoint path for Stable Diffusion # Dataset directories self.raw_dataset_dir = \"data/quick-canvas-dataset/sample\" self.processed_dataset_dir = \"mu/algorithms/esd/data\" self.dataset_type = \"unlearncanvas\" # Choices: ['unlearncanvas', 'i2p'] self.template = \"style\" # Choices: ['object', 'style', 'i2p'] self.template_name = ( \"Abstractionism\" # Choices: ['self-harm', 'Abstractionism'] ) # Output configurations self.output_dir = \"outputs/esd/finetuned_models\" self.separator = None # Device configuration self.devices = \"0,0\" self.use_sample = True # For backward compatibility self.interpolation = \"bicubic\" # Interpolation method self.ddim_eta = 0.0 # Eta for DDIM self.num_workers = 4 # Number of workers for data loading self.pin_memory = True # Pin memory for faster transfer to GPU","title":"Sample Train Config"},{"location":"unlearn/configs/esd/#sample-model-config","text":"model: base_learning_rate: 1.0e-04 target: stable_diffusion.ldm.models.diffusion.ddpm.LatentDiffusion params: linear_start: 0.00085 linear_end: 0.0120 num_timesteps_cond: 1 log_every_t: 200 timesteps: 1000 first_stage_key: \"jpg\" cond_stage_key: \"txt\" image_size: 32 channels: 4 cond_stage_trainable: false # Note: different from the one we trained before conditioning_key: crossattn monitor: val/loss_simple_ema scale_factor: 0.18215 use_ema: False scheduler_config: # 10000 warmup steps target: stable_diffusion.ldm.lr_scheduler.LambdaLinearScheduler params: warm_up_steps: [ 10000 ] cycle_lengths: [ 10000000000000 ] # incredibly large number to prevent corner cases f_start: [ 1.e-6 ] f_max: [ 1. ] f_min: [ 1. ] unet_config: target: stable_diffusion.ldm.modules.diffusionmodules.openaimodel.UNetModel params: image_size: 32 # unused in_channels: 4 out_channels: 4 model_channels: 320 attention_resolutions: [ 4, 2, 1 ] num_res_blocks: 2 channel_mult: [ 1, 2, 4, 4 ] num_heads: 8 use_spatial_transformer: True transformer_depth: 1 context_dim: 768 use_checkpoint: True legacy: False first_stage_config: target: stable_diffusion.ldm.models.autoencoder.AutoencoderKL params: embed_dim: 4 monitor: val/rec_loss ddconfig: double_z: true z_channels: 4 resolution: 256 in_channels: 3 out_ch: 3 ch: 128 ch_mult: - 1 - 2 - 4 - 4 num_res_blocks: 2 attn_resolutions: [] dropout: 0.0 lossconfig: target: torch.nn.Identity cond_stage_config: target: stable_diffusion.ldm.modules.encoders.modules.FrozenCLIPEmbedder","title":"Sample Model Config"},{"location":"unlearn/configs/esd/#description-of-arguments-being-used-in-train_config-class","text":"These are the configuration used for training a Stable Diffusion model using the ESD (Erase Stable Diffusion) method. It defines various parameters related to training, model setup, dataset handling, and output configuration. Below is a detailed description of each section and parameter: Training Parameters These parameters control the fine-tuning process, including the method of training, guidance scales, learning rate, and iteration settings. train_method: Specifies the method of training to decide which parts of the model to update. Type: str Choices: noxattn, selfattn, xattn, full, notime, xlayer, selflayer Example: xattn start_guidance: Guidance scale for generating initial images during training. Affects the diversity of the training set. Type: float Example: 0.1 negative_guidance: Guidance scale for erasing the target concept during training. Type: float Example: 0.0 iterations: Number of training iterations (similar to epochs). Type: int Example: 1 lr: Learning rate used by the optimizer for fine-tuning. Type: float Example: 5e-5 image_size: Size of images used during training and sampling (in pixels). Type: int Example: 512 ddim_steps: Number of diffusion steps used in the DDIM sampling process. Type: int Example: 50 Model Configuration These parameters specify the Stable Diffusion model checkpoint and configuration file. model_config_path: Path to the YAML file defining the model architecture and parameters. Type: str Example: mu/algorithms/esd/configs/model_config.yaml ckpt_path: Path to the finetuned Stable Diffusion model checkpoint. Type: str Example: '../models/compvis/style50/compvis.ckpt' Dataset Configuration These parameters define the dataset type and template for training, specifying whether to focus on objects, styles, or inappropriate content. dataset_type: Type of dataset used for training. Use generic as type if you want to use your own dataset. Type: str Choices: unlearncanvas, i2p, generic Example: unlearncanvas template: Type of concept or style to erase during training. Type: str Choices: object, style, i2p Example: style template_name: Specific name of the object or style to erase (e.g., \"Abstractionism\"). Type: str Example Choices: Abstractionism, self-harm Example: Abstractionism Output Configuration These parameters control where the outputs of the training process, such as fine-tuned models, are stored. output_dir: Directory where the fine-tuned model and training results will be saved. Type: str Example: outputs/esd/finetuned_models separator: Separator character used to handle multiple prompts during training. If set to null, no special handling occurs. Type: str or null Example: null Device Configuration These parameters define the compute resources for training. devices: Specifies the CUDA devices used for training. Provide a comma-separated list of device IDs. Type: str Example: 0,1 use_sample: Boolean flag indicating whether to use a sample dataset for testing or debugging. Type: bool Example: True","title":"Description of arguments being used in train_config class"},{"location":"unlearn/configs/forget_me_not/","text":"Train Ti Config class ForgetMeNotTiConfig(BaseConfig): \"\"\" Configuration class for the Forget-Me-Not textual inversion training. Mirrors the fields from the second YAML snippet. \"\"\" def __init__(self, **kwargs): # Model checkpoint path self.ckpt_path = \"models/diffuser/style50\" # Dataset directories self.raw_dataset_dir = \"data/quick-canvas-dataset/sample\" self.processed_dataset_dir = \"mu/algorithms/forget_me_not/data\" self.dataset_type = \"unlearncanvas\" self.template = \"style\" self.template_name = \"Abstractionism\" self.use_sample = True # Use the sample dataset for training # Training configuration self.initializer_tokens = self.template_name self.steps = 10 self.lr = 1e-4 self.weight_decay_ti = 0.1 self.seed = 42 self.placeholder_tokens = \"<s1>|<s2>|<s3>|<s4>\" self.placeholder_token_at_data = \"<s>|<s1><s2><s3><s4>\" self.gradient_checkpointing = False self.scale_lr = False self.gradient_accumulation_steps = 1 self.train_batch_size = 1 self.lr_warmup_steps = 100 # Output configuration self.output_dir = \"outputs/forget_me_not/ti_models\" # Device configuration self.devices = \"0\" # CUDA devices to train on (comma-separated) # Additional configurations self.tokenizer_name = \"default_tokenizer\" self.instance_prompt = \"default_prompt\" self.concept_keyword = \"default_keyword\" self.lr_scheduler = \"linear\" self.prior_generation_precision = \"fp32\" self.local_rank = 0 self.class_prompt = \"default_class_prompt\" self.num_class_images = 100 self.dataloader_num_workers = 4 self.center_crop = True self.prior_loss_weight = 0.1 Train Attn config class ForgetMeNotAttnConfig(BaseConfig): \"\"\" This class encapsulates the training configuration for the 'Forget-Me-Not' TI approach. It mirrors the fields specified in the YAML-like config snippet. \"\"\" def __init__(self, **kwargs): # Model and checkpoint paths self.ckpt_path = \"models/diffuser/style50\" # Dataset directories and setup self.raw_dataset_dir = \"data/quick-canvas-dataset/sample\" self.processed_dataset_dir = \"mu/algorithms/forget_me_not/data\" self.dataset_type = \"unlearncanvas\" self.template = \"style\" self.template_name = \"Abstractionism\" self.use_sample = True # Use the sample dataset for training # Textual Inversion config self.use_ti = True self.ti_weights_path = \"outputs/forget_me_not/finetuned_models/Abstractionism/step_inv_10.safetensors\" self.initializer_tokens = self.template_name self.placeholder_tokens = \"<s1>|<s2>|<s3>|<s4>\" # Training configuration self.mixed_precision = None # or \"fp16\", if desired self.gradient_accumulation_steps = 1 self.train_text_encoder = False self.enable_xformers_memory_efficient_attention = False self.gradient_checkpointing = False self.allow_tf32 = False self.scale_lr = False self.train_batch_size = 1 self.use_8bit_adam = False self.adam_beta1 = 0.9 self.adam_beta2 = 0.999 self.adam_weight_decay = 0.01 self.adam_epsilon = 1.0e-08 self.size = 512 self.with_prior_preservation = False self.num_train_epochs = 1 self.lr_warmup_steps = 0 self.lr_num_cycles = 1 self.lr_power = 1.0 self.max_steps = 2 # originally \"max-steps\" in config self.no_real_image = False self.max_grad_norm = 1.0 self.checkpointing_steps = 500 self.set_grads_to_none = False self.lr = 5e-5 # Output configurations self.output_dir = \"outputs/forget_me_not/finetuned_models/Abstractionism\" # Device configuration self.devices = \"0\" # CUDA devices to train on (comma-separated) self.only_xa = True # originally \"only-xa\" in config # Additional 'Forget-Me-Not' parameters self.perform_inversion = True self.continue_inversion = True self.continue_inversion_lr = 0.0001 self.learning_rate_ti = 0.001 self.learning_rate_unet = 0.0003 self.learning_rate_text = 0.0003 self.lr_scheduler = \"constant\" self.lr_scheduler_lora = \"linear\" self.lr_warmup_steps_lora = 0 self.prior_loss_weight = 1.0 self.weight_decay_lora = 0.001 self.use_face_segmentation_condition = False self.max_train_steps_ti = 500 self.max_train_steps_tuning = 1000 self.save_steps = 100 self.class_data_dir = None self.stochastic_attribute = None self.class_prompt = None self.num_class_images = 100 self.resolution = 512 self.color_jitter = False self.sample_batch_size = 1 self.lora_rank = 4 self.clip_ti_decay = True Description of Arguments in train_ti_config.yaml Pretrained Model ckpt_path : File path to the pretrained model's checkpoint file. Dataset raw_dataset_dir : Directory containing the original dataset organized by themes and classes. processed_dataset_dir : Directory where the processed datasets will be saved. dataset_type : Type of dataset to use (e.g., unlearncanvas ). Use generic as type if you want to use your own dataset. Valid choices are unlearncanvas , i2p and generic . template : Type of template to use (e.g., style ). template_name : Name of the template, defining the style or theme (e.g., Abstractionism ). use_sample : Boolean indicating whether to use the sample dataset for training. Training Configuration initializer_tokens : Tokens used to initialize the training process, referencing the template name. steps : Number of training steps. lr : Learning rate for the training optimizer. weight_decay_ti : Weight decay for Text Inversion training. seed : Random seed for reproducibility. placeholder_tokens : Tokens used as placeholders during training. placeholder_token_at_data : Placeholders used in the dataset for Text Inversion training. gradient_checkpointing : Boolean to enable or disable gradient checkpointing. scale_lr : Boolean indicating whether to scale the learning rate based on batch size. gradient_accumulation_steps : Number of steps to accumulate gradients before updating weights. train_batch_size : Batch size for training. lr_warmup_steps : Number of steps for linear warmup of the learning rate. Output Configuration output_dir : Directory path to save training results, including models and logs. Device Configuration devices : CUDA devices to train on (comma-separated). Description of Arguments in train_attn_config.yaml Key Parameters Pretrained Model ckpt_path : File path to the pretrained model's checkpoint file. Dataset raw_dataset_dir : Directory containing the original dataset organized by themes and classes. processed_dataset_dir : Directory where the processed datasets will be saved. dataset_type : Type of dataset to use (e.g., unlearncanvas ). template : Type of template to use (e.g., style ). template_name : Name of the template, defining the style or theme (e.g., Abstractionism ). use_sample : Boolean indicating whether to use the sample dataset for training. Text Inversion use_ti : Boolean indicating whether to use Text Inversion weights. ti_weights_path : File path to the Text Inversion model weights. Tokens initializer_tokens : Tokens used to initialize the training process, referencing the template name. placeholder_tokens : Tokens used as placeholders during training. Training Configuration mixed_precision : Precision type to use during training (e.g., fp16 or fp32 ). gradient_accumulation_steps : Number of steps to accumulate gradients before updating weights. train_text_encoder : Boolean to enable or disable training of the text encoder. enable_xformers_memory_efficient_attention : Boolean to enable memory-efficient attention mechanisms. gradient_checkpointing : Boolean to enable or disable gradient checkpointing. allow_tf32 : Boolean to allow TensorFloat-32 computation for faster training. scale_lr : Boolean indicating whether to scale the learning rate based on batch size. train_batch_size : Batch size for training. use_8bit_adam : Boolean to enable or disable 8-bit Adam optimizer. adam_beta1 : Beta1 parameter for the Adam optimizer. adam_beta2 : Beta2 parameter for the Adam optimizer. adam_weight_decay : Weight decay for the Adam optimizer. adam_epsilon : Epsilon value for the Adam optimizer. size : Image resolution size for training. with_prior_preservation : Boolean indicating whether to use prior preservation during training. num_train_epochs : Number of training epochs. lr_warmup_steps : Number of steps for linear warmup of the learning rate. lr_num_cycles : Number of cycles for learning rate scheduling. lr_power : Exponent to control the shape of the learning rate curve. max-steps : Maximum number of training steps. no_real_image : Boolean to skip using real images in training. max_grad_norm : Maximum norm for gradient clipping. checkpointing_steps : Number of steps between model checkpoints. set_grads_to_none : Boolean to set gradients to None instead of zeroing them out. lr : Learning rate for the training optimizer. Output Configuration output_dir : Directory path to save training results, including models and logs. Device Configuration devices : CUDA devices to train on (comma-separated). Miscellaneous only-xa : Boolean to enable additional configurations specific to the XA pipeline.","title":"Configs"},{"location":"unlearn/configs/forget_me_not/#train-ti-config","text":"class ForgetMeNotTiConfig(BaseConfig): \"\"\" Configuration class for the Forget-Me-Not textual inversion training. Mirrors the fields from the second YAML snippet. \"\"\" def __init__(self, **kwargs): # Model checkpoint path self.ckpt_path = \"models/diffuser/style50\" # Dataset directories self.raw_dataset_dir = \"data/quick-canvas-dataset/sample\" self.processed_dataset_dir = \"mu/algorithms/forget_me_not/data\" self.dataset_type = \"unlearncanvas\" self.template = \"style\" self.template_name = \"Abstractionism\" self.use_sample = True # Use the sample dataset for training # Training configuration self.initializer_tokens = self.template_name self.steps = 10 self.lr = 1e-4 self.weight_decay_ti = 0.1 self.seed = 42 self.placeholder_tokens = \"<s1>|<s2>|<s3>|<s4>\" self.placeholder_token_at_data = \"<s>|<s1><s2><s3><s4>\" self.gradient_checkpointing = False self.scale_lr = False self.gradient_accumulation_steps = 1 self.train_batch_size = 1 self.lr_warmup_steps = 100 # Output configuration self.output_dir = \"outputs/forget_me_not/ti_models\" # Device configuration self.devices = \"0\" # CUDA devices to train on (comma-separated) # Additional configurations self.tokenizer_name = \"default_tokenizer\" self.instance_prompt = \"default_prompt\" self.concept_keyword = \"default_keyword\" self.lr_scheduler = \"linear\" self.prior_generation_precision = \"fp32\" self.local_rank = 0 self.class_prompt = \"default_class_prompt\" self.num_class_images = 100 self.dataloader_num_workers = 4 self.center_crop = True self.prior_loss_weight = 0.1","title":"Train Ti Config"},{"location":"unlearn/configs/forget_me_not/#train-attn-config","text":"class ForgetMeNotAttnConfig(BaseConfig): \"\"\" This class encapsulates the training configuration for the 'Forget-Me-Not' TI approach. It mirrors the fields specified in the YAML-like config snippet. \"\"\" def __init__(self, **kwargs): # Model and checkpoint paths self.ckpt_path = \"models/diffuser/style50\" # Dataset directories and setup self.raw_dataset_dir = \"data/quick-canvas-dataset/sample\" self.processed_dataset_dir = \"mu/algorithms/forget_me_not/data\" self.dataset_type = \"unlearncanvas\" self.template = \"style\" self.template_name = \"Abstractionism\" self.use_sample = True # Use the sample dataset for training # Textual Inversion config self.use_ti = True self.ti_weights_path = \"outputs/forget_me_not/finetuned_models/Abstractionism/step_inv_10.safetensors\" self.initializer_tokens = self.template_name self.placeholder_tokens = \"<s1>|<s2>|<s3>|<s4>\" # Training configuration self.mixed_precision = None # or \"fp16\", if desired self.gradient_accumulation_steps = 1 self.train_text_encoder = False self.enable_xformers_memory_efficient_attention = False self.gradient_checkpointing = False self.allow_tf32 = False self.scale_lr = False self.train_batch_size = 1 self.use_8bit_adam = False self.adam_beta1 = 0.9 self.adam_beta2 = 0.999 self.adam_weight_decay = 0.01 self.adam_epsilon = 1.0e-08 self.size = 512 self.with_prior_preservation = False self.num_train_epochs = 1 self.lr_warmup_steps = 0 self.lr_num_cycles = 1 self.lr_power = 1.0 self.max_steps = 2 # originally \"max-steps\" in config self.no_real_image = False self.max_grad_norm = 1.0 self.checkpointing_steps = 500 self.set_grads_to_none = False self.lr = 5e-5 # Output configurations self.output_dir = \"outputs/forget_me_not/finetuned_models/Abstractionism\" # Device configuration self.devices = \"0\" # CUDA devices to train on (comma-separated) self.only_xa = True # originally \"only-xa\" in config # Additional 'Forget-Me-Not' parameters self.perform_inversion = True self.continue_inversion = True self.continue_inversion_lr = 0.0001 self.learning_rate_ti = 0.001 self.learning_rate_unet = 0.0003 self.learning_rate_text = 0.0003 self.lr_scheduler = \"constant\" self.lr_scheduler_lora = \"linear\" self.lr_warmup_steps_lora = 0 self.prior_loss_weight = 1.0 self.weight_decay_lora = 0.001 self.use_face_segmentation_condition = False self.max_train_steps_ti = 500 self.max_train_steps_tuning = 1000 self.save_steps = 100 self.class_data_dir = None self.stochastic_attribute = None self.class_prompt = None self.num_class_images = 100 self.resolution = 512 self.color_jitter = False self.sample_batch_size = 1 self.lora_rank = 4 self.clip_ti_decay = True","title":"Train Attn config"},{"location":"unlearn/configs/forget_me_not/#description-of-arguments-in-train_ti_configyaml","text":"Pretrained Model ckpt_path : File path to the pretrained model's checkpoint file. Dataset raw_dataset_dir : Directory containing the original dataset organized by themes and classes. processed_dataset_dir : Directory where the processed datasets will be saved. dataset_type : Type of dataset to use (e.g., unlearncanvas ). Use generic as type if you want to use your own dataset. Valid choices are unlearncanvas , i2p and generic . template : Type of template to use (e.g., style ). template_name : Name of the template, defining the style or theme (e.g., Abstractionism ). use_sample : Boolean indicating whether to use the sample dataset for training. Training Configuration initializer_tokens : Tokens used to initialize the training process, referencing the template name. steps : Number of training steps. lr : Learning rate for the training optimizer. weight_decay_ti : Weight decay for Text Inversion training. seed : Random seed for reproducibility. placeholder_tokens : Tokens used as placeholders during training. placeholder_token_at_data : Placeholders used in the dataset for Text Inversion training. gradient_checkpointing : Boolean to enable or disable gradient checkpointing. scale_lr : Boolean indicating whether to scale the learning rate based on batch size. gradient_accumulation_steps : Number of steps to accumulate gradients before updating weights. train_batch_size : Batch size for training. lr_warmup_steps : Number of steps for linear warmup of the learning rate. Output Configuration output_dir : Directory path to save training results, including models and logs. Device Configuration devices : CUDA devices to train on (comma-separated).","title":"Description of Arguments in train_ti_config.yaml"},{"location":"unlearn/configs/forget_me_not/#description-of-arguments-in-train_attn_configyaml","text":"","title":"Description of Arguments in train_attn_config.yaml"},{"location":"unlearn/configs/forget_me_not/#key-parameters","text":"Pretrained Model ckpt_path : File path to the pretrained model's checkpoint file. Dataset raw_dataset_dir : Directory containing the original dataset organized by themes and classes. processed_dataset_dir : Directory where the processed datasets will be saved. dataset_type : Type of dataset to use (e.g., unlearncanvas ). template : Type of template to use (e.g., style ). template_name : Name of the template, defining the style or theme (e.g., Abstractionism ). use_sample : Boolean indicating whether to use the sample dataset for training. Text Inversion use_ti : Boolean indicating whether to use Text Inversion weights. ti_weights_path : File path to the Text Inversion model weights. Tokens initializer_tokens : Tokens used to initialize the training process, referencing the template name. placeholder_tokens : Tokens used as placeholders during training. Training Configuration mixed_precision : Precision type to use during training (e.g., fp16 or fp32 ). gradient_accumulation_steps : Number of steps to accumulate gradients before updating weights. train_text_encoder : Boolean to enable or disable training of the text encoder. enable_xformers_memory_efficient_attention : Boolean to enable memory-efficient attention mechanisms. gradient_checkpointing : Boolean to enable or disable gradient checkpointing. allow_tf32 : Boolean to allow TensorFloat-32 computation for faster training. scale_lr : Boolean indicating whether to scale the learning rate based on batch size. train_batch_size : Batch size for training. use_8bit_adam : Boolean to enable or disable 8-bit Adam optimizer. adam_beta1 : Beta1 parameter for the Adam optimizer. adam_beta2 : Beta2 parameter for the Adam optimizer. adam_weight_decay : Weight decay for the Adam optimizer. adam_epsilon : Epsilon value for the Adam optimizer. size : Image resolution size for training. with_prior_preservation : Boolean indicating whether to use prior preservation during training. num_train_epochs : Number of training epochs. lr_warmup_steps : Number of steps for linear warmup of the learning rate. lr_num_cycles : Number of cycles for learning rate scheduling. lr_power : Exponent to control the shape of the learning rate curve. max-steps : Maximum number of training steps. no_real_image : Boolean to skip using real images in training. max_grad_norm : Maximum norm for gradient clipping. checkpointing_steps : Number of steps between model checkpoints. set_grads_to_none : Boolean to set gradients to None instead of zeroing them out. lr : Learning rate for the training optimizer. Output Configuration output_dir : Directory path to save training results, including models and logs. Device Configuration devices : CUDA devices to train on (comma-separated). Miscellaneous only-xa : Boolean to enable additional configurations specific to the XA pipeline.","title":"Key Parameters"},{"location":"unlearn/configs/saliency/","text":"Train Config class SaliencyUnlearningConfig(BaseConfig): def __init__(self, **kwargs): # Model configuration self.alpha = 0.1 # Alpha value for training self.epochs = 1 # Number of epochs for training self.train_method = ( \"xattn\" # Attention method: [\"noxattn\", \"selfattn\", \"xattn\", \"full\"] ) self.ckpt_path = \"models/compvis/style50/compvis.ckpt\" # Path to the checkpoint self.model_config_path = current_dir / \"model_config.yaml\" # Dataset directories self.raw_dataset_dir = ( \"data/quick-canvas-dataset/sample\" # Path to the raw dataset ) self.processed_dataset_dir = ( \"mu/algorithms/saliency_unlearning/data\" # Path to the processed dataset ) self.dataset_type = \"unlearncanvas\" # Type of the dataset self.template = \"style\" # Template type for training self.template_name = \"Abstractionism\" # Name of the template # Directory Configuration self.output_dir = \"outputs/saliency_unlearning/finetuned_models\" # Directory for output models self.mask_path = ( \"outputs/saliency_unlearning/masks/0.5.pt\" # Path to the mask file ) # Training configuration self.devices = \"0\" # CUDA devices for training (comma-separated) self.use_sample = True # Whether to use a sample dataset for training # Guidance and training parameters self.start_guidance = 0.5 # Start guidance for training self.negative_guidance = 0.5 # Negative guidance for training self.ddim_steps = 50 # Number of DDIM steps for sampling # Update properties based on provided kwargs for key, value in kwargs.items(): setattr(self, key, value) Model Config # Model Configuration alpha: 0.1 epochs: 1 train_method: \"xattn\" # Choices: [\"noxattn\", \"selfattn\", \"xattn\", \"full\" ] ckpt_path: \"models/compvis/style50/compvis.ckpt\" # Checkpoint path for Stable Diffusion model_config_path: \"mu/algorithms/saliency_unlearning/configs/model_config.yaml\" # Config path for Stable Diffusion # Dataset directories raw_dataset_dir: \"data/quick-canvas-dataset/sample\" processed_dataset_dir: \"mu/algorithms/saliency_unlearning/data\" dataset_type : \"unlearncanvas\" template : \"style\" template_name : \"Abstractionism\" # Directory Configuration output_dir: \"outputs/saliency_unlearning/finetuned_models\" # Output directory to save results mask_path: \"outputs/saliency_unlearning/masks/0.5.pt\" # Output directory to save results # Training Configuration devices: \"0\" # CUDA devices to train on (comma-separated) use_sample: true start_guidance: 0.5 negative_guidance: 0.5 ddim_steps: 50 Mask Config # Model Configuration c_guidance: 7.5 batch_size: 4 num_timesteps: 1000 image_size: 512 model_config_path: \"mu/algorithms/saliency_unlearning/configs/model_config.yaml\" # ckpt_path: \"models/compvis/style50/compvis.ckpt\" # Checkpoint path for Stable Diffusion ckpt_path: \"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\" # Dataset directories # raw_dataset_dir: \"data/quick-canvas-dataset/sample\" raw_dataset_dir: \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample\" processed_dataset_dir: \"mu/algorithms/saliency_unlearning/data\" dataset_type : \"unlearncanvas\" template : \"style\" template_name : \"Abstractionism\" threshold : 0.5 # Directory Configuration output_dir: \"outputs/saliency_unlearning/masks\" # Output directory to save results # Training Configuration lr: 0.00001 devices: \"0\" # CUDA devices to train on (comma-separated) use_sample: true Description of configs used to generate mask: Model Configuration These parameters specify settings for the Stable Diffusion model and guidance configurations. c_guidance: Guidance scale used during loss computation in the model. Higher values may emphasize certain features in mask generation. Type: float Example: 7.5 batch_size: Number of images processed in a single batch. Type: int Example: 4 ckpt_path: Path to the model checkpoint file for Stable Diffusion. Type: str Example: /path/to/compvis.ckpt model_config_path: Path to the model configuration YAML file for Stable Diffusion. Type: str Example: /path/to/model_config.yaml num_timesteps: Number of timesteps used in the diffusion process. Type: int Example: 1000 image_size: Size of the input images used for training and mask generation (in pixels). Type: int Example: 512 Dataset Configuration These parameters define the dataset paths and settings for mask generation. raw_dataset_dir: Path to the directory containing the original dataset, organized by themes and classes. Type: str Example: /path/to/raw/dataset processed_dataset_dir: Path to the directory where processed datasets will be saved after mask generation. Type: str Example: /path/to/processed/dataset dataset_type: Type of dataset being used. Choices: unlearncanvas, i2p Type: str Example: i2p template: Type of template for mask generation. Choices: object, style, i2p Type: str Example: style template_name: Specific template name for the mask generation process. Example Choices: self-harm, Abstractionism Type: str Example: Abstractionism threshold: Threshold value for mask generation to filter salient regions. Type: float Example: 0.5 Output Configuration These parameters specify the directory where the results are saved. output_dir: Directory where the generated masks will be saved. Type: str Example: outputs/saliency_unlearning/masks Training Configuration These parameters control the training process for mask generation. lr: Learning rate used for training the masking algorithm. Type: float Example: 0.00001 devices: CUDA devices used for training, specified as a comma-separated list. Type: str Example: 0 use_sample: Flag indicating whether to use a sample dataset for training and mask generation. Type: bool Example: True Description of Arguments used to train saliency unlearning. The following configs are used to fine-tune the Stable Diffusion model to perform saliency-based unlearning. This script relies on a configuration class SaliencyUnlearningConfig and supports additional runtime arguments for further customization. Below is a detailed description of each argument: General Arguments alpha: Guidance scale used to balance the loss components during training. Type: float Example: 0.1 epochs: Number of epochs to train the model. Type: int Example: 5 train_method: Specifies the training method or strategy to be used. Choices: noxattn, selfattn, xattn, full, notime, xlayer, selflayer Type: str Example: noxattn model_config_path: Path to the model configuration YAML file for Stable Diffusion. Type: str Example: 'mu/algorithms/saliency_unlearning/configs/model_config.yaml' Dataset Arguments raw_dataset_dir: Path to the directory containing the raw dataset, organized by themes and classes. Type: str Example: 'path/raw_dataset/' processed_dataset_dir: Path to the directory where the processed dataset will be saved. Type: str Example: 'path/processed_dataset_dir' dataset_type: Specifies the type of dataset to use for training. Use generic as type if you want to use your own dataset. Choices: unlearncanvas, i2p, generic Type: str Example: i2p template: Specifies the template type for training. Choices: object, style, i2p Type: str Example: style template_name: Name of the specific template used for training. Example Choices: self-harm, Abstractionism Type: str Example: Abstractionism Output Arguments output_dir: Directory where the fine-tuned model and training outputs will be saved. Type: str Example: 'output/folder_name' mask_path: Path to the saliency mask file used during training. Type: str Example:","title":"Configs"},{"location":"unlearn/configs/saliency/#train-config","text":"class SaliencyUnlearningConfig(BaseConfig): def __init__(self, **kwargs): # Model configuration self.alpha = 0.1 # Alpha value for training self.epochs = 1 # Number of epochs for training self.train_method = ( \"xattn\" # Attention method: [\"noxattn\", \"selfattn\", \"xattn\", \"full\"] ) self.ckpt_path = \"models/compvis/style50/compvis.ckpt\" # Path to the checkpoint self.model_config_path = current_dir / \"model_config.yaml\" # Dataset directories self.raw_dataset_dir = ( \"data/quick-canvas-dataset/sample\" # Path to the raw dataset ) self.processed_dataset_dir = ( \"mu/algorithms/saliency_unlearning/data\" # Path to the processed dataset ) self.dataset_type = \"unlearncanvas\" # Type of the dataset self.template = \"style\" # Template type for training self.template_name = \"Abstractionism\" # Name of the template # Directory Configuration self.output_dir = \"outputs/saliency_unlearning/finetuned_models\" # Directory for output models self.mask_path = ( \"outputs/saliency_unlearning/masks/0.5.pt\" # Path to the mask file ) # Training configuration self.devices = \"0\" # CUDA devices for training (comma-separated) self.use_sample = True # Whether to use a sample dataset for training # Guidance and training parameters self.start_guidance = 0.5 # Start guidance for training self.negative_guidance = 0.5 # Negative guidance for training self.ddim_steps = 50 # Number of DDIM steps for sampling # Update properties based on provided kwargs for key, value in kwargs.items(): setattr(self, key, value)","title":"Train Config"},{"location":"unlearn/configs/saliency/#model-config","text":"# Model Configuration alpha: 0.1 epochs: 1 train_method: \"xattn\" # Choices: [\"noxattn\", \"selfattn\", \"xattn\", \"full\" ] ckpt_path: \"models/compvis/style50/compvis.ckpt\" # Checkpoint path for Stable Diffusion model_config_path: \"mu/algorithms/saliency_unlearning/configs/model_config.yaml\" # Config path for Stable Diffusion # Dataset directories raw_dataset_dir: \"data/quick-canvas-dataset/sample\" processed_dataset_dir: \"mu/algorithms/saliency_unlearning/data\" dataset_type : \"unlearncanvas\" template : \"style\" template_name : \"Abstractionism\" # Directory Configuration output_dir: \"outputs/saliency_unlearning/finetuned_models\" # Output directory to save results mask_path: \"outputs/saliency_unlearning/masks/0.5.pt\" # Output directory to save results # Training Configuration devices: \"0\" # CUDA devices to train on (comma-separated) use_sample: true start_guidance: 0.5 negative_guidance: 0.5 ddim_steps: 50","title":"Model Config"},{"location":"unlearn/configs/saliency/#mask-config","text":"# Model Configuration c_guidance: 7.5 batch_size: 4 num_timesteps: 1000 image_size: 512 model_config_path: \"mu/algorithms/saliency_unlearning/configs/model_config.yaml\" # ckpt_path: \"models/compvis/style50/compvis.ckpt\" # Checkpoint path for Stable Diffusion ckpt_path: \"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\" # Dataset directories # raw_dataset_dir: \"data/quick-canvas-dataset/sample\" raw_dataset_dir: \"/home/ubuntu/Projects/msu_unlearningalgorithm/data/quick-canvas-dataset/sample\" processed_dataset_dir: \"mu/algorithms/saliency_unlearning/data\" dataset_type : \"unlearncanvas\" template : \"style\" template_name : \"Abstractionism\" threshold : 0.5 # Directory Configuration output_dir: \"outputs/saliency_unlearning/masks\" # Output directory to save results # Training Configuration lr: 0.00001 devices: \"0\" # CUDA devices to train on (comma-separated) use_sample: true","title":"Mask Config"},{"location":"unlearn/configs/saliency/#description-of-configs-used-to-generate-mask","text":"Model Configuration These parameters specify settings for the Stable Diffusion model and guidance configurations. c_guidance: Guidance scale used during loss computation in the model. Higher values may emphasize certain features in mask generation. Type: float Example: 7.5 batch_size: Number of images processed in a single batch. Type: int Example: 4 ckpt_path: Path to the model checkpoint file for Stable Diffusion. Type: str Example: /path/to/compvis.ckpt model_config_path: Path to the model configuration YAML file for Stable Diffusion. Type: str Example: /path/to/model_config.yaml num_timesteps: Number of timesteps used in the diffusion process. Type: int Example: 1000 image_size: Size of the input images used for training and mask generation (in pixels). Type: int Example: 512 Dataset Configuration These parameters define the dataset paths and settings for mask generation. raw_dataset_dir: Path to the directory containing the original dataset, organized by themes and classes. Type: str Example: /path/to/raw/dataset processed_dataset_dir: Path to the directory where processed datasets will be saved after mask generation. Type: str Example: /path/to/processed/dataset dataset_type: Type of dataset being used. Choices: unlearncanvas, i2p Type: str Example: i2p template: Type of template for mask generation. Choices: object, style, i2p Type: str Example: style template_name: Specific template name for the mask generation process. Example Choices: self-harm, Abstractionism Type: str Example: Abstractionism threshold: Threshold value for mask generation to filter salient regions. Type: float Example: 0.5 Output Configuration These parameters specify the directory where the results are saved. output_dir: Directory where the generated masks will be saved. Type: str Example: outputs/saliency_unlearning/masks Training Configuration These parameters control the training process for mask generation. lr: Learning rate used for training the masking algorithm. Type: float Example: 0.00001 devices: CUDA devices used for training, specified as a comma-separated list. Type: str Example: 0 use_sample: Flag indicating whether to use a sample dataset for training and mask generation. Type: bool Example: True","title":"Description of configs used to generate mask:"},{"location":"unlearn/configs/saliency/#description-of-arguments-used-to-train-saliency-unlearning","text":"The following configs are used to fine-tune the Stable Diffusion model to perform saliency-based unlearning. This script relies on a configuration class SaliencyUnlearningConfig and supports additional runtime arguments for further customization. Below is a detailed description of each argument: General Arguments alpha: Guidance scale used to balance the loss components during training. Type: float Example: 0.1 epochs: Number of epochs to train the model. Type: int Example: 5 train_method: Specifies the training method or strategy to be used. Choices: noxattn, selfattn, xattn, full, notime, xlayer, selflayer Type: str Example: noxattn model_config_path: Path to the model configuration YAML file for Stable Diffusion. Type: str Example: 'mu/algorithms/saliency_unlearning/configs/model_config.yaml' Dataset Arguments raw_dataset_dir: Path to the directory containing the raw dataset, organized by themes and classes. Type: str Example: 'path/raw_dataset/' processed_dataset_dir: Path to the directory where the processed dataset will be saved. Type: str Example: 'path/processed_dataset_dir' dataset_type: Specifies the type of dataset to use for training. Use generic as type if you want to use your own dataset. Choices: unlearncanvas, i2p, generic Type: str Example: i2p template: Specifies the template type for training. Choices: object, style, i2p Type: str Example: style template_name: Name of the specific template used for training. Example Choices: self-harm, Abstractionism Type: str Example: Abstractionism Output Arguments output_dir: Directory where the fine-tuned model and training outputs will be saved. Type: str Example: 'output/folder_name' mask_path: Path to the saliency mask file used during training. Type: str Example:","title":"Description of Arguments used to train saliency unlearning."},{"location":"unlearn/configs/scissorhands/","text":"Train config class ScissorHandsConfig(BaseConfig): def __init__(self, **kwargs): # Training parameters self.train_method = \"xattn\" # choices: [\"noxattn\", \"selfattn\", \"xattn\", \"full\", \"notime\", \"xlayer\", \"selflayer\"] self.alpha = 0.75 # Guidance of start image used to train self.epochs = 5 # Number of training epochs # Model configuration self.model_config_path = \"mu/algorithms/scissorhands/configs/model_config.yaml\" # Config path for model self.ckpt_path = \"models/compvis/style50/compvis.ckpt\" # Checkpoint path for Stable Diffusion # Dataset directories self.raw_dataset_dir = \"data/quick-canvas-dataset/sample\" self.processed_dataset_dir = \"mu/algorithms/scissorhands/data\" self.dataset_type = \"unlearncanvas\" # Choices: [\"unlearncanvas\", \"i2p\"] self.template = \"style\" # Template to use self.template_name = \"Abstractionism\" # Template name # Output configuration self.output_dir = ( \"outputs/scissorhands/finetuned_models\" # Output directory to save results ) # Sampling and image configurations self.sparsity = 0.90 # Threshold for mask sparsity self.project = False # Whether to project self.memory_num = 1 # Number of memories to use self.prune_num = 10 # Number of pruned images # Device configuration self.devices = \"0,1\" # CUDA devices to train on (comma-separated) # Additional configurations self.use_sample = True # Use sample dataset for training # Guidance configurations self.start_guidence = 0.5 # Starting guidance factor self.negative_guidance = 0.3 # Negative guidance factor self.iterations = 1000 # Number of training iterations Model Config model: base_learning_rate: 1.0e-04 target: stable_diffusion.ldm.models.diffusion.ddpm.LatentDiffusion params: linear_start: 0.00085 linear_end: 0.0120 num_timesteps_cond: 1 log_every_t: 200 timesteps: 1000 first_stage_key: \"edited\" cond_stage_key: \"edit\" image_size: 32 channels: 4 cond_stage_trainable: false # Note: different from the one we trained before conditioning_key: crossattn monitor: val/loss_simple_ema scale_factor: 0.18215 use_ema: False scheduler_config: # 10000 warmup steps target: stable_diffusion.ldm.lr_scheduler.LambdaLinearScheduler params: warm_up_steps: [ 10000 ] cycle_lengths: [ 10000000000000 ] # incredibly large number to prevent corner cases f_start: [ 1.e-6 ] f_max: [ 1. ] f_min: [ 1. ] unet_config: target: stable_diffusion.ldm.modules.diffusionmodules.openaimodel.UNetModel params: image_size: 32 # unused in_channels: 4 out_channels: 4 model_channels: 320 attention_resolutions: [ 4, 2, 1 ] num_res_blocks: 2 channel_mult: [ 1, 2, 4, 4 ] num_heads: 8 use_spatial_transformer: True transformer_depth: 1 context_dim: 768 use_checkpoint: True legacy: False first_stage_config: target: stable_diffusion.ldm.models.autoencoder.AutoencoderKL params: embed_dim: 4 monitor: val/rec_loss ddconfig: double_z: true z_channels: 4 resolution: 256 in_channels: 3 out_ch: 3 ch: 128 ch_mult: - 1 - 2 - 4 - 4 num_res_blocks: 2 attn_resolutions: [] dropout: 0.0 lossconfig: target: torch.nn.Identity cond_stage_config: target: stable_diffusion.ldm.modules.encoders.modules.FrozenCLIPEmbedder Description of Arguments in train_config.yaml Training Parameters train_method: Specifies the method of training for concept erasure. Choices: [\"noxattn\", \"selfattn\", \"xattn\", \"full\", \"notime\", \"xlayer\", \"selflayer\"] Example: \"xattn\" alpha: Guidance strength for the starting image during training. Type: float Example: 0.1 epochs: Number of epochs to train the model. Type: int Example: 1 Model Configuration model_config_path: File path to the Stable Diffusion model configuration YAML file. type: str Example: \"/path/to/model_config.yaml\" ckpt_path: File path to the checkpoint of the Stable Diffusion model. Type: str Example: \"/path/to/model_checkpoint.ckpt\" Dataset Directories raw_dataset_dir: Directory containing the raw dataset categorized by themes or classes. Type: str Example: \"/path/to/raw_dataset\" processed_dataset_dir: Directory to save the processed dataset. Type: str Example: \"/path/to/processed_dataset\" dataset_type: Specifies the dataset type for the training process. Use generic as type if you want to use your own dataset. Choices: [\"unlearncanvas\", \"i2p\", \"generic\"] Example: \"unlearncanvas\" template: Type of template to use during training. Choices: [\"object\", \"style\", \"i2p\"] Example: \"style\" template_name: Name of the specific concept or style to be erased. Choices: [\"self-harm\", \"Abstractionism\"] Example: \"Abstractionism\" Output Configurations output_dir: Directory where the fine-tuned models and results will be saved. Type: str Example: \"outputs/erase_diff/finetuned_models\" Sampling and Image Configurations sparsity: Threshold for mask. Type: float Example: 0.99 project: Type: bool Example: false memory_num: Type: Int Example: 1 prune_num: Type: Int Example: 1 Device Configuration devices: Specifies the CUDA devices to be used for training (comma-separated). Type: str (Comma separated) Example: \"0, 1\" Additional Flags use_sample: Flag to indicate whether a sample dataset should be used for training. Type: bool Example: True","title":"Configs"},{"location":"unlearn/configs/scissorhands/#train-config","text":"class ScissorHandsConfig(BaseConfig): def __init__(self, **kwargs): # Training parameters self.train_method = \"xattn\" # choices: [\"noxattn\", \"selfattn\", \"xattn\", \"full\", \"notime\", \"xlayer\", \"selflayer\"] self.alpha = 0.75 # Guidance of start image used to train self.epochs = 5 # Number of training epochs # Model configuration self.model_config_path = \"mu/algorithms/scissorhands/configs/model_config.yaml\" # Config path for model self.ckpt_path = \"models/compvis/style50/compvis.ckpt\" # Checkpoint path for Stable Diffusion # Dataset directories self.raw_dataset_dir = \"data/quick-canvas-dataset/sample\" self.processed_dataset_dir = \"mu/algorithms/scissorhands/data\" self.dataset_type = \"unlearncanvas\" # Choices: [\"unlearncanvas\", \"i2p\"] self.template = \"style\" # Template to use self.template_name = \"Abstractionism\" # Template name # Output configuration self.output_dir = ( \"outputs/scissorhands/finetuned_models\" # Output directory to save results ) # Sampling and image configurations self.sparsity = 0.90 # Threshold for mask sparsity self.project = False # Whether to project self.memory_num = 1 # Number of memories to use self.prune_num = 10 # Number of pruned images # Device configuration self.devices = \"0,1\" # CUDA devices to train on (comma-separated) # Additional configurations self.use_sample = True # Use sample dataset for training # Guidance configurations self.start_guidence = 0.5 # Starting guidance factor self.negative_guidance = 0.3 # Negative guidance factor self.iterations = 1000 # Number of training iterations","title":"Train config"},{"location":"unlearn/configs/scissorhands/#model-config","text":"model: base_learning_rate: 1.0e-04 target: stable_diffusion.ldm.models.diffusion.ddpm.LatentDiffusion params: linear_start: 0.00085 linear_end: 0.0120 num_timesteps_cond: 1 log_every_t: 200 timesteps: 1000 first_stage_key: \"edited\" cond_stage_key: \"edit\" image_size: 32 channels: 4 cond_stage_trainable: false # Note: different from the one we trained before conditioning_key: crossattn monitor: val/loss_simple_ema scale_factor: 0.18215 use_ema: False scheduler_config: # 10000 warmup steps target: stable_diffusion.ldm.lr_scheduler.LambdaLinearScheduler params: warm_up_steps: [ 10000 ] cycle_lengths: [ 10000000000000 ] # incredibly large number to prevent corner cases f_start: [ 1.e-6 ] f_max: [ 1. ] f_min: [ 1. ] unet_config: target: stable_diffusion.ldm.modules.diffusionmodules.openaimodel.UNetModel params: image_size: 32 # unused in_channels: 4 out_channels: 4 model_channels: 320 attention_resolutions: [ 4, 2, 1 ] num_res_blocks: 2 channel_mult: [ 1, 2, 4, 4 ] num_heads: 8 use_spatial_transformer: True transformer_depth: 1 context_dim: 768 use_checkpoint: True legacy: False first_stage_config: target: stable_diffusion.ldm.models.autoencoder.AutoencoderKL params: embed_dim: 4 monitor: val/rec_loss ddconfig: double_z: true z_channels: 4 resolution: 256 in_channels: 3 out_ch: 3 ch: 128 ch_mult: - 1 - 2 - 4 - 4 num_res_blocks: 2 attn_resolutions: [] dropout: 0.0 lossconfig: target: torch.nn.Identity cond_stage_config: target: stable_diffusion.ldm.modules.encoders.modules.FrozenCLIPEmbedder","title":"Model Config"},{"location":"unlearn/configs/scissorhands/#description-of-arguments-in-train_configyaml","text":"Training Parameters train_method: Specifies the method of training for concept erasure. Choices: [\"noxattn\", \"selfattn\", \"xattn\", \"full\", \"notime\", \"xlayer\", \"selflayer\"] Example: \"xattn\" alpha: Guidance strength for the starting image during training. Type: float Example: 0.1 epochs: Number of epochs to train the model. Type: int Example: 1 Model Configuration model_config_path: File path to the Stable Diffusion model configuration YAML file. type: str Example: \"/path/to/model_config.yaml\" ckpt_path: File path to the checkpoint of the Stable Diffusion model. Type: str Example: \"/path/to/model_checkpoint.ckpt\" Dataset Directories raw_dataset_dir: Directory containing the raw dataset categorized by themes or classes. Type: str Example: \"/path/to/raw_dataset\" processed_dataset_dir: Directory to save the processed dataset. Type: str Example: \"/path/to/processed_dataset\" dataset_type: Specifies the dataset type for the training process. Use generic as type if you want to use your own dataset. Choices: [\"unlearncanvas\", \"i2p\", \"generic\"] Example: \"unlearncanvas\" template: Type of template to use during training. Choices: [\"object\", \"style\", \"i2p\"] Example: \"style\" template_name: Name of the specific concept or style to be erased. Choices: [\"self-harm\", \"Abstractionism\"] Example: \"Abstractionism\" Output Configurations output_dir: Directory where the fine-tuned models and results will be saved. Type: str Example: \"outputs/erase_diff/finetuned_models\" Sampling and Image Configurations sparsity: Threshold for mask. Type: float Example: 0.99 project: Type: bool Example: false memory_num: Type: Int Example: 1 prune_num: Type: Int Example: 1 Device Configuration devices: Specifies the CUDA devices to be used for training (comma-separated). Type: str (Comma separated) Example: \"0, 1\" Additional Flags use_sample: Flag to indicate whether a sample dataset should be used for training. Type: bool Example: True","title":"Description of Arguments in train_config.yaml"},{"location":"unlearn/configs/selective_amnesia/","text":"Train config import os from mu.core.base_config import BaseConfig from pathlib import Path current_dir = Path(__file__).parent class SelectiveAmnesiaConfig(BaseConfig): def __init__(self, **kwargs): # Training parameters self.seed = 23 # Random seed self.scale_lr = True # Flag for scaling learning rate # Model configuration self.config_path = current_dir / \"train_config.yaml\" self.model_config_path = ( current_dir / \"model_config.yaml\" ) # Config path for model self.ckpt_path = \"models/compvis/style50/compvis.ckpt\" # Checkpoint path for Stable Diffusion self.full_fisher_dict_pkl_path = \"mu/algorithms/selective_amnesia/data/full_fisher_dict.pkl\" # Path for Fisher dict # Dataset directories self.raw_dataset_dir = \"data/quick-canvas-dataset/sample\" self.processed_dataset_dir = \"mu/algorithms/selective_amnesia/data\" self.dataset_type = ( \"unlearncanvas\" # Dataset type (choices: unlearncanvas, i2p) ) self.template = \"style\" # Template to use self.template_name = \"Abstractionism\" # Template name self.replay_prompt_path = \"mu/algorithms/selective_amnesia/data/fim_prompts_sample.txt\" # Path for replay prompts # Output configurations self.output_dir = \"outputs/selective_amnesia/finetuned_models\" # Output directory to save results # Device configuration self.devices = \"0,\" # CUDA devices (comma-separated) # Additional flags self.use_sample = True # Use sample dataset for training # Data configuration self.data = { \"target\": \"mu.algorithms.selective_amnesia.data_handler.SelectiveAmnesiaDataHandler\", \"params\": { \"train_batch_size\": 4, \"val_batch_size\": 6, \"num_workers\": 1, \"num_val_workers\": 0, # Avoid val dataloader issue \"train\": { \"target\": \"stable_diffusion.ldm.data.ForgettingDataset\", \"params\": { \"forget_prompt\": \"An image in Artist_Sketch style\", \"forget_dataset_path\": \"./q_dist/photo_style\", }, }, \"validation\": { \"target\": \"stable_diffusion.ldm.data.VisualizationDataset\", \"params\": { \"output_size\": 512, \"n_gpus\": 1, # Number of GPUs for validation }, }, }, } # Lightning configuration self.lightning = { \"find_unused_parameters\": False, \"modelcheckpoint\": { \"params\": {\"every_n_epochs\": 0, \"save_top_k\": 0, \"monitor\": None} }, \"callbacks\": { \"image_logger\": { \"target\": \"mu.algorithms.selective_amnesia.callbacks.ImageLogger\", \"params\": { \"batch_frequency\": 1, \"max_images\": 999, \"increase_log_steps\": False, \"log_first_step\": False, \"log_all_val\": True, \"clamp\": True, \"log_images_kwargs\": { \"ddim_eta\": 0, \"ddim_steps\": 50, \"use_ema_scope\": True, \"inpaint\": False, \"plot_progressive_rows\": False, \"plot_diffusion_rows\": False, \"N\": 6, # Number of validation prompts \"unconditional_guidance_scale\": 7.5, \"unconditional_guidance_label\": [\"\"], }, }, } }, \"trainer\": { \"benchmark\": True, \"num_sanity_val_steps\": 0, \"max_epochs\": 50, # Modify epochs here! \"check_val_every_n_epoch\": 10, }, } # Update properties based on provided kwargs for key, value in kwargs.items(): setattr(self, key, value) def validate_config(self): \"\"\" Perform basic validation on the config parameters. \"\"\" # Check if necessary directories exist if not os.path.exists(self.raw_dataset_dir): raise FileNotFoundError(f\"Directory {self.raw_dataset_dir} does not exist.\") if not os.path.exists(self.processed_dataset_dir): raise FileNotFoundError( f\"Directory {self.processed_dataset_dir} does not exist.\" ) if not os.path.exists(self.output_dir): os.makedirs(self.output_dir) # Check if model and checkpoint files exist if not os.path.exists(self.model_config_path): raise FileNotFoundError( f\"Model config file {self.model_config_path} does not exist.\" ) if not os.path.exists(self.ckpt_path): raise FileNotFoundError(f\"Checkpoint file {self.ckpt_path} does not exist.\") if not os.path.exists(self.full_fisher_dict_pkl_path): raise FileNotFoundError( f\"Fisher dictionary file {self.full_fisher_dict_pkl_path} does not exist.\" ) # Check if replay prompts file exists if not os.path.exists(self.replay_prompt_path): raise FileNotFoundError( f\"Replay prompt file {self.replay_prompt_path} does not exist.\" ) # Validate dataset type if self.dataset_type not in [\"unlearncanvas\", \"i2p\"]: raise ValueError( f\"Invalid dataset type {self.dataset_type}. Choose from ['unlearncanvas', 'i2p']\" ) # Validate batch sizes if self.data[\"params\"][\"train_batch_size\"] <= 0: raise ValueError(f\"train_batch_size should be a positive integer.\") if self.data[\"params\"][\"val_batch_size\"] <= 0: raise ValueError(f\"val_batch_size should be a positive integer.\") # Validate lightning trainer max_epochs if self.lightning[\"trainer\"][\"max_epochs\"] <= 0: raise ValueError(f\"max_epochs should be a positive integer.\") selective_amnesia_config_quick_canvas = SelectiveAmnesiaConfig() selective_amnesia_config_quick_canvas.dataset_type = \"unlearncanvas\" selective_amnesia_config_quick_canvas.raw_dataset_dir = ( \"data/quick-canvas-dataset/sample\" ) selective_amnesia_config_i2p = SelectiveAmnesiaConfig() selective_amnesia_config_i2p.dataset_type = \"i2p\" selective_amnesia_config_i2p.raw_dataset_dir = \"data/i2p-dataset/sample\" Train config yaml file # Training parameters seed : 23 scale_lr : True # Model configuration model_config_path: \"mu/algorithms/selective_amnesia/configs/model_config.yaml\" ckpt_path: \"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\" # Checkpoint path for Stable Diffusion full_fisher_dict_pkl_path : \"mu/algorithms/selective_amnesia/data/full_fisher_dict.pkl\" # Dataset directories raw_dataset_dir: \"data/quick-canvas-dataset/sample\" processed_dataset_dir: \"mu/algorithms/selective_amnesia/data\" dataset_type : \"unlearncanvas\" template : \"style\" template_name : \"Abstractionism\" replay_prompt_path: \"mu/algorithms/selective_amnesia/data/fim_prompts_sample.txt\" # Output configurations output_dir: \"outputs/selective_amnesia/finetuned_models\" # Output directory to save results # Sampling and image configurations # Device configuration devices: \"0,\" # CUDA devices to train on (comma-separated) # Additional flags use_sample: True # Use the sample dataset for training data: target: mu.algorithms.selective_amnesia.data_handler.SelectiveAmnesiaDataHandler params: train_batch_size: 4 val_batch_size: 6 num_workers: 4 num_val_workers: 0 # Avoid a weird val dataloader issue (keep unchanged) train: target: stable_diffusion.ldm.data.ForgettingDataset params: forget_prompt: An image in Artist_Sketch style forget_dataset_path: ./q_dist/photo_style validation: target: stable_diffusion.ldm.data.VisualizationDataset params: output_size: 512 n_gpus: 1 # CHANGE THIS TO NUMBER OF GPUS! small hack to sure we see all our logging samples lightning: find_unused_parameters: False modelcheckpoint: params: every_n_epochs: 0 save_top_k: 0 monitor: null callbacks: image_logger: target: mu.algorithms.selective_amnesia.callbacks.ImageLogger params: batch_frequency: 1 max_images: 999 increase_log_steps: False log_first_step: False log_all_val: True clamp: True log_images_kwargs: ddim_eta: 0 ddim_steps: 50 use_ema_scope: True inpaint: False plot_progressive_rows: False plot_diffusion_rows: False N: 6 # keep this the same as number of validation prompts! unconditional_guidance_scale: 7.5 unconditional_guidance_label: [\"\"] trainer: benchmark: True num_sanity_val_steps: 0 max_epochs: 50 # modify epochs here! check_val_every_n_epoch: 10 Configuration File description Training Parameters seed: Random seed for reproducibility. Type: int Example: 23 scale_lr: Whether to scale the base learning rate. Type: bool Example: True Model Configuration model_config_path: Path to the Stable Diffusion model configuration YAML file. Type: str Example: \"/path/to/model_config.yaml\" ckpt_path: Path to the Stable Diffusion model checkpoint. Type: str Example: \"/path/to/compvis.ckpt\" full_fisher_dict_pkl_path: Path to the full fisher dict pkl file Type: str Example: \"full_fisher_dict.pkl\" Dataset Directories raw_dataset_dir: Directory containing the raw dataset categorized by themes or classes. Type: str Example: \"/path/to/raw_dataset\" processed_dataset_dir: Directory to save the processed dataset. Type: str Example: \"/path/to/processed_dataset\" dataset_type: Specifies the dataset type for training. Use generic as type if you want to use your own dataset. Choices: [\"unlearncanvas\", \"i2p\", \"generic\"] Example: \"unlearncanvas\" template: Type of template to use during training. Choices: [\"object\", \"style\", \"i2p\"] Example: \"style\" template_name: Name of the concept or style to erase. Choices: [\"self-harm\", \"Abstractionism\"] Example: \"Abstractionism\" Output Configurations output_dir: Directory to save fine-tuned models and results. Type: str Example: \"outputs/selective_amnesia/finetuned_models\" Device Configuration devices: CUDA devices for training (comma-separated). Type: str Example: \"0\" Data Parameters train_batch_size: Batch size for training. Type: int Example: 4 val_batch_size: Batch size for validation. Type: int Example: 6 num_workers: Number of worker threads for data loading. Type: int Example: 4 forget_prompt: Prompt to specify the style or concept to forget. Type: str Example: \"An image in Artist_Sketch style\" Lightning Configuration max_epochs: Maximum number of epochs for training. Type: int Example: 50 callbacks: batch_frequency: Frequency for logging image batches. Type: int Example: 1 max_images: Maximum number of images to log. Type: int Example: 999","title":"Configs"},{"location":"unlearn/configs/selective_amnesia/#train-config","text":"import os from mu.core.base_config import BaseConfig from pathlib import Path current_dir = Path(__file__).parent class SelectiveAmnesiaConfig(BaseConfig): def __init__(self, **kwargs): # Training parameters self.seed = 23 # Random seed self.scale_lr = True # Flag for scaling learning rate # Model configuration self.config_path = current_dir / \"train_config.yaml\" self.model_config_path = ( current_dir / \"model_config.yaml\" ) # Config path for model self.ckpt_path = \"models/compvis/style50/compvis.ckpt\" # Checkpoint path for Stable Diffusion self.full_fisher_dict_pkl_path = \"mu/algorithms/selective_amnesia/data/full_fisher_dict.pkl\" # Path for Fisher dict # Dataset directories self.raw_dataset_dir = \"data/quick-canvas-dataset/sample\" self.processed_dataset_dir = \"mu/algorithms/selective_amnesia/data\" self.dataset_type = ( \"unlearncanvas\" # Dataset type (choices: unlearncanvas, i2p) ) self.template = \"style\" # Template to use self.template_name = \"Abstractionism\" # Template name self.replay_prompt_path = \"mu/algorithms/selective_amnesia/data/fim_prompts_sample.txt\" # Path for replay prompts # Output configurations self.output_dir = \"outputs/selective_amnesia/finetuned_models\" # Output directory to save results # Device configuration self.devices = \"0,\" # CUDA devices (comma-separated) # Additional flags self.use_sample = True # Use sample dataset for training # Data configuration self.data = { \"target\": \"mu.algorithms.selective_amnesia.data_handler.SelectiveAmnesiaDataHandler\", \"params\": { \"train_batch_size\": 4, \"val_batch_size\": 6, \"num_workers\": 1, \"num_val_workers\": 0, # Avoid val dataloader issue \"train\": { \"target\": \"stable_diffusion.ldm.data.ForgettingDataset\", \"params\": { \"forget_prompt\": \"An image in Artist_Sketch style\", \"forget_dataset_path\": \"./q_dist/photo_style\", }, }, \"validation\": { \"target\": \"stable_diffusion.ldm.data.VisualizationDataset\", \"params\": { \"output_size\": 512, \"n_gpus\": 1, # Number of GPUs for validation }, }, }, } # Lightning configuration self.lightning = { \"find_unused_parameters\": False, \"modelcheckpoint\": { \"params\": {\"every_n_epochs\": 0, \"save_top_k\": 0, \"monitor\": None} }, \"callbacks\": { \"image_logger\": { \"target\": \"mu.algorithms.selective_amnesia.callbacks.ImageLogger\", \"params\": { \"batch_frequency\": 1, \"max_images\": 999, \"increase_log_steps\": False, \"log_first_step\": False, \"log_all_val\": True, \"clamp\": True, \"log_images_kwargs\": { \"ddim_eta\": 0, \"ddim_steps\": 50, \"use_ema_scope\": True, \"inpaint\": False, \"plot_progressive_rows\": False, \"plot_diffusion_rows\": False, \"N\": 6, # Number of validation prompts \"unconditional_guidance_scale\": 7.5, \"unconditional_guidance_label\": [\"\"], }, }, } }, \"trainer\": { \"benchmark\": True, \"num_sanity_val_steps\": 0, \"max_epochs\": 50, # Modify epochs here! \"check_val_every_n_epoch\": 10, }, } # Update properties based on provided kwargs for key, value in kwargs.items(): setattr(self, key, value) def validate_config(self): \"\"\" Perform basic validation on the config parameters. \"\"\" # Check if necessary directories exist if not os.path.exists(self.raw_dataset_dir): raise FileNotFoundError(f\"Directory {self.raw_dataset_dir} does not exist.\") if not os.path.exists(self.processed_dataset_dir): raise FileNotFoundError( f\"Directory {self.processed_dataset_dir} does not exist.\" ) if not os.path.exists(self.output_dir): os.makedirs(self.output_dir) # Check if model and checkpoint files exist if not os.path.exists(self.model_config_path): raise FileNotFoundError( f\"Model config file {self.model_config_path} does not exist.\" ) if not os.path.exists(self.ckpt_path): raise FileNotFoundError(f\"Checkpoint file {self.ckpt_path} does not exist.\") if not os.path.exists(self.full_fisher_dict_pkl_path): raise FileNotFoundError( f\"Fisher dictionary file {self.full_fisher_dict_pkl_path} does not exist.\" ) # Check if replay prompts file exists if not os.path.exists(self.replay_prompt_path): raise FileNotFoundError( f\"Replay prompt file {self.replay_prompt_path} does not exist.\" ) # Validate dataset type if self.dataset_type not in [\"unlearncanvas\", \"i2p\"]: raise ValueError( f\"Invalid dataset type {self.dataset_type}. Choose from ['unlearncanvas', 'i2p']\" ) # Validate batch sizes if self.data[\"params\"][\"train_batch_size\"] <= 0: raise ValueError(f\"train_batch_size should be a positive integer.\") if self.data[\"params\"][\"val_batch_size\"] <= 0: raise ValueError(f\"val_batch_size should be a positive integer.\") # Validate lightning trainer max_epochs if self.lightning[\"trainer\"][\"max_epochs\"] <= 0: raise ValueError(f\"max_epochs should be a positive integer.\") selective_amnesia_config_quick_canvas = SelectiveAmnesiaConfig() selective_amnesia_config_quick_canvas.dataset_type = \"unlearncanvas\" selective_amnesia_config_quick_canvas.raw_dataset_dir = ( \"data/quick-canvas-dataset/sample\" ) selective_amnesia_config_i2p = SelectiveAmnesiaConfig() selective_amnesia_config_i2p.dataset_type = \"i2p\" selective_amnesia_config_i2p.raw_dataset_dir = \"data/i2p-dataset/sample\"","title":"Train config"},{"location":"unlearn/configs/selective_amnesia/#train-config-yaml-file","text":"# Training parameters seed : 23 scale_lr : True # Model configuration model_config_path: \"mu/algorithms/selective_amnesia/configs/model_config.yaml\" ckpt_path: \"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\" # Checkpoint path for Stable Diffusion full_fisher_dict_pkl_path : \"mu/algorithms/selective_amnesia/data/full_fisher_dict.pkl\" # Dataset directories raw_dataset_dir: \"data/quick-canvas-dataset/sample\" processed_dataset_dir: \"mu/algorithms/selective_amnesia/data\" dataset_type : \"unlearncanvas\" template : \"style\" template_name : \"Abstractionism\" replay_prompt_path: \"mu/algorithms/selective_amnesia/data/fim_prompts_sample.txt\" # Output configurations output_dir: \"outputs/selective_amnesia/finetuned_models\" # Output directory to save results # Sampling and image configurations # Device configuration devices: \"0,\" # CUDA devices to train on (comma-separated) # Additional flags use_sample: True # Use the sample dataset for training data: target: mu.algorithms.selective_amnesia.data_handler.SelectiveAmnesiaDataHandler params: train_batch_size: 4 val_batch_size: 6 num_workers: 4 num_val_workers: 0 # Avoid a weird val dataloader issue (keep unchanged) train: target: stable_diffusion.ldm.data.ForgettingDataset params: forget_prompt: An image in Artist_Sketch style forget_dataset_path: ./q_dist/photo_style validation: target: stable_diffusion.ldm.data.VisualizationDataset params: output_size: 512 n_gpus: 1 # CHANGE THIS TO NUMBER OF GPUS! small hack to sure we see all our logging samples lightning: find_unused_parameters: False modelcheckpoint: params: every_n_epochs: 0 save_top_k: 0 monitor: null callbacks: image_logger: target: mu.algorithms.selective_amnesia.callbacks.ImageLogger params: batch_frequency: 1 max_images: 999 increase_log_steps: False log_first_step: False log_all_val: True clamp: True log_images_kwargs: ddim_eta: 0 ddim_steps: 50 use_ema_scope: True inpaint: False plot_progressive_rows: False plot_diffusion_rows: False N: 6 # keep this the same as number of validation prompts! unconditional_guidance_scale: 7.5 unconditional_guidance_label: [\"\"] trainer: benchmark: True num_sanity_val_steps: 0 max_epochs: 50 # modify epochs here! check_val_every_n_epoch: 10","title":"Train config yaml file"},{"location":"unlearn/configs/selective_amnesia/#configuration-file-description","text":"","title":"Configuration File description"},{"location":"unlearn/configs/selective_amnesia/#training-parameters","text":"seed: Random seed for reproducibility. Type: int Example: 23 scale_lr: Whether to scale the base learning rate. Type: bool Example: True","title":"Training Parameters"},{"location":"unlearn/configs/selective_amnesia/#model-configuration","text":"model_config_path: Path to the Stable Diffusion model configuration YAML file. Type: str Example: \"/path/to/model_config.yaml\" ckpt_path: Path to the Stable Diffusion model checkpoint. Type: str Example: \"/path/to/compvis.ckpt\" full_fisher_dict_pkl_path: Path to the full fisher dict pkl file Type: str Example: \"full_fisher_dict.pkl\"","title":"Model Configuration"},{"location":"unlearn/configs/selective_amnesia/#dataset-directories","text":"raw_dataset_dir: Directory containing the raw dataset categorized by themes or classes. Type: str Example: \"/path/to/raw_dataset\" processed_dataset_dir: Directory to save the processed dataset. Type: str Example: \"/path/to/processed_dataset\" dataset_type: Specifies the dataset type for training. Use generic as type if you want to use your own dataset. Choices: [\"unlearncanvas\", \"i2p\", \"generic\"] Example: \"unlearncanvas\" template: Type of template to use during training. Choices: [\"object\", \"style\", \"i2p\"] Example: \"style\" template_name: Name of the concept or style to erase. Choices: [\"self-harm\", \"Abstractionism\"] Example: \"Abstractionism\"","title":"Dataset Directories"},{"location":"unlearn/configs/selective_amnesia/#output-configurations","text":"output_dir: Directory to save fine-tuned models and results. Type: str Example: \"outputs/selective_amnesia/finetuned_models\"","title":"Output Configurations"},{"location":"unlearn/configs/selective_amnesia/#device-configuration","text":"devices: CUDA devices for training (comma-separated). Type: str Example: \"0\"","title":"Device Configuration"},{"location":"unlearn/configs/selective_amnesia/#data-parameters","text":"train_batch_size: Batch size for training. Type: int Example: 4 val_batch_size: Batch size for validation. Type: int Example: 6 num_workers: Number of worker threads for data loading. Type: int Example: 4 forget_prompt: Prompt to specify the style or concept to forget. Type: str Example: \"An image in Artist_Sketch style\"","title":"Data Parameters"},{"location":"unlearn/configs/selective_amnesia/#lightning-configuration","text":"max_epochs: Maximum number of epochs for training. Type: int Example: 50 callbacks: batch_frequency: Frequency for logging image batches. Type: int Example: 1 max_images: Maximum number of images to log. Type: int Example: 999","title":"Lightning Configuration"},{"location":"unlearn/configs/semipermeable_membrane/","text":"Train Config class PretrainedModelConfig(BaseConfig): def __init__( self, name_or_path=\"CompVis/stable-diffusion-v1-4\", # Model path or name ckpt_path=\"CompVis/stable-diffusion-v1-4\", # Checkpoint path v2=False, # Version 2 of the model v_pred=False, # Version prediction clip_skip=1, # Skip layers in CLIP model ): self.name_or_path = name_or_path self.ckpt_path = ckpt_path self.v2 = v2 self.v_pred = v_pred self.clip_skip = clip_skip class NetworkConfig(BaseConfig): def __init__( self, rank=1, # Network rank alpha=1.0, # Alpha parameter for the network ): self.rank = rank self.alpha = alpha class TrainConfig(BaseConfig): def __init__( self, precision=\"float32\", # Training precision (e.g., \"float32\" or \"float16\") noise_scheduler=\"ddim\", # Noise scheduler method iterations=3000, # Number of training iterations batch_size=1, # Batch size lr=0.0001, # Learning rate for the model unet_lr=0.0001, # Learning rate for UNet text_encoder_lr=5e-05, # Learning rate for text encoder optimizer_type=\"AdamW8bit\", # Optimizer type (e.g., \"AdamW\", \"AdamW8bit\") lr_scheduler=\"cosine_with_restarts\", # Learning rate scheduler type lr_warmup_steps=500, # Steps for learning rate warm-up lr_scheduler_num_cycles=3, # Number of cycles for the learning rate scheduler max_denoising_steps=30, # Max denoising steps (for DDIM) ): self.precision = precision self.noise_scheduler = noise_scheduler self.iterations = iterations self.batch_size = batch_size self.lr = lr self.unet_lr = unet_lr self.text_encoder_lr = text_encoder_lr self.optimizer_type = optimizer_type self.lr_scheduler = lr_scheduler self.lr_warmup_steps = lr_warmup_steps self.lr_scheduler_num_cycles = lr_scheduler_num_cycles self.max_denoising_steps = max_denoising_steps class SaveConfig(BaseConfig): def __init__( self, per_steps=500, # Save model every N steps precision=\"float32\", # Precision for saving model ): self.per_steps = per_steps self.precision = precision class OtherConfig(BaseConfig): def __init__( self, use_xformers=True, # Whether to use memory-efficient attention with xformers ): self.use_xformers = use_xformers class PromptConfig(BaseConfig): def __init__( self, target=\"Abstractionism\", # Prompt target positive=\"Abstractionism\", # Positive prompt unconditional=\"\", # Unconditional prompt neutral=\"\", # Neutral prompt action=\"erase_with_la\", # Action to perform guidance_scale=\"1.0\", # Guidance scale for generation resolution=512, # Image resolution batch_size=1, # Batch size for prompt generation dynamic_resolution=True, # Flag for dynamic resolution la_strength=1000, # Strength of the latent attention sampling_batch_size=4, # Batch size for sampling ): self.target = target self.positive = positive self.unconditional = unconditional self.neutral = neutral self.action = action self.guidance_scale = guidance_scale self.resolution = resolution self.batch_size = batch_size self.dynamic_resolution = dynamic_resolution self.la_strength = la_strength self.sampling_batch_size = sampling_batch_size class SemipermeableMembraneConfig(BaseConfig): \"\"\" SemipermeableMembraneConfig stores all the configuration parameters for the semipermeable membrane training, including model, network, training, saving, and other environment details. \"\"\" def __init__(self, **kwargs): # Pretrained model configuration self.pretrained_model = PretrainedModelConfig() # Network configuration self.network = NetworkConfig() # Training configuration self.train = TrainConfig() # Save configuration self.save = SaveConfig() # Other settings self.other = OtherConfig() # Weights and Biases (wandb) configuration self.wandb_project = \"semipermeable_membrane_project\" # wandb project name self.wandb_run = \"spm_run\" # wandb run name # Dataset configuration self.use_sample = True # Use sample dataset for training self.dataset_type = ( \"unlearncanvas\" # Dataset type (e.g., \"unlearncanvas\", \"i2p\") ) self.template = \"style\" # Template type (e.g., \"style\", \"object\") self.template_name = \"Abstractionism\" # Template name # Prompt configuration self.prompt = PromptConfig( target=self.template_name, positive=self.template_name, ) # Device configuration self.devices = \"0\" # CUDA devices to train on (comma-separated) # Output configuration self.output_dir = \"outputs/semipermeable_membrane/finetuned_models\" # Directory to save models # Verbose logging self.verbose = True # Whether to log verbose information during training Description of Arguments in train_config.yaml pretrained_model ckpt_path: File path to the pretrained model's checkpoint file. v2: Boolean indicating whether the pretrained model is version 2 or not. v_pred: Boolean to enable/disable \"v-prediction\" mode for diffusion models. clip_skip: Number of CLIP layers to skip during inference. network rank: Rank of the low-rank adaptation network. alpha: Scaling factor for the network during training. train precision: Numerical precision to use during training (e.g., float32 or float16). noise_scheduler: Type of noise scheduler to use in the training loop (e.g., ddim). iterations: Number of training iterations. batch_size: Batch size for training. lr: Learning rate for the training optimizer. unet_lr: Learning rate for the U-Net model. text_encoder_lr: Learning rate for the text encoder. optimizer_type: Optimizer to use for training (e.g., AdamW8bit). lr_scheduler: Learning rate scheduler to apply during training. lr_warmup_steps: Number of steps for linear warmup of the learning rate. lr_scheduler_num_cycles: Number of cycles for a cosine-with-restarts scheduler. max_denoising_steps: Maximum denoising steps to use during training. save per_steps: Frequency of saving the model (in steps). precision: Numerical precision for saved model weights other use_xformers: Boolean to enable xformers memory-efficient attention. wandb_project and wandb_run Configuration for tracking the training progress using Weights & Biases. wandb_project: Project name in W&B. wandb_run: Specific run name in the W&B dashboard. use_sample Boolean to indicate whether to use the sample dataset for training. template Specifies the template type, choices are: object: Focus on specific objects. style: Focus on artistic styles. i2p: Intermediate style processing. template_name Name of the template, choices are: self-harm Abstractionism prompt target: Target template or concept to guide training (references template_name). positive: Positive prompt based on the template. unconditional: Unconditional prompt text. neutral: Neutral prompt text. action: Specifies the action applied to the prompt (e.g., erase_with_la). guidance_scale: Guidance scale for classifier-free guidance. resolution: Image resolution for training. batch_size: Batch size for generating prompts. dynamic_resolution: Boolean to allow dynamic resolution. la_strength: Strength of local adaptation. sampling_batch_size: Batch size for sampling images. devices CUDA devices to use for training (specified as a comma-separated list, e.g., \"0,1\"). output_dir Directory to save the fine-tuned model and other outputs. verbose Boolean flag for verbose logging during training.","title":"Configs"},{"location":"unlearn/configs/semipermeable_membrane/#train-config","text":"class PretrainedModelConfig(BaseConfig): def __init__( self, name_or_path=\"CompVis/stable-diffusion-v1-4\", # Model path or name ckpt_path=\"CompVis/stable-diffusion-v1-4\", # Checkpoint path v2=False, # Version 2 of the model v_pred=False, # Version prediction clip_skip=1, # Skip layers in CLIP model ): self.name_or_path = name_or_path self.ckpt_path = ckpt_path self.v2 = v2 self.v_pred = v_pred self.clip_skip = clip_skip class NetworkConfig(BaseConfig): def __init__( self, rank=1, # Network rank alpha=1.0, # Alpha parameter for the network ): self.rank = rank self.alpha = alpha class TrainConfig(BaseConfig): def __init__( self, precision=\"float32\", # Training precision (e.g., \"float32\" or \"float16\") noise_scheduler=\"ddim\", # Noise scheduler method iterations=3000, # Number of training iterations batch_size=1, # Batch size lr=0.0001, # Learning rate for the model unet_lr=0.0001, # Learning rate for UNet text_encoder_lr=5e-05, # Learning rate for text encoder optimizer_type=\"AdamW8bit\", # Optimizer type (e.g., \"AdamW\", \"AdamW8bit\") lr_scheduler=\"cosine_with_restarts\", # Learning rate scheduler type lr_warmup_steps=500, # Steps for learning rate warm-up lr_scheduler_num_cycles=3, # Number of cycles for the learning rate scheduler max_denoising_steps=30, # Max denoising steps (for DDIM) ): self.precision = precision self.noise_scheduler = noise_scheduler self.iterations = iterations self.batch_size = batch_size self.lr = lr self.unet_lr = unet_lr self.text_encoder_lr = text_encoder_lr self.optimizer_type = optimizer_type self.lr_scheduler = lr_scheduler self.lr_warmup_steps = lr_warmup_steps self.lr_scheduler_num_cycles = lr_scheduler_num_cycles self.max_denoising_steps = max_denoising_steps class SaveConfig(BaseConfig): def __init__( self, per_steps=500, # Save model every N steps precision=\"float32\", # Precision for saving model ): self.per_steps = per_steps self.precision = precision class OtherConfig(BaseConfig): def __init__( self, use_xformers=True, # Whether to use memory-efficient attention with xformers ): self.use_xformers = use_xformers class PromptConfig(BaseConfig): def __init__( self, target=\"Abstractionism\", # Prompt target positive=\"Abstractionism\", # Positive prompt unconditional=\"\", # Unconditional prompt neutral=\"\", # Neutral prompt action=\"erase_with_la\", # Action to perform guidance_scale=\"1.0\", # Guidance scale for generation resolution=512, # Image resolution batch_size=1, # Batch size for prompt generation dynamic_resolution=True, # Flag for dynamic resolution la_strength=1000, # Strength of the latent attention sampling_batch_size=4, # Batch size for sampling ): self.target = target self.positive = positive self.unconditional = unconditional self.neutral = neutral self.action = action self.guidance_scale = guidance_scale self.resolution = resolution self.batch_size = batch_size self.dynamic_resolution = dynamic_resolution self.la_strength = la_strength self.sampling_batch_size = sampling_batch_size class SemipermeableMembraneConfig(BaseConfig): \"\"\" SemipermeableMembraneConfig stores all the configuration parameters for the semipermeable membrane training, including model, network, training, saving, and other environment details. \"\"\" def __init__(self, **kwargs): # Pretrained model configuration self.pretrained_model = PretrainedModelConfig() # Network configuration self.network = NetworkConfig() # Training configuration self.train = TrainConfig() # Save configuration self.save = SaveConfig() # Other settings self.other = OtherConfig() # Weights and Biases (wandb) configuration self.wandb_project = \"semipermeable_membrane_project\" # wandb project name self.wandb_run = \"spm_run\" # wandb run name # Dataset configuration self.use_sample = True # Use sample dataset for training self.dataset_type = ( \"unlearncanvas\" # Dataset type (e.g., \"unlearncanvas\", \"i2p\") ) self.template = \"style\" # Template type (e.g., \"style\", \"object\") self.template_name = \"Abstractionism\" # Template name # Prompt configuration self.prompt = PromptConfig( target=self.template_name, positive=self.template_name, ) # Device configuration self.devices = \"0\" # CUDA devices to train on (comma-separated) # Output configuration self.output_dir = \"outputs/semipermeable_membrane/finetuned_models\" # Directory to save models # Verbose logging self.verbose = True # Whether to log verbose information during training","title":"Train Config"},{"location":"unlearn/configs/semipermeable_membrane/#description-of-arguments-in-train_configyaml","text":"pretrained_model ckpt_path: File path to the pretrained model's checkpoint file. v2: Boolean indicating whether the pretrained model is version 2 or not. v_pred: Boolean to enable/disable \"v-prediction\" mode for diffusion models. clip_skip: Number of CLIP layers to skip during inference. network rank: Rank of the low-rank adaptation network. alpha: Scaling factor for the network during training. train precision: Numerical precision to use during training (e.g., float32 or float16). noise_scheduler: Type of noise scheduler to use in the training loop (e.g., ddim). iterations: Number of training iterations. batch_size: Batch size for training. lr: Learning rate for the training optimizer. unet_lr: Learning rate for the U-Net model. text_encoder_lr: Learning rate for the text encoder. optimizer_type: Optimizer to use for training (e.g., AdamW8bit). lr_scheduler: Learning rate scheduler to apply during training. lr_warmup_steps: Number of steps for linear warmup of the learning rate. lr_scheduler_num_cycles: Number of cycles for a cosine-with-restarts scheduler. max_denoising_steps: Maximum denoising steps to use during training. save per_steps: Frequency of saving the model (in steps). precision: Numerical precision for saved model weights other use_xformers: Boolean to enable xformers memory-efficient attention. wandb_project and wandb_run Configuration for tracking the training progress using Weights & Biases. wandb_project: Project name in W&B. wandb_run: Specific run name in the W&B dashboard. use_sample Boolean to indicate whether to use the sample dataset for training. template Specifies the template type, choices are: object: Focus on specific objects. style: Focus on artistic styles. i2p: Intermediate style processing. template_name Name of the template, choices are: self-harm Abstractionism prompt target: Target template or concept to guide training (references template_name). positive: Positive prompt based on the template. unconditional: Unconditional prompt text. neutral: Neutral prompt text. action: Specifies the action applied to the prompt (e.g., erase_with_la). guidance_scale: Guidance scale for classifier-free guidance. resolution: Image resolution for training. batch_size: Batch size for generating prompts. dynamic_resolution: Boolean to allow dynamic resolution. la_strength: Strength of local adaptation. sampling_batch_size: Batch size for sampling images. devices CUDA devices to use for training (specified as a comma-separated list, e.g., \"0,1\"). output_dir Directory to save the fine-tuned model and other outputs. verbose Boolean flag for verbose logging during training.","title":"Description of Arguments in train_config.yaml"},{"location":"unlearn/configs/uce/","text":"Train Config class UnifiedConceptEditingConfig(BaseConfig): def __init__(self, **kwargs): # Training configuration self.train_method = \"full\" # Options: full, partial self.alpha = 0.1 # Guidance factor for training self.epochs = 1 # Number of epochs self.lr = 5e-5 # Learning rate # Model configuration self.ckpt_path = \"models/diffuser/style50\" # Path to model checkpoint # Output configuration self.output_dir = ( \"outputs/uce/finetuned_models\" # Directory to save finetuned models ) self.dataset_type = \"unlearncanvas\" # Type of dataset to be used self.template = \"style\" # Template for training self.template_name = \"Abstractionism\" # Name of the template # Device configuration self.devices = \"0\" # CUDA devices to train on (comma-separated) # Additional flags self.use_sample = True # Whether to use the sample dataset # Editing-specific configuration self.guided_concepts = ( \"A Elephant image\" # Comma-separated string of guided concepts ) self.technique = ( \"replace\" # Technique for editing (Options: \"replace\", \"tensor\") ) # Parameters for the editing technique self.preserve_scale = 0.1 # Scale for preserving the concept (float) self.preserve_number = ( None # Number of concepts to preserve (int, None for all) ) self.erase_scale = 1 # Scale for erasing self.lamb = 0.1 # Regularization weight for loss self.add_prompts = False # Whether to add additional prompts # Preserver concepts (comma-separated if multiple) self.preserver_concepts = ( \"A Lion image\" # Comma-separated string of preserver concepts ) # Base model used for editing self.base = \"stable-diffusion-v1-4\" # Base version of Stable Diffusion Description of Arguments in train_config.yaml Training Parameters train_method : Specifies the method of training for concept erasure. Choices: [\"full\", \"partial\"] Example: \"full\" alpha : Guidance strength for the starting image during training. Type: float Example: 0.1 epochs : Number of epochs to train the model. Type: int Example: 10 lr : Learning rate used for the optimizer during training. Type: float Example: 5e-5 Model Configuration * ckpt_path : File path to the checkpoint of the Stable Diffusion model. * Type: str * Example: \"/path/to/model_checkpoint.ckpt\" config_path : File path to the Stable Diffusion model configuration YAML file. Type: str Example: \"/path/to/config.yaml\" Dataset Directories dataset_type : Specifies the dataset type for the training process. Use generic as type if you want to use your own dataset. Choices: [\"unlearncanvas\", \"i2p\", \"generic\"] Example: \"unlearncanvas\" template : Type of template to use during training. Choices: [\"object\", \"style\", \"i2p\"] Example: \"style\" template_name : Name of the specific concept or style to be erased. Choices: [\"self-harm\", \"Abstractionism\"] Example: \"Abstractionism\" Output Configurations output_dir : Directory where the fine-tuned models and results will be saved. Type: str Example: \"outputs/erase_diff/finetuned_models\" Sampling and Image Configurations use_sample : Flag to indicate whether a sample dataset should be used for training. Type: bool Example: True guided_concepts : Concepts to guide the editing process. Type: str Example: \"Nature, Abstract\" technique : Specifies the editing technique. Choices: [\"replace\", \"tensor\"] Example: \"replace\" preserve_scale : Scale for preservation during the editing process. Type: float Example: 0.5 preserve_number : Number of items to preserve during editing. Type: int Example: 10 erase_scale : Scale for erasure during the editing process. Type: float Example: 0.8 lamb : Lambda parameter for controlling balance during editing. Type: float Example: 0.01 add_prompts : Flag to indicate whether additional prompts should be used. Type: bool Example: True Device Configuration devices : Specifies the CUDA devices to be used for training (comma-separated). Type: str (Comma-separated) Example: \"0,1\"","title":"Configs"},{"location":"unlearn/configs/uce/#train-config","text":"class UnifiedConceptEditingConfig(BaseConfig): def __init__(self, **kwargs): # Training configuration self.train_method = \"full\" # Options: full, partial self.alpha = 0.1 # Guidance factor for training self.epochs = 1 # Number of epochs self.lr = 5e-5 # Learning rate # Model configuration self.ckpt_path = \"models/diffuser/style50\" # Path to model checkpoint # Output configuration self.output_dir = ( \"outputs/uce/finetuned_models\" # Directory to save finetuned models ) self.dataset_type = \"unlearncanvas\" # Type of dataset to be used self.template = \"style\" # Template for training self.template_name = \"Abstractionism\" # Name of the template # Device configuration self.devices = \"0\" # CUDA devices to train on (comma-separated) # Additional flags self.use_sample = True # Whether to use the sample dataset # Editing-specific configuration self.guided_concepts = ( \"A Elephant image\" # Comma-separated string of guided concepts ) self.technique = ( \"replace\" # Technique for editing (Options: \"replace\", \"tensor\") ) # Parameters for the editing technique self.preserve_scale = 0.1 # Scale for preserving the concept (float) self.preserve_number = ( None # Number of concepts to preserve (int, None for all) ) self.erase_scale = 1 # Scale for erasing self.lamb = 0.1 # Regularization weight for loss self.add_prompts = False # Whether to add additional prompts # Preserver concepts (comma-separated if multiple) self.preserver_concepts = ( \"A Lion image\" # Comma-separated string of preserver concepts ) # Base model used for editing self.base = \"stable-diffusion-v1-4\" # Base version of Stable Diffusion","title":"Train Config"},{"location":"unlearn/configs/uce/#description-of-arguments-in-train_configyaml","text":"Training Parameters train_method : Specifies the method of training for concept erasure. Choices: [\"full\", \"partial\"] Example: \"full\" alpha : Guidance strength for the starting image during training. Type: float Example: 0.1 epochs : Number of epochs to train the model. Type: int Example: 10 lr : Learning rate used for the optimizer during training. Type: float Example: 5e-5 Model Configuration * ckpt_path : File path to the checkpoint of the Stable Diffusion model. * Type: str * Example: \"/path/to/model_checkpoint.ckpt\" config_path : File path to the Stable Diffusion model configuration YAML file. Type: str Example: \"/path/to/config.yaml\" Dataset Directories dataset_type : Specifies the dataset type for the training process. Use generic as type if you want to use your own dataset. Choices: [\"unlearncanvas\", \"i2p\", \"generic\"] Example: \"unlearncanvas\" template : Type of template to use during training. Choices: [\"object\", \"style\", \"i2p\"] Example: \"style\" template_name : Name of the specific concept or style to be erased. Choices: [\"self-harm\", \"Abstractionism\"] Example: \"Abstractionism\" Output Configurations output_dir : Directory where the fine-tuned models and results will be saved. Type: str Example: \"outputs/erase_diff/finetuned_models\" Sampling and Image Configurations use_sample : Flag to indicate whether a sample dataset should be used for training. Type: bool Example: True guided_concepts : Concepts to guide the editing process. Type: str Example: \"Nature, Abstract\" technique : Specifies the editing technique. Choices: [\"replace\", \"tensor\"] Example: \"replace\" preserve_scale : Scale for preservation during the editing process. Type: float Example: 0.5 preserve_number : Number of items to preserve during editing. Type: int Example: 10 erase_scale : Scale for erasure during the editing process. Type: float Example: 0.8 lamb : Lambda parameter for controlling balance during editing. Type: float Example: 0.01 add_prompts : Flag to indicate whether additional prompts should be used. Type: bool Example: True Device Configuration devices : Specifies the CUDA devices to be used for training (comma-separated). Type: str (Comma-separated) Example: \"0,1\"","title":"Description of Arguments in train_config.yaml"},{"location":"unlearn/examples/concept_ablation/","text":"Train your model by using Concept Ablation Algorithm. Import pre defined config classes or create your own object. Refer the config docs for details about the parameters that you can use. To test the below code snippet, you can create a file, copy the below code in eg, my_trainer.py and execute it with python my_trainer.py or use WANDB_MODE=offline python my_trainer.py for offline mode. Use pre defined config from mu.algorithms.concept_ablation.algorithm import ( ConceptAblationAlgorithm, ) from mu.algorithms.concept_ablation.configs import ( concept_ablation_train_mu, ConceptAblationConfig, ) if __name__ == \"__main__\": concept_ablation_train_mu.lightning.trainer.max_steps = 5 algorithm = ConceptAblationAlgorithm( concept_ablation_train_mu ) algorithm.run() Modify some train parameters in pre defined config class. View the config docs to see a list of available parameters. from mu.algorithms.concept_ablation.algorithm import ( ConceptAblationAlgorithm, ) from mu.algorithms.concept_ablation.configs import ( concept_ablation_train_mu, ConceptAblationConfig, ) if __name__ == \"__main__\": concept_ablation_train_mu.lightning.trainer.max_steps = 5 algorithm = ConceptAblationAlgorithm( concept_ablation_train_mu, config_path=\"mu/algorithms/concept_ablation/configs/train_config.yaml\", ckpt_path=\"machine_unlearning/models/compvis/style50/compvis.ckpt\", prompts=\"mu/algorithms/concept_ablation/data/anchor_prompts/finetune_prompts/sd_prompt_Architectures_sample.txt\", output_dir=\"/opt/dlami/nvme/outputs\", # devices=\"1\", ) algorithm.run() Create your own config object from mu.algorithms.concept_ablation.algorithm import ( ConceptAblationAlgorithm, ) from mu.algorithms.concept_ablation.configs import ( ConceptAblationConfig, ) myconfig = ConceptAblationConfig() myconfig.ckpt_path = \"machine_unlearning/models/compvis/style50/compvis.ckpt\" myconfig.raw_dataset_dir = ( \"/home/ubuntu/Projects/balaram/packaging/data/quick-canvas-dataset/sample\" ) algorithm = ConceptAblationConfig(myconfig) algorithm.run() Override the Config class itself. from mu.algorithms.concept_ablation.algorithm import ( ConceptAblationAlgorithm, ) from mu.algorithms.concept_ablation.configs import ( ConceptAblationConfig, ) class MyNewConfigClass(ConceptAblationConfig): def __init__(self, *args, **kwargs): self.new_parameter = kwargs.get(\"new_parameter\") super().__init__() new_config_object = MyNewConfigClass() algorithm = ConceptAblationAlgorithm(new_config_object) algorithm.run()","title":"Examples"},{"location":"unlearn/examples/concept_ablation/#use-pre-defined-config","text":"from mu.algorithms.concept_ablation.algorithm import ( ConceptAblationAlgorithm, ) from mu.algorithms.concept_ablation.configs import ( concept_ablation_train_mu, ConceptAblationConfig, ) if __name__ == \"__main__\": concept_ablation_train_mu.lightning.trainer.max_steps = 5 algorithm = ConceptAblationAlgorithm( concept_ablation_train_mu ) algorithm.run()","title":"Use pre defined config"},{"location":"unlearn/examples/concept_ablation/#modify-some-train-parameters-in-pre-defined-config-class","text":"View the config docs to see a list of available parameters. from mu.algorithms.concept_ablation.algorithm import ( ConceptAblationAlgorithm, ) from mu.algorithms.concept_ablation.configs import ( concept_ablation_train_mu, ConceptAblationConfig, ) if __name__ == \"__main__\": concept_ablation_train_mu.lightning.trainer.max_steps = 5 algorithm = ConceptAblationAlgorithm( concept_ablation_train_mu, config_path=\"mu/algorithms/concept_ablation/configs/train_config.yaml\", ckpt_path=\"machine_unlearning/models/compvis/style50/compvis.ckpt\", prompts=\"mu/algorithms/concept_ablation/data/anchor_prompts/finetune_prompts/sd_prompt_Architectures_sample.txt\", output_dir=\"/opt/dlami/nvme/outputs\", # devices=\"1\", ) algorithm.run()","title":"Modify some train parameters in pre defined config class."},{"location":"unlearn/examples/concept_ablation/#create-your-own-config-object","text":"from mu.algorithms.concept_ablation.algorithm import ( ConceptAblationAlgorithm, ) from mu.algorithms.concept_ablation.configs import ( ConceptAblationConfig, ) myconfig = ConceptAblationConfig() myconfig.ckpt_path = \"machine_unlearning/models/compvis/style50/compvis.ckpt\" myconfig.raw_dataset_dir = ( \"/home/ubuntu/Projects/balaram/packaging/data/quick-canvas-dataset/sample\" ) algorithm = ConceptAblationConfig(myconfig) algorithm.run()","title":"Create your own config object"},{"location":"unlearn/examples/concept_ablation/#override-the-config-class-itself","text":"from mu.algorithms.concept_ablation.algorithm import ( ConceptAblationAlgorithm, ) from mu.algorithms.concept_ablation.configs import ( ConceptAblationConfig, ) class MyNewConfigClass(ConceptAblationConfig): def __init__(self, *args, **kwargs): self.new_parameter = kwargs.get(\"new_parameter\") super().__init__() new_config_object = MyNewConfigClass() algorithm = ConceptAblationAlgorithm(new_config_object) algorithm.run()","title":"Override the Config class itself."},{"location":"unlearn/examples/erase_diff/","text":"Train your model by using Erase Diff Algorithm. Import pre defined config classes or create your own object. Refer the config docs for details about the parameters that you can use. To test the below code snippet, you can create a file, copy the below code in eg, my_trainer.py and execute it with python my_trainer.py or use WANDB_MODE=offline python my_trainer.py for offline mode. Use pre defined config from mu.algorithms.erase_diff.algorithm import EraseDiffAlgorithm from mu.algorithms.erase_diff.configs import erase_diff_train_mu algorithm = EraseDiffAlgorithm(erase_diff_train_mu) algorithm.run() Modify some train parameters in pre defined config class. View the config docs to see a list of available parameters. from mu.algorithms.erase_diff.algorithm import EraseDiffAlgorithm from mu.algorithms.erase_diff.configs import ( erase_diff_train_mu, ) algorithm = EraseDiffAlgorithm( erase_diff_train_mu, ckpt_path=\"models/compvis/style50/compvis.ckpt\", raw_dataset_dir=( \"data/quick-canvas-dataset/sample\" ), ) algorithm.run() Create your own config object from mu.algorithms.erase_diff.algorithm import EraseDiffAlgorithm from mu.algorithms.erase_diff.configs import ( EraseDiffConfig, ) myconfig = EraseDiffConfig() myconfig.ckpt_path = \"models/compvis/style50/compvis.ckpt\" myconfig.raw_dataset_dir = ( \"data/quick-canvas-dataset/sample\" ) algorithm = EraseDiffAlgorithm(myconfig) algorithm.run() Override the Config class itself. from mu.algorithms.erase_diff.algorithm import EraseDiffAlgorithm from mu.algorithms.erase_diff.configs import ( EraseDiffConfig, ) class MyNewConfigClass(EraseDiffConfig): def __init__(self, *args, **kwargs): self.new_parameter = kwargs.get(\"new_parameter\") super().__init__() new_config_object = MyNewConfigClass() algorithm = EraseDiffAlgorithm(new_config_object) algorithm.run()","title":"Examples"},{"location":"unlearn/examples/erase_diff/#use-pre-defined-config","text":"from mu.algorithms.erase_diff.algorithm import EraseDiffAlgorithm from mu.algorithms.erase_diff.configs import erase_diff_train_mu algorithm = EraseDiffAlgorithm(erase_diff_train_mu) algorithm.run()","title":"Use pre defined config"},{"location":"unlearn/examples/erase_diff/#modify-some-train-parameters-in-pre-defined-config-class","text":"View the config docs to see a list of available parameters. from mu.algorithms.erase_diff.algorithm import EraseDiffAlgorithm from mu.algorithms.erase_diff.configs import ( erase_diff_train_mu, ) algorithm = EraseDiffAlgorithm( erase_diff_train_mu, ckpt_path=\"models/compvis/style50/compvis.ckpt\", raw_dataset_dir=( \"data/quick-canvas-dataset/sample\" ), ) algorithm.run()","title":"Modify some train parameters in pre defined config class."},{"location":"unlearn/examples/erase_diff/#create-your-own-config-object","text":"from mu.algorithms.erase_diff.algorithm import EraseDiffAlgorithm from mu.algorithms.erase_diff.configs import ( EraseDiffConfig, ) myconfig = EraseDiffConfig() myconfig.ckpt_path = \"models/compvis/style50/compvis.ckpt\" myconfig.raw_dataset_dir = ( \"data/quick-canvas-dataset/sample\" ) algorithm = EraseDiffAlgorithm(myconfig) algorithm.run()","title":"Create your own config object"},{"location":"unlearn/examples/erase_diff/#override-the-config-class-itself","text":"from mu.algorithms.erase_diff.algorithm import EraseDiffAlgorithm from mu.algorithms.erase_diff.configs import ( EraseDiffConfig, ) class MyNewConfigClass(EraseDiffConfig): def __init__(self, *args, **kwargs): self.new_parameter = kwargs.get(\"new_parameter\") super().__init__() new_config_object = MyNewConfigClass() algorithm = EraseDiffAlgorithm(new_config_object) algorithm.run()","title":"Override the Config class itself."},{"location":"unlearn/examples/esd/","text":"Train your model by using Esd Algorithm. Import pre defined config classes or create your own object. Refer the config docs for details about the parameters that you can use. To test the below code snippet, you can create a file, copy the below code in eg, my_trainer.py and execute it with python my_trainer.py or use WANDB_MODE=offline python my_trainer.py for offline mode. Use pre defined config from mu.algorithms.esd.algorithm import ESDAlgorithm from mu.algorithms.esd.configs import esd_train_mu algorithm = ESDAlgorithm(esd_train_mu) algorithm.run() Modify some train parameters in pre defined config class. from mu.algorithms.esd.algorithm import ESDAlgorithm from mu.algorithms.esd.configs import ( esd_train_mu, ) algorithm = ESDAlgorithm( esd_train_mu, ckpt_path=\"models/compvis/style50/compvis.ckpt\", raw_dataset_dir=( \"data/quick-canvas-dataset/sample\" ), ) algorithm.run() Create your own config object from mu.algorithms.esd.algorithm import ESDAlgorithm from mu.algorithms.esd.configs import ( ESDConfig, ) myconfig = ESDConfig() myconfig.ckpt_path = \"models/compvis/style50/compvis.ckpt\" myconfig.raw_dataset_dir = ( \"data/quick-canvas-dataset/sample\" ) algorithm = ESDAlgorithm(myconfig) algorithm.run() Override the Config class itself. from mu.algorithms.esd.algorithm import ESDAlgorithm from mu.algorithms.esd.configs import ( ESDConfig, ) class MyNewConfigClass(ESDConfig): def __init__(self, *args, **kwargs): self.new_parameter = kwargs.get(\"new_parameter\") super().__init__() new_config_object = MyNewConfigClass() algorithm = ESDAlgorithm(new_config_object) algorithm.run()","title":"Examples"},{"location":"unlearn/examples/esd/#use-pre-defined-config","text":"from mu.algorithms.esd.algorithm import ESDAlgorithm from mu.algorithms.esd.configs import esd_train_mu algorithm = ESDAlgorithm(esd_train_mu) algorithm.run()","title":"Use pre defined config"},{"location":"unlearn/examples/esd/#modify-some-train-parameters-in-pre-defined-config-class","text":"from mu.algorithms.esd.algorithm import ESDAlgorithm from mu.algorithms.esd.configs import ( esd_train_mu, ) algorithm = ESDAlgorithm( esd_train_mu, ckpt_path=\"models/compvis/style50/compvis.ckpt\", raw_dataset_dir=( \"data/quick-canvas-dataset/sample\" ), ) algorithm.run()","title":"Modify some train parameters in pre defined config class."},{"location":"unlearn/examples/esd/#create-your-own-config-object","text":"from mu.algorithms.esd.algorithm import ESDAlgorithm from mu.algorithms.esd.configs import ( ESDConfig, ) myconfig = ESDConfig() myconfig.ckpt_path = \"models/compvis/style50/compvis.ckpt\" myconfig.raw_dataset_dir = ( \"data/quick-canvas-dataset/sample\" ) algorithm = ESDAlgorithm(myconfig) algorithm.run()","title":"Create your own config object"},{"location":"unlearn/examples/esd/#override-the-config-class-itself","text":"from mu.algorithms.esd.algorithm import ESDAlgorithm from mu.algorithms.esd.configs import ( ESDConfig, ) class MyNewConfigClass(ESDConfig): def __init__(self, *args, **kwargs): self.new_parameter = kwargs.get(\"new_parameter\") super().__init__() new_config_object = MyNewConfigClass() algorithm = ESDAlgorithm(new_config_object) algorithm.run()","title":"Override the Config class itself."},{"location":"unlearn/examples/forget_me_not/","text":"You can modify the parameters, when using config class itself. View the config docs to see a list of available parameters that you can use. Train a text inversion (train_ti) Using default config from mu.algorithms.forget_me_not.algorithm import ForgetMeNotAlgorithm from mu.algorithms.forget_me_not.configs import ( forget_me_not_train_ti_mu, ) algorithm = ForgetMeNotAlgorithm( forget_me_not_train_ti_mu ) algorithm.run(train_type=\"train_ti\") Modify some parameters in pre-defined config class from mu.algorithms.forget_me_not.algorithm import ForgetMeNotAlgorithm from mu.algorithms.forget_me_not.configs import ( forget_me_not_train_ti_mu, ) algorithm = ForgetMeNotAlgorithm( forget_me_not_train_ti_mu, ckpt_path=\"models/diffuser/style50\", raw_dataset_dir=( \"data/quick-canvas-dataset/sample\" ), steps=10 ) algorithm.run(train_type=\"train_ti\") Create your own config class from mu.algorithms.forget_me_not.algorithm import ForgetMeNotAlgorithm from mu.algorithms.forget_me_not.configs import ( ForgetMeNotTiConfig, ) myconfig = ForgetMeNotTiConfig() myconfig.ckpt_path = \"models/diffuser/style50\" myconfig.steps = 1 algorithm = ForgetMeNotAlgorithm( myconfig ) algorithm.run(train_type=\"train_ti\") Perform unlearning by using train attn. Before running the train_attn script, update the ti_weights_path parameter in the configuration file to point to the output generated from the Text Inversion (train_ti.py) stage Using default config from mu.algorithms.forget_me_not.algorithm import ForgetMeNotAlgorithm from mu.algorithms.forget_me_not.configs import ( forget_me_not_train_attn_mu, ) algorithm = ForgetMeNotAlgorithm( forget_me_not_train_attn_mu, ) algorithm.run(train_type=\"train_attn\") Modify some parameters in pre-defined config class from mu.algorithms.forget_me_not.algorithm import ForgetMeNotAlgorithm from mu.algorithms.forget_me_not.configs import ( forget_me_not_train_attn_mu, ) algorithm = ForgetMeNotAlgorithm( forget_me_not_train_attn_mu, ckpt_path=\"models/diffuser/style50\", raw_dataset_dir=( \"data/quick-canvas-dataset/sample\" ), steps=10, ti_weights_path=\"outputs/forget_me_not/finetuned_models/Abstractionism/step_inv_10.safetensors\" ) algorithm.run(train_type=\"train_attn\") Create your own config class from mu.algorithms.forget_me_not.algorithm import ForgetMeNotAlgorithm from mu.algorithms.forget_me_not.configs import ( ForgetMeNotAttnConfig, ) myconfig = ForgetMeNotAttnConfig() myconfig.ckpt_path = \"models/diffuser/style50\" myconfig.steps = 1 algorithm = ForgetMeNotAlgorithm( myconfig ) algorithm.run(train_type=\"train_attn\")","title":"Examples"},{"location":"unlearn/examples/forget_me_not/#train-a-text-inversion-train_ti","text":"Using default config from mu.algorithms.forget_me_not.algorithm import ForgetMeNotAlgorithm from mu.algorithms.forget_me_not.configs import ( forget_me_not_train_ti_mu, ) algorithm = ForgetMeNotAlgorithm( forget_me_not_train_ti_mu ) algorithm.run(train_type=\"train_ti\") Modify some parameters in pre-defined config class from mu.algorithms.forget_me_not.algorithm import ForgetMeNotAlgorithm from mu.algorithms.forget_me_not.configs import ( forget_me_not_train_ti_mu, ) algorithm = ForgetMeNotAlgorithm( forget_me_not_train_ti_mu, ckpt_path=\"models/diffuser/style50\", raw_dataset_dir=( \"data/quick-canvas-dataset/sample\" ), steps=10 ) algorithm.run(train_type=\"train_ti\") Create your own config class from mu.algorithms.forget_me_not.algorithm import ForgetMeNotAlgorithm from mu.algorithms.forget_me_not.configs import ( ForgetMeNotTiConfig, ) myconfig = ForgetMeNotTiConfig() myconfig.ckpt_path = \"models/diffuser/style50\" myconfig.steps = 1 algorithm = ForgetMeNotAlgorithm( myconfig ) algorithm.run(train_type=\"train_ti\")","title":"Train a text inversion (train_ti)"},{"location":"unlearn/examples/forget_me_not/#perform-unlearning-by-using-train-attn","text":"Before running the train_attn script, update the ti_weights_path parameter in the configuration file to point to the output generated from the Text Inversion (train_ti.py) stage Using default config from mu.algorithms.forget_me_not.algorithm import ForgetMeNotAlgorithm from mu.algorithms.forget_me_not.configs import ( forget_me_not_train_attn_mu, ) algorithm = ForgetMeNotAlgorithm( forget_me_not_train_attn_mu, ) algorithm.run(train_type=\"train_attn\") Modify some parameters in pre-defined config class from mu.algorithms.forget_me_not.algorithm import ForgetMeNotAlgorithm from mu.algorithms.forget_me_not.configs import ( forget_me_not_train_attn_mu, ) algorithm = ForgetMeNotAlgorithm( forget_me_not_train_attn_mu, ckpt_path=\"models/diffuser/style50\", raw_dataset_dir=( \"data/quick-canvas-dataset/sample\" ), steps=10, ti_weights_path=\"outputs/forget_me_not/finetuned_models/Abstractionism/step_inv_10.safetensors\" ) algorithm.run(train_type=\"train_attn\") Create your own config class from mu.algorithms.forget_me_not.algorithm import ForgetMeNotAlgorithm from mu.algorithms.forget_me_not.configs import ( ForgetMeNotAttnConfig, ) myconfig = ForgetMeNotAttnConfig() myconfig.ckpt_path = \"models/diffuser/style50\" myconfig.steps = 1 algorithm = ForgetMeNotAlgorithm( myconfig ) algorithm.run(train_type=\"train_attn\")","title":"Perform unlearning by using train attn."},{"location":"unlearn/examples/saliency/","text":"Train your model by using Saliency Unlearning Algorithm. Import pre defined config classes or create your own object. Refer to the config docs for details about the parameters that you can use. To test the below code snippet, you can create a file, copy the below code in, e.g., my_trainer.py and execute it with python my_trainer.py or use WANDB_MODE=offline python my_trainer.py for offline mode. Use Pre-defined config class from mu.algorithms.saliency_unlearning.algorithm import ( SaliencyUnlearningAlgorithm, ) from mu.algorithms.saliency_unlearning.configs import saliency_unlearning_train_mu algorithm = SaliencyUnlearningAlgorithm(saliency_unlearning_train_mu) algorithm.run() Modify some parameters in pre-defined config class from mu.algorithms.saliency_unlearning.algorithm import ( SaliencyUnlearningAlgorithm, ) from mu.algorithms.saliency_unlearning.configs import ( saliency_unlearning_train_mu, ) algorithm = SaliencyUnlearningAlgorithm( saliency_unlearning_train_mu, output_dir=\"/opt/dlami/nvme/outputs\", ) algorithm.run() Create your own config object from mu.algorithms.saliency_unlearning.algorithm import SaliencyUnlearningAlgorithm from mu.algorithms.saliency_unlearning.configs import ( SaliencyUnlearningConfig, ) myconfig = SaliencyUnlearningConfig() myconfig.output_dir = ( \"/opt/dlami/nvme/outputs\" ) algorithm = SaliencyUnlearningAlgorithm(myconfig) algorithm.run() Override the Config class itself from mu.algorithms.saliency_unlearning.algorithm import SaliencyUnlearningAlgorithm from mu.algorithms.saliency_unlearning.configs import ( SaliencyUnlearningConfig, ) class MyNewConfigClass(SaliencyUnlearningConfig): def __init__(self, *args, **kwargs): self.new_parameter = kwargs.get(\"new_parameter\") super().__init__() new_config_object = MyNewConfigClass() algorithm = SaliencyUnlearningAlgorithm(new_config_object) algorithm.run()","title":"Examples"},{"location":"unlearn/examples/saliency/#use-pre-defined-config-class","text":"from mu.algorithms.saliency_unlearning.algorithm import ( SaliencyUnlearningAlgorithm, ) from mu.algorithms.saliency_unlearning.configs import saliency_unlearning_train_mu algorithm = SaliencyUnlearningAlgorithm(saliency_unlearning_train_mu) algorithm.run()","title":"Use Pre-defined config class"},{"location":"unlearn/examples/saliency/#modify-some-parameters-in-pre-defined-config-class","text":"from mu.algorithms.saliency_unlearning.algorithm import ( SaliencyUnlearningAlgorithm, ) from mu.algorithms.saliency_unlearning.configs import ( saliency_unlearning_train_mu, ) algorithm = SaliencyUnlearningAlgorithm( saliency_unlearning_train_mu, output_dir=\"/opt/dlami/nvme/outputs\", ) algorithm.run()","title":"Modify some parameters in pre-defined config class"},{"location":"unlearn/examples/saliency/#create-your-own-config-object","text":"from mu.algorithms.saliency_unlearning.algorithm import SaliencyUnlearningAlgorithm from mu.algorithms.saliency_unlearning.configs import ( SaliencyUnlearningConfig, ) myconfig = SaliencyUnlearningConfig() myconfig.output_dir = ( \"/opt/dlami/nvme/outputs\" ) algorithm = SaliencyUnlearningAlgorithm(myconfig) algorithm.run()","title":"Create your own config object"},{"location":"unlearn/examples/saliency/#override-the-config-class-itself","text":"from mu.algorithms.saliency_unlearning.algorithm import SaliencyUnlearningAlgorithm from mu.algorithms.saliency_unlearning.configs import ( SaliencyUnlearningConfig, ) class MyNewConfigClass(SaliencyUnlearningConfig): def __init__(self, *args, **kwargs): self.new_parameter = kwargs.get(\"new_parameter\") super().__init__() new_config_object = MyNewConfigClass() algorithm = SaliencyUnlearningAlgorithm(new_config_object) algorithm.run()","title":"Override the Config class itself"},{"location":"unlearn/examples/scissorhands/","text":"Train your model by using ScissorHands Algorithm. Import pre defined config classes or create your own object. Refer the config docs for details about the parameters that you can use. To test the below code snippet, you can create a file, copy the below code in eg, my_trainer.py and execute it with python my_trainer.py or use WANDB_MODE=offline python my_trainer.py for offline mode. Use pre defined config from mu.algorithms.scissorhands.algorithm import ScissorHandsAlgorithm from mu.algorithms.scissorhands.configs import scissorhands_train_mu algorithm = ScissorHandsAlgorithm(scissorhands_train_mu) algorithm.run() Modify some train parameters in pre defined config class. View the config docs to see a list of available parameters. from mu.algorithms.scissorhands.algorithm import ScissorHandsAlgorithm from mu.algorithms.scissorhands.configs import ( scissorhands_train_mu, ) algorithm = ScissorHandsAlgorithm( scissorhands_train_mu, ckpt_path=\"models/compvis/style50/compvis.ckpt\", raw_dataset_dir=( \"data/quick-canvas-dataset/sample\" ), output_dir=\"/opt/dlami/nvme/outputs\", ) algorithm.run() Create your own config object from mu.algorithms.scissorhands.algorithm import ScissorHandsAlgorithm from mu.algorithms.scissorhands.configs import ( ScissorHandsConfig, ) myconfig = ScissorHandsConfig() myconfig.ckpt_path = \"models/compvis/style50/compvis.ckpt\" myconfig.raw_dataset_dir = ( \"data/quick-canvas-dataset/sample\" ) algorithm = ScissorHandsAlgorithm(myconfig) algorithm.run() Override the Config class itself. from mu.algorithms.scissorhands.algorithm import ScissorHandsAlgorithm from mu.algorithms.scissorhands.configs import ( ScissorHandsConfig, ) class MyNewConfigClass(ScissorHandsConfig): def __init__(self, *args, **kwargs): self.new_parameter = kwargs.get(\"new_parameter\") super().__init__() new_config_object = MyNewConfigClass() algorithm = ScissorHandsAlgorithm(new_config_object) algorithm.run()","title":"Examples"},{"location":"unlearn/examples/scissorhands/#use-pre-defined-config","text":"from mu.algorithms.scissorhands.algorithm import ScissorHandsAlgorithm from mu.algorithms.scissorhands.configs import scissorhands_train_mu algorithm = ScissorHandsAlgorithm(scissorhands_train_mu) algorithm.run()","title":"Use pre defined config"},{"location":"unlearn/examples/scissorhands/#modify-some-train-parameters-in-pre-defined-config-class","text":"View the config docs to see a list of available parameters. from mu.algorithms.scissorhands.algorithm import ScissorHandsAlgorithm from mu.algorithms.scissorhands.configs import ( scissorhands_train_mu, ) algorithm = ScissorHandsAlgorithm( scissorhands_train_mu, ckpt_path=\"models/compvis/style50/compvis.ckpt\", raw_dataset_dir=( \"data/quick-canvas-dataset/sample\" ), output_dir=\"/opt/dlami/nvme/outputs\", ) algorithm.run()","title":"Modify some train parameters in pre defined config class."},{"location":"unlearn/examples/scissorhands/#create-your-own-config-object","text":"from mu.algorithms.scissorhands.algorithm import ScissorHandsAlgorithm from mu.algorithms.scissorhands.configs import ( ScissorHandsConfig, ) myconfig = ScissorHandsConfig() myconfig.ckpt_path = \"models/compvis/style50/compvis.ckpt\" myconfig.raw_dataset_dir = ( \"data/quick-canvas-dataset/sample\" ) algorithm = ScissorHandsAlgorithm(myconfig) algorithm.run()","title":"Create your own config object"},{"location":"unlearn/examples/scissorhands/#override-the-config-class-itself","text":"from mu.algorithms.scissorhands.algorithm import ScissorHandsAlgorithm from mu.algorithms.scissorhands.configs import ( ScissorHandsConfig, ) class MyNewConfigClass(ScissorHandsConfig): def __init__(self, *args, **kwargs): self.new_parameter = kwargs.get(\"new_parameter\") super().__init__() new_config_object = MyNewConfigClass() algorithm = ScissorHandsAlgorithm(new_config_object) algorithm.run()","title":"Override the Config class itself."},{"location":"unlearn/examples/selective_amnesia/","text":"To perform training using selective amnesia. You'll need to download the full fisher file first. Download, it at mu/algorithms/selective_amnesia/data folder. Use the following command wget https://huggingface.co/ajrheng/selective-amnesia/resolve/main/full_fisher_dict.pkl Use pre defined config from mu.algorithms.selective_amnesia.algorithm import SelectiveAmnesiaAlgorithm from mu.algorithms.selective_amnesia.configs import ( selective_amnesia_config_quick_canvas, ) algorithm = SelectiveAmnesiaAlgorithm( selective_amnesia_config_quick_canvas ) algorithm.run() Modify some train parameters in pre defined config class. View the config docs to see a list of available parameters. from mu.algorithms.selective_amnesia.algorithm import SelectiveAmnesiaAlgorithm from mu.algorithms.selective_amnesia.configs import ( selective_amnesia_config_quick_canvas, ) algorithm = SelectiveAmnesiaAlgorithm( selective_amnesia_config_quick_canvas, ckpt_path=\"models/compvis/style50/compvis.ckpt\", raw_dataset_dir=( \"data/quick-canvas-dataset/sample\" ), ) algorithm.run() Create your own config object from mu.algorithms.selective_amnesia.algorithm import SelectiveAmnesiaAlgorithm from mu.algorithms.selective_amnesia.configs import ( SelectiveAmnesiaConfig, ) myconfig = SelectiveAmnesiaConfig() myconfig.ckpt_path = \"models/compvis/style50/compvis.ckpt\" myconfig.raw_dataset_dir = ( \"data/quick-canvas-dataset/sample\" ) algorithm = SelectiveAmnesiaAlgorithm(myconfig) algorithm.run() Override the Config class itself. from mu.algorithms.selective_amnesia.algorithm import SelectiveAmnesiaAlgorithm from mu.algorithms.selective_amnesia.configs import ( SelectiveAmnesiaConfig, ) class MyNewConfigClass(SelectiveAmnesiaAlgorithm): def __init__(self, *args, **kwargs): self.new_parameter = kwargs.get(\"new_parameter\") super().__init__() new_config_object = MyNewConfigClass() algorithm = SelectiveAmnesiaAlgorithm(new_config_object) algorithm.run()","title":"Examples"},{"location":"unlearn/examples/selective_amnesia/#use-pre-defined-config","text":"from mu.algorithms.selective_amnesia.algorithm import SelectiveAmnesiaAlgorithm from mu.algorithms.selective_amnesia.configs import ( selective_amnesia_config_quick_canvas, ) algorithm = SelectiveAmnesiaAlgorithm( selective_amnesia_config_quick_canvas ) algorithm.run()","title":"Use pre defined config"},{"location":"unlearn/examples/selective_amnesia/#modify-some-train-parameters-in-pre-defined-config-class","text":"View the config docs to see a list of available parameters. from mu.algorithms.selective_amnesia.algorithm import SelectiveAmnesiaAlgorithm from mu.algorithms.selective_amnesia.configs import ( selective_amnesia_config_quick_canvas, ) algorithm = SelectiveAmnesiaAlgorithm( selective_amnesia_config_quick_canvas, ckpt_path=\"models/compvis/style50/compvis.ckpt\", raw_dataset_dir=( \"data/quick-canvas-dataset/sample\" ), ) algorithm.run()","title":"Modify some train parameters in pre defined config class."},{"location":"unlearn/examples/selective_amnesia/#create-your-own-config-object","text":"from mu.algorithms.selective_amnesia.algorithm import SelectiveAmnesiaAlgorithm from mu.algorithms.selective_amnesia.configs import ( SelectiveAmnesiaConfig, ) myconfig = SelectiveAmnesiaConfig() myconfig.ckpt_path = \"models/compvis/style50/compvis.ckpt\" myconfig.raw_dataset_dir = ( \"data/quick-canvas-dataset/sample\" ) algorithm = SelectiveAmnesiaAlgorithm(myconfig) algorithm.run()","title":"Create your own config object"},{"location":"unlearn/examples/selective_amnesia/#override-the-config-class-itself","text":"from mu.algorithms.selective_amnesia.algorithm import SelectiveAmnesiaAlgorithm from mu.algorithms.selective_amnesia.configs import ( SelectiveAmnesiaConfig, ) class MyNewConfigClass(SelectiveAmnesiaAlgorithm): def __init__(self, *args, **kwargs): self.new_parameter = kwargs.get(\"new_parameter\") super().__init__() new_config_object = MyNewConfigClass() algorithm = SelectiveAmnesiaAlgorithm(new_config_object) algorithm.run()","title":"Override the Config class itself."},{"location":"unlearn/examples/semipermeable_membrane/","text":"Train your model by using Semi Permeable Membrane Algorithm. Import pre defined config classes or create your own object. Refer the config docs for details about the parameters that you can use. To test the below code snippet, you can create a file, copy the below code in eg, my_trainer.py and execute it with python my_trainer.py or use WANDB_MODE=offline python my_trainer.py for offline mode. Use Pre defined config class from mu.algorithms.semipermeable_membrane.algorithm import ( SemipermeableMembraneAlgorithm, ) from mu.algorithms.semipermeable_membrane.configs import semipermiable_membrane_train_config_quick_canvas algorithm = SemipermeableMembraneAlgorithm(semipermiable_membrane_train_config_quick_canvas) algorithm.run() Modify some parameters in pre defined config class Use config docs to view available options. You can update values within dictionaries by passing only the value that you want to change as below in when passing train={'iterations :1}`. from mu.algorithms.semipermeable_membrane.algorithm import ( SemipermeableMembraneAlgorithm, ) from mu.algorithms.semipermeable_membrane.configs import ( semipermiable_membrane_train_mu, SemipermeableMembraneConfig, ) algorithm = SemipermeableMembraneAlgorithm( semipermiable_membrane_train_mu, output_dir=\"/opt/dlami/nvme/outputs\", train={\"iterations\": 2}, ) algorithm.run() Create your own config object from mu.algorithms.semipermeable_membrane.algorithm import SemipermeableMembraneAlgorithm from mu.algorithms.semipermeable_membrane.configs import ( SemipermeableMembraneConfig, ) myconfig = SemipermeableMembraneConfig() myconfig.output_dir = ( \"/opt/dlami/nvme/outputs\" ) algorithm = SemipermeableMembraneAlgorithm(myconfig) algorithm.run() Override the Config class itself. from mu.algorithms.semipermeable_membrane.algorithm import SemipermeableMembraneAlgorithm from mu.algorithms.semipermeable_membrane.configs import ( SemipermeableMembraneConfig, ) class MyNewConfigClass(SemipermeableMembraneConfig): def __init__(self, *args, **kwargs): self.new_parameter = kwargs.get(\"new_parameter\") super().__init__() new_config_object = MyNewConfigClass() algorithm = SemipermeableMembraneAlgorithm(new_config_object) algorithm.run()","title":"Examples"},{"location":"unlearn/examples/semipermeable_membrane/#use-pre-defined-config-class","text":"from mu.algorithms.semipermeable_membrane.algorithm import ( SemipermeableMembraneAlgorithm, ) from mu.algorithms.semipermeable_membrane.configs import semipermiable_membrane_train_config_quick_canvas algorithm = SemipermeableMembraneAlgorithm(semipermiable_membrane_train_config_quick_canvas) algorithm.run()","title":"Use Pre defined config class"},{"location":"unlearn/examples/semipermeable_membrane/#modify-some-parameters-in-pre-defined-config-class","text":"Use config docs to view available options. You can update values within dictionaries by passing only the value that you want to change as below in when passing train={'iterations :1}`. from mu.algorithms.semipermeable_membrane.algorithm import ( SemipermeableMembraneAlgorithm, ) from mu.algorithms.semipermeable_membrane.configs import ( semipermiable_membrane_train_mu, SemipermeableMembraneConfig, ) algorithm = SemipermeableMembraneAlgorithm( semipermiable_membrane_train_mu, output_dir=\"/opt/dlami/nvme/outputs\", train={\"iterations\": 2}, ) algorithm.run()","title":"Modify some parameters in pre defined config class"},{"location":"unlearn/examples/semipermeable_membrane/#create-your-own-config-object","text":"from mu.algorithms.semipermeable_membrane.algorithm import SemipermeableMembraneAlgorithm from mu.algorithms.semipermeable_membrane.configs import ( SemipermeableMembraneConfig, ) myconfig = SemipermeableMembraneConfig() myconfig.output_dir = ( \"/opt/dlami/nvme/outputs\" ) algorithm = SemipermeableMembraneAlgorithm(myconfig) algorithm.run()","title":"Create your own config object"},{"location":"unlearn/examples/semipermeable_membrane/#override-the-config-class-itself","text":"from mu.algorithms.semipermeable_membrane.algorithm import SemipermeableMembraneAlgorithm from mu.algorithms.semipermeable_membrane.configs import ( SemipermeableMembraneConfig, ) class MyNewConfigClass(SemipermeableMembraneConfig): def __init__(self, *args, **kwargs): self.new_parameter = kwargs.get(\"new_parameter\") super().__init__() new_config_object = MyNewConfigClass() algorithm = SemipermeableMembraneAlgorithm(new_config_object) algorithm.run()","title":"Override the Config class itself."},{"location":"unlearn/examples/uce/","text":"Train your model by using Unified Concept Editing Algorithm. Import pre defined config classes or create your own object. Refer the config docs for details about the parameters that you can use. To test the below code snippet, you can create a file, copy the below code in eg, my_trainer.py and execute it with python my_trainer.py or use WANDB_MODE=offline python my_trainer.py for offline mode. Use Pre defined config class from mu.algorithms.unified_concept_editing.algorithm import ( UnifiedConceptEditingAlgorithm, ) from mu.algorithms.unified_concept_editing.configs import unified_concept_editing_train_mu algorithm = UnifiedConceptEditingAlgorithm(unified_concept_editing_train_mu) algorithm.run() Modify some parameters in pre defined config class from mu.algorithms.unified_concept_editing.algorithm import ( UnifiedConceptEditingAlgorithm, ) from mu.algorithms.unified_concept_editing.configs import ( unified_concept_editing_train_mu, ) algorithm = UnifiedConceptEditingAlgorithm( unified_concept_editing_train_mu, ckpt_path=\"models/diffuser/style50/\", raw_dataset_dir=( \"data/quick-canvas-dataset/sample\" ), output_dir=\"/opt/dlami/nvme/outputs\", ) algorithm.run() Create your own config object from mu.algorithms.unified_concept_editing.algorithm import UnifiedConceptEditingAlgorithm from mu.algorithms.unified_concept_editing.configs import ( UnifiedConceptEditingConfig, ) myconfig = UnifiedConceptEditingConfig() myconfig.ckpt_path = \"models/compvis/style50/compvis.ckpt\" myconfig.raw_dataset_dir = ( \"data/quick-canvas-dataset/sample\" ) algorithm = UnifiedConceptEditingAlgorithm(myconfig) algorithm.run() Override the Config class itself. from mu.algorithms.unified_concept_editing.algorithm import UnifiedConceptEditingAlgorithm from mu.algorithms.unified_concept_editing.configs import ( UnifiedConceptEditingConfig, ) UnifiedConceptEditingAlgorithm class MyNewConfigClass(UnifiedConceptEditingConfig): def __init__(self, *args, **kwargs): self.new_parameter = kwargs.get(\"new_parameter\") super().__init__() new_config_object = MyNewConfigClass() algorithm = UnifiedConceptEditingAlgorithm(new_config_object) algorithm.run()","title":"Examples"},{"location":"unlearn/examples/uce/#use-pre-defined-config-class","text":"from mu.algorithms.unified_concept_editing.algorithm import ( UnifiedConceptEditingAlgorithm, ) from mu.algorithms.unified_concept_editing.configs import unified_concept_editing_train_mu algorithm = UnifiedConceptEditingAlgorithm(unified_concept_editing_train_mu) algorithm.run()","title":"Use Pre defined config class"},{"location":"unlearn/examples/uce/#modify-some-parameters-in-pre-defined-config-class","text":"from mu.algorithms.unified_concept_editing.algorithm import ( UnifiedConceptEditingAlgorithm, ) from mu.algorithms.unified_concept_editing.configs import ( unified_concept_editing_train_mu, ) algorithm = UnifiedConceptEditingAlgorithm( unified_concept_editing_train_mu, ckpt_path=\"models/diffuser/style50/\", raw_dataset_dir=( \"data/quick-canvas-dataset/sample\" ), output_dir=\"/opt/dlami/nvme/outputs\", ) algorithm.run()","title":"Modify some parameters in pre defined config class"},{"location":"unlearn/examples/uce/#create-your-own-config-object","text":"from mu.algorithms.unified_concept_editing.algorithm import UnifiedConceptEditingAlgorithm from mu.algorithms.unified_concept_editing.configs import ( UnifiedConceptEditingConfig, ) myconfig = UnifiedConceptEditingConfig() myconfig.ckpt_path = \"models/compvis/style50/compvis.ckpt\" myconfig.raw_dataset_dir = ( \"data/quick-canvas-dataset/sample\" ) algorithm = UnifiedConceptEditingAlgorithm(myconfig) algorithm.run()","title":"Create your own config object"},{"location":"unlearn/examples/uce/#override-the-config-class-itself","text":"from mu.algorithms.unified_concept_editing.algorithm import UnifiedConceptEditingAlgorithm from mu.algorithms.unified_concept_editing.configs import ( UnifiedConceptEditingConfig, ) UnifiedConceptEditingAlgorithm class MyNewConfigClass(UnifiedConceptEditingConfig): def __init__(self, *args, **kwargs): self.new_parameter = kwargs.get(\"new_parameter\") super().__init__() new_config_object = MyNewConfigClass() algorithm = UnifiedConceptEditingAlgorithm(new_config_object) algorithm.run()","title":"Override the Config class itself."},{"location":"unlearn/mu_defense/adv_unlearn/","text":"MU_DEFENSE This repository is for mu_defense that implements adversarial unlearning by integrating a soft prompt attack into the training loop. In this process, a random prompt is selected and its embedding is adversarially perturbed\u2014either at the word or conditional embedding level\u2014to steer the model into unlearning unwanted associations while preserving overall performance. Installation Prerequisities Ensure conda is installed on your system. You can install Miniconda or Anaconda: Miniconda (recommended): https://docs.conda.io/en/latest/miniconda.html Anaconda : https://www.anaconda.com/products/distribution After installing conda , ensure it is available in your PATH by running. You may require to restart the terminal session: Step-by-Step Setup: Step 1. Create a Conda Environment Create a new Conda environment named myenv with Python 3.8.5: conda create -n myenv python=3.8.5 Step 2. Activate the Environment Activate the environment to work within it: conda activate myenv Step 3. Install Core Dependencies Install PyTorch, torchvision, CUDA Toolkit, and ONNX Runtime with specific versions: conda install pytorch==1.11.0 torchvision==0.12.0 cudatoolkit=11.3 onnxruntime==1.16.3 -c pytorch -c conda-forge Step 4. Install our unlearn_diff Package using pip: pip install unlearn_diff Step 5. Install Additional Git Dependencies: After installing unlearn_diff, install the following Git-based dependencies in the same Conda environment to ensure full functionality: pip install git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers pip install git+https://github.com/openai/CLIP.git@main#egg=clip pip install git+https://github.com/crowsonkb/k-diffusion.git pip install git+https://github.com/cocodataset/panopticapi.git pip install git+https://github.com/Phoveran/fastargs.git@main#egg=fastargs pip install git+https://github.com/boomb0om/text2image-benchmark Example usage to Run Defense for compvis To test the below code snippet, you can create a file, copy the below code in eg, mu_defense.py and execute it with python mu_defense.py or use WANDB_MODE=offline python mu_defense.py for offline mode. Run with default config from mu_defense.algorithms.adv_unlearn.algorithm import AdvUnlearnAlgorithm from mu_defense.algorithms.adv_unlearn.configs import adv_unlearn_config from mu.algorithms.erase_diff.configs import erase_diff_train_mu def mu_defense(): mu_defense = AdvUnlearnAlgorithm( config=adv_unlearn_config ) mu_defense.run() if __name__ == \"__main__\": mu_defense() Modify some train parameters in pre defined config class. View the config descriptions to see a list of available parameters. from mu_defense.algorithms.adv_unlearn.algorithm import AdvUnlearnAlgorithm from mu_defense.algorithms.adv_unlearn.configs import adv_unlearn_config from mu.algorithms.erase_diff.configs import erase_diff_train_mu def mu_defense(): mu_defense = AdvUnlearnAlgorithm( config=adv_unlearn_config, compvis_ckpt_path = \"outputs/erase_diff/erase_diff_Abstractionism_model.pth\", attack_step = 2, backend = \"compvis\", attack_method = \"fast_at\", warmup_iter = 1, iterations = 2, model_config_path = erase_diff_train_mu.model_config_path ) mu_defense.run() if __name__ == \"__main__\": mu_defense() Note: import the model_config_path from the relevant algorithm's configuration module in the mu package Example usage to Run defense for diffusers Run with default config from mu_defense.algorithms.adv_unlearn.algorithm import AdvUnlearnAlgorithm from mu_defense.algorithms.adv_unlearn.configs import adv_unlearn_config def mu_defense(): mu_defense = AdvUnlearnAlgorithm( config=adv_unlearn_config ) mu_defense.run() if __name__ == \"__main__\": mu_defense() Modify some train parameters in pre defined config class. View the config descriptions to see a list of available parameters. from mu_defense.algorithms.adv_unlearn.algorithm import AdvUnlearnAlgorithm from mu_defense.algorithms.adv_unlearn.configs import adv_unlearn_config def mu_defense(): mu_defense = AdvUnlearnAlgorithm( config=adv_unlearn_config, diffusers_model_name_or_path = \"outputs/forget_me_not/finetuned_models/Abstractionism\", attack_step = 2, backend = \"diffusers\", attack_method = \"fast_at\", warmup_iter = 1, iterations = 2 ) mu_defense.run() if __name__ == \"__main__\": mu_defense() Description of fields in config file Below is a detailed description of the configuration fields available in the adv_unlearn_config.py file. The descriptions match those provided in the help section of the command-line arguments. Inference & Model Paths model_config_path Description : Config path for stable diffusion model. Use for compvis model only. Type : str Example : configs/stable-diffusion/v1-inference.yaml compvis_ckpt_path Description : Checkpoint path for stable diffusion v1-4. Type : str Example : models/sd-v1-4-full-ema.ckpt encoder_model_name_or_path Description : Model name or path for the encoder. Type : str Example : CompVis/stable-diffusion-v1-4 cache_path Description : Directory used for caching model files. Type : str Example : .cache diffusers_model_name_or_path Description : Model name or path for the diffusers (if used). Type : str Example : outputs/forget_me_not/finetuned_models/Abstractionism target_ckpt Description : Optionally load a target checkpoint into the model for diffuser sampling. Type : Typically str or None Example : path to target checkpoint path Devices & IO devices Description : CUDA devices to train on. Type : str Example : 0,0 seperator Description : Separator used if you want to train a bunch of words separately. Type : str or None Example : None output_dir Description : Directory where output files (e.g., checkpoints, logs) are saved. Type : str Example : outputs/adv_unlearn Image & Diffusion Sampling image_size Description : Image size used to train. Type : int Example : 512 ddim_steps Description : Number of DDIM steps for inference during training. Type : int Example : 50 start_guidance Description : Guidance of start image used to train. Type : float Example : 3.0 negative_guidance Description : Guidance of negative training used to train. Type : float Example : 1.0 ddim_eta Description : DDIM eta parameter for sampling. Type : int or float Example : 0 Training Setup prompt Description : Prompt corresponding to the concept to erase. Type : str Example : nudity dataset_retain Description : Prompts corresponding to non-target concepts to retain. Type : str Choices : coco_object , coco_object_no_filter , imagenet243 , imagenet243_no_filter Example : coco_object retain_batch Description : Batch size of retaining prompts during training. Type : int Example : 5 retain_train Description : Retaining training mode; choose between iterative ( iter ) or regularization ( reg ). Type : str Choices : iter , reg Example : iter retain_step Description : Number of steps for retaining prompts. Type : int Example : 1 retain_loss_w Description : Retaining loss weight. Type : float Example : 1.0 train_method Description : Method of training. Type : str Choices : text_encoder_full , text_encoder_layer0 , text_encoder_layer01 , text_encoder_layer012 , text_encoder_layer0123 , text_encoder_layer01234 , text_encoder_layer012345 , text_encoder_layer0123456 , text_encoder_layer01234567 , text_encoder_layer012345678 , text_encoder_layer0123456789 , text_encoder_layer012345678910 , text_encoder_layer01234567891011 , text_encoder_layer0_11 , text_encoder_layer01_1011 , text_encoder_layer012_91011 , noxattn , selfattn , xattn , full , notime , xlayer , selflayer Example : text_encoder_full norm_layer Description : Flag indicating whether to update the norm layer during training. Type : bool Example : False attack_method Description : Method for adversarial attack training. Type : str Choices : pgd , multi_pgd , fast_at , free_at Example : pgd component Description : Component to apply the attack on. Type : str Choices : all , ffn , attn Example : all iterations Description : Total number of training iterations. Type : int Example : 10 (Note: The help argument may default to a higher value, e.g., 1000, but the config file sets it to 10.) save_interval Description : Interval (in iterations) at which checkpoints are saved. Type : int Example : 200 lr Description : Learning rate used during training. Type : float Example : 1e-5 Adversarial Attack Hyperparameters adv_prompt_num Description : Number of prompt tokens for adversarial soft prompt learning. Type : int Example : 1 attack_embd_type Description : The adversarial embedding type; options are word embedding or condition embedding. Type : str Choices : word_embd , condition_embd Example : word_embd attack_type Description : The type of adversarial attack applied to the prompt. Type : str Choices : replace_k , add , prefix_k , suffix_k , mid_k , insert_k , per_k_words Example : prefix_k attack_init Description : Strategy for initializing the adversarial attack; either randomly or using the latest parameters. Type : str Choices : random , latest Example : latest attack_step Description : Number of steps for the adversarial attack. Type : int Example : 30 attack_init_embd Description : Initial embedding for the attack (optional). Type : Depends on implementation; default is None Example : None adv_prompt_update_step Description : Frequency (in iterations) at which the adversarial prompt is updated. Type : int Example : 1 attack_lr Description : Learning rate for adversarial attack training. Type : float Example : 1e-3 warmup_iter Description : Number of warmup iterations before starting the adversarial attack. Type : int Example : 200 Backend backend Description : Backend framework to be used (e.g., CompVis). Type : str Example : compvis Choices : compvis or diffusers Directory Structure algorithm.py : Implementation of the AdvUnlearnAlgorithm class. configs/ : Contains configuration files for AdvUnlearn for compvis and diffusers. model.py : Implementation of the AdvUnlearnModel class for compvis and diffusers. trainer.py : Trainer for adversarial unlearning for compvis and diffusers. utils.py : Utility functions used in the project. dataset_handler.py : handles prompt cleaning and retaining dataset creation for adversarial unlearning. compvis_trainer.py : Trainer for adversarial unlearning for compvis. diffusers_trainer.py : Trainer for adversarial unlearning for diffusers.","title":"Usage"},{"location":"unlearn/mu_defense/adv_unlearn/#mu_defense","text":"This repository is for mu_defense that implements adversarial unlearning by integrating a soft prompt attack into the training loop. In this process, a random prompt is selected and its embedding is adversarially perturbed\u2014either at the word or conditional embedding level\u2014to steer the model into unlearning unwanted associations while preserving overall performance.","title":"MU_DEFENSE"},{"location":"unlearn/mu_defense/adv_unlearn/#installation","text":"","title":"Installation"},{"location":"unlearn/mu_defense/adv_unlearn/#prerequisities","text":"Ensure conda is installed on your system. You can install Miniconda or Anaconda: Miniconda (recommended): https://docs.conda.io/en/latest/miniconda.html Anaconda : https://www.anaconda.com/products/distribution After installing conda , ensure it is available in your PATH by running. You may require to restart the terminal session: Step-by-Step Setup: Step 1. Create a Conda Environment Create a new Conda environment named myenv with Python 3.8.5: conda create -n myenv python=3.8.5 Step 2. Activate the Environment Activate the environment to work within it: conda activate myenv Step 3. Install Core Dependencies Install PyTorch, torchvision, CUDA Toolkit, and ONNX Runtime with specific versions: conda install pytorch==1.11.0 torchvision==0.12.0 cudatoolkit=11.3 onnxruntime==1.16.3 -c pytorch -c conda-forge Step 4. Install our unlearn_diff Package using pip: pip install unlearn_diff Step 5. Install Additional Git Dependencies: After installing unlearn_diff, install the following Git-based dependencies in the same Conda environment to ensure full functionality: pip install git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers pip install git+https://github.com/openai/CLIP.git@main#egg=clip pip install git+https://github.com/crowsonkb/k-diffusion.git pip install git+https://github.com/cocodataset/panopticapi.git pip install git+https://github.com/Phoveran/fastargs.git@main#egg=fastargs pip install git+https://github.com/boomb0om/text2image-benchmark","title":"Prerequisities"},{"location":"unlearn/mu_defense/adv_unlearn/#example-usage-to-run-defense-for-compvis","text":"To test the below code snippet, you can create a file, copy the below code in eg, mu_defense.py and execute it with python mu_defense.py or use WANDB_MODE=offline python mu_defense.py for offline mode.","title":"Example usage to Run Defense for compvis"},{"location":"unlearn/mu_defense/adv_unlearn/#run-with-default-config","text":"from mu_defense.algorithms.adv_unlearn.algorithm import AdvUnlearnAlgorithm from mu_defense.algorithms.adv_unlearn.configs import adv_unlearn_config from mu.algorithms.erase_diff.configs import erase_diff_train_mu def mu_defense(): mu_defense = AdvUnlearnAlgorithm( config=adv_unlearn_config ) mu_defense.run() if __name__ == \"__main__\": mu_defense()","title":"Run with default config"},{"location":"unlearn/mu_defense/adv_unlearn/#modify-some-train-parameters-in-pre-defined-config-class","text":"View the config descriptions to see a list of available parameters. from mu_defense.algorithms.adv_unlearn.algorithm import AdvUnlearnAlgorithm from mu_defense.algorithms.adv_unlearn.configs import adv_unlearn_config from mu.algorithms.erase_diff.configs import erase_diff_train_mu def mu_defense(): mu_defense = AdvUnlearnAlgorithm( config=adv_unlearn_config, compvis_ckpt_path = \"outputs/erase_diff/erase_diff_Abstractionism_model.pth\", attack_step = 2, backend = \"compvis\", attack_method = \"fast_at\", warmup_iter = 1, iterations = 2, model_config_path = erase_diff_train_mu.model_config_path ) mu_defense.run() if __name__ == \"__main__\": mu_defense() Note: import the model_config_path from the relevant algorithm's configuration module in the mu package","title":"Modify some train parameters in pre defined config class."},{"location":"unlearn/mu_defense/adv_unlearn/#example-usage-to-run-defense-for-diffusers","text":"","title":"Example usage to Run defense for diffusers"},{"location":"unlearn/mu_defense/adv_unlearn/#run-with-default-config_1","text":"from mu_defense.algorithms.adv_unlearn.algorithm import AdvUnlearnAlgorithm from mu_defense.algorithms.adv_unlearn.configs import adv_unlearn_config def mu_defense(): mu_defense = AdvUnlearnAlgorithm( config=adv_unlearn_config ) mu_defense.run() if __name__ == \"__main__\": mu_defense()","title":"Run with default config"},{"location":"unlearn/mu_defense/adv_unlearn/#modify-some-train-parameters-in-pre-defined-config-class_1","text":"View the config descriptions to see a list of available parameters. from mu_defense.algorithms.adv_unlearn.algorithm import AdvUnlearnAlgorithm from mu_defense.algorithms.adv_unlearn.configs import adv_unlearn_config def mu_defense(): mu_defense = AdvUnlearnAlgorithm( config=adv_unlearn_config, diffusers_model_name_or_path = \"outputs/forget_me_not/finetuned_models/Abstractionism\", attack_step = 2, backend = \"diffusers\", attack_method = \"fast_at\", warmup_iter = 1, iterations = 2 ) mu_defense.run() if __name__ == \"__main__\": mu_defense()","title":"Modify some train parameters in pre defined config class."},{"location":"unlearn/mu_defense/adv_unlearn/#description-of-fields-in-config-file","text":"Below is a detailed description of the configuration fields available in the adv_unlearn_config.py file. The descriptions match those provided in the help section of the command-line arguments. Inference & Model Paths model_config_path Description : Config path for stable diffusion model. Use for compvis model only. Type : str Example : configs/stable-diffusion/v1-inference.yaml compvis_ckpt_path Description : Checkpoint path for stable diffusion v1-4. Type : str Example : models/sd-v1-4-full-ema.ckpt encoder_model_name_or_path Description : Model name or path for the encoder. Type : str Example : CompVis/stable-diffusion-v1-4 cache_path Description : Directory used for caching model files. Type : str Example : .cache diffusers_model_name_or_path Description : Model name or path for the diffusers (if used). Type : str Example : outputs/forget_me_not/finetuned_models/Abstractionism target_ckpt Description : Optionally load a target checkpoint into the model for diffuser sampling. Type : Typically str or None Example : path to target checkpoint path Devices & IO devices Description : CUDA devices to train on. Type : str Example : 0,0 seperator Description : Separator used if you want to train a bunch of words separately. Type : str or None Example : None output_dir Description : Directory where output files (e.g., checkpoints, logs) are saved. Type : str Example : outputs/adv_unlearn Image & Diffusion Sampling image_size Description : Image size used to train. Type : int Example : 512 ddim_steps Description : Number of DDIM steps for inference during training. Type : int Example : 50 start_guidance Description : Guidance of start image used to train. Type : float Example : 3.0 negative_guidance Description : Guidance of negative training used to train. Type : float Example : 1.0 ddim_eta Description : DDIM eta parameter for sampling. Type : int or float Example : 0 Training Setup prompt Description : Prompt corresponding to the concept to erase. Type : str Example : nudity dataset_retain Description : Prompts corresponding to non-target concepts to retain. Type : str Choices : coco_object , coco_object_no_filter , imagenet243 , imagenet243_no_filter Example : coco_object retain_batch Description : Batch size of retaining prompts during training. Type : int Example : 5 retain_train Description : Retaining training mode; choose between iterative ( iter ) or regularization ( reg ). Type : str Choices : iter , reg Example : iter retain_step Description : Number of steps for retaining prompts. Type : int Example : 1 retain_loss_w Description : Retaining loss weight. Type : float Example : 1.0 train_method Description : Method of training. Type : str Choices : text_encoder_full , text_encoder_layer0 , text_encoder_layer01 , text_encoder_layer012 , text_encoder_layer0123 , text_encoder_layer01234 , text_encoder_layer012345 , text_encoder_layer0123456 , text_encoder_layer01234567 , text_encoder_layer012345678 , text_encoder_layer0123456789 , text_encoder_layer012345678910 , text_encoder_layer01234567891011 , text_encoder_layer0_11 , text_encoder_layer01_1011 , text_encoder_layer012_91011 , noxattn , selfattn , xattn , full , notime , xlayer , selflayer Example : text_encoder_full norm_layer Description : Flag indicating whether to update the norm layer during training. Type : bool Example : False attack_method Description : Method for adversarial attack training. Type : str Choices : pgd , multi_pgd , fast_at , free_at Example : pgd component Description : Component to apply the attack on. Type : str Choices : all , ffn , attn Example : all iterations Description : Total number of training iterations. Type : int Example : 10 (Note: The help argument may default to a higher value, e.g., 1000, but the config file sets it to 10.) save_interval Description : Interval (in iterations) at which checkpoints are saved. Type : int Example : 200 lr Description : Learning rate used during training. Type : float Example : 1e-5 Adversarial Attack Hyperparameters adv_prompt_num Description : Number of prompt tokens for adversarial soft prompt learning. Type : int Example : 1 attack_embd_type Description : The adversarial embedding type; options are word embedding or condition embedding. Type : str Choices : word_embd , condition_embd Example : word_embd attack_type Description : The type of adversarial attack applied to the prompt. Type : str Choices : replace_k , add , prefix_k , suffix_k , mid_k , insert_k , per_k_words Example : prefix_k attack_init Description : Strategy for initializing the adversarial attack; either randomly or using the latest parameters. Type : str Choices : random , latest Example : latest attack_step Description : Number of steps for the adversarial attack. Type : int Example : 30 attack_init_embd Description : Initial embedding for the attack (optional). Type : Depends on implementation; default is None Example : None adv_prompt_update_step Description : Frequency (in iterations) at which the adversarial prompt is updated. Type : int Example : 1 attack_lr Description : Learning rate for adversarial attack training. Type : float Example : 1e-3 warmup_iter Description : Number of warmup iterations before starting the adversarial attack. Type : int Example : 200 Backend backend Description : Backend framework to be used (e.g., CompVis). Type : str Example : compvis Choices : compvis or diffusers","title":"Description of fields in config file"},{"location":"unlearn/mu_defense/adv_unlearn/#directory-structure","text":"algorithm.py : Implementation of the AdvUnlearnAlgorithm class. configs/ : Contains configuration files for AdvUnlearn for compvis and diffusers. model.py : Implementation of the AdvUnlearnModel class for compvis and diffusers. trainer.py : Trainer for adversarial unlearning for compvis and diffusers. utils.py : Utility functions used in the project. dataset_handler.py : handles prompt cleaning and retaining dataset creation for adversarial unlearning. compvis_trainer.py : Trainer for adversarial unlearning for compvis. diffusers_trainer.py : Trainer for adversarial unlearning for diffusers.","title":"Directory Structure"},{"location":"unlearn/mu_defense/config/","text":"Sample config for Advattack (mu_defense) #mu_defense/algorithms/adv_unlearn/configs/adv_unlearn_config.py import os from pathlib import Path from mu_defense.core.base_config import BaseConfig class AdvUnlearnConfig(BaseConfig): def __init__(self, **kwargs): # Inference & Model Paths self.model_config_path = \"configs/stable-diffusion/v1-inference.yaml\" #for compvis self.compvis_ckpt_path = \"models/sd-v1-4-full-ema.ckpt\" self.encoder_model_name_or_path = \"CompVis/stable-diffusion-v1-4\" self.cache_path = \".cache\" self.diffusers_model_name_or_path = \"\" self.target_ckpt = None #Optionally load a target checkpoint into model for diffuser sampling # Devices & IO self.devices = \"0,0\" # You can later parse this string into a list if needed. self.seperator = None self.output_dir = \"outputs/adv_unlearn\" # Image & Diffusion Sampling self.image_size = 512 self.ddim_steps = 50 self.start_guidance = 3.0 self.negative_guidance = 1.0 # Training Setup self.prompt = \"nudity\" self.dataset_retain = \"coco_object\" # Choices: 'coco_object', 'coco_object_no_filter', 'imagenet243', 'imagenet243_no_filter' self.retain_batch = 5 self.retain_train = \"iter\" # Options: 'iter' or 'reg' self.retain_step = 1 self.retain_loss_w = 1.0 self.ddim_eta = 0 self.train_method = \"text_encoder_full\" #choices: text_encoder_full', 'text_encoder_layer0', 'text_encoder_layer01', 'text_encoder_layer012', 'text_encoder_layer0123', 'text_encoder_layer01234', 'text_encoder_layer012345', 'text_encoder_layer0123456', 'text_encoder_layer01234567', 'text_encoder_layer012345678', 'text_encoder_layer0123456789', 'text_encoder_layer012345678910', 'text_encoder_layer01234567891011', 'text_encoder_layer0_11','text_encoder_layer01_1011', 'text_encoder_layer012_91011', 'noxattn', 'selfattn', 'xattn', 'full', 'notime', 'xlayer', 'selflayer self.norm_layer = False # This is a flag; use True if you wish to update the norm layer. self.attack_method = \"pgd\" # Choices: 'pgd', 'multi_pgd', 'fast_at', 'free_at' self.component = \"all\" # Choices: 'all', 'ffn', 'attn' self.iterations = 10 self.save_interval = 200 self.lr = 1e-5 # Adversarial Attack Hyperparameters self.adv_prompt_num = 1 self.attack_embd_type = \"word_embd\" # Choices: 'word_embd', 'condition_embd' self.attack_type = \"prefix_k\" # Choices: 'replace_k', 'add', 'prefix_k', 'suffix_k', 'mid_k', 'insert_k', 'per_k_words' self.attack_init = \"latest\" # Choices: 'random', 'latest' self.attack_step = 30 self.attack_init_embd = None self.adv_prompt_update_step = 1 self.attack_lr = 1e-3 self.warmup_iter = 200 #backend self.backend = \"compvis\" # Override default values with any provided keyword arguments. for key, value in kwargs.items(): setattr(self, key, value) def validate_config(self): \"\"\" Perform basic validation on the config parameters. \"\"\" if self.retain_batch <= 0: raise ValueError(\"retain_batch should be a positive integer.\") if self.lr <= 0: raise ValueError(\"Learning rate (lr) should be positive.\") if self.image_size <= 0: raise ValueError(\"Image size should be a positive integer.\") if self.iterations <= 0: raise ValueError(\"Iterations must be a positive integer.\") if not os.path.exists(self.output_dir): os.makedirs(self.output_dir) adv_unlearn_config = AdvUnlearnConfig() Sample config for image generator for mu_defense # mu_defense/algorithms/adv_unlearn/configs/example_img_generator_config.py import os from mu.core.base_config import BaseConfig class ImageGeneratorConfig(BaseConfig): def __init__(self): self.model_name = \"SD-v1-4\" self.target_ckpt = \"\" self.save_path = \"\" self.prompts_path = \"data/prompts/visualization_example.csv\" self.device = \"0\" self.guidance_scale = 7.5 self.image_size = 512 self.ddim_steps = 100 self.num_samples = 1 self.from_case = 0 self.folder_suffix = \"\" self.origin_or_target = \"target\" #target or origin self.encoder_model_name_or_path = \"CompVis/stable-diffusion-v1-4\" def validate_config(self): \"\"\" Perform basic validation on the config parameters. \"\"\" if not os.path.exists(self.prompt_path): raise FileNotFoundError(f\"Prompt dataset file {self.prompt_path} does not exist.\") example_image_generator_config = ImageGeneratorConfig() Sample config for evaluation framework for mu_defense # mu_defense/algorithms/adv_unlearn/configs/evaluation_config.py import os from mu.core.base_config import BaseConfig class MUDefenseEvaluationConfig(BaseConfig): def __init__(self): self.job = \"fid, clip\" self.gen_imgs_path = \"outputs/adv_unlearn/models_visualizations_imagenette/SD-v1-4/\" self.coco_imgs_path = \"coco_dataset/extracted_files/coco_sample\" self.prompt_path = \"data/prompts/coco_10k.csv\" self.classify_prompt_path = \"data/prompts/imagenette_5k.csv\" self.devices = \"0,0\" self.classification_model_path = \"openai/clip-vit-base-patch32\" self.output_path = \"outputs/adv_unlearn/evaluation\" def validate_config(self): \"\"\" Perform basic validation on the config parameters. \"\"\" if not os.path.exists(self.prompt_path): raise FileNotFoundError(f\"Prompt dataset file {self.prompt_path} does not exist.\") mu_defense_evaluation_config = MUDefenseEvaluationConfig()","title":"Config"},{"location":"unlearn/mu_defense/config/#sample-config-for-advattack-mu_defense","text":"#mu_defense/algorithms/adv_unlearn/configs/adv_unlearn_config.py import os from pathlib import Path from mu_defense.core.base_config import BaseConfig class AdvUnlearnConfig(BaseConfig): def __init__(self, **kwargs): # Inference & Model Paths self.model_config_path = \"configs/stable-diffusion/v1-inference.yaml\" #for compvis self.compvis_ckpt_path = \"models/sd-v1-4-full-ema.ckpt\" self.encoder_model_name_or_path = \"CompVis/stable-diffusion-v1-4\" self.cache_path = \".cache\" self.diffusers_model_name_or_path = \"\" self.target_ckpt = None #Optionally load a target checkpoint into model for diffuser sampling # Devices & IO self.devices = \"0,0\" # You can later parse this string into a list if needed. self.seperator = None self.output_dir = \"outputs/adv_unlearn\" # Image & Diffusion Sampling self.image_size = 512 self.ddim_steps = 50 self.start_guidance = 3.0 self.negative_guidance = 1.0 # Training Setup self.prompt = \"nudity\" self.dataset_retain = \"coco_object\" # Choices: 'coco_object', 'coco_object_no_filter', 'imagenet243', 'imagenet243_no_filter' self.retain_batch = 5 self.retain_train = \"iter\" # Options: 'iter' or 'reg' self.retain_step = 1 self.retain_loss_w = 1.0 self.ddim_eta = 0 self.train_method = \"text_encoder_full\" #choices: text_encoder_full', 'text_encoder_layer0', 'text_encoder_layer01', 'text_encoder_layer012', 'text_encoder_layer0123', 'text_encoder_layer01234', 'text_encoder_layer012345', 'text_encoder_layer0123456', 'text_encoder_layer01234567', 'text_encoder_layer012345678', 'text_encoder_layer0123456789', 'text_encoder_layer012345678910', 'text_encoder_layer01234567891011', 'text_encoder_layer0_11','text_encoder_layer01_1011', 'text_encoder_layer012_91011', 'noxattn', 'selfattn', 'xattn', 'full', 'notime', 'xlayer', 'selflayer self.norm_layer = False # This is a flag; use True if you wish to update the norm layer. self.attack_method = \"pgd\" # Choices: 'pgd', 'multi_pgd', 'fast_at', 'free_at' self.component = \"all\" # Choices: 'all', 'ffn', 'attn' self.iterations = 10 self.save_interval = 200 self.lr = 1e-5 # Adversarial Attack Hyperparameters self.adv_prompt_num = 1 self.attack_embd_type = \"word_embd\" # Choices: 'word_embd', 'condition_embd' self.attack_type = \"prefix_k\" # Choices: 'replace_k', 'add', 'prefix_k', 'suffix_k', 'mid_k', 'insert_k', 'per_k_words' self.attack_init = \"latest\" # Choices: 'random', 'latest' self.attack_step = 30 self.attack_init_embd = None self.adv_prompt_update_step = 1 self.attack_lr = 1e-3 self.warmup_iter = 200 #backend self.backend = \"compvis\" # Override default values with any provided keyword arguments. for key, value in kwargs.items(): setattr(self, key, value) def validate_config(self): \"\"\" Perform basic validation on the config parameters. \"\"\" if self.retain_batch <= 0: raise ValueError(\"retain_batch should be a positive integer.\") if self.lr <= 0: raise ValueError(\"Learning rate (lr) should be positive.\") if self.image_size <= 0: raise ValueError(\"Image size should be a positive integer.\") if self.iterations <= 0: raise ValueError(\"Iterations must be a positive integer.\") if not os.path.exists(self.output_dir): os.makedirs(self.output_dir) adv_unlearn_config = AdvUnlearnConfig()","title":"Sample config for Advattack (mu_defense)"},{"location":"unlearn/mu_defense/config/#sample-config-for-image-generator-for-mu_defense","text":"# mu_defense/algorithms/adv_unlearn/configs/example_img_generator_config.py import os from mu.core.base_config import BaseConfig class ImageGeneratorConfig(BaseConfig): def __init__(self): self.model_name = \"SD-v1-4\" self.target_ckpt = \"\" self.save_path = \"\" self.prompts_path = \"data/prompts/visualization_example.csv\" self.device = \"0\" self.guidance_scale = 7.5 self.image_size = 512 self.ddim_steps = 100 self.num_samples = 1 self.from_case = 0 self.folder_suffix = \"\" self.origin_or_target = \"target\" #target or origin self.encoder_model_name_or_path = \"CompVis/stable-diffusion-v1-4\" def validate_config(self): \"\"\" Perform basic validation on the config parameters. \"\"\" if not os.path.exists(self.prompt_path): raise FileNotFoundError(f\"Prompt dataset file {self.prompt_path} does not exist.\") example_image_generator_config = ImageGeneratorConfig()","title":"Sample config for image generator for mu_defense"},{"location":"unlearn/mu_defense/config/#sample-config-for-evaluation-framework-for-mu_defense","text":"# mu_defense/algorithms/adv_unlearn/configs/evaluation_config.py import os from mu.core.base_config import BaseConfig class MUDefenseEvaluationConfig(BaseConfig): def __init__(self): self.job = \"fid, clip\" self.gen_imgs_path = \"outputs/adv_unlearn/models_visualizations_imagenette/SD-v1-4/\" self.coco_imgs_path = \"coco_dataset/extracted_files/coco_sample\" self.prompt_path = \"data/prompts/coco_10k.csv\" self.classify_prompt_path = \"data/prompts/imagenette_5k.csv\" self.devices = \"0,0\" self.classification_model_path = \"openai/clip-vit-base-patch32\" self.output_path = \"outputs/adv_unlearn/evaluation\" def validate_config(self): \"\"\" Perform basic validation on the config parameters. \"\"\" if not os.path.exists(self.prompt_path): raise FileNotFoundError(f\"Prompt dataset file {self.prompt_path} does not exist.\") mu_defense_evaluation_config = MUDefenseEvaluationConfig()","title":"Sample config for evaluation framework for mu_defense"},{"location":"usage/how_to_evaluate/","text":"Evaluation: Evaluate using unlearn canvas dataset: Note: Currently it supports evaluation for unlearn canvas dataset to calculate accuracy. I2p and generic dataset support needs to be added. from mu.algorithms.erase_diff import EraseDiffEvaluator from mu.algorithms.erase_diff.configs import ( erase_diff_evaluation_config ) from evaluation.metrics.accuracy import accuracy_score from evaluation.metrics.fid import fid_score evaluator = EraseDiffEvaluator( erase_diff_evaluation_config, ckpt_path=\"outputs/erase_diff/finetuned_models/erase_diff_self-harm_model.pth\", ) generated_images_path = evaluator.generate_images() accuracy = accuracy_score(gen_image_dir=generated_images_path, dataset_type = \"unlearncanvas\", classifier_ckpt_path = \"/home/ubuntu/Projects/models/classifier_ckpt_path/style50_cls.pth\", forget_theme=\"Bricks\", seed_list = [\"188\"] ) print(accuracy['acc']) print(accuracy['loss']) reference_image_dir = \"data/quick-canvas-dataset/sample\" fid, _ = fid_score(generated_image_dir=generated_images_path, reference_image_dir=reference_image_dir ) print(fid) Link to our example usage notebooks We have kept full implementation of machine unlearning, attack and defense in these notebooks. Erase-diff (compvis model) https://github.com/RamailoTech/msu_unlearningalgorithm/blob/main/notebooks/run_erase_diff.ipynb forget-me-not (Diffuser model) https://github.com/RamailoTech/msu_unlearningalgorithm/blob/main/notebooks/run_forget_me_not.ipynb","title":"How to evaluate"},{"location":"usage/how_to_evaluate/#evaluation","text":"Evaluate using unlearn canvas dataset: Note: Currently it supports evaluation for unlearn canvas dataset to calculate accuracy. I2p and generic dataset support needs to be added. from mu.algorithms.erase_diff import EraseDiffEvaluator from mu.algorithms.erase_diff.configs import ( erase_diff_evaluation_config ) from evaluation.metrics.accuracy import accuracy_score from evaluation.metrics.fid import fid_score evaluator = EraseDiffEvaluator( erase_diff_evaluation_config, ckpt_path=\"outputs/erase_diff/finetuned_models/erase_diff_self-harm_model.pth\", ) generated_images_path = evaluator.generate_images() accuracy = accuracy_score(gen_image_dir=generated_images_path, dataset_type = \"unlearncanvas\", classifier_ckpt_path = \"/home/ubuntu/Projects/models/classifier_ckpt_path/style50_cls.pth\", forget_theme=\"Bricks\", seed_list = [\"188\"] ) print(accuracy['acc']) print(accuracy['loss']) reference_image_dir = \"data/quick-canvas-dataset/sample\" fid, _ = fid_score(generated_image_dir=generated_images_path, reference_image_dir=reference_image_dir ) print(fid) Link to our example usage notebooks We have kept full implementation of machine unlearning, attack and defense in these notebooks. Erase-diff (compvis model) https://github.com/RamailoTech/msu_unlearningalgorithm/blob/main/notebooks/run_erase_diff.ipynb forget-me-not (Diffuser model) https://github.com/RamailoTech/msu_unlearningalgorithm/blob/main/notebooks/run_forget_me_not.ipynb","title":"Evaluation:"},{"location":"usage/installation/","text":"Unlearn Diff Unlearn Diff is an open-source Python package designed to streamline the development of unlearning algorithms and establish a standardized evaluation pipeline for diffusion models. It provides researchers and practitioners with tools to implement, evaluate, and extend unlearning algorithms effectively. Usage This section contains the usage guide for the package. Installation Prerequisities Ensure conda is installed on your system. You can install Miniconda or Anaconda: Miniconda (recommended): https://docs.conda.io/en/latest/miniconda.html Anaconda : https://www.anaconda.com/products/distribution After installing conda , ensure it is available in your PATH by running. You may require to restart the terminal session: Before installing the unlearn_diff package, follow these steps to set up your environment correctly. These instructions ensure compatibility with the required dependencies, including Python, PyTorch, and ONNX Runtime. Step-by-Step Setup: Create a Conda Environment Create a new Conda environment named myenv with Python 3.8.5: conda create -n myenv python=3.8.5 Activate the Environment Activate the environment to work within it: conda activate myenv Install Core Dependencies Install PyTorch, torchvision, CUDA Toolkit, and ONNX Runtime with specific versions: conda install pytorch==1.11.0 torchvision==0.12.0 cudatoolkit=11.3 onnxruntime==1.16.3 -c pytorch -c conda-forge Install our unlearn_diff Package using pip: pip install unlearn_diff Install Additional Git Dependencies: After installing unlearn_diff, install the following Git-based dependencies in the same Conda environment to ensure full functionality: pip install git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers pip install git+https://github.com/openai/CLIP.git@main#egg=clip pip install git+https://github.com/crowsonkb/k-diffusion.git pip install git+https://github.com/cocodataset/panopticapi.git pip install git+https://github.com/Phoveran/fastargs.git@main#egg=fastargs pip install git+https://github.com/boomb0om/text2image-benchmark Downloading data and models. After you install the package, you can use the following commands to download. Dataset : i2p : Sample : download_data sample i2p Full : download_data full i2p quick_canvas : Sample : download_data sample quick_canvas Full : download_data full quick_canvas Model : compvis : download_model compvis diffuser : download_model diffuser Download best.onnx model download_best_onnx","title":"Installation"},{"location":"usage/installation/#unlearn-diff","text":"Unlearn Diff is an open-source Python package designed to streamline the development of unlearning algorithms and establish a standardized evaluation pipeline for diffusion models. It provides researchers and practitioners with tools to implement, evaluate, and extend unlearning algorithms effectively.","title":"Unlearn Diff"},{"location":"usage/installation/#usage","text":"This section contains the usage guide for the package.","title":"Usage"},{"location":"usage/installation/#installation","text":"","title":"Installation"},{"location":"usage/installation/#prerequisities","text":"Ensure conda is installed on your system. You can install Miniconda or Anaconda: Miniconda (recommended): https://docs.conda.io/en/latest/miniconda.html Anaconda : https://www.anaconda.com/products/distribution After installing conda , ensure it is available in your PATH by running. You may require to restart the terminal session: Before installing the unlearn_diff package, follow these steps to set up your environment correctly. These instructions ensure compatibility with the required dependencies, including Python, PyTorch, and ONNX Runtime. Step-by-Step Setup: Create a Conda Environment Create a new Conda environment named myenv with Python 3.8.5: conda create -n myenv python=3.8.5 Activate the Environment Activate the environment to work within it: conda activate myenv Install Core Dependencies Install PyTorch, torchvision, CUDA Toolkit, and ONNX Runtime with specific versions: conda install pytorch==1.11.0 torchvision==0.12.0 cudatoolkit=11.3 onnxruntime==1.16.3 -c pytorch -c conda-forge Install our unlearn_diff Package using pip: pip install unlearn_diff Install Additional Git Dependencies: After installing unlearn_diff, install the following Git-based dependencies in the same Conda environment to ensure full functionality: pip install git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers pip install git+https://github.com/openai/CLIP.git@main#egg=clip pip install git+https://github.com/crowsonkb/k-diffusion.git pip install git+https://github.com/cocodataset/panopticapi.git pip install git+https://github.com/Phoveran/fastargs.git@main#egg=fastargs pip install git+https://github.com/boomb0om/text2image-benchmark","title":"Prerequisities"},{"location":"usage/installation/#downloading-data-and-models","text":"After you install the package, you can use the following commands to download. Dataset : i2p : Sample : download_data sample i2p Full : download_data full i2p quick_canvas : Sample : download_data sample quick_canvas Full : download_data full quick_canvas Model : compvis : download_model compvis diffuser : download_model diffuser Download best.onnx model download_best_onnx","title":"Downloading data and models."},{"location":"usage/unlearn_algorithm_usage/","text":"Run Train Each algorithm has their own script to run the algorithm, Some also have different process all together. Follow usage section in readme for the algorithm you want to run with the help of the github repository. You will need to run the code snippet provided in usage section with necessary configuration passed. Example usage for erase_diff algorithm (CompVis model) Note: Currently we only have support for unlearn canvas dataset. I2p and generic dataset support needs to be added. The default configuration for training is provided by erase_diff_train_mu. You can run the training with the default settings as follows: Using the Default Configuration from mu.algorithms.erase_diff.algorithm import EraseDiffAlgorithm from mu.algorithms.erase_diff.configs import erase_diff_train_mu algorithm = EraseDiffAlgorithm( erase_diff_train_mu ) algorithm.run() Overriding the Default Configuration If you need to override the existing configuration settings, you can specify your custom parameters (such as ckpt_path and raw_dataset_dir) directly when initializing the algorithm. For example: Machine unlearning using unlearn canvas dataset: from mu.algorithms.erase_diff.algorithm import EraseDiffAlgorithm from mu.algorithms.erase_diff.configs import erase_diff_train_mu algorithm = EraseDiffAlgorithm( erase_diff_train_mu, ckpt_path=\"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\", #replace it with your ckpt path raw_dataset_dir=\"data/quick-canvas-dataset/sample\", use_sample = True, #uses sample dataset template_name = \"Abstractionism\", dataset_type = \"unlearncanvas\", devices = \"0\" ) algorithm.run() Note: When fine-tuning the model, if you want to use a sample dataset, set use_sample=True (default).Otherwise, set use_sample=False to use the full dataset. Note You can choose from a set of predefined template_name options to erase specific concept when working with the unlearncanvas dataset to perform unlearning. For instance, in the i2p context, the available choices include: \"Abstractionism\", \"Artist_Sketch\", \"Blossom_Season\", \"Bricks\", \"Byzantine\", \"Cartoon\", \"Cold_Warm\", \"Color_Fantasy\", \"Comic_Etch\", \"Crayon\", \"Cubism\", \"Dadaism\", \"Dapple\", \"Defoliation\", \"Early_Autumn\", \"Expressionism\", \"Fauvism\", \"French\", \"Glowing_Sunset\", \"Gorgeous_Love\", \"Greenfield\", \"Impressionism\", \"Ink_Art\", \"Joy\", \"Liquid_Dreams\", \"Magic_Cube\", \"Meta_Physics\", \"Meteor_Shower\", \"Monet\", \"Mosaic\", \"Neon_Lines\", \"On_Fire\", \"Pastel\", \"Pencil_Drawing\", \"Picasso\", \"Pop_Art\", \"Red_Blue_Ink\", \"Rust\", \"Seed_Images\", \"Sketch\", \"Sponge_Dabbed\", \"Structuralism\", \"Superstring\", \"Surrealism\", \"Ukiyoe\", \"Van_Gogh\", \"Vibrant_Flow\", \"Warm_Love\", \"Warm_Smear\", \"Watercolor\", \"Winter\" Machine unlearning with i2p dataset from mu.algorithms.erase_diff.algorithm import EraseDiffAlgorithm from mu.algorithms.erase_diff.configs import erase_diff_train_i2p algorithm = EraseDiffAlgorithm( erase_diff_train_i2p, ckpt_path=\"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\", #replace it with your ckpt path raw_dataset_dir=\"data/i2p-dataset/sample\", num_samples = 1, dataset_type = \"i2p\", template = \"i2p\", template_name = \"self-harm\", #concept to erase use_sample = True, #uses sample dataset devices = \"0\" ) algorithm.run() Note You can choose from a set of predefined template_name options to erase specific concept when working with the i2p dataset to perform unlearning. For instance, in the i2p context, the available choices include: 'shocking', 'harassment', 'hate', 'self-harm', 'sexual', 'illegal activity', 'violence' Add your own unlearning algorithms: For detailed instructions on adding your own algorithm, please see the Contribution section.","title":"How to use existing unlearning algorithms"},{"location":"usage/unlearn_algorithm_usage/#run-train","text":"Each algorithm has their own script to run the algorithm, Some also have different process all together. Follow usage section in readme for the algorithm you want to run with the help of the github repository. You will need to run the code snippet provided in usage section with necessary configuration passed. Example usage for erase_diff algorithm (CompVis model) Note: Currently we only have support for unlearn canvas dataset. I2p and generic dataset support needs to be added. The default configuration for training is provided by erase_diff_train_mu. You can run the training with the default settings as follows: Using the Default Configuration from mu.algorithms.erase_diff.algorithm import EraseDiffAlgorithm from mu.algorithms.erase_diff.configs import erase_diff_train_mu algorithm = EraseDiffAlgorithm( erase_diff_train_mu ) algorithm.run() Overriding the Default Configuration If you need to override the existing configuration settings, you can specify your custom parameters (such as ckpt_path and raw_dataset_dir) directly when initializing the algorithm. For example: Machine unlearning using unlearn canvas dataset: from mu.algorithms.erase_diff.algorithm import EraseDiffAlgorithm from mu.algorithms.erase_diff.configs import erase_diff_train_mu algorithm = EraseDiffAlgorithm( erase_diff_train_mu, ckpt_path=\"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\", #replace it with your ckpt path raw_dataset_dir=\"data/quick-canvas-dataset/sample\", use_sample = True, #uses sample dataset template_name = \"Abstractionism\", dataset_type = \"unlearncanvas\", devices = \"0\" ) algorithm.run() Note: When fine-tuning the model, if you want to use a sample dataset, set use_sample=True (default).Otherwise, set use_sample=False to use the full dataset. Note You can choose from a set of predefined template_name options to erase specific concept when working with the unlearncanvas dataset to perform unlearning. For instance, in the i2p context, the available choices include: \"Abstractionism\", \"Artist_Sketch\", \"Blossom_Season\", \"Bricks\", \"Byzantine\", \"Cartoon\", \"Cold_Warm\", \"Color_Fantasy\", \"Comic_Etch\", \"Crayon\", \"Cubism\", \"Dadaism\", \"Dapple\", \"Defoliation\", \"Early_Autumn\", \"Expressionism\", \"Fauvism\", \"French\", \"Glowing_Sunset\", \"Gorgeous_Love\", \"Greenfield\", \"Impressionism\", \"Ink_Art\", \"Joy\", \"Liquid_Dreams\", \"Magic_Cube\", \"Meta_Physics\", \"Meteor_Shower\", \"Monet\", \"Mosaic\", \"Neon_Lines\", \"On_Fire\", \"Pastel\", \"Pencil_Drawing\", \"Picasso\", \"Pop_Art\", \"Red_Blue_Ink\", \"Rust\", \"Seed_Images\", \"Sketch\", \"Sponge_Dabbed\", \"Structuralism\", \"Superstring\", \"Surrealism\", \"Ukiyoe\", \"Van_Gogh\", \"Vibrant_Flow\", \"Warm_Love\", \"Warm_Smear\", \"Watercolor\", \"Winter\" Machine unlearning with i2p dataset from mu.algorithms.erase_diff.algorithm import EraseDiffAlgorithm from mu.algorithms.erase_diff.configs import erase_diff_train_i2p algorithm = EraseDiffAlgorithm( erase_diff_train_i2p, ckpt_path=\"/home/ubuntu/Projects/UnlearnCanvas/UnlearnCanvas/machine_unlearning/models/compvis/style50/compvis.ckpt\", #replace it with your ckpt path raw_dataset_dir=\"data/i2p-dataset/sample\", num_samples = 1, dataset_type = \"i2p\", template = \"i2p\", template_name = \"self-harm\", #concept to erase use_sample = True, #uses sample dataset devices = \"0\" ) algorithm.run() Note You can choose from a set of predefined template_name options to erase specific concept when working with the i2p dataset to perform unlearning. For instance, in the i2p context, the available choices include: 'shocking', 'harassment', 'hate', 'self-harm', 'sexual', 'illegal activity', 'violence'","title":"Run Train"},{"location":"usage/unlearn_algorithm_usage/#add-your-own-unlearning-algorithms","text":"For detailed instructions on adding your own algorithm, please see the Contribution section.","title":"Add your own unlearning algorithms:"}]}